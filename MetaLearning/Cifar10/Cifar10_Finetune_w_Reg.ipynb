{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 256\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(batch_size=64):\n",
    "    # Old data\n",
    "    train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=tf)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False,download=True, transform=tf)\n",
    "\n",
    "    source_size = int(0.7 * len(train_dataset))\n",
    "    target_size = int(0.1 * len(train_dataset))\n",
    "    val_size =int(0.2 * len(train_dataset))\n",
    "\n",
    "    train_dataset, target_dataset, val_dataset = random_split(train_dataset, [source_size, target_size, val_size])\n",
    "\n",
    "    source_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    target_dl = DataLoader(target_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "    \n",
    "    test_dl =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "\n",
    "    return source_dl, target_dl, test_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "source_dl, target_dl, test_dl, val_dl = load_data_set(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, out_dim = 10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, out_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def pre_train(criterion, optimizer, model, num_epochs, trainloader, testloader, valloader, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        test_acc = cal_acc(model, testloader, device)\n",
    "        val_acc = cal_acc(model, valloader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Test Acc: {test_acc:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from file.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('source.pth'):\n",
    "        model = VGG()\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "        num_epochs = 30\n",
    "        pre_train(criterion, optimizer, model, num_epochs, source_dl, test_dl, val_dl, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "                torch.save(model.state_dict(), 'source.pth')\n",
    "else:\n",
    "        model = VGG().to(device)\n",
    "        model.load_state_dict(torch.load('source.pth'))\n",
    "        print(\"Loaded model from file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(criterion, optimizer, model, num_epochs, trainloader, testloader, valloader, device):\n",
    "    best_model_wts = None\n",
    "    leader = VGG().to(device)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_num = 0\n",
    "        total, correct = 0., 0.\n",
    "        # if (best_model_wts):\n",
    "            # model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "            model.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            reg_loss = 0\n",
    "            for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
    "                reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
    "            \n",
    "            classification_loss = criterion(outputs, labels)\n",
    "            loss = classification_loss + alpha * reg_loss\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in valloader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    outputs = model(val_inputs)\n",
    "                    batch_loss = criterion(outputs, val_labels)\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    leader.load_state_dict(best_model_wts)\n",
    "            \n",
    "            print(f\"Batch num: {batch_num}, c_loss: {classification_loss.item():.4f}, Val Loss: {val_loss:.4f}, loss : {loss.item():.4f}\")\n",
    "            batch_num += 1\n",
    "\n",
    "        test_acc = cal_acc(model, testloader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Train Acc: {correct/total:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num: 0, c_loss: 1.5263, Val Loss: 62.5958, loss : 5.7539\n",
      "Batch num: 1, c_loss: 1.5618, Val Loss: 63.5568, loss : 1.5618\n",
      "Batch num: 2, c_loss: 1.5836, Val Loss: 63.9667, loss : 1.6168\n",
      "Batch num: 3, c_loss: 1.6129, Val Loss: 63.9123, loss : 1.6605\n",
      "Batch num: 4, c_loss: 1.5978, Val Loss: 64.0024, loss : 1.6561\n",
      "Batch num: 5, c_loss: 1.5620, Val Loss: 63.5031, loss : 1.6279\n",
      "Batch num: 6, c_loss: 1.5753, Val Loss: 63.2309, loss : 1.6465\n",
      "Batch num: 7, c_loss: 1.5581, Val Loss: 62.9128, loss : 1.6337\n",
      "Batch num: 8, c_loss: 1.5871, Val Loss: 62.3527, loss : 1.6667\n",
      "Batch num: 9, c_loss: 1.5621, Val Loss: 62.1038, loss : 1.5621\n",
      "Batch num: 10, c_loss: 1.5870, Val Loss: 62.1098, loss : 1.5870\n",
      "Batch num: 11, c_loss: 1.5598, Val Loss: 62.0841, loss : 1.5721\n",
      "Batch num: 12, c_loss: 1.5599, Val Loss: 62.1422, loss : 1.5599\n",
      "Batch num: 13, c_loss: 1.5541, Val Loss: 62.0479, loss : 1.5647\n",
      "Batch num: 14, c_loss: 1.5575, Val Loss: 62.0243, loss : 1.5575\n",
      "Batch num: 15, c_loss: 1.5810, Val Loss: 61.9960, loss : 1.5810\n",
      "Batch num: 16, c_loss: 1.5724, Val Loss: 61.9880, loss : 1.5724\n",
      "Batch num: 17, c_loss: 1.5684, Val Loss: 61.9568, loss : 1.5684\n",
      "Batch num: 18, c_loss: 1.5827, Val Loss: 62.0137, loss : 1.5827\n",
      "Epoch [1/30], Loss: 1.5827, Train Acc: 0.8896, Test Accuracy: 0.7753\n",
      "Batch num: 0, c_loss: 1.5394, Val Loss: 61.9745, loss : 1.5487\n",
      "Batch num: 1, c_loss: 1.5510, Val Loss: 61.9032, loss : 1.5671\n",
      "Batch num: 2, c_loss: 1.5654, Val Loss: 61.8069, loss : 1.5654\n",
      "Batch num: 3, c_loss: 1.5714, Val Loss: 61.8248, loss : 1.5714\n",
      "Batch num: 4, c_loss: 1.5643, Val Loss: 61.8095, loss : 1.5736\n",
      "Batch num: 5, c_loss: 1.5337, Val Loss: 61.7883, loss : 1.5501\n",
      "Batch num: 6, c_loss: 1.5415, Val Loss: 61.7602, loss : 1.5415\n",
      "Batch num: 7, c_loss: 1.5502, Val Loss: 61.7149, loss : 1.5502\n",
      "Batch num: 8, c_loss: 1.5625, Val Loss: 61.6860, loss : 1.5625\n",
      "Batch num: 9, c_loss: 1.5329, Val Loss: 61.8062, loss : 1.5329\n",
      "Batch num: 10, c_loss: 1.5850, Val Loss: 61.9326, loss : 1.5951\n",
      "Batch num: 11, c_loss: 1.5568, Val Loss: 62.0089, loss : 1.5748\n",
      "Batch num: 12, c_loss: 1.5365, Val Loss: 62.0166, loss : 1.5613\n",
      "Batch num: 13, c_loss: 1.5178, Val Loss: 62.0556, loss : 1.5483\n",
      "Batch num: 14, c_loss: 1.5618, Val Loss: 62.0313, loss : 1.5971\n",
      "Batch num: 15, c_loss: 1.5512, Val Loss: 62.0238, loss : 1.5911\n",
      "Batch num: 16, c_loss: 1.5480, Val Loss: 62.0170, loss : 1.5922\n",
      "Batch num: 17, c_loss: 1.5499, Val Loss: 62.0214, loss : 1.5978\n",
      "Batch num: 18, c_loss: 1.5757, Val Loss: 62.0773, loss : 1.6271\n",
      "Epoch [2/30], Loss: 1.6271, Train Acc: 0.9093, Test Accuracy: 0.7767\n",
      "Batch num: 0, c_loss: 1.5051, Val Loss: 62.0728, loss : 1.5599\n",
      "Batch num: 1, c_loss: 1.5396, Val Loss: 62.1927, loss : 1.5976\n",
      "Batch num: 2, c_loss: 1.5415, Val Loss: 62.4277, loss : 1.6025\n",
      "Batch num: 3, c_loss: 1.5780, Val Loss: 62.5436, loss : 1.6419\n",
      "Batch num: 4, c_loss: 1.5537, Val Loss: 62.4312, loss : 1.6199\n",
      "Batch num: 5, c_loss: 1.5299, Val Loss: 62.2507, loss : 1.5978\n",
      "Batch num: 6, c_loss: 1.5382, Val Loss: 62.1136, loss : 1.6078\n",
      "Batch num: 7, c_loss: 1.5352, Val Loss: 61.9055, loss : 1.6060\n",
      "Batch num: 8, c_loss: 1.5645, Val Loss: 61.5230, loss : 1.6362\n",
      "Batch num: 9, c_loss: 1.5266, Val Loss: 61.2053, loss : 1.5266\n",
      "Batch num: 10, c_loss: 1.5689, Val Loss: 61.0223, loss : 1.5689\n",
      "Batch num: 11, c_loss: 1.5286, Val Loss: 60.9768, loss : 1.5286\n",
      "Batch num: 12, c_loss: 1.5335, Val Loss: 61.0336, loss : 1.5335\n",
      "Batch num: 13, c_loss: 1.5008, Val Loss: 61.1120, loss : 1.5089\n",
      "Batch num: 14, c_loss: 1.5207, Val Loss: 61.1842, loss : 1.5349\n",
      "Batch num: 15, c_loss: 1.5305, Val Loss: 61.1939, loss : 1.5494\n",
      "Batch num: 16, c_loss: 1.5468, Val Loss: 61.1987, loss : 1.5696\n",
      "Batch num: 17, c_loss: 1.5390, Val Loss: 61.2149, loss : 1.5649\n",
      "Batch num: 18, c_loss: 1.5775, Val Loss: 61.1708, loss : 1.6062\n",
      "Epoch [3/30], Loss: 1.6062, Train Acc: 0.9208, Test Accuracy: 0.7897\n",
      "Batch num: 0, c_loss: 1.5046, Val Loss: 61.1240, loss : 1.5358\n",
      "Batch num: 1, c_loss: 1.5408, Val Loss: 60.9983, loss : 1.5744\n",
      "Batch num: 2, c_loss: 1.5286, Val Loss: 60.9047, loss : 1.5644\n",
      "Batch num: 3, c_loss: 1.5528, Val Loss: 60.8694, loss : 1.5528\n",
      "Batch num: 4, c_loss: 1.5380, Val Loss: 60.8616, loss : 1.5380\n",
      "Batch num: 5, c_loss: 1.5258, Val Loss: 60.8671, loss : 1.5258\n",
      "Batch num: 6, c_loss: 1.5234, Val Loss: 60.8485, loss : 1.5299\n",
      "Batch num: 7, c_loss: 1.5284, Val Loss: 60.8482, loss : 1.5284\n",
      "Batch num: 8, c_loss: 1.5482, Val Loss: 60.8744, loss : 1.5482\n",
      "Batch num: 9, c_loss: 1.5053, Val Loss: 60.8759, loss : 1.5113\n",
      "Batch num: 10, c_loss: 1.5533, Val Loss: 60.8836, loss : 1.5632\n",
      "Batch num: 11, c_loss: 1.5146, Val Loss: 60.8339, loss : 1.5276\n",
      "Batch num: 12, c_loss: 1.5317, Val Loss: 60.7740, loss : 1.5317\n",
      "Batch num: 13, c_loss: 1.4975, Val Loss: 60.7498, loss : 1.4975\n",
      "Batch num: 14, c_loss: 1.5142, Val Loss: 60.7407, loss : 1.5142\n",
      "Batch num: 15, c_loss: 1.5210, Val Loss: 60.7572, loss : 1.5210\n",
      "Batch num: 16, c_loss: 1.5265, Val Loss: 60.7979, loss : 1.5331\n",
      "Batch num: 17, c_loss: 1.5195, Val Loss: 60.8654, loss : 1.5308\n",
      "Batch num: 18, c_loss: 1.5461, Val Loss: 60.9002, loss : 1.5611\n",
      "Epoch [4/30], Loss: 1.5611, Train Acc: 0.9340, Test Accuracy: 0.8042\n",
      "Batch num: 0, c_loss: 1.5014, Val Loss: 60.9341, loss : 1.5192\n",
      "Batch num: 1, c_loss: 1.5298, Val Loss: 60.8882, loss : 1.5499\n",
      "Batch num: 2, c_loss: 1.5254, Val Loss: 60.8565, loss : 1.5478\n",
      "Batch num: 3, c_loss: 1.5299, Val Loss: 60.8617, loss : 1.5552\n",
      "Batch num: 4, c_loss: 1.5242, Val Loss: 60.8810, loss : 1.5524\n",
      "Batch num: 5, c_loss: 1.5177, Val Loss: 60.8864, loss : 1.5485\n",
      "Batch num: 6, c_loss: 1.5088, Val Loss: 60.9252, loss : 1.5416\n",
      "Batch num: 7, c_loss: 1.5204, Val Loss: 60.9909, loss : 1.5549\n",
      "Batch num: 8, c_loss: 1.5369, Val Loss: 61.0628, loss : 1.5727\n",
      "Batch num: 9, c_loss: 1.5014, Val Loss: 61.1748, loss : 1.5384\n",
      "Batch num: 10, c_loss: 1.5556, Val Loss: 61.3052, loss : 1.5939\n",
      "Batch num: 11, c_loss: 1.5004, Val Loss: 61.4038, loss : 1.5401\n",
      "Batch num: 12, c_loss: 1.5166, Val Loss: 61.4462, loss : 1.5577\n",
      "Batch num: 13, c_loss: 1.4916, Val Loss: 61.4270, loss : 1.5337\n",
      "Batch num: 14, c_loss: 1.5098, Val Loss: 61.3833, loss : 1.5526\n",
      "Batch num: 15, c_loss: 1.5096, Val Loss: 61.3419, loss : 1.5532\n",
      "Batch num: 16, c_loss: 1.5202, Val Loss: 61.3341, loss : 1.5647\n",
      "Batch num: 17, c_loss: 1.5227, Val Loss: 61.2386, loss : 1.5679\n",
      "Batch num: 18, c_loss: 1.5389, Val Loss: 61.1369, loss : 1.5845\n",
      "Epoch [5/30], Loss: 1.5845, Train Acc: 0.9426, Test Accuracy: 0.8004\n",
      "Batch num: 0, c_loss: 1.4978, Val Loss: 61.0352, loss : 1.5443\n",
      "Batch num: 1, c_loss: 1.5197, Val Loss: 60.9879, loss : 1.5669\n",
      "Batch num: 2, c_loss: 1.5209, Val Loss: 60.8988, loss : 1.5687\n",
      "Batch num: 3, c_loss: 1.5273, Val Loss: 60.8356, loss : 1.5756\n",
      "Batch num: 4, c_loss: 1.5309, Val Loss: 60.7943, loss : 1.5797\n",
      "Batch num: 5, c_loss: 1.5069, Val Loss: 60.7199, loss : 1.5561\n",
      "Batch num: 6, c_loss: 1.5086, Val Loss: 60.7645, loss : 1.5086\n",
      "Batch num: 7, c_loss: 1.5185, Val Loss: 60.9709, loss : 1.5260\n",
      "Batch num: 8, c_loss: 1.5322, Val Loss: 61.1966, loss : 1.5449\n",
      "Batch num: 9, c_loss: 1.4961, Val Loss: 61.3395, loss : 1.5125\n",
      "Batch num: 10, c_loss: 1.5444, Val Loss: 61.3019, loss : 1.5637\n",
      "Batch num: 11, c_loss: 1.5047, Val Loss: 61.1951, loss : 1.5261\n",
      "Batch num: 12, c_loss: 1.5118, Val Loss: 61.1335, loss : 1.5349\n",
      "Batch num: 13, c_loss: 1.4888, Val Loss: 60.9635, loss : 1.5134\n",
      "Batch num: 14, c_loss: 1.5041, Val Loss: 60.8717, loss : 1.5300\n",
      "Batch num: 15, c_loss: 1.5036, Val Loss: 60.7997, loss : 1.5304\n",
      "Batch num: 16, c_loss: 1.5159, Val Loss: 60.7578, loss : 1.5435\n",
      "Batch num: 17, c_loss: 1.5161, Val Loss: 60.7306, loss : 1.5443\n",
      "Batch num: 18, c_loss: 1.5378, Val Loss: 60.7051, loss : 1.5664\n",
      "Epoch [6/30], Loss: 1.5664, Train Acc: 0.9463, Test Accuracy: 0.8027\n",
      "Batch num: 0, c_loss: 1.4938, Val Loss: 60.7010, loss : 1.4938\n",
      "Batch num: 1, c_loss: 1.5199, Val Loss: 60.7101, loss : 1.5199\n",
      "Batch num: 2, c_loss: 1.5158, Val Loss: 60.7048, loss : 1.5198\n",
      "Batch num: 3, c_loss: 1.5240, Val Loss: 60.6935, loss : 1.5303\n",
      "Batch num: 4, c_loss: 1.5236, Val Loss: 60.6919, loss : 1.5236\n",
      "Batch num: 5, c_loss: 1.4965, Val Loss: 60.6775, loss : 1.4965\n",
      "Batch num: 6, c_loss: 1.5042, Val Loss: 60.6728, loss : 1.5042\n",
      "Batch num: 7, c_loss: 1.5117, Val Loss: 60.6551, loss : 1.5117\n",
      "Batch num: 8, c_loss: 1.5303, Val Loss: 60.6463, loss : 1.5303\n",
      "Batch num: 9, c_loss: 1.4884, Val Loss: 60.6503, loss : 1.4884\n",
      "Batch num: 10, c_loss: 1.5391, Val Loss: 60.6507, loss : 1.5434\n",
      "Batch num: 11, c_loss: 1.5004, Val Loss: 60.6314, loss : 1.5071\n",
      "Batch num: 12, c_loss: 1.5097, Val Loss: 60.6049, loss : 1.5097\n",
      "Batch num: 13, c_loss: 1.4855, Val Loss: 60.5706, loss : 1.4855\n",
      "Batch num: 14, c_loss: 1.5040, Val Loss: 60.5656, loss : 1.5040\n",
      "Batch num: 15, c_loss: 1.4965, Val Loss: 60.5779, loss : 1.4965\n",
      "Batch num: 16, c_loss: 1.5171, Val Loss: 60.5718, loss : 1.5207\n",
      "Batch num: 17, c_loss: 1.5155, Val Loss: 60.5779, loss : 1.5207\n",
      "Batch num: 18, c_loss: 1.5230, Val Loss: 60.5917, loss : 1.5289\n",
      "Epoch [7/30], Loss: 1.5289, Train Acc: 0.9507, Test Accuracy: 0.8104\n",
      "Batch num: 0, c_loss: 1.4920, Val Loss: 60.6068, loss : 1.4989\n",
      "Batch num: 1, c_loss: 1.5194, Val Loss: 60.5899, loss : 1.5266\n",
      "Batch num: 2, c_loss: 1.5173, Val Loss: 60.5682, loss : 1.5249\n",
      "Batch num: 3, c_loss: 1.5225, Val Loss: 60.5411, loss : 1.5306\n",
      "Batch num: 4, c_loss: 1.5234, Val Loss: 60.5303, loss : 1.5234\n",
      "Batch num: 5, c_loss: 1.4963, Val Loss: 60.5454, loss : 1.4963\n",
      "Batch num: 6, c_loss: 1.5030, Val Loss: 60.5534, loss : 1.5062\n",
      "Batch num: 7, c_loss: 1.5038, Val Loss: 60.5522, loss : 1.5077\n",
      "Batch num: 8, c_loss: 1.5200, Val Loss: 60.5485, loss : 1.5250\n",
      "Batch num: 9, c_loss: 1.4891, Val Loss: 60.5608, loss : 1.4943\n",
      "Batch num: 10, c_loss: 1.5351, Val Loss: 60.5639, loss : 1.5408\n",
      "Batch num: 11, c_loss: 1.5012, Val Loss: 60.5466, loss : 1.5072\n",
      "Batch num: 12, c_loss: 1.5073, Val Loss: 60.5417, loss : 1.5137\n",
      "Batch num: 13, c_loss: 1.4846, Val Loss: 60.5389, loss : 1.4914\n",
      "Batch num: 14, c_loss: 1.5038, Val Loss: 60.5378, loss : 1.5107\n",
      "Batch num: 15, c_loss: 1.4942, Val Loss: 60.5409, loss : 1.5012\n",
      "Batch num: 16, c_loss: 1.5159, Val Loss: 60.5420, loss : 1.5236\n",
      "Batch num: 17, c_loss: 1.5155, Val Loss: 60.5449, loss : 1.5237\n",
      "Batch num: 18, c_loss: 1.5193, Val Loss: 60.5503, loss : 1.5278\n",
      "Epoch [8/30], Loss: 1.5278, Train Acc: 0.9523, Test Accuracy: 0.8103\n",
      "Batch num: 0, c_loss: 1.4911, Val Loss: 60.5515, loss : 1.4998\n",
      "Batch num: 1, c_loss: 1.5168, Val Loss: 60.5420, loss : 1.5256\n",
      "Batch num: 2, c_loss: 1.5158, Val Loss: 60.5457, loss : 1.5248\n",
      "Batch num: 3, c_loss: 1.5180, Val Loss: 60.5422, loss : 1.5271\n",
      "Batch num: 4, c_loss: 1.5232, Val Loss: 60.5395, loss : 1.5329\n",
      "Batch num: 5, c_loss: 1.4964, Val Loss: 60.5503, loss : 1.5067\n",
      "Batch num: 6, c_loss: 1.5004, Val Loss: 60.5529, loss : 1.5110\n",
      "Batch num: 7, c_loss: 1.5038, Val Loss: 60.5624, loss : 1.5148\n",
      "Batch num: 8, c_loss: 1.5199, Val Loss: 60.5650, loss : 1.5312\n",
      "Batch num: 9, c_loss: 1.4883, Val Loss: 60.5790, loss : 1.4998\n",
      "Batch num: 10, c_loss: 1.5352, Val Loss: 60.5944, loss : 1.5466\n",
      "Batch num: 11, c_loss: 1.5004, Val Loss: 60.5847, loss : 1.5117\n",
      "Batch num: 12, c_loss: 1.5055, Val Loss: 60.5703, loss : 1.5166\n",
      "Batch num: 13, c_loss: 1.4876, Val Loss: 60.5464, loss : 1.4988\n",
      "Batch num: 14, c_loss: 1.5004, Val Loss: 60.5416, loss : 1.5118\n",
      "Batch num: 15, c_loss: 1.4890, Val Loss: 60.5366, loss : 1.5007\n",
      "Batch num: 16, c_loss: 1.5158, Val Loss: 60.5389, loss : 1.5278\n",
      "Batch num: 17, c_loss: 1.5153, Val Loss: 60.5414, loss : 1.5274\n",
      "Batch num: 18, c_loss: 1.5194, Val Loss: 60.5344, loss : 1.5316\n",
      "Epoch [9/30], Loss: 1.5316, Train Acc: 0.9537, Test Accuracy: 0.8095\n",
      "Batch num: 0, c_loss: 1.4885, Val Loss: 60.5407, loss : 1.5008\n",
      "Batch num: 1, c_loss: 1.5156, Val Loss: 60.5403, loss : 1.5278\n",
      "Batch num: 2, c_loss: 1.5157, Val Loss: 60.5401, loss : 1.5277\n",
      "Batch num: 3, c_loss: 1.5164, Val Loss: 60.5390, loss : 1.5281\n",
      "Batch num: 4, c_loss: 1.5233, Val Loss: 60.5310, loss : 1.5348\n",
      "Batch num: 5, c_loss: 1.4962, Val Loss: 60.5357, loss : 1.5072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.0005\u001b[39m)\n\u001b[0;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(criterion, optimizer, model, num_epochs, trainloader, testloader, valloader, device)\u001b[0m\n\u001b[0;32m     33\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_inputs, val_labels \u001b[38;5;129;01min\u001b[39;00m valloader:\n\u001b[0;32m     36\u001b[0m         val_inputs, val_labels \u001b[38;5;241m=\u001b[39m val_inputs\u001b[38;5;241m.\u001b[39mto(device), val_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(val_inputs)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\functional.py:153\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(pic\u001b[38;5;241m.\u001b[39mgetbands()))\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VGG().to(device)\n",
    "model.load_state_dict(torch.load('source.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "num_epochs = 30\n",
    "fine_tune(criterion, optimizer, model, num_epochs, target_dl, test_dl, val_dl, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
