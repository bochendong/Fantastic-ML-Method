{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OT8NfOcbkbzR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.backends.cudnn as cudnn\n",
        "import copy\n",
        "from torch.autograd import Function\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-DsLK_NnkbzR"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 256\n",
        "image_size = 28*28\n",
        "alpha = 0.005\n",
        "DANN_EPOCHES = 10\n",
        "DANN_TRAINING_BATCH = 50\n",
        "dann_path = 'dann.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzl9AkwhkbzR",
        "outputId": "ceec5923-7085-42c8-a3af-c3034d429f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Split training set for training and validation\n",
        "train_size = int(0.8 * len(train_set))\n",
        "val_size = len(train_set) - train_size\n",
        "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "# DataLoader for validation set\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last = True)\n",
        "train_loader =  DataLoader(train_set, batch_size=batch_size, shuffle=False, drop_last = True)\n",
        "test_loader =  DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BPHNhlPkbzR"
      },
      "source": [
        "# DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0CWhtn0jkbzS"
      },
      "outputs": [],
      "source": [
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "    \n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1), \n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Classifier_Small(nn.Module):\n",
        "    def __init__(self, out_dim = 10):\n",
        "        super(Classifier_Small, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "             nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4 * 4 * 256, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, 4 * 4 * 256)  # Adjusted for the added depth\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dXz9YIu4kbzS"
      },
      "outputs": [],
      "source": [
        "class DANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DANN, self).__init__()\n",
        "        self.autoencoder = ConvAutoencoder()\n",
        "        self.classifier = Classifier_Small(out_dim = 10)\n",
        "        self.domain_classifier = Classifier_Small(out_dim = 2)\n",
        "\n",
        "    def forward(self, input_data, alpha):\n",
        "        feature = self.autoencoder(input_data)\n",
        "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
        "        class_output = self.classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "        \n",
        "        return class_output, domain_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p38-3bPBkbzS"
      },
      "outputs": [],
      "source": [
        "def train_DANN(train_loader, model, criterion, optimizer, epoches):\n",
        "    model.train()\n",
        "    src_domain_label = torch.ones(batch_size).long().to(device)\n",
        "    tgt_domain_label = torch.zeros(batch_size).long().to(device)\n",
        "\n",
        "    for e in range(epoches):\n",
        "        data_target_iter = iter(train_loader)\n",
        "        correct_source_domain, correct_tgt_domain = 0, 0\n",
        "        total = 0\n",
        "        for i in range(DANN_TRAINING_BATCH):\n",
        "            # Src\n",
        "            optimizer.zero_grad()\n",
        "            source, source_label = next(data_target_iter)\n",
        "            total += source.size(0)\n",
        "\n",
        "            source, source_label = source.to(device), source_label.to(device)\n",
        "\n",
        "            class_output, domain_output = model(source, alpha)\n",
        "\n",
        "            loss_s_label = criterion(class_output, source_label)\n",
        "            loss_s_domain = criterion(domain_output, src_domain_label)\n",
        "\n",
        "            _, predicted = torch.max(class_output.data, 1)\n",
        "            correct_source_domain += predicted.eq(source_label.data).cpu().sum().item()\n",
        "\n",
        "            # Tgt\n",
        "            target, target_label  = next(data_target_iter)\n",
        "            target, target_label = target.to(device), target_label.to(device)\n",
        "\n",
        "            class_output, domain_output = model(target, alpha)\n",
        "\n",
        "            loss_t_label = criterion(class_output, target_label)\n",
        "            loss_t_domain = criterion(domain_output, tgt_domain_label)\n",
        "\n",
        "            _, predicted = torch.max(class_output.data, 1)\n",
        "            correct_tgt_domain += predicted.eq(target_label.data).cpu().sum().item()\n",
        "\n",
        "            loss = loss_s_label + loss_s_domain + loss_t_domain + loss_t_label\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if ((e + 1) % 5 == 0):\n",
        "            print(f\"Epoch: [{e}/{epoches}]: source correct: {correct_source_domain/total}, target correct: {correct_tgt_domain/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ffFuN6kbzS",
        "outputId": "88381c15-b074-4449-d32d-8b714644a87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from file.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(dann_path):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    dann = DANN().to(device)\n",
        "    optimizer = optim.Adam(dann.parameters(), lr=0.001)\n",
        "\n",
        "    train_DANN(train_loader, dann, criterion, optimizer, DANN_EPOCHES)\n",
        "    with torch.no_grad():\n",
        "        torch.save(dann.state_dict(), dann_path)\n",
        "else:\n",
        "    dann = DANN().to(device)\n",
        "    dann.load_state_dict(torch.load(dann_path))\n",
        "    print(\"Loaded model from file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyMOw2MnkbzS"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, out_dim = 10):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, out_dim),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, 512)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, label in testloader:\n",
        "        images, label = images.to(device), label.to(device)\n",
        "        features = dann.autoencoder(images)\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, epoches, criterion, optimizer):\n",
        "    best_model_wts = None\n",
        "    leader = VGG().to(device)\n",
        "    best_loss = float('inf')\n",
        "    batch_num = 0\n",
        "    warm_up_batch = 3\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        if (best_model_wts):\n",
        "            model.load_state_dict(best_model_wts)\n",
        "        for epoch in range(epoches):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                features = dann.autoencoder(inputs)\n",
        "            outputs = model(features)\n",
        "            if (warm_up_batch < batch_num):\n",
        "                for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
        "                    reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
        "                loss = criterion(outputs, labels) + reg_loss\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for val_inputs, val_labels in val_loader:\n",
        "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "                    features = dann.autoencoder(val_inputs)\n",
        "                    outputs = model(features)\n",
        "                    batch_loss = criterion(outputs, val_labels)\n",
        "                    val_loss += batch_loss.item()\n",
        "\n",
        "                if val_loss < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    leader.load_state_dict(best_model_wts)\n",
        "\n",
        "            print(f\"Batch: {batch_num}, epoch: {epoch}, Train Loss: {loss.item()}, Val Loss: {val_loss}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_acc = test(model, test_loader)\n",
        "                \n",
        "        print(f\"epoch: {epoch}, Test Acc: {test_acc}\")\n",
        "\n",
        "        batch_num += 1\n",
        "\n",
        "    return best_model_wts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch: 0, epoch: 0, Train Loss: 2.299405574798584, Val Loss: 89.79897046089172\n",
            "Batch: 0, epoch: 1, Train Loss: 2.261664390563965, Val Loss: 89.79429197311401\n",
            "Batch: 0, epoch: 2, Train Loss: 2.2087221145629883, Val Loss: 89.79136157035828\n",
            "Batch: 0, epoch: 3, Train Loss: 2.1589226722717285, Val Loss: 89.79161405563354\n",
            "Batch: 0, epoch: 4, Train Loss: 2.095796823501587, Val Loss: 89.79613041877747\n",
            "Batch: 0, epoch: 5, Train Loss: 2.0450117588043213, Val Loss: 89.80266499519348\n",
            "Batch: 0, epoch: 6, Train Loss: 1.985438585281372, Val Loss: 89.81656622886658\n",
            "Batch: 0, epoch: 7, Train Loss: 1.9195525646209717, Val Loss: 89.83870077133179\n",
            "Batch: 0, epoch: 8, Train Loss: 1.8682588338851929, Val Loss: 89.85702419281006\n",
            "Batch: 0, epoch: 9, Train Loss: 1.810956597328186, Val Loss: 89.86341190338135\n",
            "Batch: 0, epoch: 10, Train Loss: 1.7549476623535156, Val Loss: 89.88233304023743\n",
            "Batch: 0, epoch: 11, Train Loss: 1.7215338945388794, Val Loss: 89.9250226020813\n",
            "Batch: 0, epoch: 12, Train Loss: 1.6784061193466187, Val Loss: 89.98306083679199\n",
            "Batch: 0, epoch: 13, Train Loss: 1.6453020572662354, Val Loss: 90.07885909080505\n",
            "Batch: 0, epoch: 14, Train Loss: 1.6190686225891113, Val Loss: 90.19232392311096\n",
            "Batch: 0, epoch: 15, Train Loss: 1.5861834287643433, Val Loss: 90.24094223976135\n",
            "Batch: 0, epoch: 16, Train Loss: 1.554753065109253, Val Loss: 90.27450346946716\n",
            "Batch: 0, epoch: 17, Train Loss: 1.5307645797729492, Val Loss: 90.32806706428528\n",
            "Batch: 0, epoch: 18, Train Loss: 1.5130462646484375, Val Loss: 90.55436444282532\n",
            "Batch: 0, epoch: 19, Train Loss: 1.5005784034729004, Val Loss: 90.75914239883423\n",
            "epoch: 19, Test Acc: 0.10016025641025642\n",
            "Batch: 1, epoch: 0, Train Loss: 1.9558300971984863, Val Loss: 90.68273735046387\n",
            "Batch: 1, epoch: 1, Train Loss: 1.813035011291504, Val Loss: 90.12822794914246\n",
            "Batch: 1, epoch: 2, Train Loss: 1.7088857889175415, Val Loss: 89.74938559532166\n",
            "Batch: 1, epoch: 3, Train Loss: 1.6469612121582031, Val Loss: 89.49002313613892\n",
            "Batch: 1, epoch: 4, Train Loss: 1.6073272228240967, Val Loss: 89.42791104316711\n",
            "Batch: 1, epoch: 5, Train Loss: 1.582543134689331, Val Loss: 89.22797060012817\n",
            "Batch: 1, epoch: 6, Train Loss: 1.5611495971679688, Val Loss: 88.99859046936035\n",
            "Batch: 1, epoch: 7, Train Loss: 1.5444786548614502, Val Loss: 88.63950681686401\n",
            "Batch: 1, epoch: 8, Train Loss: 1.5278856754302979, Val Loss: 88.28900599479675\n",
            "Batch: 1, epoch: 9, Train Loss: 1.5181567668914795, Val Loss: 88.08750367164612\n",
            "Batch: 1, epoch: 10, Train Loss: 1.505991816520691, Val Loss: 87.93811583518982\n",
            "Batch: 1, epoch: 11, Train Loss: 1.4966074228286743, Val Loss: 87.79415893554688\n",
            "Batch: 1, epoch: 12, Train Loss: 1.4902817010879517, Val Loss: 87.41435217857361\n",
            "Batch: 1, epoch: 13, Train Loss: 1.4868041276931763, Val Loss: 86.81202554702759\n",
            "Batch: 1, epoch: 14, Train Loss: 1.4819278717041016, Val Loss: 86.02252912521362\n",
            "Batch: 1, epoch: 15, Train Loss: 1.4809025526046753, Val Loss: 85.11279463768005\n",
            "Batch: 1, epoch: 16, Train Loss: 1.4772831201553345, Val Loss: 84.29866600036621\n",
            "Batch: 1, epoch: 17, Train Loss: 1.4745107889175415, Val Loss: 83.5292706489563\n",
            "Batch: 1, epoch: 18, Train Loss: 1.473378300666809, Val Loss: 82.5842514038086\n",
            "Batch: 1, epoch: 19, Train Loss: 1.4734693765640259, Val Loss: 81.7541835308075\n",
            "epoch: 19, Test Acc: 0.34815705128205127\n",
            "Batch: 2, epoch: 0, Train Loss: 1.8775925636291504, Val Loss: 78.84111070632935\n",
            "Batch: 2, epoch: 1, Train Loss: 1.7411001920700073, Val Loss: 79.0669572353363\n",
            "Batch: 2, epoch: 2, Train Loss: 1.6668763160705566, Val Loss: 79.59014999866486\n",
            "Batch: 2, epoch: 3, Train Loss: 1.632190227508545, Val Loss: 79.70977783203125\n",
            "Batch: 2, epoch: 4, Train Loss: 1.6088727712631226, Val Loss: 78.76336896419525\n",
            "Batch: 2, epoch: 5, Train Loss: 1.5882846117019653, Val Loss: 78.30823802947998\n",
            "Batch: 2, epoch: 6, Train Loss: 1.569218635559082, Val Loss: 78.15955460071564\n",
            "Batch: 2, epoch: 7, Train Loss: 1.5517783164978027, Val Loss: 77.94779682159424\n",
            "Batch: 2, epoch: 8, Train Loss: 1.5396599769592285, Val Loss: 78.1672420501709\n",
            "Batch: 2, epoch: 9, Train Loss: 1.5327024459838867, Val Loss: 78.21962690353394\n",
            "Batch: 2, epoch: 10, Train Loss: 1.5233824253082275, Val Loss: 78.21544814109802\n",
            "Batch: 2, epoch: 11, Train Loss: 1.5172983407974243, Val Loss: 78.41662335395813\n",
            "Batch: 2, epoch: 12, Train Loss: 1.511578917503357, Val Loss: 78.67058205604553\n",
            "Batch: 2, epoch: 13, Train Loss: 1.5078834295272827, Val Loss: 78.71123373508453\n",
            "Batch: 2, epoch: 14, Train Loss: 1.5026451349258423, Val Loss: 78.74158704280853\n",
            "Batch: 2, epoch: 15, Train Loss: 1.5033742189407349, Val Loss: 78.70461523532867\n",
            "Batch: 2, epoch: 16, Train Loss: 1.499861717224121, Val Loss: 78.89687216281891\n",
            "Batch: 2, epoch: 17, Train Loss: 1.4964503049850464, Val Loss: 79.26399838924408\n",
            "Batch: 2, epoch: 18, Train Loss: 1.4916125535964966, Val Loss: 79.35637271404266\n",
            "Batch: 2, epoch: 19, Train Loss: 1.4852079153060913, Val Loss: 79.20220470428467\n",
            "epoch: 19, Test Acc: 0.4016426282051282\n",
            "Batch: 3, epoch: 0, Train Loss: 1.9549939632415771, Val Loss: 77.17124450206757\n",
            "Batch: 3, epoch: 1, Train Loss: 1.8104151487350464, Val Loss: 75.37501442432404\n",
            "Batch: 3, epoch: 2, Train Loss: 1.7323318719863892, Val Loss: 75.58863592147827\n",
            "Batch: 3, epoch: 3, Train Loss: 1.6861709356307983, Val Loss: 76.54098451137543\n",
            "Batch: 3, epoch: 4, Train Loss: 1.6573634147644043, Val Loss: 77.44677090644836\n",
            "Batch: 3, epoch: 5, Train Loss: 1.6350957155227661, Val Loss: 77.42360770702362\n",
            "Batch: 3, epoch: 6, Train Loss: 1.6171239614486694, Val Loss: 77.14298558235168\n",
            "Batch: 3, epoch: 7, Train Loss: 1.6067264080047607, Val Loss: 77.03208684921265\n",
            "Batch: 3, epoch: 8, Train Loss: 1.590287446975708, Val Loss: 77.1809549331665\n",
            "Batch: 3, epoch: 9, Train Loss: 1.5734010934829712, Val Loss: 77.62008464336395\n",
            "Batch: 3, epoch: 10, Train Loss: 1.5565955638885498, Val Loss: 78.58308732509613\n",
            "Batch: 3, epoch: 11, Train Loss: 1.5429991483688354, Val Loss: 78.74956905841827\n",
            "Batch: 3, epoch: 12, Train Loss: 1.5298084020614624, Val Loss: 78.09569549560547\n",
            "Batch: 3, epoch: 13, Train Loss: 1.5224615335464478, Val Loss: 77.35646748542786\n",
            "Batch: 3, epoch: 14, Train Loss: 1.5136958360671997, Val Loss: 77.02304649353027\n",
            "Batch: 3, epoch: 15, Train Loss: 1.4976706504821777, Val Loss: 77.39081180095673\n",
            "Batch: 3, epoch: 16, Train Loss: 1.4918415546417236, Val Loss: 78.17691838741302\n",
            "Batch: 3, epoch: 17, Train Loss: 1.481913685798645, Val Loss: 79.24021995067596\n",
            "Batch: 3, epoch: 18, Train Loss: 1.4783376455307007, Val Loss: 78.859978556633\n",
            "Batch: 3, epoch: 19, Train Loss: 1.4725031852722168, Val Loss: 78.48652172088623\n",
            "epoch: 19, Test Acc: 0.44481169871794873\n",
            "Batch: 4, epoch: 0, Train Loss: 1.8864837884902954, Val Loss: 75.60845172405243\n",
            "Batch: 4, epoch: 1, Train Loss: 1.772558331489563, Val Loss: 79.67870581150055\n",
            "epoch: 1, Test Acc: 0.38060897435897434\n",
            "Batch: 5, epoch: 0, Train Loss: 1.8971426486968994, Val Loss: 76.74430549144745\n",
            "Batch: 5, epoch: 1, Train Loss: 1.7958322763442993, Val Loss: 79.98613345623016\n",
            "epoch: 1, Test Acc: 0.37439903846153844\n",
            "Batch: 6, epoch: 0, Train Loss: 1.9428881406784058, Val Loss: 75.39562141895294\n",
            "Batch: 6, epoch: 1, Train Loss: 1.8381739854812622, Val Loss: 76.94993698596954\n",
            "epoch: 1, Test Acc: 0.4476161858974359\n",
            "Batch: 7, epoch: 0, Train Loss: 1.8571057319641113, Val Loss: 75.51526463031769\n",
            "Batch: 7, epoch: 1, Train Loss: 1.7723761796951294, Val Loss: 77.51444828510284\n",
            "epoch: 1, Test Acc: 0.426181891025641\n",
            "Batch: 8, epoch: 0, Train Loss: 1.8678630590438843, Val Loss: 74.23590290546417\n",
            "Batch: 8, epoch: 1, Train Loss: 1.7580804824829102, Val Loss: 75.43712115287781\n",
            "epoch: 1, Test Acc: 0.4717548076923077\n",
            "Batch: 9, epoch: 0, Train Loss: 1.8397164344787598, Val Loss: 74.38350212574005\n",
            "Batch: 9, epoch: 1, Train Loss: 1.7692790031433105, Val Loss: 75.44577777385712\n",
            "epoch: 1, Test Acc: 0.47215544871794873\n",
            "Batch: 10, epoch: 0, Train Loss: 1.8632978200912476, Val Loss: 72.85875642299652\n",
            "Batch: 10, epoch: 1, Train Loss: 1.757546067237854, Val Loss: 74.32638216018677\n",
            "epoch: 1, Test Acc: 0.5017027243589743\n",
            "Batch: 11, epoch: 0, Train Loss: 1.8515028953552246, Val Loss: 73.89578783512115\n",
            "Batch: 11, epoch: 1, Train Loss: 1.790597677230835, Val Loss: 74.13751947879791\n",
            "epoch: 1, Test Acc: 0.5052083333333334\n",
            "Batch: 12, epoch: 0, Train Loss: 1.820589542388916, Val Loss: 73.19463586807251\n",
            "Batch: 12, epoch: 1, Train Loss: 1.7220414876937866, Val Loss: 74.41795015335083\n",
            "epoch: 1, Test Acc: 0.49368990384615385\n",
            "Batch: 13, epoch: 0, Train Loss: 1.8266758918762207, Val Loss: 73.48305702209473\n",
            "Batch: 13, epoch: 1, Train Loss: 1.745619297027588, Val Loss: 74.55881083011627\n",
            "epoch: 1, Test Acc: 0.49008413461538464\n",
            "Batch: 14, epoch: 0, Train Loss: 1.8710447549819946, Val Loss: 72.77448177337646\n",
            "Batch: 14, epoch: 1, Train Loss: 1.781934380531311, Val Loss: 75.09036087989807\n",
            "epoch: 1, Test Acc: 0.48066907051282054\n",
            "Batch: 15, epoch: 0, Train Loss: 1.8512474298477173, Val Loss: 74.3244686126709\n",
            "Batch: 15, epoch: 1, Train Loss: 1.7802283763885498, Val Loss: 76.03481805324554\n",
            "epoch: 1, Test Acc: 0.45713141025641024\n",
            "Batch: 16, epoch: 0, Train Loss: 1.8651436567306519, Val Loss: 74.15694904327393\n",
            "Batch: 16, epoch: 1, Train Loss: 1.7797691822052002, Val Loss: 77.58875930309296\n",
            "epoch: 1, Test Acc: 0.42397836538461536\n",
            "Batch: 17, epoch: 0, Train Loss: 1.810744047164917, Val Loss: 74.00372517108917\n",
            "Batch: 17, epoch: 1, Train Loss: 1.7391619682312012, Val Loss: 76.49197924137115\n",
            "epoch: 1, Test Acc: 0.44701522435897434\n",
            "Batch: 18, epoch: 0, Train Loss: 1.7819660902023315, Val Loss: 73.20769453048706\n",
            "Batch: 18, epoch: 1, Train Loss: 1.7297090291976929, Val Loss: 74.33026659488678\n",
            "epoch: 1, Test Acc: 0.49829727564102566\n",
            "Batch: 19, epoch: 0, Train Loss: 1.7924457788467407, Val Loss: 73.46519863605499\n",
            "Batch: 19, epoch: 1, Train Loss: 1.7145181894302368, Val Loss: 75.17223942279816\n",
            "epoch: 1, Test Acc: 0.4699519230769231\n",
            "Batch: 20, epoch: 0, Train Loss: 1.8097565174102783, Val Loss: 74.2072081565857\n",
            "Batch: 20, epoch: 1, Train Loss: 1.7214144468307495, Val Loss: 75.75023746490479\n",
            "epoch: 1, Test Acc: 0.4596354166666667\n",
            "Batch: 21, epoch: 0, Train Loss: 1.838563084602356, Val Loss: 75.45732390880585\n",
            "Batch: 21, epoch: 1, Train Loss: 1.7620484828948975, Val Loss: 76.9272609949112\n",
            "epoch: 1, Test Acc: 0.43599759615384615\n",
            "Batch: 22, epoch: 0, Train Loss: 1.8508052825927734, Val Loss: 75.54054474830627\n",
            "Batch: 22, epoch: 1, Train Loss: 1.7839298248291016, Val Loss: 78.32537114620209\n",
            "epoch: 1, Test Acc: 0.4055488782051282\n",
            "Batch: 23, epoch: 0, Train Loss: 1.808375597000122, Val Loss: 74.80544126033783\n",
            "Batch: 23, epoch: 1, Train Loss: 1.7447758913040161, Val Loss: 77.45184791088104\n",
            "epoch: 1, Test Acc: 0.4241786858974359\n",
            "Batch: 24, epoch: 0, Train Loss: 1.8576561212539673, Val Loss: 75.04069375991821\n",
            "Batch: 24, epoch: 1, Train Loss: 1.7947474718093872, Val Loss: 77.2835978269577\n",
            "epoch: 1, Test Acc: 0.4306891025641026\n",
            "Batch: 25, epoch: 0, Train Loss: 1.7940531969070435, Val Loss: 73.75504243373871\n",
            "Batch: 25, epoch: 1, Train Loss: 1.7437725067138672, Val Loss: 75.82734656333923\n",
            "epoch: 1, Test Acc: 0.4616386217948718\n",
            "Batch: 26, epoch: 0, Train Loss: 1.8066786527633667, Val Loss: 72.9052631855011\n",
            "Batch: 26, epoch: 1, Train Loss: 1.7624317407608032, Val Loss: 73.44898355007172\n",
            "epoch: 1, Test Acc: 0.5150240384615384\n",
            "Batch: 27, epoch: 0, Train Loss: 1.8106130361557007, Val Loss: 72.89225661754608\n",
            "Batch: 27, epoch: 1, Train Loss: 1.7344486713409424, Val Loss: 74.79713416099548\n",
            "epoch: 1, Test Acc: 0.4817708333333333\n",
            "Batch: 28, epoch: 0, Train Loss: 1.8292052745819092, Val Loss: 73.41977596282959\n",
            "Batch: 28, epoch: 1, Train Loss: 1.7518917322158813, Val Loss: 75.46597003936768\n",
            "epoch: 1, Test Acc: 0.46724759615384615\n",
            "Batch: 29, epoch: 0, Train Loss: 1.757460117340088, Val Loss: 73.12795376777649\n",
            "Batch: 29, epoch: 1, Train Loss: 1.7091784477233887, Val Loss: 75.06058716773987\n",
            "epoch: 1, Test Acc: 0.4762620192307692\n",
            "Batch: 30, epoch: 0, Train Loss: 1.7900105714797974, Val Loss: 72.47952365875244\n",
            "Batch: 30, epoch: 1, Train Loss: 1.7216750383377075, Val Loss: 73.39591872692108\n",
            "epoch: 1, Test Acc: 0.5104166666666666\n",
            "Batch: 31, epoch: 0, Train Loss: 1.8226779699325562, Val Loss: 74.22153532505035\n",
            "Batch: 31, epoch: 1, Train Loss: 1.7590664625167847, Val Loss: 76.47557735443115\n",
            "epoch: 1, Test Acc: 0.4361979166666667\n",
            "Batch: 32, epoch: 0, Train Loss: 1.858406662940979, Val Loss: 73.91111516952515\n",
            "Batch: 32, epoch: 1, Train Loss: 1.7984851598739624, Val Loss: 74.8842921257019\n",
            "epoch: 1, Test Acc: 0.4814703525641026\n",
            "Batch: 33, epoch: 0, Train Loss: 1.816206932067871, Val Loss: 72.73282098770142\n",
            "Batch: 33, epoch: 1, Train Loss: 1.7517828941345215, Val Loss: 73.49572002887726\n",
            "epoch: 1, Test Acc: 0.5161258012820513\n",
            "Batch: 34, epoch: 0, Train Loss: 1.8210475444793701, Val Loss: 72.74006581306458\n",
            "Batch: 34, epoch: 1, Train Loss: 1.7577115297317505, Val Loss: 73.43152105808258\n",
            "epoch: 1, Test Acc: 0.5155248397435898\n",
            "Batch: 35, epoch: 0, Train Loss: 1.8174633979797363, Val Loss: 73.51797115802765\n",
            "Batch: 35, epoch: 1, Train Loss: 1.7664965391159058, Val Loss: 75.70390737056732\n",
            "epoch: 1, Test Acc: 0.4651442307692308\n",
            "Batch: 36, epoch: 0, Train Loss: 1.807929277420044, Val Loss: 73.75322365760803\n",
            "Batch: 36, epoch: 1, Train Loss: 1.729177474975586, Val Loss: 76.76689982414246\n",
            "epoch: 1, Test Acc: 0.44140625\n",
            "Batch: 37, epoch: 0, Train Loss: 1.8143953084945679, Val Loss: 73.22873294353485\n",
            "Batch: 37, epoch: 1, Train Loss: 1.7473969459533691, Val Loss: 76.2614518404007\n",
            "epoch: 1, Test Acc: 0.46464342948717946\n",
            "Batch: 38, epoch: 0, Train Loss: 1.7981317043304443, Val Loss: 72.42724239826202\n",
            "Batch: 38, epoch: 1, Train Loss: 1.7280949354171753, Val Loss: 73.7764003276825\n",
            "epoch: 1, Test Acc: 0.5155248397435898\n",
            "Batch: 39, epoch: 0, Train Loss: 1.8106776475906372, Val Loss: 74.32325220108032\n",
            "Batch: 39, epoch: 1, Train Loss: 1.7426209449768066, Val Loss: 75.62913525104523\n",
            "epoch: 1, Test Acc: 0.47115384615384615\n",
            "Batch: 40, epoch: 0, Train Loss: 1.832216501235962, Val Loss: 73.97220730781555\n",
            "Batch: 40, epoch: 1, Train Loss: 1.7618050575256348, Val Loss: 76.81483101844788\n",
            "epoch: 1, Test Acc: 0.44200721153846156\n",
            "Batch: 41, epoch: 0, Train Loss: 1.8182426691055298, Val Loss: 74.37864768505096\n",
            "Batch: 41, epoch: 1, Train Loss: 1.7504509687423706, Val Loss: 75.99854063987732\n",
            "epoch: 1, Test Acc: 0.45332532051282054\n",
            "Batch: 42, epoch: 0, Train Loss: 1.8442223072052002, Val Loss: 73.64992225170135\n",
            "Batch: 42, epoch: 1, Train Loss: 1.7570629119873047, Val Loss: 75.13973438739777\n",
            "epoch: 1, Test Acc: 0.48066907051282054\n",
            "Batch: 43, epoch: 0, Train Loss: 1.7810550928115845, Val Loss: 73.51294994354248\n",
            "Batch: 43, epoch: 1, Train Loss: 1.7116283178329468, Val Loss: 75.5015938282013\n",
            "epoch: 1, Test Acc: 0.4675480769230769\n",
            "Batch: 44, epoch: 0, Train Loss: 1.833008885383606, Val Loss: 73.42290735244751\n",
            "Batch: 44, epoch: 1, Train Loss: 1.7595804929733276, Val Loss: 74.82524383068085\n",
            "epoch: 1, Test Acc: 0.4940905448717949\n",
            "Batch: 45, epoch: 0, Train Loss: 1.7731455564498901, Val Loss: 72.66808998584747\n",
            "Batch: 45, epoch: 1, Train Loss: 1.72291898727417, Val Loss: 73.84351348876953\n",
            "epoch: 1, Test Acc: 0.5055088141025641\n",
            "Batch: 46, epoch: 0, Train Loss: 1.82355535030365, Val Loss: 73.92706441879272\n",
            "Batch: 46, epoch: 1, Train Loss: 1.7395492792129517, Val Loss: 76.12753903865814\n",
            "epoch: 1, Test Acc: 0.4557291666666667\n",
            "Batch: 47, epoch: 0, Train Loss: 1.8416173458099365, Val Loss: 72.59363090991974\n",
            "Batch: 47, epoch: 1, Train Loss: 1.7665832042694092, Val Loss: 72.7570208311081\n",
            "epoch: 1, Test Acc: 0.5240384615384616\n",
            "Batch: 48, epoch: 0, Train Loss: 1.7764619588851929, Val Loss: 73.00027453899384\n",
            "Batch: 48, epoch: 1, Train Loss: 1.7065719366073608, Val Loss: 74.18631398677826\n",
            "epoch: 1, Test Acc: 0.4989983974358974\n",
            "Batch: 49, epoch: 0, Train Loss: 1.8208750486373901, Val Loss: 72.76414716243744\n",
            "Batch: 49, epoch: 1, Train Loss: 1.7411051988601685, Val Loss: 73.07468104362488\n",
            "epoch: 1, Test Acc: 0.5262419871794872\n",
            "Batch: 50, epoch: 0, Train Loss: 1.810517430305481, Val Loss: 72.3563107252121\n",
            "Batch: 50, epoch: 1, Train Loss: 1.7425795793533325, Val Loss: 73.47545671463013\n",
            "epoch: 1, Test Acc: 0.5169270833333334\n",
            "Batch: 51, epoch: 0, Train Loss: 1.7931978702545166, Val Loss: 72.58451020717621\n",
            "Batch: 51, epoch: 1, Train Loss: 1.731451392173767, Val Loss: 73.69775938987732\n",
            "epoch: 1, Test Acc: 0.5174278846153846\n",
            "Batch: 52, epoch: 0, Train Loss: 1.8245947360992432, Val Loss: 72.79695129394531\n",
            "Batch: 52, epoch: 1, Train Loss: 1.7576643228530884, Val Loss: 74.3677726984024\n",
            "epoch: 1, Test Acc: 0.487880608974359\n",
            "Batch: 53, epoch: 0, Train Loss: 1.800443410873413, Val Loss: 74.01795506477356\n",
            "Batch: 53, epoch: 1, Train Loss: 1.7350711822509766, Val Loss: 75.2498322725296\n",
            "epoch: 1, Test Acc: 0.469150641025641\n",
            "Batch: 54, epoch: 0, Train Loss: 1.83174467086792, Val Loss: 73.61376678943634\n",
            "Batch: 54, epoch: 1, Train Loss: 1.74717378616333, Val Loss: 75.20443880558014\n",
            "epoch: 1, Test Acc: 0.47415865384615385\n",
            "Batch: 55, epoch: 0, Train Loss: 1.8307369947433472, Val Loss: 73.20055520534515\n",
            "Batch: 55, epoch: 1, Train Loss: 1.7643964290618896, Val Loss: 74.4841673374176\n",
            "epoch: 1, Test Acc: 0.49389022435897434\n",
            "Batch: 56, epoch: 0, Train Loss: 1.8341655731201172, Val Loss: 73.13560044765472\n",
            "Batch: 56, epoch: 1, Train Loss: 1.7670226097106934, Val Loss: 74.72871387004852\n",
            "epoch: 1, Test Acc: 0.4857772435897436\n",
            "Batch: 57, epoch: 0, Train Loss: 1.8225017786026, Val Loss: 73.54612529277802\n",
            "Batch: 57, epoch: 1, Train Loss: 1.7584059238433838, Val Loss: 75.97582340240479\n",
            "epoch: 1, Test Acc: 0.45913461538461536\n",
            "Batch: 58, epoch: 0, Train Loss: 1.8104411363601685, Val Loss: 74.26132071018219\n",
            "Batch: 58, epoch: 1, Train Loss: 1.729648232460022, Val Loss: 75.79988777637482\n",
            "epoch: 1, Test Acc: 0.4671474358974359\n",
            "Batch: 59, epoch: 0, Train Loss: 1.7641762495040894, Val Loss: 72.69527518749237\n",
            "Batch: 59, epoch: 1, Train Loss: 1.7110909223556519, Val Loss: 73.815061211586\n",
            "epoch: 1, Test Acc: 0.5122195512820513\n",
            "Batch: 60, epoch: 0, Train Loss: 1.8255709409713745, Val Loss: 73.1266280412674\n",
            "Batch: 60, epoch: 1, Train Loss: 1.777039885520935, Val Loss: 74.95532667636871\n",
            "epoch: 1, Test Acc: 0.4827724358974359\n",
            "Batch: 61, epoch: 0, Train Loss: 1.7765947580337524, Val Loss: 74.3207460641861\n",
            "Batch: 61, epoch: 1, Train Loss: 1.7201025485992432, Val Loss: 76.74087810516357\n",
            "epoch: 1, Test Acc: 0.4428084935897436\n",
            "Batch: 62, epoch: 0, Train Loss: 1.799475908279419, Val Loss: 74.54008340835571\n",
            "Batch: 62, epoch: 1, Train Loss: 1.743889331817627, Val Loss: 75.43331944942474\n",
            "epoch: 1, Test Acc: 0.4733573717948718\n",
            "Batch: 63, epoch: 0, Train Loss: 1.8553352355957031, Val Loss: 72.1356110572815\n",
            "Batch: 63, epoch: 1, Train Loss: 1.7925010919570923, Val Loss: 73.16287958621979\n",
            "epoch: 1, Test Acc: 0.5221354166666666\n",
            "Batch: 64, epoch: 0, Train Loss: 1.824118971824646, Val Loss: 73.50637900829315\n",
            "Batch: 64, epoch: 1, Train Loss: 1.7606984376907349, Val Loss: 78.00575125217438\n",
            "epoch: 1, Test Acc: 0.40775240384615385\n",
            "Batch: 65, epoch: 0, Train Loss: 1.839785099029541, Val Loss: 73.90352296829224\n",
            "Batch: 65, epoch: 1, Train Loss: 1.7878241539001465, Val Loss: 77.09328055381775\n",
            "epoch: 1, Test Acc: 0.4289863782051282\n",
            "Batch: 66, epoch: 0, Train Loss: 1.8110805749893188, Val Loss: 73.67923402786255\n",
            "Batch: 66, epoch: 1, Train Loss: 1.7590768337249756, Val Loss: 75.2663505077362\n",
            "epoch: 1, Test Acc: 0.4736578525641026\n",
            "Batch: 67, epoch: 0, Train Loss: 1.794684886932373, Val Loss: 73.49915826320648\n",
            "Batch: 67, epoch: 1, Train Loss: 1.741599440574646, Val Loss: 74.5018253326416\n",
            "epoch: 1, Test Acc: 0.49368990384615385\n",
            "Batch: 68, epoch: 0, Train Loss: 1.8097010850906372, Val Loss: 72.09767639636993\n",
            "Batch: 68, epoch: 1, Train Loss: 1.7382750511169434, Val Loss: 73.08233666419983\n",
            "epoch: 1, Test Acc: 0.5277443910256411\n",
            "Batch: 69, epoch: 0, Train Loss: 1.793592929840088, Val Loss: 72.95868158340454\n",
            "Batch: 69, epoch: 1, Train Loss: 1.7294411659240723, Val Loss: 73.81857419013977\n",
            "epoch: 1, Test Acc: 0.5112179487179487\n",
            "Batch: 70, epoch: 0, Train Loss: 1.7743988037109375, Val Loss: 72.62635660171509\n",
            "Batch: 70, epoch: 1, Train Loss: 1.7155790328979492, Val Loss: 74.83621120452881\n",
            "epoch: 1, Test Acc: 0.4791666666666667\n",
            "Batch: 71, epoch: 0, Train Loss: 1.7974790334701538, Val Loss: 73.00763273239136\n",
            "Batch: 71, epoch: 1, Train Loss: 1.7299747467041016, Val Loss: 76.66639685630798\n",
            "epoch: 1, Test Acc: 0.4427083333333333\n",
            "Batch: 72, epoch: 0, Train Loss: 1.7739866971969604, Val Loss: 73.65436100959778\n",
            "Batch: 72, epoch: 1, Train Loss: 1.7091176509857178, Val Loss: 78.04520654678345\n",
            "epoch: 1, Test Acc: 0.41065705128205127\n",
            "Batch: 73, epoch: 0, Train Loss: 1.774435043334961, Val Loss: 72.90952265262604\n",
            "Batch: 73, epoch: 1, Train Loss: 1.7135475873947144, Val Loss: 73.59410858154297\n",
            "epoch: 1, Test Acc: 0.5198317307692307\n",
            "Batch: 74, epoch: 0, Train Loss: 1.7540005445480347, Val Loss: 72.00022006034851\n",
            "Batch: 74, epoch: 1, Train Loss: 1.676040768623352, Val Loss: 73.64130568504333\n",
            "epoch: 1, Test Acc: 0.5181290064102564\n",
            "Batch: 75, epoch: 0, Train Loss: 1.8326642513275146, Val Loss: 73.53182172775269\n",
            "Batch: 75, epoch: 1, Train Loss: 1.756715178489685, Val Loss: 74.86449193954468\n",
            "epoch: 1, Test Acc: 0.4912860576923077\n",
            "Batch: 76, epoch: 0, Train Loss: 1.80381178855896, Val Loss: 73.00906443595886\n",
            "Batch: 76, epoch: 1, Train Loss: 1.751598834991455, Val Loss: 74.0467072725296\n",
            "epoch: 1, Test Acc: 0.5139222756410257\n",
            "Batch: 77, epoch: 0, Train Loss: 1.8223981857299805, Val Loss: 72.71371638774872\n",
            "Batch: 77, epoch: 1, Train Loss: 1.7369197607040405, Val Loss: 73.33972072601318\n",
            "epoch: 1, Test Acc: 0.5218349358974359\n",
            "Batch: 78, epoch: 0, Train Loss: 1.7833399772644043, Val Loss: 71.89895272254944\n",
            "Batch: 78, epoch: 1, Train Loss: 1.7200225591659546, Val Loss: 72.99455726146698\n",
            "epoch: 1, Test Acc: 0.5367588141025641\n",
            "Batch: 79, epoch: 0, Train Loss: 1.8199633359909058, Val Loss: 71.97048902511597\n",
            "Batch: 79, epoch: 1, Train Loss: 1.750807285308838, Val Loss: 73.64843654632568\n",
            "epoch: 1, Test Acc: 0.5253405448717948\n",
            "Batch: 80, epoch: 0, Train Loss: 1.859521508216858, Val Loss: 71.33655273914337\n",
            "Batch: 80, epoch: 1, Train Loss: 1.7689096927642822, Val Loss: 71.9430536031723\n",
            "epoch: 1, Test Acc: 0.5619991987179487\n",
            "Batch: 81, epoch: 0, Train Loss: 1.8134517669677734, Val Loss: 72.0109646320343\n",
            "Batch: 81, epoch: 1, Train Loss: 1.7419030666351318, Val Loss: 73.09440672397614\n",
            "epoch: 1, Test Acc: 0.5315504807692307\n",
            "Batch: 82, epoch: 0, Train Loss: 1.791420578956604, Val Loss: 72.08326470851898\n",
            "Batch: 82, epoch: 1, Train Loss: 1.731740951538086, Val Loss: 73.99938547611237\n",
            "epoch: 1, Test Acc: 0.5138221153846154\n",
            "Batch: 83, epoch: 0, Train Loss: 1.8245346546173096, Val Loss: 72.24353778362274\n",
            "Batch: 83, epoch: 1, Train Loss: 1.7468235492706299, Val Loss: 74.77090430259705\n",
            "epoch: 1, Test Acc: 0.4914863782051282\n",
            "Batch: 84, epoch: 0, Train Loss: 1.8593624830245972, Val Loss: 72.85396730899811\n",
            "Batch: 84, epoch: 1, Train Loss: 1.7802631855010986, Val Loss: 74.17168653011322\n",
            "epoch: 1, Test Acc: 0.5019030448717948\n",
            "Batch: 85, epoch: 0, Train Loss: 1.8291984796524048, Val Loss: 71.15471172332764\n",
            "Batch: 85, epoch: 1, Train Loss: 1.779950499534607, Val Loss: 72.31627202033997\n",
            "epoch: 1, Test Acc: 0.5463741987179487\n",
            "Batch: 86, epoch: 0, Train Loss: 1.763877034187317, Val Loss: 72.54674482345581\n",
            "Batch: 86, epoch: 1, Train Loss: 1.7142009735107422, Val Loss: 73.02205717563629\n",
            "epoch: 1, Test Acc: 0.5288461538461539\n",
            "Batch: 87, epoch: 0, Train Loss: 1.7979544401168823, Val Loss: 71.11088764667511\n",
            "Batch: 87, epoch: 1, Train Loss: 1.735658049583435, Val Loss: 72.08679473400116\n",
            "epoch: 1, Test Acc: 0.559395032051282\n",
            "Batch: 88, epoch: 0, Train Loss: 1.8048795461654663, Val Loss: 72.06467866897583\n",
            "Batch: 88, epoch: 1, Train Loss: 1.7505522966384888, Val Loss: 73.80967247486115\n",
            "epoch: 1, Test Acc: 0.5129206730769231\n",
            "Batch: 89, epoch: 0, Train Loss: 1.7886834144592285, Val Loss: 71.46231997013092\n",
            "Batch: 89, epoch: 1, Train Loss: 1.724656343460083, Val Loss: 72.66524636745453\n",
            "epoch: 1, Test Acc: 0.5401642628205128\n",
            "Batch: 90, epoch: 0, Train Loss: 1.87886643409729, Val Loss: 71.03646349906921\n",
            "Batch: 90, epoch: 1, Train Loss: 1.811535358428955, Val Loss: 72.83243894577026\n",
            "epoch: 1, Test Acc: 0.5384615384615384\n",
            "Batch: 91, epoch: 0, Train Loss: 1.797100305557251, Val Loss: 72.09714376926422\n",
            "Batch: 91, epoch: 1, Train Loss: 1.738265037536621, Val Loss: 74.13051152229309\n",
            "epoch: 1, Test Acc: 0.5098157051282052\n",
            "Batch: 92, epoch: 0, Train Loss: 1.793280005455017, Val Loss: 71.4639424085617\n",
            "Batch: 92, epoch: 1, Train Loss: 1.7162455320358276, Val Loss: 72.9783045053482\n",
            "epoch: 1, Test Acc: 0.534354967948718\n",
            "Batch: 93, epoch: 0, Train Loss: 1.7762446403503418, Val Loss: 70.77292597293854\n",
            "Batch: 93, epoch: 1, Train Loss: 1.7333576679229736, Val Loss: 71.90148377418518\n",
            "epoch: 1, Test Acc: 0.5497796474358975\n",
            "Batch: 94, epoch: 0, Train Loss: 1.7722722291946411, Val Loss: 72.06668484210968\n",
            "Batch: 94, epoch: 1, Train Loss: 1.7029342651367188, Val Loss: 74.12071990966797\n",
            "epoch: 1, Test Acc: 0.4979967948717949\n",
            "Batch: 95, epoch: 0, Train Loss: 1.7902214527130127, Val Loss: 72.33207750320435\n",
            "Batch: 95, epoch: 1, Train Loss: 1.727654218673706, Val Loss: 73.85773766040802\n",
            "epoch: 1, Test Acc: 0.5067107371794872\n",
            "Batch: 96, epoch: 0, Train Loss: 1.7961876392364502, Val Loss: 71.50040543079376\n",
            "Batch: 96, epoch: 1, Train Loss: 1.7134140729904175, Val Loss: 72.45016992092133\n",
            "epoch: 1, Test Acc: 0.5389623397435898\n",
            "Batch: 97, epoch: 0, Train Loss: 1.838497519493103, Val Loss: 71.12726473808289\n",
            "Batch: 97, epoch: 1, Train Loss: 1.7739405632019043, Val Loss: 71.90167665481567\n",
            "epoch: 1, Test Acc: 0.5497796474358975\n",
            "Batch: 98, epoch: 0, Train Loss: 1.807276725769043, Val Loss: 70.77319943904877\n",
            "Batch: 98, epoch: 1, Train Loss: 1.74240243434906, Val Loss: 71.7621020078659\n",
            "epoch: 1, Test Acc: 0.5528846153846154\n",
            "Batch: 99, epoch: 0, Train Loss: 1.8321640491485596, Val Loss: 71.47039794921875\n",
            "Batch: 99, epoch: 1, Train Loss: 1.7624220848083496, Val Loss: 72.80150353908539\n",
            "epoch: 1, Test Acc: 0.5338541666666666\n",
            "Batch: 100, epoch: 0, Train Loss: 1.7884846925735474, Val Loss: 71.05963253974915\n",
            "Batch: 100, epoch: 1, Train Loss: 1.7272703647613525, Val Loss: 72.27027940750122\n",
            "epoch: 1, Test Acc: 0.5481770833333334\n",
            "Batch: 101, epoch: 0, Train Loss: 1.8269617557525635, Val Loss: 71.09671247005463\n",
            "Batch: 101, epoch: 1, Train Loss: 1.7429851293563843, Val Loss: 72.35305321216583\n",
            "epoch: 1, Test Acc: 0.5399639423076923\n",
            "Batch: 102, epoch: 0, Train Loss: 1.789303183555603, Val Loss: 71.65381598472595\n",
            "Batch: 102, epoch: 1, Train Loss: 1.7229949235916138, Val Loss: 74.17178535461426\n",
            "epoch: 1, Test Acc: 0.5010016025641025\n",
            "Batch: 103, epoch: 0, Train Loss: 1.8018629550933838, Val Loss: 71.8920385837555\n",
            "Batch: 103, epoch: 1, Train Loss: 1.746034860610962, Val Loss: 74.50651788711548\n",
            "epoch: 1, Test Acc: 0.49008413461538464\n",
            "Batch: 104, epoch: 0, Train Loss: 1.8285491466522217, Val Loss: 71.3379499912262\n",
            "Batch: 104, epoch: 1, Train Loss: 1.761344075202942, Val Loss: 72.85287404060364\n",
            "epoch: 1, Test Acc: 0.5350560897435898\n",
            "Batch: 105, epoch: 0, Train Loss: 1.8112163543701172, Val Loss: 71.28907072544098\n",
            "Batch: 105, epoch: 1, Train Loss: 1.7307041883468628, Val Loss: 73.57162404060364\n",
            "epoch: 1, Test Acc: 0.5154246794871795\n",
            "Batch: 106, epoch: 0, Train Loss: 1.7666257619857788, Val Loss: 71.346630692482\n",
            "Batch: 106, epoch: 1, Train Loss: 1.7163242101669312, Val Loss: 72.49494206905365\n",
            "epoch: 1, Test Acc: 0.534354967948718\n",
            "Batch: 107, epoch: 0, Train Loss: 1.824630618095398, Val Loss: 71.5100429058075\n",
            "Batch: 107, epoch: 1, Train Loss: 1.7576349973678589, Val Loss: 72.65484261512756\n",
            "epoch: 1, Test Acc: 0.5387620192307693\n",
            "Batch: 108, epoch: 0, Train Loss: 1.7852002382278442, Val Loss: 70.86018228530884\n",
            "Batch: 108, epoch: 1, Train Loss: 1.7181514501571655, Val Loss: 72.32231545448303\n",
            "epoch: 1, Test Acc: 0.5491786858974359\n",
            "Batch: 109, epoch: 0, Train Loss: 1.810959815979004, Val Loss: 71.5511006116867\n",
            "Batch: 109, epoch: 1, Train Loss: 1.7283333539962769, Val Loss: 73.6752051115036\n",
            "epoch: 1, Test Acc: 0.5153245192307693\n",
            "Batch: 110, epoch: 0, Train Loss: 1.797302007675171, Val Loss: 71.55026733875275\n",
            "Batch: 110, epoch: 1, Train Loss: 1.7338035106658936, Val Loss: 72.24660384654999\n",
            "epoch: 1, Test Acc: 0.5479767628205128\n",
            "Batch: 111, epoch: 0, Train Loss: 1.8004690408706665, Val Loss: 70.94750428199768\n",
            "Batch: 111, epoch: 1, Train Loss: 1.7496774196624756, Val Loss: 72.83478903770447\n",
            "epoch: 1, Test Acc: 0.5339543269230769\n",
            "Batch: 112, epoch: 0, Train Loss: 1.805980920791626, Val Loss: 71.21764075756073\n",
            "Batch: 112, epoch: 1, Train Loss: 1.7587870359420776, Val Loss: 73.98240351676941\n",
            "epoch: 1, Test Acc: 0.5132211538461539\n",
            "Batch: 113, epoch: 0, Train Loss: 1.8064634799957275, Val Loss: 70.8899313211441\n",
            "Batch: 113, epoch: 1, Train Loss: 1.7242428064346313, Val Loss: 72.90594947338104\n",
            "epoch: 1, Test Acc: 0.5419671474358975\n",
            "Batch: 114, epoch: 0, Train Loss: 1.812291145324707, Val Loss: 70.99808609485626\n",
            "Batch: 114, epoch: 1, Train Loss: 1.7661311626434326, Val Loss: 72.3093296289444\n",
            "epoch: 1, Test Acc: 0.5529847756410257\n",
            "Batch: 115, epoch: 0, Train Loss: 1.7857716083526611, Val Loss: 70.9327849149704\n",
            "Batch: 115, epoch: 1, Train Loss: 1.7149096727371216, Val Loss: 73.00762128829956\n",
            "epoch: 1, Test Acc: 0.5315504807692307\n",
            "Batch: 116, epoch: 0, Train Loss: 1.8301069736480713, Val Loss: 71.04041683673859\n",
            "Batch: 116, epoch: 1, Train Loss: 1.7374125719070435, Val Loss: 72.1342579126358\n",
            "epoch: 1, Test Acc: 0.5479767628205128\n",
            "Batch: 117, epoch: 0, Train Loss: 1.7836782932281494, Val Loss: 70.57798480987549\n",
            "Batch: 117, epoch: 1, Train Loss: 1.7253527641296387, Val Loss: 70.823535323143\n",
            "epoch: 1, Test Acc: 0.5788261217948718\n",
            "Batch: 118, epoch: 0, Train Loss: 1.8044401407241821, Val Loss: 70.85756230354309\n",
            "Batch: 118, epoch: 1, Train Loss: 1.7611865997314453, Val Loss: 72.01786518096924\n",
            "epoch: 1, Test Acc: 0.5598958333333334\n",
            "Batch: 119, epoch: 0, Train Loss: 1.7923028469085693, Val Loss: 71.16502010822296\n",
            "Batch: 119, epoch: 1, Train Loss: 1.72098970413208, Val Loss: 73.69196939468384\n",
            "epoch: 1, Test Acc: 0.515625\n",
            "Batch: 120, epoch: 0, Train Loss: 1.790093183517456, Val Loss: 71.39453494548798\n",
            "Batch: 120, epoch: 1, Train Loss: 1.7341301441192627, Val Loss: 74.41713976860046\n",
            "epoch: 1, Test Acc: 0.5004006410256411\n",
            "Batch: 121, epoch: 0, Train Loss: 1.8073995113372803, Val Loss: 71.19515025615692\n",
            "Batch: 121, epoch: 1, Train Loss: 1.7366526126861572, Val Loss: 73.53852713108063\n",
            "epoch: 1, Test Acc: 0.5262419871794872\n",
            "Batch: 122, epoch: 0, Train Loss: 1.7697349786758423, Val Loss: 70.97053182125092\n",
            "Batch: 122, epoch: 1, Train Loss: 1.7247767448425293, Val Loss: 72.83300054073334\n",
            "epoch: 1, Test Acc: 0.5386618589743589\n",
            "Batch: 123, epoch: 0, Train Loss: 1.8373453617095947, Val Loss: 71.01605677604675\n",
            "Batch: 123, epoch: 1, Train Loss: 1.7500356435775757, Val Loss: 73.78654992580414\n",
            "epoch: 1, Test Acc: 0.5116185897435898\n",
            "Batch: 124, epoch: 0, Train Loss: 1.8069875240325928, Val Loss: 71.44823026657104\n",
            "Batch: 124, epoch: 1, Train Loss: 1.7334996461868286, Val Loss: 75.66833078861237\n",
            "epoch: 1, Test Acc: 0.464443108974359\n",
            "Batch: 125, epoch: 0, Train Loss: 1.7969814538955688, Val Loss: 71.25402688980103\n",
            "Batch: 125, epoch: 1, Train Loss: 1.7368592023849487, Val Loss: 74.06648528575897\n",
            "epoch: 1, Test Acc: 0.5072115384615384\n",
            "Batch: 126, epoch: 0, Train Loss: 1.827944040298462, Val Loss: 71.86351776123047\n",
            "Batch: 126, epoch: 1, Train Loss: 1.7382662296295166, Val Loss: 74.85280978679657\n",
            "epoch: 1, Test Acc: 0.4850761217948718\n",
            "Batch: 127, epoch: 0, Train Loss: 1.811921238899231, Val Loss: 71.66302680969238\n",
            "Batch: 127, epoch: 1, Train Loss: 1.7386174201965332, Val Loss: 74.38010263442993\n",
            "epoch: 1, Test Acc: 0.49979967948717946\n",
            "Batch: 128, epoch: 0, Train Loss: 1.808172583580017, Val Loss: 70.8938592672348\n",
            "Batch: 128, epoch: 1, Train Loss: 1.7556970119476318, Val Loss: 72.14267468452454\n",
            "epoch: 1, Test Acc: 0.5482772435897436\n",
            "Batch: 129, epoch: 0, Train Loss: 1.7919222116470337, Val Loss: 71.08244204521179\n",
            "Batch: 129, epoch: 1, Train Loss: 1.7241660356521606, Val Loss: 73.13782358169556\n",
            "epoch: 1, Test Acc: 0.5279447115384616\n",
            "Batch: 130, epoch: 0, Train Loss: 1.8327994346618652, Val Loss: 71.73399722576141\n",
            "Batch: 130, epoch: 1, Train Loss: 1.7768936157226562, Val Loss: 74.19761765003204\n",
            "epoch: 1, Test Acc: 0.5009014423076923\n",
            "Batch: 131, epoch: 0, Train Loss: 1.7562652826309204, Val Loss: 71.56861674785614\n",
            "Batch: 131, epoch: 1, Train Loss: 1.6902395486831665, Val Loss: 73.32811999320984\n",
            "epoch: 1, Test Acc: 0.5233373397435898\n",
            "Batch: 132, epoch: 0, Train Loss: 1.7820322513580322, Val Loss: 70.75719118118286\n",
            "Batch: 132, epoch: 1, Train Loss: 1.7102171182632446, Val Loss: 71.85296058654785\n",
            "epoch: 1, Test Acc: 0.5576923076923077\n",
            "Batch: 133, epoch: 0, Train Loss: 1.802185297012329, Val Loss: 71.27233421802521\n",
            "Batch: 133, epoch: 1, Train Loss: 1.743550181388855, Val Loss: 73.0921356678009\n",
            "epoch: 1, Test Acc: 0.5250400641025641\n",
            "Batch: 134, epoch: 0, Train Loss: 1.764769196510315, Val Loss: 71.02331674098969\n",
            "Batch: 134, epoch: 1, Train Loss: 1.7310551404953003, Val Loss: 71.73190689086914\n",
            "epoch: 1, Test Acc: 0.5569911858974359\n",
            "Batch: 135, epoch: 0, Train Loss: 1.7949836254119873, Val Loss: 70.97512686252594\n",
            "Batch: 135, epoch: 1, Train Loss: 1.7045055627822876, Val Loss: 73.26506471633911\n",
            "epoch: 1, Test Acc: 0.5302483974358975\n",
            "Batch: 136, epoch: 0, Train Loss: 1.7755173444747925, Val Loss: 70.90439248085022\n",
            "Batch: 136, epoch: 1, Train Loss: 1.7225384712219238, Val Loss: 71.8537427186966\n",
            "epoch: 1, Test Acc: 0.5618990384615384\n",
            "Batch: 137, epoch: 0, Train Loss: 1.8117220401763916, Val Loss: 70.73430502414703\n",
            "Batch: 137, epoch: 1, Train Loss: 1.7671948671340942, Val Loss: 71.88707971572876\n",
            "epoch: 1, Test Acc: 0.5581931089743589\n",
            "Batch: 138, epoch: 0, Train Loss: 1.8078503608703613, Val Loss: 70.51634728908539\n",
            "Batch: 138, epoch: 1, Train Loss: 1.7374861240386963, Val Loss: 71.85019290447235\n",
            "epoch: 1, Test Acc: 0.5660056089743589\n",
            "Batch: 139, epoch: 0, Train Loss: 1.7667533159255981, Val Loss: 71.74206304550171\n",
            "Batch: 139, epoch: 1, Train Loss: 1.7274267673492432, Val Loss: 73.01252782344818\n",
            "epoch: 1, Test Acc: 0.5413661858974359\n",
            "Batch: 140, epoch: 0, Train Loss: 1.767940878868103, Val Loss: 71.04737401008606\n",
            "Batch: 140, epoch: 1, Train Loss: 1.685960054397583, Val Loss: 72.13137090206146\n",
            "epoch: 1, Test Acc: 0.5610977564102564\n",
            "Batch: 141, epoch: 0, Train Loss: 1.8281564712524414, Val Loss: 71.28513669967651\n",
            "Batch: 141, epoch: 1, Train Loss: 1.7544097900390625, Val Loss: 73.09010910987854\n",
            "epoch: 1, Test Acc: 0.5350560897435898\n",
            "Batch: 142, epoch: 0, Train Loss: 1.8220343589782715, Val Loss: 71.76484191417694\n",
            "Batch: 142, epoch: 1, Train Loss: 1.744649887084961, Val Loss: 73.85309278964996\n",
            "epoch: 1, Test Acc: 0.5180288461538461\n",
            "Batch: 143, epoch: 0, Train Loss: 1.7970260381698608, Val Loss: 71.83947575092316\n",
            "Batch: 143, epoch: 1, Train Loss: 1.7300156354904175, Val Loss: 74.28246068954468\n",
            "epoch: 1, Test Acc: 0.5017027243589743\n",
            "Batch: 144, epoch: 0, Train Loss: 1.780848741531372, Val Loss: 71.32142388820648\n",
            "Batch: 144, epoch: 1, Train Loss: 1.7171134948730469, Val Loss: 72.76185703277588\n",
            "epoch: 1, Test Acc: 0.5420673076923077\n",
            "Batch: 145, epoch: 0, Train Loss: 1.797573447227478, Val Loss: 70.63115048408508\n",
            "Batch: 145, epoch: 1, Train Loss: 1.7377471923828125, Val Loss: 71.50230705738068\n",
            "epoch: 1, Test Acc: 0.5639022435897436\n",
            "Batch: 146, epoch: 0, Train Loss: 1.775780200958252, Val Loss: 70.70084190368652\n",
            "Batch: 146, epoch: 1, Train Loss: 1.691642165184021, Val Loss: 72.1720439195633\n",
            "epoch: 1, Test Acc: 0.5485777243589743\n",
            "Batch: 147, epoch: 0, Train Loss: 1.7946364879608154, Val Loss: 70.84558689594269\n",
            "Batch: 147, epoch: 1, Train Loss: 1.73197340965271, Val Loss: 72.92680144309998\n",
            "epoch: 1, Test Acc: 0.5347556089743589\n",
            "Batch: 148, epoch: 0, Train Loss: 1.7609844207763672, Val Loss: 70.71002471446991\n",
            "Batch: 148, epoch: 1, Train Loss: 1.6967350244522095, Val Loss: 71.84639573097229\n",
            "epoch: 1, Test Acc: 0.5603966346153846\n",
            "Batch: 149, epoch: 0, Train Loss: 1.7881375551223755, Val Loss: 70.69908666610718\n",
            "Batch: 149, epoch: 1, Train Loss: 1.7155903577804565, Val Loss: 71.85974156856537\n",
            "epoch: 1, Test Acc: 0.5657051282051282\n",
            "Batch: 150, epoch: 0, Train Loss: 1.7907540798187256, Val Loss: 71.37372612953186\n",
            "Batch: 150, epoch: 1, Train Loss: 1.7104357481002808, Val Loss: 73.16600406169891\n",
            "epoch: 1, Test Acc: 0.53125\n",
            "Batch: 151, epoch: 0, Train Loss: 1.7909202575683594, Val Loss: 71.31279468536377\n",
            "Batch: 151, epoch: 1, Train Loss: 1.728485345840454, Val Loss: 72.83094668388367\n",
            "epoch: 1, Test Acc: 0.5401642628205128\n",
            "Batch: 152, epoch: 0, Train Loss: 1.7870327234268188, Val Loss: 71.2751977443695\n",
            "Batch: 152, epoch: 1, Train Loss: 1.7062649726867676, Val Loss: 72.47284984588623\n",
            "epoch: 1, Test Acc: 0.5426682692307693\n",
            "Batch: 153, epoch: 0, Train Loss: 1.7909873723983765, Val Loss: 70.61930644512177\n",
            "Batch: 153, epoch: 1, Train Loss: 1.7306606769561768, Val Loss: 71.70188689231873\n",
            "epoch: 1, Test Acc: 0.5677083333333334\n",
            "Batch: 154, epoch: 0, Train Loss: 1.7961506843566895, Val Loss: 70.77814900875092\n",
            "Batch: 154, epoch: 1, Train Loss: 1.7347385883331299, Val Loss: 71.9047441482544\n",
            "epoch: 1, Test Acc: 0.5567908653846154\n",
            "Batch: 155, epoch: 0, Train Loss: 1.7784446477890015, Val Loss: 70.5827624797821\n",
            "Batch: 155, epoch: 1, Train Loss: 1.711931586265564, Val Loss: 71.20094788074493\n",
            "epoch: 1, Test Acc: 0.5697115384615384\n"
          ]
        }
      ],
      "source": [
        "model = VGG().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "best_model_wts = train(model, 20, criterion, optimizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
