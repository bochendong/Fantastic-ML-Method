{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import copy\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 256\n",
    "image_size = 28*28\n",
    "alpha = 0.005\n",
    "DANN_EPOCHES = 50\n",
    "DANN_TRAINING_BATCH = 40\n",
    "dann_path = 'dann.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Split training set for training and validation\n",
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = len(train_set) - train_size\n",
    "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
    "\n",
    "# DataLoader for validation set\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "train_loader =  DataLoader(train_set, batch_size=256, shuffle=False, drop_last = True)\n",
    "test_loader =  DataLoader(test_set, batch_size=256, shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        input_data = input_data.expand(input_data.data.shape[0], 3, 32, 32)\n",
    "        feature = self.feature(input_data)\n",
    "        feature = feature.view(-1, 512)\n",
    "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
    "        class_output = self.classifier(feature)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DANN(train_loader, model, criterion, optimizer, epoches):\n",
    "    model.train()\n",
    "    src_domain_label = torch.ones(batch_size).long().to(device)\n",
    "    tgt_domain_label = torch.zeros(batch_size).long().to(device)\n",
    "\n",
    "    for e in range(epoches):\n",
    "        data_target_iter = iter(train_loader)\n",
    "        correct_source_domain, correct_tgt_domain = 0, 0\n",
    "        total = 0\n",
    "        for i in range(DANN_TRAINING_BATCH):\n",
    "            # Src\n",
    "            source, source_label = next(data_target_iter)\n",
    "            total += source.size(0)\n",
    "\n",
    "            source, source_label = source.to(device), source_label.to(device)\n",
    "\n",
    "            class_output, domain_output = model(source, alpha)\n",
    "\n",
    "            loss_s_label = criterion(class_output, source_label)\n",
    "            loss_s_domain = criterion(domain_output, src_domain_label)\n",
    "\n",
    "            _, predicted = torch.max(domain_output.data, 1)\n",
    "            correct_source_domain += predicted.eq(src_domain_label.data).cpu().sum().item()\n",
    "\n",
    "            # Tgt\n",
    "            target, target_label  = next(data_target_iter)\n",
    "            target, target_label = target.to(device), target_label.to(device)\n",
    "\n",
    "            class_output, domain_output = model(target, alpha)\n",
    "\n",
    "            loss_t_label = criterion(class_output, target_label)\n",
    "            loss_t_domain = criterion(domain_output, tgt_domain_label)\n",
    "\n",
    "            _, predicted = torch.max(domain_output.data, 1)\n",
    "            correct_tgt_domain += predicted.eq(tgt_domain_label.data).cpu().sum().item()\n",
    "\n",
    "            loss = loss_s_label + loss_s_domain + loss_t_domain + loss_t_label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if ((e + 1) % 5 == 0):\n",
    "            print(f\"{e}: source correct: {correct_source_domain/total}, \\\n",
    "                        target correct: {correct_tgt_domain/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dann_path):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    dann = DANN().to(device)\n",
    "    optimizer = optim.Adam(dann.parameters(), lr=0.001)\n",
    "\n",
    "    train_DANN(train_loader, dann, criterion, optimizer, DANN_EPOCHES)\n",
    "    with torch.no_grad():\n",
    "        torch.save(dann.state_dict(), dann_path)\n",
    "else:\n",
    "    dann = DANN().to(device)\n",
    "    dann.load_state_dict(torch.load(dann_path))\n",
    "    print(\"Loaded model from file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear (512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear (512, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear (512, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, label in testloader:\n",
    "        images, label = images.to(device), label.to(device)\n",
    "        feature = dann.feature(images)\n",
    "        outputs = model(feature)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Linear(model, epoches, criterion, optimizer):\n",
    "    best_model_wts = None\n",
    "    best_loss = float('inf')\n",
    "    batch_num = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        if (best_model_wts):\n",
    "            model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for epoch in range(epoches):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                feature = dann.feature(inputs)\n",
    "\n",
    "            outputs = model(feature)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    outputs = model(val_inputs)\n",
    "                    batch_loss = criterion(outputs, val_labels)\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                test_acc = test(model, test_loader)\n",
    "\n",
    "            print(f\"Batch: {batch_num}, epoch: {epoch}, Train Loss: {loss.item()}, \n",
    "                    Val Loss: {val_loss}, Test Acc: {test_acc}\")\n",
    "        batch_num += 1\n",
    "\n",
    "    return best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "best_model_wts = train_Linear(model, 20, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
