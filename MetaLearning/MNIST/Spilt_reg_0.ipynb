{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 4\n",
    "pre_heat_batches = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        print(dataset.root)\n",
    "        label_cache_filename = dataset.root + '/' +'_'+str(len(dataset))+'.pth'\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        '''\n",
    "        :param dataset: (CacheClassLabel)\n",
    "        :param class_list: (list) A list of integers\n",
    "        :param remap: (bool) Ex: remap class [2,4,6 ...] to [0,1,2 ...]\n",
    "        '''\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST(dataroot, train_aug=False):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n",
      "./data\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = MNIST('./data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_boundaries: [0, 2, 4, 6, 8, 10]\n",
      "{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_task(model, train_loader, optimizer, criterion, \n",
    "                  valid_out_dim, best_model_wts, task_num, task_names):\n",
    "    leader = MLP400().to(device)\n",
    "    best_loss = float('inf')\n",
    "    best_pre_heat_loss = float('inf')\n",
    "    if (best_model_wts):\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "    else:\n",
    "        leader.load_state_dict(model.state_dict())\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            model.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                leader_outputs = leader(images)\n",
    "\n",
    "            follower_outputs = model(images)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\n",
    "            loss = c_loss + alpha * torch.mean(torch.abs(follower_outputs - leader_outputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = AverageMeter()\n",
    "\n",
    "                for task in range(task_num + 1):\n",
    "                    val_name = task_names[task]\n",
    "                    val_data = val_dataset_splits[val_name]\n",
    "                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    for i, (input, target, _) in enumerate(val_loader):\n",
    "                        input, target = input.to(device), target.to(device)\n",
    "                        output = model(input)\n",
    "                        loss_v = criterion(output, target).item()\n",
    "\n",
    "                        val_loss.update(loss_v, len(target))\n",
    "\n",
    "                print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss:.4f}\", end = \" \")\n",
    "                \n",
    "                if(batch_num < pre_heat_batches):\n",
    "                    if (val_loss.avg < best_pre_heat_loss):\n",
    "                        best_pre_heat_loss = val_loss.avg\n",
    "                        best_pre_heat_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    print()\n",
    "                elif (batch_num == pre_heat_batches):\n",
    "                    best_model_wts = best_pre_heat_model_wts\n",
    "                    best_loss = best_pre_heat_loss\n",
    "                    print(f\"Leader changed with val acc {best_loss: .4f}\")\n",
    "                    leader.load_state_dict(best_model_wts) \n",
    "                else:\n",
    "                    if val_loss.avg < best_loss:\n",
    "                        best_loss = val_loss.avg\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        print(f\"Leader changed with val acc {best_loss: .4f}\")\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "                    else:\n",
    "                        print()\n",
    "            batch_num += 1\n",
    "    return best_model_wts, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(acc_table, model, train_name, task_names, task_index):\n",
    "    acc_table[train_name] = OrderedDict()\n",
    "\n",
    "    for j in range(task_index+1):\n",
    "        val_name = task_names[j]\n",
    "        val_data = val_dataset_splits[val_name]\n",
    "        val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        model.eval()\n",
    "        val_acc = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "        acc_table[val_name][train_name] = val_acc.avg\n",
    "    \n",
    "    return acc_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task_names):\n",
    "    leader_acc_table = OrderedDict()\n",
    "    follower_acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    best_loss = float('inf')\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\n",
    "    \n",
    "        follower_acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        leader = MLP400().to(device)\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "        eval(follower_acc_table, model, train_name, task_names, i)\n",
    "        eval(leader_acc_table, leader, train_name, task_names, i)\n",
    "\n",
    "        print(follower_acc_table)\n",
    "        print(leader_acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += follower_acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('follower Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    leader_avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += leader_acc_table[val_name][train_name]\n",
    "\n",
    "        leader_avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('leader Task', train_name, 'average acc:', leader_avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history, leader_avg_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task order: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Task: 1=====\n",
      "batch_num: 0, c_loss:0.8795, val_loss: 2.2131, loss:0.8795 \n",
      "batch_num: 1, c_loss:0.2279, val_loss: 2.1898, loss:1.3537 \n",
      "batch_num: 2, c_loss:0.1773, val_loss: 2.1823, loss:1.4582 \n",
      "batch_num: 3, c_loss:0.1746, val_loss: 2.1883, loss:1.3798 \n",
      "batch_num: 4, c_loss:0.2254, val_loss: 2.1975, loss:1.2263 \n",
      "batch_num: 5, c_loss:0.2942, val_loss: 2.2164, loss:1.1961 \n",
      "batch_num: 6, c_loss:0.3776, val_loss: 2.2345, loss:1.1339 \n",
      "batch_num: 7, c_loss:0.5058, val_loss: 2.2490, loss:1.1719 \n",
      "batch_num: 8, c_loss:0.5853, val_loss: 2.2483, loss:1.1879 \n",
      "batch_num: 9, c_loss:0.6251, val_loss: 2.2266, loss:1.2243 \n",
      "batch_num: 10, c_loss:0.5538, val_loss: 2.1947, loss:1.1729 \n",
      "batch_num: 11, c_loss:0.5223, val_loss: 2.1575, loss:1.1505 \n",
      "batch_num: 12, c_loss:0.4508, val_loss: 2.1255, loss:1.1076 \n",
      "batch_num: 13, c_loss:0.4228, val_loss: 2.1011, loss:1.0994 \n",
      "batch_num: 14, c_loss:0.4045, val_loss: 2.0830, loss:1.0838 \n",
      "batch_num: 15, c_loss:0.3879, val_loss: 2.0718, loss:1.1736 \n",
      "batch_num: 16, c_loss:0.4234, val_loss: 2.0596, loss:1.1374 \n",
      "batch_num: 17, c_loss:0.3992, val_loss: 2.0489, loss:1.0988 \n",
      "batch_num: 18, c_loss:0.3922, val_loss: 2.0437, loss:1.1025 \n",
      "batch_num: 19, c_loss:0.3942, val_loss: 2.0414, loss:1.0935 \n",
      "batch_num: 20, c_loss:0.4245, val_loss: 2.0398, loss:1.1163 \n",
      "batch_num: 21, c_loss:0.4400, val_loss: 2.0388, loss:1.1307 \n",
      "batch_num: 22, c_loss:0.4552, val_loss: 2.0342, loss:1.1172 \n",
      "batch_num: 23, c_loss:0.4605, val_loss: 2.0256, loss:1.1008 \n",
      "batch_num: 24, c_loss:0.4482, val_loss: 2.0220, loss:1.0862 \n",
      "batch_num: 25, c_loss:0.4820, val_loss: 2.0226, loss:1.1036 \n",
      "batch_num: 26, c_loss:0.4834, val_loss: 2.0273, loss:1.0533 \n",
      "batch_num: 27, c_loss:0.4937, val_loss: 2.0306, loss:1.1107 \n",
      "batch_num: 28, c_loss:0.4931, val_loss: 2.0264, loss:1.0829 \n",
      "batch_num: 29, c_loss:0.4724, val_loss: 1.9986, loss:1.0402 \n",
      "batch_num: 30, c_loss:0.4502, val_loss: 1.9563, loss:1.0568 Leader changed with val acc  1.9986\n",
      "batch_num: 31, c_loss:0.4577, val_loss: 1.9156, loss:0.5853 Leader changed with val acc  1.9156\n",
      "batch_num: 32, c_loss:0.4250, val_loss: 1.8399, loss:0.4250 Leader changed with val acc  1.8399\n",
      "batch_num: 33, c_loss:0.3667, val_loss: 1.7444, loss:0.3667 Leader changed with val acc  1.7444\n",
      "batch_num: 34, c_loss:0.3133, val_loss: 1.6301, loss:0.3133 Leader changed with val acc  1.6301\n",
      "batch_num: 35, c_loss:0.2744, val_loss: 1.5024, loss:0.2744 Leader changed with val acc  1.5024\n",
      "batch_num: 36, c_loss:0.2135, val_loss: 1.3684, loss:0.2135 Leader changed with val acc  1.3684\n",
      "batch_num: 37, c_loss:0.1641, val_loss: 1.2367, loss:0.1641 Leader changed with val acc  1.2367\n",
      "batch_num: 38, c_loss:0.1279, val_loss: 1.1127, loss:0.1279 Leader changed with val acc  1.1127\n",
      "batch_num: 39, c_loss:0.0998, val_loss: 1.0026, loss:0.0998 Leader changed with val acc  1.0026\n",
      "batch_num: 40, c_loss:0.0911, val_loss: 0.8995, loss:0.0911 Leader changed with val acc  0.8995\n",
      "batch_num: 41, c_loss:0.0806, val_loss: 0.8158, loss:0.0806 Leader changed with val acc  0.8158\n",
      "batch_num: 42, c_loss:0.0594, val_loss: 0.7423, loss:0.0594 Leader changed with val acc  0.7423\n",
      "batch_num: 43, c_loss:0.0522, val_loss: 0.6825, loss:0.0522 Leader changed with val acc  0.6825\n",
      "batch_num: 44, c_loss:0.0475, val_loss: 0.6279, loss:0.0475 Leader changed with val acc  0.6279\n",
      "batch_num: 45, c_loss:0.0453, val_loss: 0.5833, loss:0.0453 Leader changed with val acc  0.5833\n",
      "batch_num: 46, c_loss:0.0308, val_loss: 0.5445, loss:0.0308 Leader changed with val acc  0.5445\n",
      "batch_num: 47, c_loss:0.0365, val_loss: 0.5106, loss:0.0365 Leader changed with val acc  0.5106\n",
      "batch_num: 48, c_loss:0.0456, val_loss: 0.4838, loss:0.0456 Leader changed with val acc  0.4838\n",
      "batch_num: 49, c_loss:0.0234, val_loss: 0.4610, loss:0.0234 Leader changed with val acc  0.4610\n",
      "batch_num: 50, c_loss:0.0415, val_loss: 0.4403, loss:0.0415 Leader changed with val acc  0.4403\n",
      "batch_num: 51, c_loss:0.0190, val_loss: 0.4257, loss:0.0190 Leader changed with val acc  0.4257\n",
      "batch_num: 52, c_loss:0.0192, val_loss: 0.4097, loss:0.0192 Leader changed with val acc  0.4097\n",
      "batch_num: 53, c_loss:0.0195, val_loss: 0.3968, loss:0.0195 Leader changed with val acc  0.3968\n",
      "batch_num: 54, c_loss:0.0194, val_loss: 0.3882, loss:0.0194 Leader changed with val acc  0.3882\n",
      "batch_num: 55, c_loss:0.0190, val_loss: 0.3802, loss:0.0190 Leader changed with val acc  0.3802\n",
      "batch_num: 56, c_loss:0.0166, val_loss: 0.3755, loss:0.0166 Leader changed with val acc  0.3755\n",
      "batch_num: 57, c_loss:0.0170, val_loss: 0.3717, loss:0.0170 Leader changed with val acc  0.3717\n",
      "batch_num: 58, c_loss:0.0133, val_loss: 0.3676, loss:0.0133 Leader changed with val acc  0.3676\n",
      "batch_num: 59, c_loss:0.0352, val_loss: 0.3653, loss:0.0352 Leader changed with val acc  0.3653\n",
      "batch_num: 60, c_loss:0.0087, val_loss: 0.3616, loss:0.0087 Leader changed with val acc  0.3616\n",
      "batch_num: 61, c_loss:0.0168, val_loss: 0.3603, loss:0.0168 Leader changed with val acc  0.3603\n",
      "batch_num: 62, c_loss:0.0134, val_loss: 0.3577, loss:0.0134 Leader changed with val acc  0.3577\n",
      "batch_num: 63, c_loss:0.0132, val_loss: 0.3549, loss:0.0132 Leader changed with val acc  0.3549\n",
      "batch_num: 64, c_loss:0.0144, val_loss: 0.3562, loss:0.0144 \n",
      "batch_num: 65, c_loss:0.0107, val_loss: 0.3911, loss:0.0335 \n",
      "batch_num: 66, c_loss:0.0140, val_loss: 0.3817, loss:0.1693 \n",
      "batch_num: 67, c_loss:0.0097, val_loss: 0.3569, loss:0.1384 \n",
      "batch_num: 68, c_loss:0.0110, val_loss: 0.3659, loss:0.1444 \n",
      "batch_num: 69, c_loss:0.0101, val_loss: 0.3906, loss:0.1518 \n",
      "batch_num: 70, c_loss:0.0232, val_loss: 0.4098, loss:0.1459 \n",
      "batch_num: 71, c_loss:0.0133, val_loss: 0.4058, loss:0.1405 \n",
      "batch_num: 72, c_loss:0.0333, val_loss: 0.3967, loss:0.1738 \n",
      "batch_num: 73, c_loss:0.0165, val_loss: 0.3968, loss:0.1387 \n",
      "batch_num: 74, c_loss:0.0093, val_loss: 0.4030, loss:0.1491 \n",
      "batch_num: 75, c_loss:0.0106, val_loss: 0.4189, loss:0.1430 \n",
      "batch_num: 76, c_loss:0.0154, val_loss: 0.4360, loss:0.1408 \n",
      "batch_num: 77, c_loss:0.0109, val_loss: 0.4429, loss:0.1487 \n",
      "batch_num: 78, c_loss:0.0100, val_loss: 0.4505, loss:0.1496 \n",
      "batch_num: 79, c_loss:0.0491, val_loss: 0.4358, loss:0.1862 \n",
      "batch_num: 80, c_loss:0.0144, val_loss: 0.4313, loss:0.1553 \n",
      "batch_num: 81, c_loss:0.0078, val_loss: 0.4354, loss:0.1539 \n",
      "batch_num: 82, c_loss:0.0107, val_loss: 0.4359, loss:0.1543 \n",
      "batch_num: 83, c_loss:0.0134, val_loss: 0.4259, loss:0.1558 \n",
      "batch_num: 84, c_loss:0.0165, val_loss: 0.4260, loss:0.1443 \n",
      "batch_num: 85, c_loss:0.0099, val_loss: 0.4373, loss:0.1516 \n",
      "batch_num: 86, c_loss:0.0141, val_loss: 0.4505, loss:0.1590 \n",
      "batch_num: 87, c_loss:0.0141, val_loss: 0.4496, loss:0.1575 \n",
      "batch_num: 88, c_loss:0.0117, val_loss: 0.4425, loss:0.1620 \n",
      "batch_num: 89, c_loss:0.0199, val_loss: 0.4384, loss:0.1669 \n",
      "batch_num: 90, c_loss:0.0117, val_loss: 0.4368, loss:0.1589 \n",
      "batch_num: 91, c_loss:0.0103, val_loss: 0.4389, loss:0.1571 \n",
      "batch_num: 92, c_loss:0.0098, val_loss: 0.4435, loss:0.1544 \n",
      "batch_num: 93, c_loss:0.0102, val_loss: 0.4462, loss:0.1541 \n",
      "batch_num: 94, c_loss:0.0312, val_loss: 0.4525, loss:0.1837 \n",
      "batch_num: 95, c_loss:0.0186, val_loss: 0.4552, loss:0.1700 \n",
      "batch_num: 96, c_loss:0.0182, val_loss: 0.4460, loss:0.1745 \n",
      "batch_num: 97, c_loss:0.0099, val_loss: 0.4438, loss:0.1504 \n",
      "batch_num: 98, c_loss:0.0109, val_loss: 0.4498, loss:0.1751 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.81087470449172)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.76359338061465)]))])\n",
      "=====Task: 2=====\n",
      "batch_num: 0, c_loss:2.2957, val_loss: 1.4346, loss:2.5324 \n",
      "batch_num: 1, c_loss:2.0598, val_loss: 1.3873, loss:2.3028 \n",
      "batch_num: 2, c_loss:1.8859, val_loss: 1.3381, loss:2.2181 \n",
      "batch_num: 3, c_loss:1.7713, val_loss: 1.2918, loss:2.2128 \n",
      "batch_num: 4, c_loss:1.5400, val_loss: 1.2487, loss:2.1380 \n",
      "batch_num: 5, c_loss:1.4095, val_loss: 1.2170, loss:2.1071 \n",
      "batch_num: 6, c_loss:1.2492, val_loss: 1.1913, loss:2.0353 \n",
      "batch_num: 7, c_loss:1.0828, val_loss: 1.1733, loss:1.9864 \n",
      "batch_num: 8, c_loss:0.9738, val_loss: 1.1604, loss:1.9542 \n",
      "batch_num: 9, c_loss:0.9528, val_loss: 1.1512, loss:2.0167 \n",
      "batch_num: 10, c_loss:0.8367, val_loss: 1.1461, loss:1.9452 \n",
      "batch_num: 11, c_loss:0.7560, val_loss: 1.1375, loss:1.9529 \n",
      "batch_num: 12, c_loss:0.7135, val_loss: 1.1312, loss:1.9761 \n",
      "batch_num: 13, c_loss:0.6523, val_loss: 1.1255, loss:1.9750 \n",
      "batch_num: 14, c_loss:0.6234, val_loss: 1.1201, loss:1.9759 \n",
      "batch_num: 15, c_loss:0.6161, val_loss: 1.1167, loss:2.0075 \n",
      "batch_num: 16, c_loss:0.6130, val_loss: 1.1052, loss:1.9885 \n",
      "batch_num: 17, c_loss:0.5547, val_loss: 1.0915, loss:1.9533 \n",
      "batch_num: 18, c_loss:0.5638, val_loss: 1.0776, loss:1.9281 \n",
      "batch_num: 19, c_loss:0.6232, val_loss: 1.0599, loss:2.0067 \n",
      "batch_num: 20, c_loss:0.5807, val_loss: 1.0393, loss:1.9601 \n",
      "batch_num: 21, c_loss:0.5696, val_loss: 1.0199, loss:1.9508 \n",
      "batch_num: 22, c_loss:0.6021, val_loss: 1.0062, loss:1.9566 \n",
      "batch_num: 23, c_loss:0.6316, val_loss: 0.9956, loss:1.9474 \n",
      "batch_num: 24, c_loss:0.6657, val_loss: 0.9832, loss:1.9696 \n",
      "batch_num: 25, c_loss:0.6810, val_loss: 0.9708, loss:1.9421 \n",
      "batch_num: 26, c_loss:0.7236, val_loss: 0.9578, loss:1.9388 \n",
      "batch_num: 27, c_loss:0.6831, val_loss: 0.9420, loss:1.9088 \n",
      "batch_num: 28, c_loss:0.7929, val_loss: 0.9300, loss:2.0288 \n",
      "batch_num: 29, c_loss:0.7095, val_loss: 0.9129, loss:1.8987 \n",
      "batch_num: 30, c_loss:0.7364, val_loss: 0.9044, loss:1.9527 Leader changed with val acc  0.9129\n",
      "batch_num: 31, c_loss:0.7536, val_loss: 0.8986, loss:0.8372 Leader changed with val acc  0.8986\n",
      "batch_num: 32, c_loss:0.6843, val_loss: 0.8943, loss:0.6843 Leader changed with val acc  0.8943\n",
      "batch_num: 33, c_loss:0.7236, val_loss: 0.8959, loss:0.7236 \n",
      "batch_num: 34, c_loss:0.5372, val_loss: 0.9075, loss:0.6744 \n",
      "batch_num: 35, c_loss:0.5633, val_loss: 0.9226, loss:0.7562 \n",
      "batch_num: 36, c_loss:0.5321, val_loss: 0.9404, loss:0.7622 \n",
      "batch_num: 37, c_loss:0.5837, val_loss: 0.9555, loss:0.8404 \n",
      "batch_num: 38, c_loss:0.5165, val_loss: 0.9637, loss:0.7980 \n",
      "batch_num: 39, c_loss:0.4524, val_loss: 0.9578, loss:0.7481 \n",
      "batch_num: 40, c_loss:0.5037, val_loss: 0.9490, loss:0.8036 \n",
      "batch_num: 41, c_loss:0.5135, val_loss: 0.9383, loss:0.8043 \n",
      "batch_num: 42, c_loss:0.5114, val_loss: 0.9321, loss:0.8023 \n",
      "batch_num: 43, c_loss:0.5017, val_loss: 0.9262, loss:0.7828 \n",
      "batch_num: 44, c_loss:0.5033, val_loss: 0.9265, loss:0.7904 \n",
      "batch_num: 45, c_loss:0.5555, val_loss: 0.9265, loss:0.8389 \n",
      "batch_num: 46, c_loss:0.5337, val_loss: 0.9327, loss:0.7961 \n",
      "batch_num: 47, c_loss:0.5241, val_loss: 0.9445, loss:0.7915 \n",
      "batch_num: 48, c_loss:0.5778, val_loss: 0.9545, loss:0.8463 \n",
      "batch_num: 49, c_loss:0.5413, val_loss: 0.9617, loss:0.8165 \n",
      "batch_num: 50, c_loss:0.5478, val_loss: 0.9686, loss:0.8121 \n",
      "batch_num: 51, c_loss:0.5529, val_loss: 0.9745, loss:0.8175 \n",
      "batch_num: 52, c_loss:0.5569, val_loss: 0.9794, loss:0.8184 \n",
      "batch_num: 53, c_loss:0.5568, val_loss: 0.9799, loss:0.8460 \n",
      "batch_num: 54, c_loss:0.5110, val_loss: 0.9783, loss:0.7974 \n",
      "batch_num: 55, c_loss:0.5549, val_loss: 0.9736, loss:0.8233 \n",
      "batch_num: 56, c_loss:0.5523, val_loss: 0.9652, loss:0.8207 \n",
      "batch_num: 57, c_loss:0.5551, val_loss: 0.9569, loss:0.8193 \n",
      "batch_num: 58, c_loss:0.5801, val_loss: 0.9502, loss:0.8394 \n",
      "batch_num: 59, c_loss:0.5512, val_loss: 0.9471, loss:0.8082 \n",
      "batch_num: 60, c_loss:0.5950, val_loss: 0.9431, loss:0.8618 \n",
      "batch_num: 61, c_loss:0.5647, val_loss: 0.9403, loss:0.8279 \n",
      "batch_num: 62, c_loss:0.5154, val_loss: 0.9381, loss:0.7802 \n",
      "batch_num: 63, c_loss:0.5643, val_loss: 0.9387, loss:0.8293 \n",
      "batch_num: 64, c_loss:0.5384, val_loss: 0.9394, loss:0.8136 \n",
      "batch_num: 65, c_loss:0.5876, val_loss: 0.9430, loss:0.8626 \n",
      "batch_num: 66, c_loss:0.5334, val_loss: 0.9431, loss:0.8075 \n",
      "batch_num: 67, c_loss:0.5531, val_loss: 0.9429, loss:0.8241 \n",
      "batch_num: 68, c_loss:0.5529, val_loss: 0.9432, loss:0.8203 \n",
      "batch_num: 69, c_loss:0.5847, val_loss: 0.9478, loss:0.8645 \n",
      "batch_num: 70, c_loss:0.5729, val_loss: 0.9562, loss:0.8386 \n",
      "batch_num: 71, c_loss:0.5860, val_loss: 0.9632, loss:0.8611 \n",
      "batch_num: 72, c_loss:0.5559, val_loss: 0.9700, loss:0.8157 \n",
      "batch_num: 73, c_loss:0.5901, val_loss: 0.9751, loss:0.8562 \n",
      "batch_num: 74, c_loss:0.5414, val_loss: 0.9734, loss:0.8208 \n",
      "batch_num: 75, c_loss:0.5549, val_loss: 0.9738, loss:0.8270 \n",
      "batch_num: 76, c_loss:0.5876, val_loss: 0.9683, loss:0.8770 \n",
      "batch_num: 77, c_loss:0.5554, val_loss: 0.9639, loss:0.8403 \n",
      "batch_num: 78, c_loss:0.5768, val_loss: 0.9600, loss:0.8697 \n",
      "batch_num: 79, c_loss:0.5293, val_loss: 0.9582, loss:0.8220 \n",
      "batch_num: 80, c_loss:0.5805, val_loss: 0.9573, loss:0.8767 \n",
      "batch_num: 81, c_loss:0.5306, val_loss: 0.9597, loss:0.8187 \n",
      "batch_num: 82, c_loss:0.5330, val_loss: 0.9628, loss:0.8350 \n",
      "batch_num: 83, c_loss:0.5412, val_loss: 0.9706, loss:0.8512 \n",
      "batch_num: 84, c_loss:0.5324, val_loss: 0.9776, loss:0.8468 \n",
      "batch_num: 85, c_loss:0.5971, val_loss: 0.9831, loss:0.8976 \n",
      "batch_num: 86, c_loss:0.5805, val_loss: 0.9855, loss:0.8845 \n",
      "batch_num: 87, c_loss:0.5589, val_loss: 0.9849, loss:0.8602 \n",
      "batch_num: 88, c_loss:0.5207, val_loss: 0.9804, loss:0.8200 \n",
      "batch_num: 89, c_loss:0.5663, val_loss: 0.9756, loss:0.8626 \n",
      "batch_num: 90, c_loss:0.5409, val_loss: 0.9717, loss:0.8541 \n",
      "batch_num: 91, c_loss:0.5437, val_loss: 0.9749, loss:0.8464 \n",
      "batch_num: 92, c_loss:0.5258, val_loss: 0.9802, loss:0.8178 \n",
      "batch_num: 93, c_loss:0.5442, val_loss: 0.9842, loss:0.8371 \n",
      "batch_num: 94, c_loss:0.5975, val_loss: 0.9847, loss:0.9013 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.81087470449172), ('2', 88.74704491725768)])), ('2', OrderedDict([('2', 91.96865817825662)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.76359338061465), ('2', 96.07565011820331)])), ('2', OrderedDict([('2', 88.54064642507346)]))])\n",
      "=====Task: 3=====\n",
      "batch_num: 0, c_loss:2.3081, val_loss: 1.4308, loss:2.6072 \n",
      "batch_num: 1, c_loss:2.2440, val_loss: 1.4003, loss:2.5145 \n",
      "batch_num: 2, c_loss:2.1257, val_loss: 1.3695, loss:2.4584 \n",
      "batch_num: 3, c_loss:1.9266, val_loss: 1.3419, loss:2.3505 \n",
      "batch_num: 4, c_loss:1.7901, val_loss: 1.3124, loss:2.3246 \n",
      "batch_num: 5, c_loss:1.5898, val_loss: 1.2879, loss:2.2341 \n",
      "batch_num: 6, c_loss:1.4315, val_loss: 1.2651, loss:2.2075 \n",
      "batch_num: 7, c_loss:1.3529, val_loss: 1.2466, loss:2.1956 \n",
      "batch_num: 8, c_loss:1.2632, val_loss: 1.2305, loss:2.1835 \n",
      "batch_num: 9, c_loss:1.0612, val_loss: 1.2190, loss:2.0867 \n",
      "batch_num: 10, c_loss:1.0038, val_loss: 1.2098, loss:2.0546 \n",
      "batch_num: 11, c_loss:0.9114, val_loss: 1.1999, loss:2.0302 \n",
      "batch_num: 12, c_loss:0.8485, val_loss: 1.1937, loss:2.0205 \n",
      "batch_num: 13, c_loss:0.8156, val_loss: 1.1879, loss:2.0284 \n",
      "batch_num: 14, c_loss:0.7811, val_loss: 1.1823, loss:2.0000 \n",
      "batch_num: 15, c_loss:0.7259, val_loss: 1.1714, loss:1.9779 \n",
      "batch_num: 16, c_loss:0.7170, val_loss: 1.1600, loss:1.9879 \n",
      "batch_num: 17, c_loss:0.6557, val_loss: 1.1480, loss:1.9857 \n",
      "batch_num: 18, c_loss:0.6701, val_loss: 1.1414, loss:2.0011 \n",
      "batch_num: 19, c_loss:0.6045, val_loss: 1.1370, loss:1.9511 \n",
      "batch_num: 20, c_loss:0.5948, val_loss: 1.1396, loss:1.9674 \n",
      "batch_num: 21, c_loss:0.5770, val_loss: 1.1414, loss:1.9644 \n",
      "batch_num: 22, c_loss:0.5854, val_loss: 1.1493, loss:1.9741 \n",
      "batch_num: 23, c_loss:0.5718, val_loss: 1.1546, loss:2.0020 \n",
      "batch_num: 24, c_loss:0.5860, val_loss: 1.1563, loss:2.0051 \n",
      "batch_num: 25, c_loss:0.5230, val_loss: 1.1531, loss:1.9581 \n",
      "batch_num: 26, c_loss:0.4896, val_loss: 1.1464, loss:1.9488 \n",
      "batch_num: 27, c_loss:0.5591, val_loss: 1.1355, loss:1.9838 \n",
      "batch_num: 28, c_loss:0.5443, val_loss: 1.1221, loss:1.9499 \n",
      "batch_num: 29, c_loss:0.5599, val_loss: 1.1130, loss:1.9358 \n",
      "batch_num: 30, c_loss:0.5296, val_loss: 1.1024, loss:1.9210 Leader changed with val acc  1.1130\n",
      "batch_num: 31, c_loss:0.5541, val_loss: 1.1082, loss:0.6348 Leader changed with val acc  1.1082\n",
      "batch_num: 32, c_loss:0.5470, val_loss: 1.1231, loss:0.5470 \n",
      "batch_num: 33, c_loss:0.5259, val_loss: 1.1337, loss:0.6282 \n",
      "batch_num: 34, c_loss:0.5210, val_loss: 1.1371, loss:0.6502 \n",
      "batch_num: 35, c_loss:0.4934, val_loss: 1.1348, loss:0.6375 \n",
      "batch_num: 36, c_loss:0.4862, val_loss: 1.1312, loss:0.6469 \n",
      "batch_num: 37, c_loss:0.4767, val_loss: 1.1296, loss:0.6464 \n",
      "batch_num: 38, c_loss:0.5175, val_loss: 1.1310, loss:0.6859 \n",
      "batch_num: 39, c_loss:0.4892, val_loss: 1.1368, loss:0.6723 \n",
      "batch_num: 40, c_loss:0.5136, val_loss: 1.1448, loss:0.7031 \n",
      "batch_num: 41, c_loss:0.4865, val_loss: 1.1461, loss:0.6914 \n",
      "batch_num: 42, c_loss:0.4766, val_loss: 1.1459, loss:0.6842 \n",
      "batch_num: 43, c_loss:0.4611, val_loss: 1.1385, loss:0.6558 \n",
      "batch_num: 44, c_loss:0.4384, val_loss: 1.1248, loss:0.6417 \n",
      "batch_num: 45, c_loss:0.4672, val_loss: 1.1148, loss:0.6713 \n",
      "batch_num: 46, c_loss:0.4917, val_loss: 1.1131, loss:0.7038 \n",
      "batch_num: 47, c_loss:0.4793, val_loss: 1.1225, loss:0.6807 \n",
      "batch_num: 48, c_loss:0.4872, val_loss: 1.1357, loss:0.6935 \n",
      "batch_num: 49, c_loss:0.5040, val_loss: 1.1456, loss:0.7104 \n",
      "batch_num: 50, c_loss:0.4660, val_loss: 1.1485, loss:0.6764 \n",
      "batch_num: 51, c_loss:0.4765, val_loss: 1.1429, loss:0.6833 \n",
      "batch_num: 52, c_loss:0.4677, val_loss: 1.1342, loss:0.6852 \n",
      "batch_num: 53, c_loss:0.5020, val_loss: 1.1320, loss:0.7234 \n",
      "batch_num: 54, c_loss:0.4996, val_loss: 1.1314, loss:0.7109 \n",
      "batch_num: 55, c_loss:0.5007, val_loss: 1.1323, loss:0.7043 \n",
      "batch_num: 56, c_loss:0.5070, val_loss: 1.1370, loss:0.7157 \n",
      "batch_num: 57, c_loss:0.4852, val_loss: 1.1428, loss:0.7091 \n",
      "batch_num: 58, c_loss:0.4477, val_loss: 1.1470, loss:0.6692 \n",
      "batch_num: 59, c_loss:0.4910, val_loss: 1.1484, loss:0.7065 \n",
      "batch_num: 60, c_loss:0.5211, val_loss: 1.1484, loss:0.7367 \n",
      "batch_num: 61, c_loss:0.4981, val_loss: 1.1455, loss:0.7132 \n",
      "batch_num: 62, c_loss:0.5070, val_loss: 1.1449, loss:0.7184 \n",
      "batch_num: 63, c_loss:0.4829, val_loss: 1.1429, loss:0.6943 \n",
      "batch_num: 64, c_loss:0.5058, val_loss: 1.1437, loss:0.7190 \n",
      "batch_num: 65, c_loss:0.5214, val_loss: 1.1432, loss:0.7472 \n",
      "batch_num: 66, c_loss:0.4701, val_loss: 1.1458, loss:0.7056 \n",
      "batch_num: 67, c_loss:0.5020, val_loss: 1.1475, loss:0.7306 \n",
      "batch_num: 68, c_loss:0.5019, val_loss: 1.1453, loss:0.7294 \n",
      "batch_num: 69, c_loss:0.4672, val_loss: 1.1433, loss:0.7032 \n",
      "batch_num: 70, c_loss:0.4784, val_loss: 1.1393, loss:0.6986 \n",
      "batch_num: 71, c_loss:0.4862, val_loss: 1.1377, loss:0.7080 \n",
      "batch_num: 72, c_loss:0.4978, val_loss: 1.1411, loss:0.7320 \n",
      "batch_num: 73, c_loss:0.5109, val_loss: 1.1491, loss:0.7413 \n",
      "batch_num: 74, c_loss:0.4682, val_loss: 1.1593, loss:0.6958 \n",
      "batch_num: 75, c_loss:0.4485, val_loss: 1.1687, loss:0.6722 \n",
      "batch_num: 76, c_loss:0.4580, val_loss: 1.1682, loss:0.6915 \n",
      "batch_num: 77, c_loss:0.4738, val_loss: 1.1642, loss:0.7068 \n",
      "batch_num: 78, c_loss:0.5277, val_loss: 1.1559, loss:0.7602 \n",
      "batch_num: 79, c_loss:0.4960, val_loss: 1.1471, loss:0.7250 \n",
      "batch_num: 80, c_loss:0.4733, val_loss: 1.1382, loss:0.6923 \n",
      "batch_num: 81, c_loss:0.4734, val_loss: 1.1367, loss:0.6982 \n",
      "batch_num: 82, c_loss:0.5042, val_loss: 1.1404, loss:0.7295 \n",
      "batch_num: 83, c_loss:0.4934, val_loss: 1.1464, loss:0.7185 \n",
      "batch_num: 84, c_loss:0.5245, val_loss: 1.1544, loss:0.7407 \n",
      "batch_num: 85, c_loss:0.4976, val_loss: 1.1580, loss:0.7183 \n",
      "batch_num: 86, c_loss:0.4730, val_loss: 1.1529, loss:0.6915 \n",
      "batch_num: 87, c_loss:0.4894, val_loss: 1.1504, loss:0.7220 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.81087470449172), ('2', 88.74704491725768), ('3', 40.8983451536643)])), ('2', OrderedDict([('2', 91.96865817825662), ('3', 25.318315377081294)])), ('3', OrderedDict([('3', 98.82604055496265)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.76359338061465), ('2', 96.07565011820331), ('3', 40.709219858156025)])), ('2', OrderedDict([('2', 88.54064642507346), ('3', 29.040156709108718)])), ('3', OrderedDict([('3', 97.81216648879402)]))])\n",
      "=====Task: 4=====\n",
      "batch_num: 0, c_loss:2.9870, val_loss: 1.5765, loss:3.2092 \n",
      "batch_num: 1, c_loss:2.8590, val_loss: 1.5306, loss:3.0879 \n",
      "batch_num: 2, c_loss:2.6860, val_loss: 1.4826, loss:2.9912 \n",
      "batch_num: 3, c_loss:2.5125, val_loss: 1.4387, loss:2.9181 \n",
      "batch_num: 4, c_loss:2.3818, val_loss: 1.3995, loss:2.8798 \n",
      "batch_num: 5, c_loss:2.2120, val_loss: 1.3615, loss:2.8123 \n",
      "batch_num: 6, c_loss:1.9692, val_loss: 1.3246, loss:2.7021 \n",
      "batch_num: 7, c_loss:1.8655, val_loss: 1.2911, loss:2.6944 \n",
      "batch_num: 8, c_loss:1.6162, val_loss: 1.2616, loss:2.5555 \n",
      "batch_num: 9, c_loss:1.4742, val_loss: 1.2359, loss:2.4973 \n",
      "batch_num: 10, c_loss:1.2884, val_loss: 1.2204, loss:2.4189 \n",
      "batch_num: 11, c_loss:1.2249, val_loss: 1.2119, loss:2.4082 \n",
      "batch_num: 12, c_loss:1.0443, val_loss: 1.2102, loss:2.3056 \n",
      "batch_num: 13, c_loss:1.0122, val_loss: 1.2122, loss:2.3831 \n",
      "batch_num: 14, c_loss:0.8628, val_loss: 1.2161, loss:2.2871 \n",
      "batch_num: 15, c_loss:0.7422, val_loss: 1.2261, loss:2.2350 \n",
      "batch_num: 16, c_loss:0.7682, val_loss: 1.2399, loss:2.2955 \n",
      "batch_num: 17, c_loss:0.6387, val_loss: 1.2567, loss:2.2477 \n",
      "batch_num: 18, c_loss:0.6541, val_loss: 1.2706, loss:2.2730 \n",
      "batch_num: 19, c_loss:0.6250, val_loss: 1.2816, loss:2.2831 \n",
      "batch_num: 20, c_loss:0.5920, val_loss: 1.2910, loss:2.2925 \n",
      "batch_num: 21, c_loss:0.5417, val_loss: 1.2966, loss:2.2979 \n",
      "batch_num: 22, c_loss:0.5624, val_loss: 1.2997, loss:2.3305 \n",
      "batch_num: 23, c_loss:0.4623, val_loss: 1.3002, loss:2.2098 \n",
      "batch_num: 24, c_loss:0.4839, val_loss: 1.3063, loss:2.2598 \n",
      "batch_num: 25, c_loss:0.4656, val_loss: 1.3095, loss:2.2468 \n",
      "batch_num: 26, c_loss:0.4567, val_loss: 1.3121, loss:2.2166 \n",
      "batch_num: 27, c_loss:0.4483, val_loss: 1.3089, loss:2.2393 \n",
      "batch_num: 28, c_loss:0.4752, val_loss: 1.3073, loss:2.2174 \n",
      "batch_num: 29, c_loss:0.5024, val_loss: 1.3045, loss:2.2171 \n",
      "batch_num: 30, c_loss:0.4859, val_loss: 1.3021, loss:2.1941 Leader changed with val acc  1.2102\n",
      "batch_num: 31, c_loss:0.4896, val_loss: 1.3046, loss:1.3164 \n",
      "batch_num: 32, c_loss:0.4756, val_loss: 1.3071, loss:1.2751 \n",
      "batch_num: 33, c_loss:0.4992, val_loss: 1.3087, loss:1.2545 \n",
      "batch_num: 34, c_loss:0.5001, val_loss: 1.3067, loss:1.2436 \n",
      "batch_num: 35, c_loss:0.4773, val_loss: 1.2970, loss:1.1988 \n",
      "batch_num: 36, c_loss:0.4883, val_loss: 1.2900, loss:1.1939 \n",
      "batch_num: 37, c_loss:0.4735, val_loss: 1.2865, loss:1.1976 \n",
      "batch_num: 38, c_loss:0.4873, val_loss: 1.2824, loss:1.1828 \n",
      "batch_num: 39, c_loss:0.4540, val_loss: 1.2792, loss:1.1596 \n",
      "batch_num: 40, c_loss:0.4626, val_loss: 1.2766, loss:1.1661 \n",
      "batch_num: 41, c_loss:0.4948, val_loss: 1.2780, loss:1.1789 \n",
      "batch_num: 42, c_loss:0.5040, val_loss: 1.2778, loss:1.1556 \n",
      "batch_num: 43, c_loss:0.5213, val_loss: 1.2800, loss:1.1536 \n",
      "batch_num: 44, c_loss:0.5289, val_loss: 1.2840, loss:1.1486 \n",
      "batch_num: 45, c_loss:0.5266, val_loss: 1.2844, loss:1.1104 \n",
      "batch_num: 46, c_loss:0.5442, val_loss: 1.2832, loss:1.1241 \n",
      "batch_num: 47, c_loss:0.5714, val_loss: 1.2778, loss:1.1264 \n",
      "batch_num: 48, c_loss:0.5405, val_loss: 1.2731, loss:1.1085 \n",
      "batch_num: 49, c_loss:0.5748, val_loss: 1.2690, loss:1.1232 \n",
      "batch_num: 50, c_loss:0.5776, val_loss: 1.2719, loss:1.1336 \n",
      "batch_num: 51, c_loss:0.5825, val_loss: 1.2731, loss:1.1471 \n",
      "batch_num: 52, c_loss:0.5883, val_loss: 1.2766, loss:1.1207 \n",
      "batch_num: 53, c_loss:0.5761, val_loss: 1.2809, loss:1.1296 \n",
      "batch_num: 54, c_loss:0.5571, val_loss: 1.2819, loss:1.0952 \n",
      "batch_num: 55, c_loss:0.5541, val_loss: 1.2790, loss:1.0728 \n",
      "batch_num: 56, c_loss:0.5635, val_loss: 1.2772, loss:1.0799 \n",
      "batch_num: 57, c_loss:0.5923, val_loss: 1.2769, loss:1.1139 \n",
      "batch_num: 58, c_loss:0.5894, val_loss: 1.2770, loss:1.1014 \n",
      "batch_num: 59, c_loss:0.5660, val_loss: 1.2769, loss:1.0702 \n",
      "batch_num: 60, c_loss:0.5590, val_loss: 1.2745, loss:1.0673 \n",
      "batch_num: 61, c_loss:0.5633, val_loss: 1.2744, loss:1.0806 \n",
      "batch_num: 62, c_loss:0.5705, val_loss: 1.2740, loss:1.0843 \n",
      "batch_num: 63, c_loss:0.5869, val_loss: 1.2729, loss:1.1019 \n",
      "batch_num: 64, c_loss:0.5615, val_loss: 1.2732, loss:1.0699 \n",
      "batch_num: 65, c_loss:0.5810, val_loss: 1.2753, loss:1.0872 \n",
      "batch_num: 66, c_loss:0.5868, val_loss: 1.2789, loss:1.0949 \n",
      "batch_num: 67, c_loss:0.6007, val_loss: 1.2778, loss:1.1113 \n",
      "batch_num: 68, c_loss:0.5426, val_loss: 1.2767, loss:1.0515 \n",
      "batch_num: 69, c_loss:0.5986, val_loss: 1.2742, loss:1.0849 \n",
      "batch_num: 70, c_loss:0.5747, val_loss: 1.2722, loss:1.0813 \n",
      "batch_num: 71, c_loss:0.5857, val_loss: 1.2732, loss:1.0985 \n",
      "batch_num: 72, c_loss:0.5773, val_loss: 1.2730, loss:1.0794 \n",
      "batch_num: 73, c_loss:0.5700, val_loss: 1.2738, loss:1.0755 \n",
      "batch_num: 74, c_loss:0.5614, val_loss: 1.2754, loss:1.0714 \n",
      "batch_num: 75, c_loss:0.5395, val_loss: 1.2822, loss:1.0400 \n",
      "batch_num: 76, c_loss:0.5883, val_loss: 1.2899, loss:1.1041 \n",
      "batch_num: 77, c_loss:0.5384, val_loss: 1.2944, loss:1.0514 \n",
      "batch_num: 78, c_loss:0.5671, val_loss: 1.2938, loss:1.0901 \n",
      "batch_num: 79, c_loss:0.5878, val_loss: 1.2866, loss:1.1118 \n",
      "batch_num: 80, c_loss:0.5521, val_loss: 1.2812, loss:1.0690 \n",
      "batch_num: 81, c_loss:0.5543, val_loss: 1.2765, loss:1.0574 \n",
      "batch_num: 82, c_loss:0.5537, val_loss: 1.2763, loss:1.0678 \n",
      "batch_num: 83, c_loss:0.5419, val_loss: 1.2791, loss:1.0468 \n",
      "batch_num: 84, c_loss:0.5345, val_loss: 1.2826, loss:1.0358 \n",
      "batch_num: 85, c_loss:0.5356, val_loss: 1.2828, loss:1.0505 \n",
      "batch_num: 86, c_loss:0.5966, val_loss: 1.2833, loss:1.1063 \n",
      "batch_num: 87, c_loss:0.5468, val_loss: 1.2840, loss:1.0576 \n",
      "batch_num: 88, c_loss:0.5266, val_loss: 1.2827, loss:1.0450 \n",
      "batch_num: 89, c_loss:0.5432, val_loss: 1.2818, loss:1.0588 \n",
      "batch_num: 90, c_loss:0.5469, val_loss: 1.2795, loss:1.0652 \n",
      "batch_num: 91, c_loss:0.5474, val_loss: 1.2817, loss:1.0498 \n",
      "batch_num: 92, c_loss:0.5466, val_loss: 1.2803, loss:1.0451 \n",
      "batch_num: 93, c_loss:0.5530, val_loss: 1.2814, loss:1.0380 \n",
      "batch_num: 94, c_loss:0.5651, val_loss: 1.2813, loss:1.0532 \n",
      "batch_num: 95, c_loss:0.5380, val_loss: 1.2815, loss:1.0441 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.81087470449172), ('2', 88.74704491725768), ('3', 40.8983451536643), ('4', 15.839243498817966)])), ('2', OrderedDict([('2', 91.96865817825662), ('3', 25.318315377081294), ('4', 22.38001958863859)])), ('3', OrderedDict([('3', 98.82604055496265), ('4', 64.4076840981857)])), ('4', OrderedDict([('4', 98.48942598187311)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.76359338061465), ('2', 96.07565011820331), ('3', 40.709219858156025), ('4', 51.39479905437352)])), ('2', OrderedDict([('2', 88.54064642507346), ('3', 29.040156709108718), ('4', 29.08912830558276)])), ('3', OrderedDict([('3', 97.81216648879402), ('4', 89.75453575240128)])), ('4', OrderedDict([('4', 86.5558912386707)]))])\n",
      "=====Task: 5=====\n",
      "batch_num: 0, c_loss:3.7309, val_loss: 1.7304, loss:4.2327 \n",
      "batch_num: 1, c_loss:3.7179, val_loss: 1.6961, loss:4.1812 \n",
      "batch_num: 2, c_loss:3.5143, val_loss: 1.6528, loss:3.9743 \n",
      "batch_num: 3, c_loss:3.3827, val_loss: 1.6051, loss:3.8623 \n",
      "batch_num: 4, c_loss:3.1284, val_loss: 1.5603, loss:3.6472 \n",
      "batch_num: 5, c_loss:3.0245, val_loss: 1.5219, loss:3.6090 \n",
      "batch_num: 6, c_loss:2.7986, val_loss: 1.4899, loss:3.4715 \n",
      "batch_num: 7, c_loss:2.5756, val_loss: 1.4619, loss:3.3811 \n",
      "batch_num: 8, c_loss:2.4035, val_loss: 1.4371, loss:3.3312 \n",
      "batch_num: 9, c_loss:2.1819, val_loss: 1.4134, loss:3.2303 \n",
      "batch_num: 10, c_loss:2.1103, val_loss: 1.3967, loss:3.2418 \n",
      "batch_num: 11, c_loss:1.9497, val_loss: 1.3825, loss:3.1934 \n",
      "batch_num: 12, c_loss:1.7305, val_loss: 1.3716, loss:3.1021 \n",
      "batch_num: 13, c_loss:1.6113, val_loss: 1.3688, loss:2.9921 \n",
      "batch_num: 14, c_loss:1.4612, val_loss: 1.3712, loss:2.9430 \n",
      "batch_num: 15, c_loss:1.4074, val_loss: 1.3753, loss:2.9326 \n",
      "batch_num: 16, c_loss:1.2194, val_loss: 1.3809, loss:2.8358 \n",
      "batch_num: 17, c_loss:1.1716, val_loss: 1.3924, loss:2.8346 \n",
      "batch_num: 18, c_loss:1.0260, val_loss: 1.4016, loss:2.7107 \n",
      "batch_num: 19, c_loss:0.9780, val_loss: 1.4127, loss:2.7260 \n",
      "batch_num: 20, c_loss:0.8907, val_loss: 1.4286, loss:2.6967 \n",
      "batch_num: 21, c_loss:0.8748, val_loss: 1.4438, loss:2.7573 \n",
      "batch_num: 22, c_loss:0.8157, val_loss: 1.4617, loss:2.6983 \n",
      "batch_num: 23, c_loss:0.7748, val_loss: 1.4776, loss:2.6845 \n",
      "batch_num: 24, c_loss:0.7308, val_loss: 1.4872, loss:2.6747 \n",
      "batch_num: 25, c_loss:0.7256, val_loss: 1.4926, loss:2.6962 \n",
      "batch_num: 26, c_loss:0.6440, val_loss: 1.4922, loss:2.6260 \n",
      "batch_num: 27, c_loss:0.6627, val_loss: 1.4894, loss:2.6571 \n",
      "batch_num: 28, c_loss:0.6706, val_loss: 1.4865, loss:2.6599 \n",
      "batch_num: 29, c_loss:0.6459, val_loss: 1.4853, loss:2.6598 \n",
      "batch_num: 30, c_loss:0.6279, val_loss: 1.4799, loss:2.6450 Leader changed with val acc  1.3688\n",
      "batch_num: 31, c_loss:0.5866, val_loss: 1.4849, loss:1.7196 \n",
      "batch_num: 32, c_loss:0.7099, val_loss: 1.4968, loss:1.8143 \n",
      "batch_num: 33, c_loss:0.6200, val_loss: 1.5068, loss:1.7141 \n",
      "batch_num: 34, c_loss:0.5834, val_loss: 1.5220, loss:1.6274 \n",
      "batch_num: 35, c_loss:0.5380, val_loss: 1.5333, loss:1.5845 \n",
      "batch_num: 36, c_loss:0.5288, val_loss: 1.5404, loss:1.5306 \n",
      "batch_num: 37, c_loss:0.5454, val_loss: 1.5436, loss:1.5624 \n",
      "batch_num: 38, c_loss:0.5638, val_loss: 1.5436, loss:1.5710 \n",
      "batch_num: 39, c_loss:0.5880, val_loss: 1.5396, loss:1.5917 \n",
      "batch_num: 40, c_loss:0.5792, val_loss: 1.5359, loss:1.5616 \n",
      "batch_num: 41, c_loss:0.5365, val_loss: 1.5319, loss:1.5120 \n",
      "batch_num: 42, c_loss:0.5967, val_loss: 1.5333, loss:1.5507 \n",
      "batch_num: 43, c_loss:0.6404, val_loss: 1.5337, loss:1.5712 \n",
      "batch_num: 44, c_loss:0.5950, val_loss: 1.5357, loss:1.4914 \n",
      "batch_num: 45, c_loss:0.6022, val_loss: 1.5377, loss:1.4788 \n",
      "batch_num: 46, c_loss:0.6888, val_loss: 1.5394, loss:1.5475 \n",
      "batch_num: 47, c_loss:0.6184, val_loss: 1.5369, loss:1.4681 \n",
      "batch_num: 48, c_loss:0.6087, val_loss: 1.5320, loss:1.4674 \n",
      "batch_num: 49, c_loss:0.6069, val_loss: 1.5236, loss:1.4425 \n",
      "batch_num: 50, c_loss:0.6190, val_loss: 1.5171, loss:1.4464 \n",
      "batch_num: 51, c_loss:0.6820, val_loss: 1.5112, loss:1.5000 \n",
      "batch_num: 52, c_loss:0.7481, val_loss: 1.5074, loss:1.5198 \n",
      "batch_num: 53, c_loss:0.6939, val_loss: 1.5066, loss:1.4750 \n",
      "batch_num: 54, c_loss:0.6876, val_loss: 1.5066, loss:1.4645 \n",
      "batch_num: 55, c_loss:0.6535, val_loss: 1.5087, loss:1.4425 \n",
      "batch_num: 56, c_loss:0.6817, val_loss: 1.5114, loss:1.4426 \n",
      "batch_num: 57, c_loss:0.6509, val_loss: 1.5149, loss:1.4044 \n",
      "batch_num: 58, c_loss:0.7080, val_loss: 1.5139, loss:1.4627 \n",
      "batch_num: 59, c_loss:0.6775, val_loss: 1.5126, loss:1.4347 \n",
      "batch_num: 60, c_loss:0.7058, val_loss: 1.5118, loss:1.4375 \n",
      "batch_num: 61, c_loss:0.7347, val_loss: 1.5095, loss:1.4731 \n",
      "batch_num: 62, c_loss:0.6761, val_loss: 1.5071, loss:1.4063 \n",
      "batch_num: 63, c_loss:0.7703, val_loss: 1.5061, loss:1.4943 \n",
      "batch_num: 64, c_loss:0.7130, val_loss: 1.5055, loss:1.4304 \n",
      "batch_num: 65, c_loss:0.7169, val_loss: 1.5011, loss:1.4276 \n",
      "batch_num: 66, c_loss:0.6544, val_loss: 1.4976, loss:1.3852 \n",
      "batch_num: 67, c_loss:0.6688, val_loss: 1.4973, loss:1.3996 \n",
      "batch_num: 68, c_loss:0.6564, val_loss: 1.4979, loss:1.3746 \n",
      "batch_num: 69, c_loss:0.6790, val_loss: 1.5006, loss:1.3980 \n",
      "batch_num: 70, c_loss:0.6916, val_loss: 1.5045, loss:1.4080 \n",
      "batch_num: 71, c_loss:0.7242, val_loss: 1.5078, loss:1.4473 \n",
      "batch_num: 72, c_loss:0.6801, val_loss: 1.5075, loss:1.4307 \n",
      "batch_num: 73, c_loss:0.6822, val_loss: 1.5102, loss:1.4030 \n",
      "batch_num: 74, c_loss:0.7222, val_loss: 1.5090, loss:1.4537 \n",
      "batch_num: 75, c_loss:0.6437, val_loss: 1.5106, loss:1.3655 \n",
      "batch_num: 76, c_loss:0.6545, val_loss: 1.5109, loss:1.3811 \n",
      "batch_num: 77, c_loss:0.6387, val_loss: 1.5119, loss:1.3574 \n",
      "batch_num: 78, c_loss:0.6738, val_loss: 1.5097, loss:1.4112 \n",
      "batch_num: 79, c_loss:0.6359, val_loss: 1.5064, loss:1.3585 \n",
      "batch_num: 80, c_loss:0.7530, val_loss: 1.5034, loss:1.4801 \n",
      "batch_num: 81, c_loss:0.7033, val_loss: 1.5049, loss:1.4345 \n",
      "batch_num: 82, c_loss:0.6431, val_loss: 1.5058, loss:1.3672 \n",
      "batch_num: 83, c_loss:0.6896, val_loss: 1.5075, loss:1.4025 \n",
      "batch_num: 84, c_loss:0.6630, val_loss: 1.5101, loss:1.3942 \n",
      "batch_num: 85, c_loss:0.6799, val_loss: 1.5120, loss:1.3964 \n",
      "batch_num: 86, c_loss:0.6856, val_loss: 1.5120, loss:1.4133 \n",
      "batch_num: 87, c_loss:0.7110, val_loss: 1.5093, loss:1.4264 \n",
      "batch_num: 88, c_loss:0.6061, val_loss: 1.5071, loss:1.3282 \n",
      "batch_num: 89, c_loss:0.6448, val_loss: 1.5099, loss:1.3668 \n",
      "batch_num: 90, c_loss:0.7056, val_loss: 1.5142, loss:1.4213 \n",
      "batch_num: 91, c_loss:0.6231, val_loss: 1.5156, loss:1.3404 \n",
      "batch_num: 92, c_loss:0.6506, val_loss: 1.5132, loss:1.4048 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.81087470449172), ('2', 88.74704491725768), ('3', 40.8983451536643), ('4', 15.839243498817966), ('5', 25.95744680851064)])), ('2', OrderedDict([('2', 91.96865817825662), ('3', 25.318315377081294), ('4', 22.38001958863859), ('5', 7.541625857002939)])), ('3', OrderedDict([('3', 98.82604055496265), ('4', 64.4076840981857), ('5', 32.817502668089645)])), ('4', OrderedDict([('4', 98.48942598187311), ('5', 31.973816717019133)])), ('5', OrderedDict([('5', 96.97428139183056)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.76359338061465), ('2', 96.07565011820331), ('3', 40.709219858156025), ('4', 51.39479905437352), ('5', 68.84160756501183)])), ('2', OrderedDict([('2', 88.54064642507346), ('3', 29.040156709108718), ('4', 29.08912830558276), ('5', 21.00881488736533)])), ('3', OrderedDict([('3', 97.81216648879402), ('4', 89.75453575240128), ('5', 81.85699039487727)])), ('4', OrderedDict([('4', 86.5558912386707), ('5', 67.6233635448137)])), ('5', OrderedDict([('5', 69.28895612708018)]))])\n",
      "follower Task 1 average acc: 99.81087470449172\n",
      "follower Task 2 average acc: 90.35785154775715\n",
      "follower Task 3 average acc: 55.01423369523608\n",
      "follower Task 4 average acc: 50.27909329187884\n",
      "follower Task 5 average acc: 39.05293468849059\n",
      "leader Task 1 average acc: 99.76359338061465\n",
      "leader Task 2 average acc: 92.30814827163839\n",
      "leader Task 3 average acc: 55.853847685352925\n",
      "leader Task 4 average acc: 64.19858858775707\n",
      "leader Task 5 average acc: 61.723946503829666\n"
     ]
    }
   ],
   "source": [
    "avg_acc_history,leader_avg_acc_history = train(task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torchvision\\nimport os\\nfrom os import path\\nimport copy\\nimport numpy as np\\nimport torch.utils.data as data\\nfrom torchvision import transforms\\nfrom collections import OrderedDict\\n\\nbatch_size = 128\\nrepeat = 10\\nepoches = 1\\nalpha = 4\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\nclass CacheClassLabel(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that has a quick access to all labels of data.\\n    \"\"\"\\n    def __init__(self, dataset):\\n        super(CacheClassLabel, self).__init__()\\n        self.dataset = dataset\\n        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\\n        print(dataset.root)\\n        label_cache_filename = dataset.root + \\'/\\' +\\'_\\'+str(len(dataset))+\\'.pth\\'\\n        if path.exists(label_cache_filename):\\n            self.labels = torch.load(label_cache_filename)\\n        else:\\n            for i, data in enumerate(dataset):\\n                self.labels[i] = data[1]\\n            torch.save(self.labels, label_cache_filename)\\n        self.number_classes = len(torch.unique(self.labels))\\n\\n    def __len__(self):\\n        return len(self.dataset)\\n\\n    def __getitem__(self, index):\\n        img,target = self.dataset[index]\\n        return img, target\\n    \\nclass AppendName(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that also return the name of the dataset/task\\n    \"\"\"\\n    def __init__(self, dataset, name, first_class_ind=0):\\n        super(AppendName,self).__init__()\\n        self.dataset = dataset\\n        self.name = name\\n        self.first_class_ind = first_class_ind  # For remapping the class index\\n\\n    def __len__(self):\\n        return len(self.dataset)\\n\\n    def __getitem__(self, index):\\n        img,target = self.dataset[index]\\n        target = target + self.first_class_ind\\n        return img, target, self.name\\n    \\nclass Subclass(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\\n    \"\"\"\\n    def __init__(self, dataset, class_list, remap=True):\\n        super(Subclass,self).__init__()\\n        assert isinstance(dataset, CacheClassLabel), \\'dataset must be wrapped by CacheClassLabel\\'\\n        self.dataset = dataset\\n        self.class_list = class_list\\n        self.remap = remap\\n        self.indices = []\\n        for c in class_list:\\n            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\\n        if remap:\\n            self.class_mapping = {c: i for i, c in enumerate(class_list)}\\n\\n    def __len__(self):\\n        return len(self.indices)\\n    def __getitem__(self, index):\\n        img,target = self.dataset[self.indices[index]]\\n        if self.remap:\\n            raw_target = target.item() if isinstance(target,torch.Tensor) else target\\n            target = self.class_mapping[raw_target]\\n        return img, target\\n\\ndef SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\\n    assert train_dataset.number_classes==val_dataset.number_classes,\\'Train/Val has different number of classes\\'\\n    num_classes =  train_dataset.number_classes\\n\\n    # Calculate the boundary index of classes for splits\\n    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\\n    split_boundaries = [0, first_split_sz]\\n    while split_boundaries[-1]<num_classes:\\n        split_boundaries.append(split_boundaries[-1]+other_split_sz)\\n    print(\\'split_boundaries:\\',split_boundaries)\\n    assert split_boundaries[-1]==num_classes,\\'Invalid split size\\'\\n\\n    # Assign classes to each splits\\n    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\\n    if not rand_split:\\n        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\\n    else:\\n        randseq = torch.randperm(num_classes)\\n        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\\n    print(class_lists)\\n\\n    # Generate the dicts of splits\\n    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\\n    train_dataset_splits = {}\\n    val_dataset_splits = {}\\n    task_output_space = {}\\n    for name,class_list in class_lists.items():\\n        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\\n        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\\n        task_output_space[name] = len(class_list)\\n\\n    return train_dataset_splits, val_dataset_splits, task_output_space\\n\\ndef MNIST(dataroot, train_aug=False):\\n    val_transform = transforms.Compose([\\n        transforms.Pad(2, fill=0, padding_mode=\\'constant\\'),\\n        transforms.ToTensor(),\\n        transforms.Normalize([0.5], [0.5]),\\n    ])\\n    train_transform = val_transform\\n    if train_aug:\\n        train_transform = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5], [0.5]),\\n        ])\\n\\n    train_dataset = torchvision.datasets.MNIST(\\n        root=dataroot,\\n        train=True,\\n        download=True,\\n        transform=train_transform\\n    )\\n    train_dataset = CacheClassLabel(train_dataset)\\n\\n    val_dataset = torchvision.datasets.MNIST(\\n        dataroot,\\n        train=False,\\n        transform=val_transform\\n    )\\n    val_dataset = CacheClassLabel(val_dataset)\\n\\n    return train_dataset, val_dataset\\n\\ntrain_dataset, val_dataset = MNIST(\\'./data\\', False)\\n\\ntrain_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\\n                                                                          first_split_sz=2,\\n                                                                          other_split_sz=2,\\n                                                                          rand_split=False,\\n                                                                          remap_class=False)\\n\\nclass MLP(nn.Module):\\n    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\\n        super(MLP, self).__init__()\\n        self.in_dim = in_channel*img_sz*img_sz\\n        self.linear = nn.Sequential(\\n            nn.Linear(self.in_dim, hidden_dim),\\n            nn.BatchNorm1d(hidden_dim),\\n            nn.ReLU(inplace=True),\\n            nn.Linear(hidden_dim, hidden_dim),\\n            nn.BatchNorm1d(hidden_dim),\\n            nn.ReLU(inplace=True),\\n        )\\n        self.last = nn.Linear(hidden_dim, out_dim)\\n\\n    def features(self, x):\\n        x = self.linear(x.view(-1,self.in_dim))\\n        return x\\n\\n    def logits(self, x):\\n        x = self.last(x)\\n        return x\\n\\n    def forward(self, x):\\n        x = self.features(x)\\n        x = self.logits(x)\\n        return x\\n\\ndef MLP400():\\n    return MLP(hidden_dim=400)\\n\\nclass AverageMeter(object):\\n    def __init__(self):\\n        self.reset()\\n\\n    def reset(self):\\n        self.val = 0\\n        self.avg = 0\\n        self.sum = 0\\n        self.count = 0\\n\\n    def update(self, val, n=1):\\n        self.val = val\\n        self.sum += val * n\\n        self.count += n\\n        self.avg = float(self.sum) / self.count\\n\\ndef accuracy(output, target):\\n    with torch.no_grad():\\n        _, predicted = torch.max(output.data, 1)\\n        batch_size = target.size(0)\\n        correct = (predicted == target).sum().item() * 100\\n    return correct / batch_size\\n\\ndef accumulate_acc(output, target, meter):\\n    acc = accuracy(output, target)\\n    meter.update(acc, len(target))\\n    return meter\\n\\ndef criterion_fn(criterion, preds, targets, valid_out_dim):\\n    if valid_out_dim != 0:\\n        pred = preds[:,:valid_out_dim]\\n    loss = criterion(pred, targets)\\n    return loss\\n\\ndef train_on_task(model, train_loader, optimizer, criterion, \\n                  valid_out_dim, best_model_wts, task_num, task_names):\\n    leader = MLP400().to(device)\\n    best_loss = float(\\'inf\\')\\n    if (best_model_wts):\\n        leader.load_state_dict(best_model_wts)\\n\\n    for epoch in range(epoches):\\n        train_acc = AverageMeter()\\n        batch_num = 0\\n        for images, labels, _ in train_loader:\\n            images, labels = images.to(device), labels.to(device)\\n\\n            with torch.no_grad():\\n                leader_outputs = leader(images)\\n\\n            model.train()\\n            follower_outputs = model(images)\\n\\n            # reg_loss = 0\\n            # for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\\n                # reg_loss += torch.norm(follower_para - lead_para, p = 2)\\n            \\n            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\\n            loss = c_loss + alpha * torch.mean((follower_outputs - leader_outputs) ** 2)\\n\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n\\n            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\\n            \\n            model.eval()\\n            with torch.no_grad():\\n                val_loss = AverageMeter()\\n\\n                for task in range(task_num + 1):\\n                    val_name = task_names[task]\\n                    val_data = val_dataset_splits[val_name]\\n                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\\n\\n                    for i, (input, target, _) in enumerate(val_loader):\\n                        input, target = input.to(device), target.to(device)\\n                        output = model(input)\\n                        loss_v = criterion(output, target).item()\\n\\n                        val_loss.update(loss_v, len(target))\\n\\n                    if val_loss.avg < best_loss:\\n                        best_loss = val_loss.avg\\n                        best_model_wts = copy.deepcopy(model.state_dict())\\n                        leader.load_state_dict(best_model_wts) \\n            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss_v:.4f}\")\\n            batch_num += 1\\n    return best_model_wts, best_loss\\n\\ndef train(task_names):\\n    acc_table = OrderedDict()\\n    valid_out_dim = 0\\n\\n    model = MLP400().to(device)\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\\n\\n    best_model_wts = None\\n    for i in range(len(task_names)):\\n        valid_out_dim += 2\\n        train_name = task_names[i]\\n        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\\n        \\n        print(f\\'=====Task: {train_name}=====\\')\\n        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\\n    \\n        acc_table[train_name] = OrderedDict()\\n\\n        for j in range(i+1):\\n            val_name = task_names[j]\\n            val_data = val_dataset_splits[val_name]\\n            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\\n            model.eval()\\n            val_acc = AverageMeter()\\n            with torch.no_grad():\\n                for i, (input, target, _) in enumerate(val_loader):\\n                    input, target = input.to(device), target.to(device)\\n                    output = model(input)\\n                    val_acc = accumulate_acc(output, target, val_acc)\\n\\n            acc_table[val_name][train_name] = val_acc.avg\\n\\n        print(acc_table)\\n\\n    avg_acc_history = [0] * len(task_names)\\n    for i in range(len(task_names)):\\n        train_name = task_names[i]\\n        cls_acc_sum = 0\\n        for j in range(i + 1):\\n            val_name = task_names[j]\\n            cls_acc_sum += acc_table[val_name][train_name]\\n\\n        avg_acc_history[i] = cls_acc_sum / (i + 1)\\n        print(\\'Task\\', train_name, \\'average acc:\\', avg_acc_history[i])\\n    \\n    return avg_acc_history\\n\\ntask_names = sorted(list(task_output_space.keys()), key=int)\\nprint(\\'Task order:\\',task_names)\\n\\navg_acc_history = train(task_names)\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        print(dataset.root)\n",
    "        label_cache_filename = dataset.root + '/' +'_'+str(len(dataset))+'.pth'\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target\n",
    "\n",
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space\n",
    "\n",
    "def MNIST(dataroot, train_aug=False):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = MNIST('./data', False)\n",
    "\n",
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count\n",
    "\n",
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size\n",
    "\n",
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter\n",
    "\n",
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss\n",
    "\n",
    "def train_on_task(model, train_loader, optimizer, criterion, \n",
    "                  valid_out_dim, best_model_wts, task_num, task_names):\n",
    "    leader = MLP400().to(device)\n",
    "    best_loss = float('inf')\n",
    "    if (best_model_wts):\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                leader_outputs = leader(images)\n",
    "\n",
    "            model.train()\n",
    "            follower_outputs = model(images)\n",
    "\n",
    "            # reg_loss = 0\n",
    "            # for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
    "                # reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\n",
    "            loss = c_loss + alpha * torch.mean((follower_outputs - leader_outputs) ** 2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = AverageMeter()\n",
    "\n",
    "                for task in range(task_num + 1):\n",
    "                    val_name = task_names[task]\n",
    "                    val_data = val_dataset_splits[val_name]\n",
    "                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    for i, (input, target, _) in enumerate(val_loader):\n",
    "                        input, target = input.to(device), target.to(device)\n",
    "                        output = model(input)\n",
    "                        loss_v = criterion(output, target).item()\n",
    "\n",
    "                        val_loss.update(loss_v, len(target))\n",
    "\n",
    "                    if val_loss.avg < best_loss:\n",
    "                        best_loss = val_loss.avg\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss_v:.4f}\")\n",
    "            batch_num += 1\n",
    "    return best_model_wts, best_loss\n",
    "\n",
    "def train(task_names):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\n",
    "    \n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "            model.eval()\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "        print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history\n",
    "\n",
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)\n",
    "\n",
    "avg_acc_history = train(task_names)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
