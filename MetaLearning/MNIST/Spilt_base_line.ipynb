{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongpochen/opt/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from os import path\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "repeat = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        label_cache_filename = path.join(dataset.root, str(type(dataset))+'_'+str(len(dataset))+'.pth')\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        '''\n",
    "        :param dataset: (CacheClassLabel)\n",
    "        :param class_list: (list) A list of integers\n",
    "        :param remap: (bool) Ex: remap class [2,4,6 ...] to [0,1,2 ...]\n",
    "        '''\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST(dataroot, train_aug=False):\n",
    "    # Add padding to make 32x32\n",
    "    #normalize = transforms.Normalize(mean=(0.1307,), std=(0.3081,))  # for 28x28\n",
    "    normalize = transforms.Normalize(mean=(0.1000,), std=(0.2752,))  # for 32x32\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = MNIST('data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_boundaries: [0, 2, 4, 6, 8, 10]\n",
      "{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            #nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            #nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)  # Subject to be replaced dependent on task\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MLP400()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(agent.parameters(), 0.001)\n",
    "\n",
    "n_feat = agent.last.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task order: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_fn(preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum().item()\n",
    "            res.append(correct_k*100.0 / batch_size)\n",
    "\n",
    "        if len(res)==1:\n",
    "            return res[0]\n",
    "        else:\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_acc(output, target, meter):\n",
    "    meter.update(accuracy(output, target), len(target))\n",
    "    return meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 99.70617042115573), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 99.46638207043756), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 99.59718026183283), ('5', 0.0)])), ('5', OrderedDict([('5', 98.73928391326274)]))])\n",
      "Task 1 average acc: 99.66903073286052\n",
      "Task 2 average acc: 49.853085210577866\n",
      "Task 3 average acc: 33.15546069014585\n",
      "Task 4 average acc: 24.899295065458208\n",
      "Task 5 average acc: 19.747856782652548\n",
      "===Summary of experiment repeats: 1 / 10 ===\n",
      "[19.74785678  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "mean: 1.9747856782652549 std: 5.924357034795765\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 13.180362860192103), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 98.84189325276938), ('5', 0.0)])), ('5', OrderedDict([('5', 98.84014120020171)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 4.393454286730701\n",
      "Task 4 average acc: 24.710473313192345\n",
      "Task 5 average acc: 19.768028240040344\n",
      "===Summary of experiment repeats: 2 / 10 ===\n",
      "[19.74785678 19.76802824  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "mean: 3.9515885022692894 std: 7.903178291640145\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 0.0), ('5', 0.0)])), ('5', OrderedDict([('5', 98.73928391326274)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 0.0\n",
      "Task 5 average acc: 19.747856782652548\n",
      "===Summary of experiment repeats: 3 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "mean: 5.926374180534544 std: 9.052687591197877\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 0.0), ('5', 0.0)])), ('5', OrderedDict([('5', 99.34442763489662)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 0.0\n",
      "Task 5 average acc: 19.868885526979323\n",
      "===Summary of experiment repeats: 4 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678 19.86888553  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "mean: 7.913262733232477 std: 9.69177990230842\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 0.0), ('5', 0.0)])), ('5', OrderedDict([('5', 99.4957135653051)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 0.0\n",
      "Task 5 average acc: 19.899142713061018\n",
      "===Summary of experiment repeats: 5 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678 19.86888553 19.89914271  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "mean: 9.90317700453858 std: 9.9032821856297\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 0.0), ('5', 0.0)])), ('5', OrderedDict([('5', 99.59657085224407)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 0.0\n",
      "Task 5 average acc: 19.919314170448814\n",
      "===Summary of experiment repeats: 6 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678 19.86888553 19.89914271 19.91931417\n",
      "  0.          0.          0.          0.        ]\n",
      "mean: 11.895108421583462 std: 9.712477344751484\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 0.0), ('5', 0.0)])), ('5', OrderedDict([('5', 99.44528492183561)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 0.0\n",
      "Task 5 average acc: 19.889056984367123\n",
      "===Summary of experiment repeats: 7 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678 19.86888553 19.89914271 19.91931417\n",
      " 19.88905698  0.          0.          0.        ]\n",
      "mean: 13.884014120020174 std: 9.089413139546323\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 10.171198388721047), ('5', 0.0)])), ('5', OrderedDict([('5', 99.54614220877458)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 2.5427995971802617\n",
      "Task 5 average acc: 19.909228441754916\n",
      "===Summary of experiment repeats: 8 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678 19.86888553 19.89914271 19.91931417\n",
      " 19.88905698 19.90922844  0.          0.        ]\n",
      "mean: 15.874936964195664 std: 7.937719660185115\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 41.69184290030211), ('5', 0.0)])), ('5', OrderedDict([('5', 99.4957135653051)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 10.422960725075528\n",
      "Task 5 average acc: 19.899142713061018\n",
      "===Summary of experiment repeats: 9 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678 19.86888553 19.89914271 19.91931417\n",
      " 19.88905698 19.90922844 19.89914271  0.        ]\n",
      "mean: 17.864851235501767 std: 5.955308172189171\n",
      "OrderedDict([('1', OrderedDict([('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 0.0), ('5', 0.0)])), ('5', OrderedDict([('5', 99.3948562783661)]))])\n",
      "Task 1 average acc: 0.0\n",
      "Task 2 average acc: 0.0\n",
      "Task 3 average acc: 0.0\n",
      "Task 4 average acc: 0.0\n",
      "Task 5 average acc: 19.87897125567322\n",
      "===Summary of experiment repeats: 10 / 10 ===\n",
      "[19.74785678 19.76802824 19.74785678 19.86888553 19.89914271 19.91931417\n",
      " 19.88905698 19.90922844 19.89914271 19.87897126]\n",
      "mean: 19.852748361069086 std: 0.06585911333258425\n"
     ]
    }
   ],
   "source": [
    "avg_final_acc = np.zeros(repeat)\n",
    "\n",
    "for r in range (repeat):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name],\n",
    "                                                            batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset_splits[train_name],\n",
    "                                                        batch_size=batch_size, shuffle=False)\n",
    "        # Train\n",
    "        for epoch in range(4):\n",
    "            train_acc = AverageMeter()\n",
    "            for (input, target, task) in train_loader:\n",
    "                agent.train()\n",
    "                input, target = input.to(device), target.to(device)\n",
    "\n",
    "                output = agent(input)\n",
    "                loss = criterion_fn(output, target, valid_out_dim)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_acc = accumulate_acc(output, target, train_acc)\n",
    "\n",
    "        # Eval\n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False,)\n",
    "\n",
    "            agent.eval()\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, task) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = agent(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "    print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    avg_final_acc[r] = avg_acc_history[-1]\n",
    "    print('===Summary of experiment repeats:',r+1,'/',repeat,'===')\n",
    "    print(avg_final_acc)\n",
    "    print('mean:', avg_final_acc.mean(), 'std:', avg_final_acc.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
