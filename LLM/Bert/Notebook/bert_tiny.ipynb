{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongpochen/opt/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path = './data/eng-fra.txt', vocab = None , seq_len = 20):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_path = corpus_path\n",
    "        self.lines = []\n",
    "\n",
    "        # Reopen the file to read the lines\n",
    "        with open(self.corpus_path , \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm.tqdm(f, desc=\"Loading Dataset\"):\n",
    "                self.lines.append(line.replace('\\n', '').split('\\t'))\n",
    "\n",
    "        self.corpus_lines = len(self.lines)\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "    \n",
    "    def get_random_line(self):\n",
    "        return self.lines[random.randrange(self.corpus_lines)][1]\n",
    "    \n",
    "    def random_sent(self, index):\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # output_text, label(isNotNext:0, isNext:1)\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "        \n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = self.vocab.mask_index\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = random.randrange(len(self.vocab))\n",
    "\n",
    "                # 10% randomly change token to current token\n",
    "                else:\n",
    "                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "\n",
    "                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "\n",
    "            else:\n",
    "                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "                output_label.append(0)\n",
    "\n",
    "        return tokens, output_label\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        t1, t2, is_next_label = self.random_sent(item)\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]\n",
    "        t2 = t2_random + [self.vocab.eos_index]\n",
    "\n",
    "        t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n",
    "        t2_label = t2_label + [self.vocab.pad_index]\n",
    "\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "\n",
    "        padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input, \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label, \"is_next\": is_next_label}\n",
    "        \n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, counter, specials, max_size = None, min_freq = 1):\n",
    "        self.freqs = counter\n",
    "        self.itos = list(specials)\n",
    "\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "\n",
    "        for token in specials:\n",
    "            del counter[token]\n",
    "\n",
    "        words_and_freqs = sorted(counter.items(), key = lambda tup: tup[0])\n",
    "        words_and_freqs.sort(key = lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_freqs:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        self.stoi = {token: i for i, token in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 135842it [00:00, 263929.66it/s]\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "with open('./data/eng-fra.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm.tqdm(f, desc=\"Loading Dataset\"):\n",
    "        if isinstance(line, list):\n",
    "            words = line\n",
    "        else:\n",
    "            words = line.replace(\"\\n\", \"\").replace(\"\\t\", \" \").split()\n",
    "\n",
    "        for word in words:\n",
    "            counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " '<eos>': 2,\n",
       " '<sos>': 3,\n",
       " '<mask>': 4,\n",
       " 'I': 5,\n",
       " 'to': 6,\n",
       " 'de': 7,\n",
       " 'Je': 8,\n",
       " 'a': 9,\n",
       " 'you': 10,\n",
       " 'the': 11,\n",
       " '?': 12,\n",
       " 'pas': 13,\n",
       " 'que': 14,\n",
       " 'à': 15,\n",
       " 'Tom': 16,\n",
       " 'ne': 17,\n",
       " 'la': 18,\n",
       " 'le': 19,\n",
       " 'Il': 20,\n",
       " 'is': 21,\n",
       " 'me': 22,\n",
       " 'vous': 23,\n",
       " 'est': 24,\n",
       " 'of': 25,\n",
       " 'un': 26,\n",
       " 'He': 27,\n",
       " 'in': 28,\n",
       " 'ce': 29,\n",
       " 'en': 30,\n",
       " 'have': 31,\n",
       " 'was': 32,\n",
       " 'une': 33,\n",
       " 'for': 34,\n",
       " 'je': 35,\n",
       " 'your': 36,\n",
       " 'that': 37,\n",
       " 'pour': 38,\n",
       " 'suis': 39,\n",
       " 'You': 40,\n",
       " \"don't\": 41,\n",
       " \"I'm\": 42,\n",
       " 'les': 43,\n",
       " \"J'ai\": 44,\n",
       " '!': 45,\n",
       " 'be': 46,\n",
       " 'not': 47,\n",
       " 'The': 48,\n",
       " 'want': 49,\n",
       " 'my': 50,\n",
       " 'Elle': 51,\n",
       " 'do': 52,\n",
       " 'She': 53,\n",
       " 'Nous': 54,\n",
       " 'tu': 55,\n",
       " 'Vous': 56,\n",
       " 'Tu': 57,\n",
       " 'this': 58,\n",
       " 'like': 59,\n",
       " 'on': 60,\n",
       " 'it': 61,\n",
       " 'with': 62,\n",
       " 'are': 63,\n",
       " 'dans': 64,\n",
       " 'des': 65,\n",
       " 'you.': 66,\n",
       " \"C'est\": 67,\n",
       " 'nous': 68,\n",
       " 'We': 69,\n",
       " 'know': 70,\n",
       " 'plus': 71,\n",
       " 'te': 72,\n",
       " 'faire': 73,\n",
       " 'at': 74,\n",
       " 'what': 75,\n",
       " 'se': 76,\n",
       " 'du': 77,\n",
       " 'his': 78,\n",
       " 'as': 79,\n",
       " 'avec': 80,\n",
       " 'veux': 81,\n",
       " 'au': 82,\n",
       " 'all': 83,\n",
       " 'qui': 84,\n",
       " 'think': 85,\n",
       " 'he': 86,\n",
       " 'Do': 87,\n",
       " \"can't\": 88,\n",
       " 'What': 89,\n",
       " 'and': 90,\n",
       " \"qu'il\": 91,\n",
       " 'fait': 92,\n",
       " 'Le': 93,\n",
       " 'about': 94,\n",
       " \"You're\": 95,\n",
       " 'very': 96,\n",
       " 'They': 97,\n",
       " 'et': 98,\n",
       " 'go': 99,\n",
       " 'sont': 100,\n",
       " 'son': 101,\n",
       " \"Don't\": 102,\n",
       " 'can': 103,\n",
       " 'mon': 104,\n",
       " 'did': 105,\n",
       " 'him': 106,\n",
       " 'me.': 107,\n",
       " 'tout': 108,\n",
       " 'It': 109,\n",
       " 'had': 110,\n",
       " 'get': 111,\n",
       " 'Ne': 112,\n",
       " 'This': 113,\n",
       " \"didn't\": 114,\n",
       " \"It's\": 115,\n",
       " 'How': 116,\n",
       " 'will': 117,\n",
       " 'it.': 118,\n",
       " 'être': 119,\n",
       " 'y': 120,\n",
       " 'très': 121,\n",
       " 'votre': 122,\n",
       " 'La': 123,\n",
       " 'si': 124,\n",
       " 'just': 125,\n",
       " 'were': 126,\n",
       " 'Ce': 127,\n",
       " 'sur': 128,\n",
       " 'has': 129,\n",
       " 'été': 130,\n",
       " 'Les': 131,\n",
       " 'we': 132,\n",
       " 'pense': 133,\n",
       " 'lui': 134,\n",
       " 'her': 135,\n",
       " 'Ils': 136,\n",
       " \"n'est\": 137,\n",
       " 'peux': 138,\n",
       " 'Are': 139,\n",
       " 'était': 140,\n",
       " 'cette': 141,\n",
       " \"n'ai\": 142,\n",
       " 'out': 143,\n",
       " 'so': 144,\n",
       " 'an': 145,\n",
       " \"I'll\": 146,\n",
       " 'going': 147,\n",
       " 'My': 148,\n",
       " 'jamais': 149,\n",
       " 'by': 150,\n",
       " 'from': 151,\n",
       " 'see': 152,\n",
       " 'need': 153,\n",
       " 'il': 154,\n",
       " 'dit': 155,\n",
       " 'up': 156,\n",
       " 'par': 157,\n",
       " 'one': 158,\n",
       " 'no': 159,\n",
       " 'Tom.': 160,\n",
       " 'êtes': 161,\n",
       " 'ça': 162,\n",
       " 'really': 163,\n",
       " 'ton': 164,\n",
       " 'temps': 165,\n",
       " 'should': 166,\n",
       " 'quelque': 167,\n",
       " 'ma': 168,\n",
       " 'vraiment': 169,\n",
       " 'time': 170,\n",
       " 'been': 171,\n",
       " 'good': 172,\n",
       " 'chose': 173,\n",
       " 'Pourquoi': 174,\n",
       " 'Why': 175,\n",
       " 'if': 176,\n",
       " 'tell': 177,\n",
       " 'never': 178,\n",
       " 'that.': 179,\n",
       " 'Mary': 180,\n",
       " 'you?': 181,\n",
       " 'sa': 182,\n",
       " 'sais': 183,\n",
       " 'would': 184,\n",
       " \"I've\": 185,\n",
       " 'more': 186,\n",
       " 'got': 187,\n",
       " 'train': 188,\n",
       " 'sommes': 189,\n",
       " 'here.': 190,\n",
       " 'Comment': 191,\n",
       " 'Ça': 192,\n",
       " 'but': 193,\n",
       " 'than': 194,\n",
       " 'come': 195,\n",
       " 'Is': 196,\n",
       " \"I'd\": 197,\n",
       " 'ont': 198,\n",
       " 'besoin': 199,\n",
       " 'him.': 200,\n",
       " 'dire': 201,\n",
       " 'avons': 202,\n",
       " \"That's\": 203,\n",
       " \"s'est\": 204,\n",
       " 'make': 205,\n",
       " 'bien': 206,\n",
       " 'es': 207,\n",
       " 'pas.': 208,\n",
       " 'tous': 209,\n",
       " 'Can': 210,\n",
       " \"m'a\": 211,\n",
       " 'some': 212,\n",
       " 'too': 213,\n",
       " 'beaucoup': 214,\n",
       " 'how': 215,\n",
       " 'take': 216,\n",
       " 'any': 217,\n",
       " 'thought': 218,\n",
       " 'she': 219,\n",
       " 'trop': 220,\n",
       " 'quoi': 221,\n",
       " 'when': 222,\n",
       " 'could': 223,\n",
       " \"We're\": 224,\n",
       " 'On': 225,\n",
       " 'am': 226,\n",
       " 'Elles': 227,\n",
       " 'Did': 228,\n",
       " 'peu': 229,\n",
       " 'help': 230,\n",
       " 'ça.': 231,\n",
       " 'mais': 232,\n",
       " 'moi.': 233,\n",
       " 'That': 234,\n",
       " 'toujours': 235,\n",
       " 'Please': 236,\n",
       " 'time.': 237,\n",
       " 'ai': 238,\n",
       " 'comme': 239,\n",
       " \"n'a\": 240,\n",
       " 'encore': 241,\n",
       " \"you're\": 242,\n",
       " 'avoir': 243,\n",
       " 'faut': 244,\n",
       " 'there': 245,\n",
       " 'must': 246,\n",
       " 'mes': 247,\n",
       " 'much': 248,\n",
       " \"c'est\": 249,\n",
       " 'last': 250,\n",
       " 'made': 251,\n",
       " \"doesn't\": 252,\n",
       " \"j'ai\": 253,\n",
       " 'they': 254,\n",
       " 'Tout': 255,\n",
       " 'peut': 256,\n",
       " 'If': 257,\n",
       " 'something': 258,\n",
       " 'deux': 259,\n",
       " 'this.': 260,\n",
       " 'Que': 261,\n",
       " 'now.': 262,\n",
       " 'ses': 263,\n",
       " 'Est-ce': 264,\n",
       " 'rien': 265,\n",
       " 'went': 266,\n",
       " 'va': 267,\n",
       " 'feel': 268,\n",
       " 'soit': 269,\n",
       " 'A': 270,\n",
       " 'ta': 271,\n",
       " 'told': 272,\n",
       " 'aller': 273,\n",
       " 'ici': 274,\n",
       " 'toi.': 275,\n",
       " 'voir': 276,\n",
       " 'Mon': 277,\n",
       " 'avez': 278,\n",
       " 'ici.': 279,\n",
       " 'moi': 280,\n",
       " 'There': 281,\n",
       " 'faire.': 282,\n",
       " 'vais': 283,\n",
       " 'Have': 284,\n",
       " 'Où': 285,\n",
       " 'who': 286,\n",
       " 'dois': 287,\n",
       " 'talk': 288,\n",
       " 'believe': 289,\n",
       " 'monde': 290,\n",
       " \"J'aimerais\": 291,\n",
       " 'Si': 292,\n",
       " 'long': 293,\n",
       " 'avait': 294,\n",
       " 'chez': 295,\n",
       " 'aussi': 296,\n",
       " 'look': 297,\n",
       " 'où': 298,\n",
       " \"l'air\": 299,\n",
       " 'eu': 300,\n",
       " 'love': 301,\n",
       " \"He's\": 302,\n",
       " 'here': 303,\n",
       " 'juste': 304,\n",
       " 'sure': 305,\n",
       " 'only': 306,\n",
       " 'parler': 307,\n",
       " \"s'il\": 308,\n",
       " 'always': 309,\n",
       " 'still': 310,\n",
       " 'bon': 311,\n",
       " 'many': 312,\n",
       " 'même': 313,\n",
       " 'say': 314,\n",
       " 'us': 315,\n",
       " 'vous.': 316,\n",
       " 'cela': 317,\n",
       " 'Where': 318,\n",
       " 'our': 319,\n",
       " \"n'y\": 320,\n",
       " 'pensais': 321,\n",
       " 'back': 322,\n",
       " 'right': 323,\n",
       " 'lot': 324,\n",
       " 'vu': 325,\n",
       " 'give': 326,\n",
       " \"Let's\": 327,\n",
       " 'déjà': 328,\n",
       " 'À': 329,\n",
       " 'people': 330,\n",
       " 'As-tu': 331,\n",
       " 'me?': 332,\n",
       " \"d'un\": 333,\n",
       " 'little': 334,\n",
       " 'Avez-vous': 335,\n",
       " 'maison': 336,\n",
       " 'anything': 337,\n",
       " 'new': 338,\n",
       " 'simplement': 339,\n",
       " \"d'être\": 340,\n",
       " 'where': 341,\n",
       " 'said': 342,\n",
       " 'or': 343,\n",
       " 'prendre': 344,\n",
       " 'better': 345,\n",
       " 'into': 346,\n",
       " \"l'ai\": 347,\n",
       " 'When': 348,\n",
       " 'toutes': 349,\n",
       " 'sans': 350,\n",
       " 'work': 351,\n",
       " 'Personne': 352,\n",
       " 'Combien': 353,\n",
       " 'assez': 354,\n",
       " 'put': 355,\n",
       " 'fut': 356,\n",
       " 'Ma': 357,\n",
       " 'Mary.': 358,\n",
       " 'saw': 359,\n",
       " 'Êtes-vous': 360,\n",
       " 'aux': 361,\n",
       " 'No': 362,\n",
       " 'two': 363,\n",
       " 'temps.': 364,\n",
       " 'toute': 365,\n",
       " 'Cette': 366,\n",
       " 'Qui': 367,\n",
       " 'it?': 368,\n",
       " \"won't\": 369,\n",
       " 'quand': 370,\n",
       " 'fois': 371,\n",
       " 'bonne': 372,\n",
       " 'ever': 373,\n",
       " 'find': 374,\n",
       " 'là': 375,\n",
       " 'do.': 376,\n",
       " 'way': 377,\n",
       " 'dû': 378,\n",
       " \"qu'elle\": 379,\n",
       " 'wanted': 380,\n",
       " 'again.': 381,\n",
       " 'ou': 382,\n",
       " 'asked': 383,\n",
       " 'Un': 384,\n",
       " \"n'arrive\": 385,\n",
       " 'off': 386,\n",
       " 'a-t-il': 387,\n",
       " 'Who': 388,\n",
       " \"n'est-ce\": 389,\n",
       " 'tes': 390,\n",
       " 'eat': 391,\n",
       " 'old': 392,\n",
       " 'devrais': 393,\n",
       " 'notre': 394,\n",
       " 'every': 395,\n",
       " 'gens': 396,\n",
       " \"it's\": 397,\n",
       " \"isn't\": 398,\n",
       " 'go.': 399,\n",
       " 'Would': 400,\n",
       " \"d'une\": 401,\n",
       " 'voiture': 402,\n",
       " 'ces': 403,\n",
       " 'pris': 404,\n",
       " 'avant': 405,\n",
       " 'day.': 406,\n",
       " 'mieux': 407,\n",
       " 'speak': 408,\n",
       " 'let': 409,\n",
       " 'vos': 410,\n",
       " 'us.': 411,\n",
       " 'elle': 412,\n",
       " 'doing': 413,\n",
       " 'home.': 414,\n",
       " 'car': 415,\n",
       " 'lui.': 416,\n",
       " 'money.': 417,\n",
       " 'over': 418,\n",
       " \"What's\": 419,\n",
       " 'up.': 420,\n",
       " 'before': 421,\n",
       " 'père': 422,\n",
       " 'being': 423,\n",
       " 'Quel': 424,\n",
       " 'her.': 425,\n",
       " 'His': 426,\n",
       " 'Puis-je': 427,\n",
       " 'does': 428,\n",
       " 'today.': 429,\n",
       " 'after': 430,\n",
       " 'may': 431,\n",
       " 'next': 432,\n",
       " 'fais': 433,\n",
       " 'night.': 434,\n",
       " 'comment': 435,\n",
       " 'hope': 436,\n",
       " \"l'a\": 437,\n",
       " 'quelques': 438,\n",
       " 'êtes-vous': 439,\n",
       " 'ask': 440,\n",
       " 'their': 441,\n",
       " 'left': 442,\n",
       " 'why': 443,\n",
       " 'took': 444,\n",
       " 'livre': 445,\n",
       " 'leave': 446,\n",
       " 'money': 447,\n",
       " 'trois': 448,\n",
       " 'personne': 449,\n",
       " 'without': 450,\n",
       " 'gave': 451,\n",
       " \"They're\": 452,\n",
       " 'cela.': 453,\n",
       " \"There's\": 454,\n",
       " 'pouvez': 455,\n",
       " 'Cela': 456,\n",
       " 'Quand': 457,\n",
       " 'buy': 458,\n",
       " 'voulais': 459,\n",
       " 'heard': 460,\n",
       " 'them': 461,\n",
       " \"J'aime\": 462,\n",
       " 'out.': 463,\n",
       " 'travail': 464,\n",
       " \"couldn't\": 465,\n",
       " 'down': 466,\n",
       " 'that?': 467,\n",
       " 'understand': 468,\n",
       " 'live': 469,\n",
       " 'maison.': 470,\n",
       " 'savoir': 471,\n",
       " 'sens': 472,\n",
       " \"aujourd'hui.\": 473,\n",
       " 'remember': 474,\n",
       " 'father': 475,\n",
       " 'parents': 476,\n",
       " 'stay': 477,\n",
       " 'book': 478,\n",
       " 'read': 479,\n",
       " 'there.': 480,\n",
       " 'toi': 481,\n",
       " 'three': 482,\n",
       " 'alone.': 483,\n",
       " 'came': 484,\n",
       " 'home': 485,\n",
       " 'manger': 486,\n",
       " 'used': 487,\n",
       " 'call': 488,\n",
       " 'hear': 489,\n",
       " 'man': 490,\n",
       " 'veut': 491,\n",
       " 'car.': 492,\n",
       " 'job.': 493,\n",
       " 'porte': 494,\n",
       " 'bien.': 495,\n",
       " 'croire': 496,\n",
       " 'pourquoi': 497,\n",
       " 'yesterday.': 498,\n",
       " 'Your': 499,\n",
       " 'Quelle': 500,\n",
       " 'rendre': 501,\n",
       " 'En': 502,\n",
       " \"J'espère\": 503,\n",
       " 'found': 504,\n",
       " 'All': 505,\n",
       " 'seen': 506,\n",
       " 'keep': 507,\n",
       " 'happy.': 508,\n",
       " 'maintenant.': 509,\n",
       " 'Es-tu': 510,\n",
       " \"n'êtes\": 511,\n",
       " 'work.': 512,\n",
       " \"Qu'est-ce\": 513,\n",
       " 'kind': 514,\n",
       " 'Let': 515,\n",
       " 'après': 516,\n",
       " 'looking': 517,\n",
       " 'enough': 518,\n",
       " 'tomorrow.': 519,\n",
       " 'as-tu': 520,\n",
       " 'mal': 521,\n",
       " 'choses': 522,\n",
       " 'sait': 523,\n",
       " 'wish': 524,\n",
       " 'Could': 525,\n",
       " 'well.': 526,\n",
       " 'hard': 527,\n",
       " 'pendant': 528,\n",
       " 'Boston.': 529,\n",
       " \"C'était\": 530,\n",
       " \"J'étais\": 531,\n",
       " 'cet': 532,\n",
       " 'part': 533,\n",
       " \"haven't\": 534,\n",
       " 'mère': 535,\n",
       " \"quelqu'un\": 536,\n",
       " 'vie': 537,\n",
       " 'house': 538,\n",
       " 'voiture.': 539,\n",
       " 'entendu': 540,\n",
       " 'nothing': 541,\n",
       " 'leur': 542,\n",
       " 'travail.': 543,\n",
       " 'help.': 544,\n",
       " 'even': 545,\n",
       " \"n'en\": 546,\n",
       " \"wasn't\": 547,\n",
       " 'might': 548,\n",
       " 'use': 549,\n",
       " 'first': 550,\n",
       " 'pu': 551,\n",
       " 'point': 552,\n",
       " 'quelle': 553,\n",
       " 'suppose': 554,\n",
       " 'chose.': 555,\n",
       " 'depuis': 556,\n",
       " 'venir': 557,\n",
       " 'chien': 558,\n",
       " 'demain.': 559,\n",
       " 'here?': 560,\n",
       " 'lost': 561,\n",
       " 'right.': 562,\n",
       " 'De': 563,\n",
       " 'knew': 564,\n",
       " 'semble': 565,\n",
       " 'rester': 566,\n",
       " 'same': 567,\n",
       " 'sois': 568,\n",
       " 'play': 569,\n",
       " 'things': 570,\n",
       " 'already': 571,\n",
       " 'avez-vous': 572,\n",
       " 'done': 573,\n",
       " 'Une': 574,\n",
       " 'other': 575,\n",
       " 'such': 576,\n",
       " 'aller.': 577,\n",
       " 'nouveau': 578,\n",
       " 'enfants': 579,\n",
       " 'parle': 580,\n",
       " 'care': 581,\n",
       " 'demandé': 582,\n",
       " 'soyez': 583,\n",
       " 'moment.': 584,\n",
       " 'room.': 585,\n",
       " 'passé': 586,\n",
       " \"Tom's\": 587,\n",
       " 'grand': 588,\n",
       " 'moins': 589,\n",
       " 'hier': 590,\n",
       " 'everything': 591,\n",
       " 'day': 592,\n",
       " 'getting': 593,\n",
       " 'nuit': 594,\n",
       " 'Marie': 595,\n",
       " 'these': 596,\n",
       " 'afraid': 597,\n",
       " 'anyone': 598,\n",
       " 'school': 599,\n",
       " 'this?': 600,\n",
       " 'trouver': 601,\n",
       " 'train.': 602,\n",
       " 'dog': 603,\n",
       " 'question.': 604,\n",
       " 'hier.': 605,\n",
       " \"n'aime\": 606,\n",
       " \"you'd\": 607,\n",
       " 'try': 608,\n",
       " 'Son': 609,\n",
       " 'pourrait': 610,\n",
       " 'Y': 611,\n",
       " 'dont': 612,\n",
       " 'often': 613,\n",
       " 'school.': 614,\n",
       " 'dire.': 615,\n",
       " 'glad': 616,\n",
       " 'homme': 617,\n",
       " 'petit': 618,\n",
       " 'étaient': 619,\n",
       " 'bought': 620,\n",
       " 'ans.': 621,\n",
       " 'friends.': 622,\n",
       " \"t'ai\": 623,\n",
       " 'parler.': 624,\n",
       " 'pensé': 625,\n",
       " 'few': 626,\n",
       " 'house.': 627,\n",
       " 'looks': 628,\n",
       " 'tout.': 629,\n",
       " 'souvent': 630,\n",
       " 'wants': 631,\n",
       " 'soir.': 632,\n",
       " 'met': 633,\n",
       " 'sera': 634,\n",
       " 'ceci': 635,\n",
       " 'tellement': 636,\n",
       " 'sorry': 637,\n",
       " \"She's\": 638,\n",
       " 'autre': 639,\n",
       " \"m'en\": 640,\n",
       " 'because': 641,\n",
       " 'happy': 642,\n",
       " 'fille': 643,\n",
       " \"jusqu'à\": 644,\n",
       " 'mother': 645,\n",
       " 'devriez': 646,\n",
       " 'fait.': 647,\n",
       " 'là.': 648,\n",
       " 'mind': 649,\n",
       " 'presque': 650,\n",
       " 'anything.': 651,\n",
       " 'fort': 652,\n",
       " 'morning.': 653,\n",
       " 'allé': 654,\n",
       " 'pouvons': 655,\n",
       " 'able': 656,\n",
       " \"qu'on\": 657,\n",
       " 'seul.': 658,\n",
       " 'big': 659,\n",
       " \"j'étais\": 660,\n",
       " 'bad': 661,\n",
       " 'happened': 662,\n",
       " 'sûr': 663,\n",
       " 'femme': 664,\n",
       " 'Get': 665,\n",
       " 'bus.': 666,\n",
       " 'Pouvez-vous': 667,\n",
       " 'felt': 668,\n",
       " 'know.': 669,\n",
       " 'late.': 670,\n",
       " 'peur': 671,\n",
       " 'another': 672,\n",
       " 'place': 673,\n",
       " 'meet': 674,\n",
       " 'around': 675,\n",
       " 'doit': 676,\n",
       " 'Veuillez': 677,\n",
       " 'acheté': 678,\n",
       " 'davantage': 679,\n",
       " 'room': 680,\n",
       " 'yet.': 681,\n",
       " \"qu'ils\": 682,\n",
       " 'bus': 683,\n",
       " 'heures.': 684,\n",
       " 'knows': 685,\n",
       " 'maintenant': 686,\n",
       " 'best': 687,\n",
       " 'tried': 688,\n",
       " \"aren't\": 689,\n",
       " 'est-il': 690,\n",
       " \"n'es\": 691,\n",
       " \"n'était\": 692,\n",
       " 'perdu': 693,\n",
       " 'seul': 694,\n",
       " \"wouldn't\": 695,\n",
       " 'lorsque': 696,\n",
       " 'almost': 697,\n",
       " 'away.': 698,\n",
       " 'Take': 699,\n",
       " 'trouvé': 700,\n",
       " 'great': 701,\n",
       " 'hate': 702,\n",
       " \"J'ignore\": 703,\n",
       " 'please.': 704,\n",
       " 'seems': 705,\n",
       " 'aime': 706,\n",
       " 'friend.': 707,\n",
       " 'to.': 708,\n",
       " 'anymore.': 709,\n",
       " 'chaque': 710,\n",
       " 'problème': 711,\n",
       " 'nouvelle': 712,\n",
       " 'combien': 713,\n",
       " 'difficile': 714,\n",
       " 'years': 715,\n",
       " 'Peux-tu': 716,\n",
       " 'all.': 717,\n",
       " 'plaît.': 718,\n",
       " 'plus.': 719,\n",
       " 'Sa': 720,\n",
       " \"You've\": 721,\n",
       " 'back.': 722,\n",
       " 'jouer': 723,\n",
       " 'lire': 724,\n",
       " 'police': 725,\n",
       " 'Be': 726,\n",
       " 'partir': 727,\n",
       " 'Will': 728,\n",
       " 'quel': 729,\n",
       " 'trouve': 730,\n",
       " 'way.': 731,\n",
       " 'problème.': 732,\n",
       " 'stop': 733,\n",
       " 'Everyone': 734,\n",
       " 'away': 735,\n",
       " 'demande': 736,\n",
       " 'plan': 737,\n",
       " 'week.': 738,\n",
       " 'write': 739,\n",
       " 'devons': 740,\n",
       " 'jour': 741,\n",
       " 'question': 742,\n",
       " 'talking': 743,\n",
       " 'looked': 744,\n",
       " 'passer': 745,\n",
       " 'them.': 746,\n",
       " 'wait': 747,\n",
       " 'dit.': 748,\n",
       " 'mis': 749,\n",
       " 'Does': 750,\n",
       " 'aucun': 751,\n",
       " \"d'aller\": 752,\n",
       " 'moment': 753,\n",
       " 'likes': 754,\n",
       " 'crois': 755,\n",
       " 'friend': 756,\n",
       " 'partir.': 757,\n",
       " 'wrong.': 758,\n",
       " 'each': 759,\n",
       " 'one.': 760,\n",
       " 'aucune': 761,\n",
       " 'idée': 762,\n",
       " 'May': 763,\n",
       " 'nice': 764,\n",
       " 'much.': 765,\n",
       " \"l'école.\": 766,\n",
       " 'nous.': 767,\n",
       " \"J'adore\": 768,\n",
       " 'fois.': 769,\n",
       " 'forget': 770,\n",
       " 'heures': 771,\n",
       " 'problem.': 772,\n",
       " 'viens': 773,\n",
       " 'Tell': 774,\n",
       " 'bed.': 775,\n",
       " 'devez': 776,\n",
       " 'grande': 777,\n",
       " 'jours.': 778,\n",
       " 'thing': 779,\n",
       " 'while': 780,\n",
       " 'own': 781,\n",
       " \"d'autre\": 782,\n",
       " 'show': 783,\n",
       " 'answer': 784,\n",
       " 'connais': 785,\n",
       " 'book.': 786,\n",
       " 'happened.': 787,\n",
       " 'arrivé': 788,\n",
       " 'well': 789,\n",
       " 'voulez': 790,\n",
       " \"you've\": 791,\n",
       " 'pay': 792,\n",
       " 'Stop': 793,\n",
       " 'heure': 794,\n",
       " 'matter': 795,\n",
       " 'mettre': 796,\n",
       " 'non': 797,\n",
       " 'quite': 798,\n",
       " 'sortir': 799,\n",
       " \"d'argent\": 800,\n",
       " 'voir.': 801,\n",
       " \"S'il\": 802,\n",
       " 'Veux-tu': 803,\n",
       " 'matin.': 804,\n",
       " 'soon': 805,\n",
       " 'makes': 806,\n",
       " 'petite': 807,\n",
       " 'something.': 808,\n",
       " \"Quelqu'un\": 809,\n",
       " 'donné': 810,\n",
       " 'important': 811,\n",
       " 'nom': 812,\n",
       " 'prie': 813,\n",
       " \"shouldn't\": 814,\n",
       " 'monde.': 815,\n",
       " \"d'avoir\": 816,\n",
       " 'life.': 817,\n",
       " 'serais': 818,\n",
       " 'Just': 819,\n",
       " 'français.': 820,\n",
       " 'friends': 821,\n",
       " 'idea': 822,\n",
       " 'myself.': 823,\n",
       " 'someone': 824,\n",
       " 'voulait': 825,\n",
       " 'Come': 826,\n",
       " 'English': 827,\n",
       " 'Merci': 828,\n",
       " 'door.': 829,\n",
       " 'seem': 830,\n",
       " 'Give': 831,\n",
       " 'Boston': 832,\n",
       " 'on.': 833,\n",
       " 'turn': 834,\n",
       " 'spend': 835,\n",
       " 'started': 836,\n",
       " 'travailler': 837,\n",
       " 'accident.': 838,\n",
       " 'please': 839,\n",
       " 'savais': 840,\n",
       " \"We'll\": 841,\n",
       " 'change': 842,\n",
       " 'faites': 843,\n",
       " 'livre.': 844,\n",
       " 'Voulez-vous': 845,\n",
       " 'beautiful': 846,\n",
       " 'trying': 847,\n",
       " 'vie.': 848,\n",
       " 'busy': 849,\n",
       " 'good.': 850,\n",
       " 'children': 851,\n",
       " \"l'école\": 852,\n",
       " 'laissé': 853,\n",
       " 'ready': 854,\n",
       " 'until': 855,\n",
       " 'Tous': 856,\n",
       " 'chambre': 857,\n",
       " 'down.': 858,\n",
       " 'heureux': 859,\n",
       " 'year.': 860,\n",
       " '«': 861,\n",
       " 'advised': 862,\n",
       " 'both': 863,\n",
       " 'seule': 864,\n",
       " 'seulement': 865,\n",
       " 'table.': 866,\n",
       " 'venu': 867,\n",
       " 'déteste': 868,\n",
       " 'mine.': 869,\n",
       " 'chance.': 870,\n",
       " \"d'argent.\": 871,\n",
       " 'dix': 872,\n",
       " 'jour.': 873,\n",
       " 'you,': 874,\n",
       " 'In': 875,\n",
       " 'Our': 876,\n",
       " 'dernière': 877,\n",
       " 'es-tu': 878,\n",
       " 'job': 879,\n",
       " 'turned': 880,\n",
       " 'étiez': 881,\n",
       " 'Her': 882,\n",
       " 'dernière.': 883,\n",
       " 'donner': 884,\n",
       " 'manière': 885,\n",
       " \"n'avais\": 886,\n",
       " 'photo': 887,\n",
       " 'raison.': 888,\n",
       " 'appreciate': 889,\n",
       " 'pretty': 890,\n",
       " 'près': 891,\n",
       " 'ran': 892,\n",
       " 'teacher.': 893,\n",
       " 'door': 894,\n",
       " 'retard.': 895,\n",
       " \"J'en\": 896,\n",
       " 'come.': 897,\n",
       " 'garçon': 898,\n",
       " 'lettre': 899,\n",
       " 'possible.': 900,\n",
       " 'yourself.': 901,\n",
       " 'having': 902,\n",
       " \"j'avais\": 903,\n",
       " 'most': 904,\n",
       " 'partie': 905,\n",
       " 'pouvoir': 906,\n",
       " 'water.': 907,\n",
       " 'questions.': 908,\n",
       " 'fini': 909,\n",
       " 'fit': 910,\n",
       " 'heureux.': 911,\n",
       " 'mangé': 912,\n",
       " 'nos': 913,\n",
       " 'personnes': 914,\n",
       " 'voudrais': 915,\n",
       " 'ami': 916,\n",
       " 'devrait': 917,\n",
       " 'else': 918,\n",
       " 'parents.': 919,\n",
       " \"that's\": 920,\n",
       " 'walk': 921,\n",
       " 'autant': 922,\n",
       " 'enfants.': 923,\n",
       " 'envie': 924,\n",
       " 'serait': 925,\n",
       " \"you'll\": 926,\n",
       " 'hurt': 927,\n",
       " 'sous': 928,\n",
       " 'tôt.': 929,\n",
       " 'French.': 930,\n",
       " 'comprends': 931,\n",
       " 'semaine': 932,\n",
       " 'watch': 933,\n",
       " \"what's\": 934,\n",
       " 'alors': 935,\n",
       " 'amis.': 936,\n",
       " 'coming': 937,\n",
       " 'rien.': 938,\n",
       " 'nuit.': 939,\n",
       " 'open': 940,\n",
       " 'peut-être': 941,\n",
       " '»': 942,\n",
       " 'drink': 943,\n",
       " 'finished': 944,\n",
       " 'laisser': 945,\n",
       " 'longtemps.': 946,\n",
       " 'serai': 947,\n",
       " 'tôt': 948,\n",
       " 'exactement': 949,\n",
       " 'gone': 950,\n",
       " \"he's\": 951,\n",
       " 'pourrais': 952,\n",
       " 'waiting': 953,\n",
       " 'before.': 954,\n",
       " 'broke': 955,\n",
       " 'caught': 956,\n",
       " 'coup': 957,\n",
       " 'far': 958,\n",
       " 'life': 959,\n",
       " 'minutes': 960,\n",
       " 'study': 961,\n",
       " 'true.': 962,\n",
       " 'ceci.': 963,\n",
       " 'is.': 964,\n",
       " 'première': 965,\n",
       " 'puisse': 966,\n",
       " 'frère': 967,\n",
       " 'regarder': 968,\n",
       " 'téléphone': 969,\n",
       " 'under': 970,\n",
       " 'vérité.': 971,\n",
       " 'leurs': 972,\n",
       " 'mal.': 973,\n",
       " 'now?': 974,\n",
       " 'tant': 975,\n",
       " 'late': 976,\n",
       " 'plutôt': 977,\n",
       " 'entre': 978,\n",
       " 'est-ce': 979,\n",
       " 'secret.': 980,\n",
       " 'ten': 981,\n",
       " 'acheter': 982,\n",
       " 'busy.': 983,\n",
       " 'everything.': 984,\n",
       " 'learn': 985,\n",
       " 'main': 986,\n",
       " 'miss': 987,\n",
       " 'supposed': 988,\n",
       " 'Which': 989,\n",
       " 'contre': 990,\n",
       " \"d'accord\": 991,\n",
       " 'devrions': 992,\n",
       " 'dis': 993,\n",
       " 'since': 994,\n",
       " \"You'll\": 995,\n",
       " 'demain': 996,\n",
       " 'in.': 997,\n",
       " 'père.': 998,\n",
       " 'sœur': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi.get('', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 135842it [00:00, 273806.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BERTDataset(corpus_path = './data/eng-fra.txt', vocab = vocab , seq_len = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "    \n",
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # paper noted they used 4*hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden, nhead=attn_heads, \n",
    "                                                   dim_feedforward = hidden * 4, dropout = dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        x = self.transformer_encoder(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "\n",
    "class BERTLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, criterion, optim_schedule):\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        data = {key:value for key, value in data.items()}\n",
    "        next_sentence_out, mask_lm_out = model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "\n",
    "        next_loss = criterion(next_sentence_out, data[\"is_next\"])\n",
    "\n",
    "        mask_loss = criterion(mask_lm_out.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "        loss = next_loss + mask_loss\n",
    "\n",
    "        optim_schedule.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_schedule.step_and_update_lr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index = 0)\n",
    "bert_hidden = 768\n",
    "warmup_steps = 10000\n",
    "lr = 1e-4\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 0.01\n",
    "\n",
    "bert = BERT(len(vocab))\n",
    "model = BERTLM(bert, len(vocab))\n",
    "\n",
    "optim = Adam(model.parameters(), lr = lr, betas = betas, weight_decay = weight_decay)\n",
    "optim_schedule = ScheduledOptim(optim, bert.hidden, n_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model = model, \n",
    "      train_data_loader = train_data_loader, \n",
    "      criterion = criterion, \n",
    "      optim_schedule = optim_schedule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
