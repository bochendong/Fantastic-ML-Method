{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 4\n",
    "pre_heat_batches = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        print(dataset.root)\n",
    "        label_cache_filename = dataset.root + '/' +'_'+str(len(dataset))+'.pth'\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        '''\n",
    "        :param dataset: (CacheClassLabel)\n",
    "        :param class_list: (list) A list of integers\n",
    "        :param remap: (bool) Ex: remap class [2,4,6 ...] to [0,1,2 ...]\n",
    "        '''\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST(dataroot, train_aug=False):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n",
      "./data\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = MNIST('./data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_boundaries: [0, 2, 4, 6, 8, 10]\n",
      "{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_task(model, train_loader, optimizer, criterion, \n",
    "                  valid_out_dim, best_model_wts, task_num, task_names):\n",
    "    leader = MLP400().to(device)\n",
    "    best_loss = float('inf')\n",
    "    best_pre_heat_loss = float('inf')\n",
    "    if (best_model_wts):\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "    else:\n",
    "        leader.load_state_dict(model.state_dict())\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            model.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                leader_outputs = leader(images)\n",
    "\n",
    "            follower_outputs = model(images)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\n",
    "\n",
    "            if (valid_out_dim == 2):\n",
    "                reg_loss = alpha * torch.mean(torch.abs(follower_outputs - leader_outputs))\n",
    "            else:\n",
    "                follower_outputs = follower_outputs[:,:valid_out_dim - 2]\n",
    "                leader_outputs = leader_outputs[:,:valid_out_dim - 2]\n",
    "                reg_loss = alpha * torch.mean(torch.abs(follower_outputs - leader_outputs)) \n",
    "\n",
    "            loss = c_loss + reg_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = AverageMeter()\n",
    "\n",
    "                for task in range(task_num + 1):\n",
    "                    val_name = task_names[task]\n",
    "                    val_data = val_dataset_splits[val_name]\n",
    "                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    for i, (input, target, _) in enumerate(val_loader):\n",
    "                        input, target = input.to(device), target.to(device)\n",
    "                        output = model(input)\n",
    "                        loss_v = criterion(output, target).item()\n",
    "\n",
    "                        val_loss.update(loss_v, len(target))\n",
    "\n",
    "                print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss:.4f}\", end = \" \")\n",
    "                \n",
    "                if(batch_num < pre_heat_batches):\n",
    "                    if (val_loss.avg < best_pre_heat_loss):\n",
    "                        best_pre_heat_loss = val_loss.avg\n",
    "                        best_pre_heat_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    print()\n",
    "                elif (batch_num == pre_heat_batches):\n",
    "                    best_model_wts = best_pre_heat_model_wts\n",
    "                    best_loss = best_pre_heat_loss\n",
    "                    print(f\"Leader changed with val acc {best_loss: .4f}\")\n",
    "                    leader.load_state_dict(best_model_wts) \n",
    "                else:\n",
    "                    if val_loss.avg < best_loss:\n",
    "                        best_loss = val_loss.avg\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        print(f\"Leader changed with val acc {best_loss: .4f}\")\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "                    else:\n",
    "                        print()\n",
    "            batch_num += 1\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return best_model_wts, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(acc_table, model, train_name, task_names, task_index):\n",
    "    acc_table[train_name] = OrderedDict()\n",
    "\n",
    "    for j in range(task_index+1):\n",
    "        val_name = task_names[j]\n",
    "        val_data = val_dataset_splits[val_name]\n",
    "        val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        model.eval()\n",
    "        val_acc = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "        acc_table[val_name][train_name] = val_acc.avg\n",
    "    \n",
    "    return acc_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task_names):\n",
    "    leader_acc_table = OrderedDict()\n",
    "    follower_acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    best_loss = float('inf')\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\n",
    "    \n",
    "        follower_acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        leader = MLP400().to(device)\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "        eval(follower_acc_table, model, train_name, task_names, i)\n",
    "        eval(leader_acc_table, leader, train_name, task_names, i)\n",
    "\n",
    "        print(follower_acc_table)\n",
    "        print(leader_acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += follower_acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('follower Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    leader_avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += leader_acc_table[val_name][train_name]\n",
    "\n",
    "        leader_avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('leader Task', train_name, 'average acc:', leader_avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history, leader_avg_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task order: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Task: 1=====\n",
      "batch_num: 0, c_loss:0.6989, val_loss: 2.2008, loss:0.6989 \n",
      "batch_num: 1, c_loss:0.2050, val_loss: 2.1703, loss:1.1964 \n",
      "batch_num: 2, c_loss:0.1701, val_loss: 2.1700, loss:1.3915 \n",
      "batch_num: 3, c_loss:0.1890, val_loss: 2.1798, loss:1.3272 \n",
      "batch_num: 4, c_loss:0.2339, val_loss: 2.1943, loss:1.1772 \n",
      "batch_num: 5, c_loss:0.2970, val_loss: 2.2234, loss:1.1467 \n",
      "batch_num: 6, c_loss:0.3660, val_loss: 2.2591, loss:1.1465 \n",
      "batch_num: 7, c_loss:0.4497, val_loss: 2.2944, loss:1.0708 \n",
      "batch_num: 8, c_loss:0.5644, val_loss: 2.3081, loss:1.0582 \n",
      "batch_num: 9, c_loss:0.5977, val_loss: 2.2964, loss:1.0861 \n",
      "batch_num: 10, c_loss:0.5736, val_loss: 2.2712, loss:1.0498 \n",
      "batch_num: 11, c_loss:0.5076, val_loss: 2.2423, loss:1.0177 \n",
      "batch_num: 12, c_loss:0.4772, val_loss: 2.2169, loss:1.0231 \n",
      "batch_num: 13, c_loss:0.4198, val_loss: 2.2009, loss:1.0493 \n",
      "batch_num: 14, c_loss:0.4132, val_loss: 2.1967, loss:1.0275 \n",
      "batch_num: 15, c_loss:0.4003, val_loss: 2.1964, loss:1.0009 \n",
      "batch_num: 16, c_loss:0.3926, val_loss: 2.1955, loss:1.0013 \n",
      "batch_num: 17, c_loss:0.3928, val_loss: 2.1991, loss:1.0093 \n",
      "batch_num: 18, c_loss:0.4128, val_loss: 2.1992, loss:1.0348 \n",
      "batch_num: 19, c_loss:0.4418, val_loss: 2.1948, loss:0.9985 \n",
      "batch_num: 20, c_loss:0.4511, val_loss: 2.1901, loss:1.0051 \n",
      "batch_num: 21, c_loss:0.4493, val_loss: 2.1890, loss:0.9803 \n",
      "batch_num: 22, c_loss:0.4639, val_loss: 2.1926, loss:0.9618 \n",
      "batch_num: 23, c_loss:0.4773, val_loss: 2.2014, loss:1.0038 \n",
      "batch_num: 24, c_loss:0.4830, val_loss: 2.2120, loss:1.0101 \n",
      "batch_num: 25, c_loss:0.4838, val_loss: 2.2222, loss:0.9917 \n",
      "batch_num: 26, c_loss:0.4887, val_loss: 2.2299, loss:0.9961 \n",
      "batch_num: 27, c_loss:0.4827, val_loss: 2.2280, loss:0.9928 \n",
      "batch_num: 28, c_loss:0.5100, val_loss: 2.2104, loss:0.9969 \n",
      "batch_num: 29, c_loss:0.4645, val_loss: 2.1927, loss:1.0293 \n",
      "batch_num: 30, c_loss:0.4858, val_loss: 2.1760, loss:0.9982 Leader changed with val acc  2.1700\n",
      "batch_num: 31, c_loss:0.4652, val_loss: 2.0975, loss:1.4531 Leader changed with val acc  2.0975\n",
      "batch_num: 32, c_loss:0.3959, val_loss: 1.9963, loss:0.3959 Leader changed with val acc  1.9963\n",
      "batch_num: 33, c_loss:0.3508, val_loss: 1.8768, loss:0.3508 Leader changed with val acc  1.8768\n",
      "batch_num: 34, c_loss:0.2813, val_loss: 1.7485, loss:0.2813 Leader changed with val acc  1.7485\n",
      "batch_num: 35, c_loss:0.2302, val_loss: 1.6146, loss:0.2302 Leader changed with val acc  1.6146\n",
      "batch_num: 36, c_loss:0.1797, val_loss: 1.4832, loss:0.1797 Leader changed with val acc  1.4832\n",
      "batch_num: 37, c_loss:0.1490, val_loss: 1.3607, loss:0.1490 Leader changed with val acc  1.3607\n",
      "batch_num: 38, c_loss:0.1352, val_loss: 1.2430, loss:0.1352 Leader changed with val acc  1.2430\n",
      "batch_num: 39, c_loss:0.1105, val_loss: 1.1411, loss:0.1105 Leader changed with val acc  1.1411\n",
      "batch_num: 40, c_loss:0.0809, val_loss: 1.0501, loss:0.0809 Leader changed with val acc  1.0501\n",
      "batch_num: 41, c_loss:0.0732, val_loss: 0.9682, loss:0.0732 Leader changed with val acc  0.9682\n",
      "batch_num: 42, c_loss:0.0586, val_loss: 0.8984, loss:0.0586 Leader changed with val acc  0.8984\n",
      "batch_num: 43, c_loss:0.0629, val_loss: 0.8426, loss:0.0629 Leader changed with val acc  0.8426\n",
      "batch_num: 44, c_loss:0.0520, val_loss: 0.7933, loss:0.0520 Leader changed with val acc  0.7933\n",
      "batch_num: 45, c_loss:0.0501, val_loss: 0.7496, loss:0.0501 Leader changed with val acc  0.7496\n",
      "batch_num: 46, c_loss:0.0475, val_loss: 0.7105, loss:0.0475 Leader changed with val acc  0.7105\n",
      "batch_num: 47, c_loss:0.0292, val_loss: 0.6818, loss:0.0292 Leader changed with val acc  0.6818\n",
      "batch_num: 48, c_loss:0.0316, val_loss: 0.6545, loss:0.0316 Leader changed with val acc  0.6545\n",
      "batch_num: 49, c_loss:0.0460, val_loss: 0.6348, loss:0.0460 Leader changed with val acc  0.6348\n",
      "batch_num: 50, c_loss:0.0393, val_loss: 0.6100, loss:0.0393 Leader changed with val acc  0.6100\n",
      "batch_num: 51, c_loss:0.0569, val_loss: 0.5933, loss:0.0569 Leader changed with val acc  0.5933\n",
      "batch_num: 52, c_loss:0.0219, val_loss: 0.5794, loss:0.0219 Leader changed with val acc  0.5794\n",
      "batch_num: 53, c_loss:0.0181, val_loss: 0.5674, loss:0.0181 Leader changed with val acc  0.5674\n",
      "batch_num: 54, c_loss:0.0277, val_loss: 0.5539, loss:0.0277 Leader changed with val acc  0.5539\n",
      "batch_num: 55, c_loss:0.0185, val_loss: 0.5447, loss:0.0185 Leader changed with val acc  0.5447\n",
      "batch_num: 56, c_loss:0.0250, val_loss: 0.5356, loss:0.0250 Leader changed with val acc  0.5356\n",
      "batch_num: 57, c_loss:0.0251, val_loss: 0.5283, loss:0.0251 Leader changed with val acc  0.5283\n",
      "batch_num: 58, c_loss:0.0256, val_loss: 0.5216, loss:0.0256 Leader changed with val acc  0.5216\n",
      "batch_num: 59, c_loss:0.0180, val_loss: 0.5166, loss:0.0180 Leader changed with val acc  0.5166\n",
      "batch_num: 60, c_loss:0.0162, val_loss: 0.5157, loss:0.0162 Leader changed with val acc  0.5157\n",
      "batch_num: 61, c_loss:0.0203, val_loss: 0.5145, loss:0.0203 Leader changed with val acc  0.5145\n",
      "batch_num: 62, c_loss:0.0393, val_loss: 0.5078, loss:0.0393 Leader changed with val acc  0.5078\n",
      "batch_num: 63, c_loss:0.0237, val_loss: 0.5069, loss:0.0237 Leader changed with val acc  0.5069\n",
      "batch_num: 64, c_loss:0.0227, val_loss: 0.5022, loss:0.0227 Leader changed with val acc  0.5022\n",
      "batch_num: 65, c_loss:0.0112, val_loss: 0.4989, loss:0.0112 Leader changed with val acc  0.4989\n",
      "batch_num: 66, c_loss:0.0134, val_loss: 0.4957, loss:0.0134 Leader changed with val acc  0.4957\n",
      "batch_num: 67, c_loss:0.0133, val_loss: 0.4943, loss:0.0133 Leader changed with val acc  0.4943\n",
      "batch_num: 68, c_loss:0.0093, val_loss: 0.4917, loss:0.0093 Leader changed with val acc  0.4917\n",
      "batch_num: 69, c_loss:0.0106, val_loss: 0.4898, loss:0.0106 Leader changed with val acc  0.4898\n",
      "batch_num: 70, c_loss:0.0112, val_loss: 0.4862, loss:0.0112 Leader changed with val acc  0.4862\n",
      "batch_num: 71, c_loss:0.0092, val_loss: 0.4855, loss:0.0092 Leader changed with val acc  0.4855\n",
      "batch_num: 72, c_loss:0.0117, val_loss: 0.4817, loss:0.0117 Leader changed with val acc  0.4817\n",
      "batch_num: 73, c_loss:0.0114, val_loss: 0.4817, loss:0.0114 Leader changed with val acc  0.4817\n",
      "batch_num: 74, c_loss:0.0065, val_loss: 0.4811, loss:0.0065 Leader changed with val acc  0.4811\n",
      "batch_num: 75, c_loss:0.0180, val_loss: 0.4789, loss:0.0180 Leader changed with val acc  0.4789\n",
      "batch_num: 76, c_loss:0.0162, val_loss: 0.4765, loss:0.0162 Leader changed with val acc  0.4765\n",
      "batch_num: 77, c_loss:0.0102, val_loss: 0.4723, loss:0.0102 Leader changed with val acc  0.4723\n",
      "batch_num: 78, c_loss:0.0081, val_loss: 0.4706, loss:0.0081 Leader changed with val acc  0.4706\n",
      "batch_num: 79, c_loss:0.0106, val_loss: 0.4669, loss:0.0106 Leader changed with val acc  0.4669\n",
      "batch_num: 80, c_loss:0.0103, val_loss: 0.4647, loss:0.0103 Leader changed with val acc  0.4647\n",
      "batch_num: 81, c_loss:0.0075, val_loss: 0.4613, loss:0.0075 Leader changed with val acc  0.4613\n",
      "batch_num: 82, c_loss:0.0114, val_loss: 0.4603, loss:0.0114 Leader changed with val acc  0.4603\n",
      "batch_num: 83, c_loss:0.0099, val_loss: 0.4562, loss:0.0099 Leader changed with val acc  0.4562\n",
      "batch_num: 84, c_loss:0.0135, val_loss: 0.4543, loss:0.0135 Leader changed with val acc  0.4543\n",
      "batch_num: 85, c_loss:0.0139, val_loss: 0.4561, loss:0.0139 \n",
      "batch_num: 86, c_loss:0.0235, val_loss: 0.4878, loss:0.0356 \n",
      "batch_num: 87, c_loss:0.0133, val_loss: 0.4835, loss:0.2023 \n",
      "batch_num: 88, c_loss:0.0091, val_loss: 0.4596, loss:0.1424 \n",
      "batch_num: 89, c_loss:0.0081, val_loss: 0.4567, loss:0.1463 \n",
      "batch_num: 90, c_loss:0.0109, val_loss: 0.4828, loss:0.1691 \n",
      "batch_num: 91, c_loss:0.0082, val_loss: 0.5017, loss:0.1587 \n",
      "batch_num: 92, c_loss:0.0076, val_loss: 0.5052, loss:0.1511 \n",
      "batch_num: 93, c_loss:0.0056, val_loss: 0.4963, loss:0.1607 \n",
      "batch_num: 94, c_loss:0.0079, val_loss: 0.4831, loss:0.1664 \n",
      "batch_num: 95, c_loss:0.0287, val_loss: 0.4766, loss:0.1861 \n",
      "batch_num: 96, c_loss:0.0067, val_loss: 0.4760, loss:0.1626 \n",
      "batch_num: 97, c_loss:0.0129, val_loss: 0.4853, loss:0.1752 \n",
      "batch_num: 98, c_loss:0.0101, val_loss: 0.5000, loss:0.1715 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.71631205673759)]))])\n",
      "=====Task: 2=====\n",
      "batch_num: 0, c_loss:2.0662, val_loss: 1.3534, loss:2.2920 \n",
      "batch_num: 1, c_loss:1.9813, val_loss: 1.2935, loss:2.5255 \n",
      "batch_num: 2, c_loss:1.7517, val_loss: 1.2164, loss:2.7605 \n",
      "batch_num: 3, c_loss:1.5182, val_loss: 1.1251, loss:2.3305 \n",
      "batch_num: 4, c_loss:1.4759, val_loss: 1.0209, loss:2.3269 \n",
      "batch_num: 5, c_loss:1.3000, val_loss: 0.9074, loss:2.1992 \n",
      "batch_num: 6, c_loss:1.1965, val_loss: 0.8125, loss:2.0072 \n",
      "batch_num: 7, c_loss:1.0915, val_loss: 0.7351, loss:1.9497 \n",
      "batch_num: 8, c_loss:0.8756, val_loss: 0.6761, loss:1.5786 \n",
      "batch_num: 9, c_loss:0.7556, val_loss: 0.6261, loss:1.4447 \n",
      "batch_num: 10, c_loss:0.6171, val_loss: 0.5885, loss:1.3608 \n",
      "batch_num: 11, c_loss:0.5736, val_loss: 0.5539, loss:1.4356 \n",
      "batch_num: 12, c_loss:0.5634, val_loss: 0.5212, loss:1.3614 \n",
      "batch_num: 13, c_loss:0.5035, val_loss: 0.4975, loss:1.4018 \n",
      "batch_num: 14, c_loss:0.4184, val_loss: 0.4837, loss:1.2016 \n",
      "batch_num: 15, c_loss:0.3998, val_loss: 0.4747, loss:1.1219 \n",
      "batch_num: 16, c_loss:0.4295, val_loss: 0.4864, loss:1.1854 \n",
      "batch_num: 17, c_loss:0.3288, val_loss: 0.5041, loss:1.1887 \n",
      "batch_num: 18, c_loss:0.3379, val_loss: 0.5411, loss:1.1452 \n",
      "batch_num: 19, c_loss:0.3082, val_loss: 0.5873, loss:1.0746 \n",
      "batch_num: 20, c_loss:0.2556, val_loss: 0.6230, loss:1.1207 \n",
      "batch_num: 21, c_loss:0.2326, val_loss: 0.6595, loss:1.0980 \n",
      "batch_num: 22, c_loss:0.2606, val_loss: 0.6745, loss:1.0509 \n",
      "batch_num: 23, c_loss:0.2236, val_loss: 0.6911, loss:1.0585 \n",
      "batch_num: 24, c_loss:0.2120, val_loss: 0.7192, loss:0.9773 \n",
      "batch_num: 25, c_loss:0.1945, val_loss: 0.7557, loss:0.9568 \n",
      "batch_num: 26, c_loss:0.1277, val_loss: 0.7947, loss:1.0047 \n",
      "batch_num: 27, c_loss:0.1437, val_loss: 0.8320, loss:0.9368 \n",
      "batch_num: 28, c_loss:0.1722, val_loss: 0.8894, loss:1.0146 \n",
      "batch_num: 29, c_loss:0.1907, val_loss: 0.9570, loss:0.8997 \n",
      "batch_num: 30, c_loss:0.2828, val_loss: 1.0176, loss:1.1327 Leader changed with val acc  0.4747\n",
      "batch_num: 31, c_loss:0.2149, val_loss: 1.0717, loss:1.0710 \n",
      "batch_num: 32, c_loss:0.1566, val_loss: 1.1233, loss:0.9683 \n",
      "batch_num: 33, c_loss:0.1495, val_loss: 1.1623, loss:0.9219 \n",
      "batch_num: 34, c_loss:0.1247, val_loss: 1.1901, loss:0.8365 \n",
      "batch_num: 35, c_loss:0.1418, val_loss: 1.2064, loss:0.9022 \n",
      "batch_num: 36, c_loss:0.1940, val_loss: 1.2331, loss:0.9155 \n",
      "batch_num: 37, c_loss:0.1382, val_loss: 1.2547, loss:0.8052 \n",
      "batch_num: 38, c_loss:0.1322, val_loss: 1.2810, loss:0.7608 \n",
      "batch_num: 39, c_loss:0.1132, val_loss: 1.3077, loss:0.7778 \n",
      "batch_num: 40, c_loss:0.1401, val_loss: 1.3422, loss:0.7682 \n",
      "batch_num: 41, c_loss:0.1215, val_loss: 1.3745, loss:0.7519 \n",
      "batch_num: 42, c_loss:0.1581, val_loss: 1.3859, loss:0.7464 \n",
      "batch_num: 43, c_loss:0.1456, val_loss: 1.3840, loss:0.7307 \n",
      "batch_num: 44, c_loss:0.2000, val_loss: 1.3729, loss:0.7987 \n",
      "batch_num: 45, c_loss:0.1512, val_loss: 1.3694, loss:0.7597 \n",
      "batch_num: 46, c_loss:0.0606, val_loss: 1.3992, loss:0.6804 \n",
      "batch_num: 47, c_loss:0.1601, val_loss: 1.4295, loss:0.7199 \n",
      "batch_num: 48, c_loss:0.0955, val_loss: 1.4847, loss:0.6302 \n",
      "batch_num: 49, c_loss:0.0739, val_loss: 1.5284, loss:0.5747 \n",
      "batch_num: 50, c_loss:0.1359, val_loss: 1.5796, loss:0.6150 \n",
      "batch_num: 51, c_loss:0.0794, val_loss: 1.6158, loss:0.6172 \n",
      "batch_num: 52, c_loss:0.0637, val_loss: 1.6368, loss:0.5810 \n",
      "batch_num: 53, c_loss:0.1220, val_loss: 1.6466, loss:0.6649 \n",
      "batch_num: 54, c_loss:0.1603, val_loss: 1.6501, loss:0.6622 \n",
      "batch_num: 55, c_loss:0.0606, val_loss: 1.6399, loss:0.6297 \n",
      "batch_num: 56, c_loss:0.0846, val_loss: 1.6474, loss:0.6538 \n",
      "batch_num: 57, c_loss:0.1255, val_loss: 1.6595, loss:0.6735 \n",
      "batch_num: 58, c_loss:0.1127, val_loss: 1.6731, loss:0.6258 \n",
      "batch_num: 59, c_loss:0.1187, val_loss: 1.7032, loss:0.5837 \n",
      "batch_num: 60, c_loss:0.1240, val_loss: 1.6881, loss:0.6158 \n",
      "batch_num: 61, c_loss:0.0905, val_loss: 1.7034, loss:0.6279 \n",
      "batch_num: 62, c_loss:0.1159, val_loss: 1.7197, loss:0.6362 \n",
      "batch_num: 63, c_loss:0.0848, val_loss: 1.7392, loss:0.5294 \n",
      "batch_num: 64, c_loss:0.1176, val_loss: 1.7620, loss:0.5383 \n",
      "batch_num: 65, c_loss:0.0727, val_loss: 1.7762, loss:0.5567 \n",
      "batch_num: 66, c_loss:0.1108, val_loss: 1.7899, loss:0.5363 \n",
      "batch_num: 67, c_loss:0.0928, val_loss: 1.8108, loss:0.5993 \n",
      "batch_num: 68, c_loss:0.0648, val_loss: 1.8304, loss:0.5375 \n",
      "batch_num: 69, c_loss:0.1354, val_loss: 1.8602, loss:0.5906 \n",
      "batch_num: 70, c_loss:0.0989, val_loss: 1.8707, loss:0.5699 \n",
      "batch_num: 71, c_loss:0.0842, val_loss: 1.8876, loss:0.5031 \n",
      "batch_num: 72, c_loss:0.0743, val_loss: 1.8924, loss:0.5005 \n",
      "batch_num: 73, c_loss:0.1169, val_loss: 1.8760, loss:0.5607 \n",
      "batch_num: 74, c_loss:0.1036, val_loss: 1.8838, loss:0.5571 \n",
      "batch_num: 75, c_loss:0.0871, val_loss: 1.9027, loss:0.5332 \n",
      "batch_num: 76, c_loss:0.0453, val_loss: 1.9396, loss:0.5136 \n",
      "batch_num: 77, c_loss:0.1316, val_loss: 1.9695, loss:0.5847 \n",
      "batch_num: 78, c_loss:0.0603, val_loss: 1.9762, loss:0.4875 \n",
      "batch_num: 79, c_loss:0.0958, val_loss: 1.9883, loss:0.5229 \n",
      "batch_num: 80, c_loss:0.1148, val_loss: 1.9795, loss:0.5440 \n",
      "batch_num: 81, c_loss:0.0742, val_loss: 1.9807, loss:0.5059 \n",
      "batch_num: 82, c_loss:0.0548, val_loss: 1.9746, loss:0.4861 \n",
      "batch_num: 83, c_loss:0.1928, val_loss: 1.9839, loss:0.5969 \n",
      "batch_num: 84, c_loss:0.0755, val_loss: 1.9882, loss:0.5061 \n",
      "batch_num: 85, c_loss:0.0548, val_loss: 1.9845, loss:0.4868 \n",
      "batch_num: 86, c_loss:0.0980, val_loss: 1.9787, loss:0.5392 \n",
      "batch_num: 87, c_loss:0.0858, val_loss: 1.9734, loss:0.4816 \n",
      "batch_num: 88, c_loss:0.0616, val_loss: 1.9802, loss:0.4853 \n",
      "batch_num: 89, c_loss:0.0961, val_loss: 1.9822, loss:0.5625 \n",
      "batch_num: 90, c_loss:0.0784, val_loss: 1.9838, loss:0.5031 \n",
      "batch_num: 91, c_loss:0.1131, val_loss: 1.9966, loss:0.5143 \n",
      "batch_num: 92, c_loss:0.0657, val_loss: 1.9927, loss:0.4791 \n",
      "batch_num: 93, c_loss:0.0584, val_loss: 1.9923, loss:0.5355 \n",
      "batch_num: 94, c_loss:0.1302, val_loss: 2.0241, loss:0.5838 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 0.0)])), ('2', OrderedDict([('2', 98.38393731635651)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.71631205673759), ('2', 91.06382978723404)])), ('2', OrderedDict([('2', 92.01762977473065)]))])\n",
      "=====Task: 3=====\n",
      "batch_num: 0, c_loss:5.8402, val_loss: 3.0124, loss:10.1133 \n",
      "batch_num: 1, c_loss:5.6847, val_loss: 2.8735, loss:9.8508 \n",
      "batch_num: 2, c_loss:5.4990, val_loss: 2.6921, loss:9.4392 \n",
      "batch_num: 3, c_loss:4.9921, val_loss: 2.4718, loss:8.5928 \n",
      "batch_num: 4, c_loss:4.6643, val_loss: 2.2308, loss:7.9445 \n",
      "batch_num: 5, c_loss:4.1275, val_loss: 1.9861, loss:7.0102 \n",
      "batch_num: 6, c_loss:3.6092, val_loss: 1.7495, loss:6.0892 \n",
      "batch_num: 7, c_loss:3.0714, val_loss: 1.5217, loss:5.2063 \n",
      "batch_num: 8, c_loss:2.5018, val_loss: 1.3292, loss:4.3447 \n",
      "batch_num: 9, c_loss:2.0609, val_loss: 1.1584, loss:3.7701 \n",
      "batch_num: 10, c_loss:1.6474, val_loss: 1.0320, loss:3.1002 \n",
      "batch_num: 11, c_loss:1.3695, val_loss: 0.9595, loss:2.5011 \n",
      "batch_num: 12, c_loss:1.1188, val_loss: 0.9494, loss:2.3051 \n",
      "batch_num: 13, c_loss:0.9017, val_loss: 0.9813, loss:2.1069 \n",
      "batch_num: 14, c_loss:0.7592, val_loss: 1.0463, loss:2.1445 \n",
      "batch_num: 15, c_loss:0.6037, val_loss: 1.1295, loss:2.2137 \n",
      "batch_num: 16, c_loss:0.5120, val_loss: 1.2314, loss:2.2067 \n",
      "batch_num: 17, c_loss:0.4123, val_loss: 1.3364, loss:2.1856 \n",
      "batch_num: 18, c_loss:0.4261, val_loss: 1.4548, loss:2.2772 \n",
      "batch_num: 19, c_loss:0.3108, val_loss: 1.5637, loss:2.0455 \n",
      "batch_num: 20, c_loss:0.2664, val_loss: 1.6627, loss:2.0319 \n",
      "batch_num: 21, c_loss:0.2463, val_loss: 1.7452, loss:1.9734 \n",
      "batch_num: 22, c_loss:0.2192, val_loss: 1.8352, loss:1.8492 \n",
      "batch_num: 23, c_loss:0.2681, val_loss: 1.9085, loss:1.7635 \n",
      "batch_num: 24, c_loss:0.2254, val_loss: 1.9618, loss:1.6854 \n",
      "batch_num: 25, c_loss:0.1844, val_loss: 2.0243, loss:1.5271 \n",
      "batch_num: 26, c_loss:0.1774, val_loss: 2.0818, loss:1.4191 \n",
      "batch_num: 27, c_loss:0.1998, val_loss: 2.1142, loss:1.2448 \n",
      "batch_num: 28, c_loss:0.2220, val_loss: 2.1299, loss:1.1627 \n",
      "batch_num: 29, c_loss:0.1532, val_loss: 2.1663, loss:1.0024 \n",
      "batch_num: 30, c_loss:0.1515, val_loss: 2.1951, loss:0.9712 Leader changed with val acc  0.9494\n",
      "batch_num: 31, c_loss:0.1975, val_loss: 2.2355, loss:1.4775 \n",
      "batch_num: 32, c_loss:0.1623, val_loss: 2.2678, loss:1.4194 \n",
      "batch_num: 33, c_loss:0.1692, val_loss: 2.3181, loss:1.5058 \n",
      "batch_num: 34, c_loss:0.1155, val_loss: 2.3859, loss:1.3124 \n",
      "batch_num: 35, c_loss:0.1303, val_loss: 2.4654, loss:1.2985 \n",
      "batch_num: 36, c_loss:0.1264, val_loss: 2.5420, loss:1.2806 \n",
      "batch_num: 37, c_loss:0.1495, val_loss: 2.5905, loss:1.2379 \n",
      "batch_num: 38, c_loss:0.1307, val_loss: 2.6665, loss:1.1321 \n",
      "batch_num: 39, c_loss:0.1483, val_loss: 2.7144, loss:1.0316 \n",
      "batch_num: 40, c_loss:0.0831, val_loss: 2.7695, loss:0.9009 \n",
      "batch_num: 41, c_loss:0.1274, val_loss: 2.8162, loss:0.8700 \n",
      "batch_num: 42, c_loss:0.0725, val_loss: 2.8591, loss:0.7180 \n",
      "batch_num: 43, c_loss:0.0843, val_loss: 2.9324, loss:0.7617 \n",
      "batch_num: 44, c_loss:0.1013, val_loss: 2.9683, loss:0.6961 \n",
      "batch_num: 45, c_loss:0.0673, val_loss: 3.0073, loss:0.6927 \n",
      "batch_num: 46, c_loss:0.0608, val_loss: 3.0425, loss:0.6726 \n",
      "batch_num: 47, c_loss:0.0735, val_loss: 3.0646, loss:0.7129 \n",
      "batch_num: 48, c_loss:0.0602, val_loss: 3.0589, loss:0.7011 \n",
      "batch_num: 49, c_loss:0.0527, val_loss: 3.0518, loss:0.6771 \n",
      "batch_num: 50, c_loss:0.0780, val_loss: 3.0582, loss:0.6348 \n",
      "batch_num: 51, c_loss:0.0701, val_loss: 3.0497, loss:0.6178 \n",
      "batch_num: 52, c_loss:0.0886, val_loss: 3.0227, loss:0.5957 \n",
      "batch_num: 53, c_loss:0.0608, val_loss: 3.0035, loss:0.5027 \n",
      "batch_num: 54, c_loss:0.0531, val_loss: 2.9836, loss:0.5158 \n",
      "batch_num: 55, c_loss:0.0579, val_loss: 2.9604, loss:0.4696 \n",
      "batch_num: 56, c_loss:0.0452, val_loss: 2.9602, loss:0.5039 \n",
      "batch_num: 57, c_loss:0.0520, val_loss: 2.9664, loss:0.4861 \n",
      "batch_num: 58, c_loss:0.0770, val_loss: 2.9630, loss:0.4859 \n",
      "batch_num: 59, c_loss:0.0617, val_loss: 2.9793, loss:0.4774 \n",
      "batch_num: 60, c_loss:0.0507, val_loss: 2.9776, loss:0.4529 \n",
      "batch_num: 61, c_loss:0.1012, val_loss: 2.9628, loss:0.4953 \n",
      "batch_num: 62, c_loss:0.0474, val_loss: 2.9864, loss:0.4549 \n",
      "batch_num: 63, c_loss:0.0612, val_loss: 3.0020, loss:0.4365 \n",
      "batch_num: 64, c_loss:0.0546, val_loss: 3.0206, loss:0.4534 \n",
      "batch_num: 65, c_loss:0.0625, val_loss: 3.0592, loss:0.4912 \n",
      "batch_num: 66, c_loss:0.0702, val_loss: 3.0811, loss:0.4523 \n",
      "batch_num: 67, c_loss:0.0580, val_loss: 3.1159, loss:0.4487 \n",
      "batch_num: 68, c_loss:0.0501, val_loss: 3.1362, loss:0.4221 \n",
      "batch_num: 69, c_loss:0.0765, val_loss: 3.1427, loss:0.4653 \n",
      "batch_num: 70, c_loss:0.0419, val_loss: 3.1558, loss:0.4129 \n",
      "batch_num: 71, c_loss:0.0512, val_loss: 3.1564, loss:0.3950 \n",
      "batch_num: 72, c_loss:0.0709, val_loss: 3.1588, loss:0.4260 \n",
      "batch_num: 73, c_loss:0.0569, val_loss: 3.1543, loss:0.4070 \n",
      "batch_num: 74, c_loss:0.0451, val_loss: 3.1538, loss:0.3869 \n",
      "batch_num: 75, c_loss:0.0623, val_loss: 3.1376, loss:0.4120 \n",
      "batch_num: 76, c_loss:0.0484, val_loss: 3.1562, loss:0.4341 \n",
      "batch_num: 77, c_loss:0.0711, val_loss: 3.1763, loss:0.4227 \n",
      "batch_num: 78, c_loss:0.0731, val_loss: 3.2204, loss:0.4007 \n",
      "batch_num: 79, c_loss:0.0454, val_loss: 3.2297, loss:0.4236 \n",
      "batch_num: 80, c_loss:0.0492, val_loss: 3.2391, loss:0.3885 \n",
      "batch_num: 81, c_loss:0.0446, val_loss: 3.2353, loss:0.3846 \n",
      "batch_num: 82, c_loss:0.0591, val_loss: 3.2540, loss:0.4057 \n",
      "batch_num: 83, c_loss:0.0456, val_loss: 3.2455, loss:0.3977 \n",
      "batch_num: 84, c_loss:0.0568, val_loss: 3.2417, loss:0.4300 \n",
      "batch_num: 85, c_loss:0.0484, val_loss: 3.2289, loss:0.3843 \n",
      "batch_num: 86, c_loss:0.0787, val_loss: 3.2462, loss:0.4222 \n",
      "batch_num: 87, c_loss:0.0584, val_loss: 3.2153, loss:0.3590 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 0.0), ('3', 0.0)])), ('2', OrderedDict([('2', 98.38393731635651), ('3', 0.0)])), ('3', OrderedDict([('3', 99.62646744930629)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.71631205673759), ('2', 91.06382978723404), ('3', 63.78250591016548)])), ('2', OrderedDict([('2', 92.01762977473065), ('3', 67.43388834476004)])), ('3', OrderedDict([('3', 80.14941302027748)]))])\n",
      "=====Task: 4=====\n",
      "batch_num: 0, c_loss:6.0383, val_loss: 3.7661, loss:10.2491 \n",
      "batch_num: 1, c_loss:5.6690, val_loss: 3.6342, loss:9.6735 \n",
      "batch_num: 2, c_loss:5.4872, val_loss: 3.4349, loss:9.4163 \n",
      "batch_num: 3, c_loss:5.1266, val_loss: 3.2059, loss:8.8025 \n",
      "batch_num: 4, c_loss:4.5783, val_loss: 2.9443, loss:8.0099 \n",
      "batch_num: 5, c_loss:4.0335, val_loss: 2.6721, loss:7.2601 \n",
      "batch_num: 6, c_loss:3.4946, val_loss: 2.4084, loss:6.4500 \n",
      "batch_num: 7, c_loss:2.8601, val_loss: 2.1405, loss:5.5225 \n",
      "batch_num: 8, c_loss:2.4601, val_loss: 1.8938, loss:4.8708 \n",
      "batch_num: 9, c_loss:2.0276, val_loss: 1.6655, loss:4.1645 \n",
      "batch_num: 10, c_loss:1.5422, val_loss: 1.4743, loss:3.3769 \n",
      "batch_num: 11, c_loss:1.0297, val_loss: 1.3518, loss:2.7557 \n",
      "batch_num: 12, c_loss:0.7782, val_loss: 1.3005, loss:2.2053 \n",
      "batch_num: 13, c_loss:0.6419, val_loss: 1.3257, loss:1.8284 \n",
      "batch_num: 14, c_loss:0.4540, val_loss: 1.4212, loss:1.4780 \n",
      "batch_num: 15, c_loss:0.3683, val_loss: 1.5558, loss:1.2493 \n",
      "batch_num: 16, c_loss:0.3548, val_loss: 1.7061, loss:1.1961 \n",
      "batch_num: 17, c_loss:0.2532, val_loss: 1.8727, loss:1.1557 \n",
      "batch_num: 18, c_loss:0.2319, val_loss: 2.0306, loss:1.2068 \n",
      "batch_num: 19, c_loss:0.1734, val_loss: 2.1733, loss:1.2531 \n",
      "batch_num: 20, c_loss:0.1457, val_loss: 2.3070, loss:1.2783 \n",
      "batch_num: 21, c_loss:0.1165, val_loss: 2.4348, loss:1.3200 \n",
      "batch_num: 22, c_loss:0.1266, val_loss: 2.5549, loss:1.3485 \n",
      "batch_num: 23, c_loss:0.1167, val_loss: 2.6490, loss:1.3544 \n",
      "batch_num: 24, c_loss:0.0990, val_loss: 2.7251, loss:1.3045 \n",
      "batch_num: 25, c_loss:0.1097, val_loss: 2.7644, loss:1.2717 \n",
      "batch_num: 26, c_loss:0.1043, val_loss: 2.8038, loss:1.2592 \n",
      "batch_num: 27, c_loss:0.0938, val_loss: 2.8591, loss:1.1297 \n",
      "batch_num: 28, c_loss:0.0612, val_loss: 2.9139, loss:1.0276 \n",
      "batch_num: 29, c_loss:0.0772, val_loss: 2.9472, loss:0.9270 \n",
      "batch_num: 30, c_loss:0.0574, val_loss: 2.9826, loss:0.8644 Leader changed with val acc  1.3005\n",
      "batch_num: 31, c_loss:0.0867, val_loss: 2.9836, loss:1.4537 \n",
      "batch_num: 32, c_loss:0.0647, val_loss: 3.0108, loss:1.3380 \n",
      "batch_num: 33, c_loss:0.0568, val_loss: 3.0684, loss:1.2048 \n",
      "batch_num: 34, c_loss:0.0466, val_loss: 3.0793, loss:1.0844 \n",
      "batch_num: 35, c_loss:0.0900, val_loss: 3.1186, loss:1.0240 \n",
      "batch_num: 36, c_loss:0.0791, val_loss: 3.1412, loss:0.9196 \n",
      "batch_num: 37, c_loss:0.0513, val_loss: 3.1697, loss:0.8286 \n",
      "batch_num: 38, c_loss:0.0377, val_loss: 3.1858, loss:0.7807 \n",
      "batch_num: 39, c_loss:0.0371, val_loss: 3.1877, loss:0.7790 \n",
      "batch_num: 40, c_loss:0.0371, val_loss: 3.1757, loss:0.7189 \n",
      "batch_num: 41, c_loss:0.0695, val_loss: 3.1691, loss:0.7712 \n",
      "batch_num: 42, c_loss:0.0502, val_loss: 3.1524, loss:0.7646 \n",
      "batch_num: 43, c_loss:0.0370, val_loss: 3.1501, loss:0.7130 \n",
      "batch_num: 44, c_loss:0.0560, val_loss: 3.1526, loss:0.7599 \n",
      "batch_num: 45, c_loss:0.0385, val_loss: 3.1684, loss:0.7359 \n",
      "batch_num: 46, c_loss:0.0532, val_loss: 3.1618, loss:0.6917 \n",
      "batch_num: 47, c_loss:0.0473, val_loss: 3.1778, loss:0.6781 \n",
      "batch_num: 48, c_loss:0.0400, val_loss: 3.1917, loss:0.7137 \n",
      "batch_num: 49, c_loss:0.0482, val_loss: 3.1961, loss:0.6693 \n",
      "batch_num: 50, c_loss:0.0253, val_loss: 3.2141, loss:0.5995 \n",
      "batch_num: 51, c_loss:0.0399, val_loss: 3.2090, loss:0.5577 \n",
      "batch_num: 52, c_loss:0.0315, val_loss: 3.2305, loss:0.5673 \n",
      "batch_num: 53, c_loss:0.0499, val_loss: 3.2266, loss:0.5467 \n",
      "batch_num: 54, c_loss:0.0317, val_loss: 3.2545, loss:0.5085 \n",
      "batch_num: 55, c_loss:0.0478, val_loss: 3.2459, loss:0.5628 \n",
      "batch_num: 56, c_loss:0.0309, val_loss: 3.2575, loss:0.4832 \n",
      "batch_num: 57, c_loss:0.0421, val_loss: 3.2559, loss:0.5268 \n",
      "batch_num: 58, c_loss:0.0421, val_loss: 3.2610, loss:0.5111 \n",
      "batch_num: 59, c_loss:0.0248, val_loss: 3.2496, loss:0.4972 \n",
      "batch_num: 60, c_loss:0.0359, val_loss: 3.2596, loss:0.4989 \n",
      "batch_num: 61, c_loss:0.0290, val_loss: 3.2623, loss:0.4339 \n",
      "batch_num: 62, c_loss:0.0301, val_loss: 3.2492, loss:0.3961 \n",
      "batch_num: 63, c_loss:0.0421, val_loss: 3.2481, loss:0.4278 \n",
      "batch_num: 64, c_loss:0.0307, val_loss: 3.2564, loss:0.3944 \n",
      "batch_num: 65, c_loss:0.0287, val_loss: 3.2621, loss:0.3847 \n",
      "batch_num: 66, c_loss:0.0221, val_loss: 3.2650, loss:0.3912 \n",
      "batch_num: 67, c_loss:0.0346, val_loss: 3.2682, loss:0.3779 \n",
      "batch_num: 68, c_loss:0.0337, val_loss: 3.2765, loss:0.3884 \n",
      "batch_num: 69, c_loss:0.0315, val_loss: 3.2907, loss:0.3715 \n",
      "batch_num: 70, c_loss:0.0256, val_loss: 3.3076, loss:0.3603 \n",
      "batch_num: 71, c_loss:0.0516, val_loss: 3.3117, loss:0.3710 \n",
      "batch_num: 72, c_loss:0.0223, val_loss: 3.3311, loss:0.3359 \n",
      "batch_num: 73, c_loss:0.0494, val_loss: 3.3468, loss:0.3954 \n",
      "batch_num: 74, c_loss:0.0329, val_loss: 3.3587, loss:0.3551 \n",
      "batch_num: 75, c_loss:0.0312, val_loss: 3.3654, loss:0.3586 \n",
      "batch_num: 76, c_loss:0.0294, val_loss: 3.3709, loss:0.3644 \n",
      "batch_num: 77, c_loss:0.0295, val_loss: 3.4024, loss:0.3554 \n",
      "batch_num: 78, c_loss:0.0305, val_loss: 3.4339, loss:0.3535 \n",
      "batch_num: 79, c_loss:0.0568, val_loss: 3.4441, loss:0.3384 \n",
      "batch_num: 80, c_loss:0.0306, val_loss: 3.4435, loss:0.3117 \n",
      "batch_num: 81, c_loss:0.0257, val_loss: 3.4512, loss:0.3321 \n",
      "batch_num: 82, c_loss:0.0279, val_loss: 3.4632, loss:0.3204 \n",
      "batch_num: 83, c_loss:0.0274, val_loss: 3.4672, loss:0.3185 \n",
      "batch_num: 84, c_loss:0.0237, val_loss: 3.4764, loss:0.3089 \n",
      "batch_num: 85, c_loss:0.0446, val_loss: 3.4803, loss:0.3459 \n",
      "batch_num: 86, c_loss:0.0560, val_loss: 3.4894, loss:0.3689 \n",
      "batch_num: 87, c_loss:0.0331, val_loss: 3.4841, loss:0.3352 \n",
      "batch_num: 88, c_loss:0.0275, val_loss: 3.4975, loss:0.3021 \n",
      "batch_num: 89, c_loss:0.0181, val_loss: 3.5201, loss:0.2978 \n",
      "batch_num: 90, c_loss:0.0317, val_loss: 3.5195, loss:0.3162 \n",
      "batch_num: 91, c_loss:0.0324, val_loss: 3.5237, loss:0.2963 \n",
      "batch_num: 92, c_loss:0.0388, val_loss: 3.5188, loss:0.3111 \n",
      "batch_num: 93, c_loss:0.0326, val_loss: 3.5070, loss:0.3288 \n",
      "batch_num: 94, c_loss:0.0313, val_loss: 3.5205, loss:0.3393 \n",
      "batch_num: 95, c_loss:0.0299, val_loss: 3.5329, loss:0.3182 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 0.0), ('3', 0.0), ('4', 0.0)])), ('2', OrderedDict([('2', 98.38393731635651), ('3', 0.0), ('4', 0.0)])), ('3', OrderedDict([('3', 99.62646744930629), ('4', 0.05336179295624333)])), ('4', OrderedDict([('4', 99.59718026183283)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.71631205673759), ('2', 91.06382978723404), ('3', 63.78250591016548), ('4', 1.3711583924349882)])), ('2', OrderedDict([('2', 92.01762977473065), ('3', 67.43388834476004), ('4', 11.655239960822723)])), ('3', OrderedDict([('3', 80.14941302027748), ('4', 90.50160085378869)])), ('4', OrderedDict([('4', 92.69889224572005)]))])\n",
      "=====Task: 5=====\n",
      "batch_num: 0, c_loss:7.0566, val_loss: 4.0846, loss:10.1931 \n",
      "batch_num: 1, c_loss:6.8013, val_loss: 3.8925, loss:9.7937 \n",
      "batch_num: 2, c_loss:6.6942, val_loss: 3.6912, loss:9.6342 \n",
      "batch_num: 3, c_loss:6.0427, val_loss: 3.4394, loss:8.7295 \n",
      "batch_num: 4, c_loss:5.5471, val_loss: 3.1902, loss:8.0696 \n",
      "batch_num: 5, c_loss:5.0835, val_loss: 2.9410, loss:7.4285 \n",
      "batch_num: 6, c_loss:4.7504, val_loss: 2.6848, loss:6.8399 \n",
      "batch_num: 7, c_loss:4.0701, val_loss: 2.4330, loss:5.9091 \n",
      "batch_num: 8, c_loss:3.6179, val_loss: 2.2024, loss:5.2933 \n",
      "batch_num: 9, c_loss:3.1686, val_loss: 1.9895, loss:4.5905 \n",
      "batch_num: 10, c_loss:2.6851, val_loss: 1.8090, loss:3.8937 \n",
      "batch_num: 11, c_loss:2.2713, val_loss: 1.6664, loss:3.2556 \n",
      "batch_num: 12, c_loss:1.9371, val_loss: 1.5771, loss:2.7915 \n",
      "batch_num: 13, c_loss:1.5899, val_loss: 1.5432, loss:2.3819 \n",
      "batch_num: 14, c_loss:1.3243, val_loss: 1.5690, loss:2.1367 \n",
      "batch_num: 15, c_loss:1.0631, val_loss: 1.6472, loss:1.8654 \n",
      "batch_num: 16, c_loss:0.9642, val_loss: 1.7585, loss:1.7824 \n",
      "batch_num: 17, c_loss:0.8096, val_loss: 1.8967, loss:1.7472 \n",
      "batch_num: 18, c_loss:0.6564, val_loss: 2.0530, loss:1.6654 \n",
      "batch_num: 19, c_loss:0.5997, val_loss: 2.2162, loss:1.6882 \n",
      "batch_num: 20, c_loss:0.5279, val_loss: 2.3919, loss:1.6763 \n",
      "batch_num: 21, c_loss:0.4459, val_loss: 2.5524, loss:1.6223 \n",
      "batch_num: 22, c_loss:0.3938, val_loss: 2.7136, loss:1.5584 \n",
      "batch_num: 23, c_loss:0.3721, val_loss: 2.8574, loss:1.5081 \n",
      "batch_num: 24, c_loss:0.2885, val_loss: 2.9953, loss:1.4458 \n",
      "batch_num: 25, c_loss:0.3139, val_loss: 3.1172, loss:1.3871 \n",
      "batch_num: 26, c_loss:0.2641, val_loss: 3.2150, loss:1.3117 \n",
      "batch_num: 27, c_loss:0.2295, val_loss: 3.3171, loss:1.1856 \n",
      "batch_num: 28, c_loss:0.2275, val_loss: 3.3912, loss:1.1338 \n",
      "batch_num: 29, c_loss:0.1964, val_loss: 3.4650, loss:1.0419 \n",
      "batch_num: 30, c_loss:0.2042, val_loss: 3.5379, loss:0.9710 Leader changed with val acc  1.5432\n",
      "batch_num: 31, c_loss:0.1905, val_loss: 3.5963, loss:1.0677 \n",
      "batch_num: 32, c_loss:0.2018, val_loss: 3.6549, loss:1.0488 \n",
      "batch_num: 33, c_loss:0.1284, val_loss: 3.7149, loss:0.8899 \n",
      "batch_num: 34, c_loss:0.1213, val_loss: 3.7807, loss:0.8371 \n",
      "batch_num: 35, c_loss:0.1574, val_loss: 3.8386, loss:0.8358 \n",
      "batch_num: 36, c_loss:0.1653, val_loss: 3.8923, loss:0.8087 \n",
      "batch_num: 37, c_loss:0.1268, val_loss: 3.9352, loss:0.6804 \n",
      "batch_num: 38, c_loss:0.1588, val_loss: 3.9842, loss:0.7669 \n",
      "batch_num: 39, c_loss:0.1043, val_loss: 4.0221, loss:0.7014 \n",
      "batch_num: 40, c_loss:0.1253, val_loss: 4.0565, loss:0.7112 \n",
      "batch_num: 41, c_loss:0.0967, val_loss: 4.0868, loss:0.6592 \n",
      "batch_num: 42, c_loss:0.1443, val_loss: 4.1155, loss:0.7041 \n",
      "batch_num: 43, c_loss:0.0983, val_loss: 4.1089, loss:0.6524 \n",
      "batch_num: 44, c_loss:0.1207, val_loss: 4.1104, loss:0.6276 \n",
      "batch_num: 45, c_loss:0.1303, val_loss: 4.1039, loss:0.6377 \n",
      "batch_num: 46, c_loss:0.1537, val_loss: 4.1013, loss:0.5987 \n",
      "batch_num: 47, c_loss:0.1139, val_loss: 4.1197, loss:0.5551 \n",
      "batch_num: 48, c_loss:0.1774, val_loss: 4.1111, loss:0.5669 \n",
      "batch_num: 49, c_loss:0.0972, val_loss: 4.1231, loss:0.4967 \n",
      "batch_num: 50, c_loss:0.1283, val_loss: 4.1455, loss:0.5137 \n",
      "batch_num: 51, c_loss:0.1079, val_loss: 4.1610, loss:0.4635 \n",
      "batch_num: 52, c_loss:0.1306, val_loss: 4.1639, loss:0.5341 \n",
      "batch_num: 53, c_loss:0.1039, val_loss: 4.1736, loss:0.4784 \n",
      "batch_num: 54, c_loss:0.1339, val_loss: 4.1914, loss:0.4950 \n",
      "batch_num: 55, c_loss:0.1646, val_loss: 4.2090, loss:0.5079 \n",
      "batch_num: 56, c_loss:0.1064, val_loss: 4.2272, loss:0.4488 \n",
      "batch_num: 57, c_loss:0.1291, val_loss: 4.2447, loss:0.4518 \n",
      "batch_num: 58, c_loss:0.1063, val_loss: 4.2557, loss:0.4091 \n",
      "batch_num: 59, c_loss:0.0937, val_loss: 4.2503, loss:0.3811 \n",
      "batch_num: 60, c_loss:0.0806, val_loss: 4.2631, loss:0.3577 \n",
      "batch_num: 61, c_loss:0.1428, val_loss: 4.2690, loss:0.4334 \n",
      "batch_num: 62, c_loss:0.1113, val_loss: 4.2632, loss:0.3875 \n",
      "batch_num: 63, c_loss:0.0770, val_loss: 4.2668, loss:0.3684 \n",
      "batch_num: 64, c_loss:0.0812, val_loss: 4.2839, loss:0.3652 \n",
      "batch_num: 65, c_loss:0.0846, val_loss: 4.2873, loss:0.3387 \n",
      "batch_num: 66, c_loss:0.0780, val_loss: 4.2906, loss:0.3456 \n",
      "batch_num: 67, c_loss:0.0826, val_loss: 4.3118, loss:0.3396 \n",
      "batch_num: 68, c_loss:0.1276, val_loss: 4.3239, loss:0.3986 \n",
      "batch_num: 69, c_loss:0.0797, val_loss: 4.3221, loss:0.3360 \n",
      "batch_num: 70, c_loss:0.1212, val_loss: 4.3482, loss:0.3765 \n",
      "batch_num: 71, c_loss:0.0647, val_loss: 4.3347, loss:0.3179 \n",
      "batch_num: 72, c_loss:0.1072, val_loss: 4.3424, loss:0.3645 \n",
      "batch_num: 73, c_loss:0.0896, val_loss: 4.3582, loss:0.3610 \n",
      "batch_num: 74, c_loss:0.0711, val_loss: 4.3934, loss:0.3474 \n",
      "batch_num: 75, c_loss:0.1054, val_loss: 4.3999, loss:0.3652 \n",
      "batch_num: 76, c_loss:0.0808, val_loss: 4.4150, loss:0.3452 \n",
      "batch_num: 77, c_loss:0.1351, val_loss: 4.4424, loss:0.3706 \n",
      "batch_num: 78, c_loss:0.0493, val_loss: 4.4479, loss:0.2968 \n",
      "batch_num: 79, c_loss:0.0757, val_loss: 4.4515, loss:0.3241 \n",
      "batch_num: 80, c_loss:0.1341, val_loss: 4.4592, loss:0.3655 \n",
      "batch_num: 81, c_loss:0.0873, val_loss: 4.4472, loss:0.3199 \n",
      "batch_num: 82, c_loss:0.0880, val_loss: 4.4606, loss:0.3178 \n",
      "batch_num: 83, c_loss:0.1585, val_loss: 4.4528, loss:0.3779 \n",
      "batch_num: 84, c_loss:0.1128, val_loss: 4.4783, loss:0.3514 \n",
      "batch_num: 85, c_loss:0.0673, val_loss: 4.4923, loss:0.3031 \n",
      "batch_num: 86, c_loss:0.0997, val_loss: 4.4887, loss:0.3160 \n",
      "batch_num: 87, c_loss:0.1303, val_loss: 4.5005, loss:0.3566 \n",
      "batch_num: 88, c_loss:0.0879, val_loss: 4.5053, loss:0.3234 \n",
      "batch_num: 89, c_loss:0.0958, val_loss: 4.5123, loss:0.3230 \n",
      "batch_num: 90, c_loss:0.1245, val_loss: 4.5315, loss:0.3561 \n",
      "batch_num: 91, c_loss:0.0596, val_loss: 4.5357, loss:0.2751 \n",
      "batch_num: 92, c_loss:0.0571, val_loss: 4.5540, loss:0.3087 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('2', OrderedDict([('2', 98.38393731635651), ('3', 0.0), ('4', 0.0), ('5', 0.0)])), ('3', OrderedDict([('3', 99.62646744930629), ('4', 0.05336179295624333), ('5', 0.0)])), ('4', OrderedDict([('4', 99.59718026183283), ('5', 0.0)])), ('5', OrderedDict([('5', 98.08371154815936)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.71631205673759), ('2', 91.06382978723404), ('3', 63.78250591016548), ('4', 1.3711583924349882), ('5', 0.0)])), ('2', OrderedDict([('2', 92.01762977473065), ('3', 67.43388834476004), ('4', 11.655239960822723), ('5', 5.533790401567091)])), ('3', OrderedDict([('3', 80.14941302027748), ('4', 90.50160085378869), ('5', 83.1376734258271)])), ('4', OrderedDict([('4', 92.69889224572005), ('5', 90.38267875125881)])), ('5', OrderedDict([('5', 54.563792233988906)]))])\n",
      "follower Task 1 average acc: 99.66903073286052\n",
      "follower Task 2 average acc: 49.191968658178254\n",
      "follower Task 3 average acc: 33.2088224831021\n",
      "follower Task 4 average acc: 24.912635513697268\n",
      "follower Task 5 average acc: 19.61674230963187\n",
      "leader Task 1 average acc: 99.71631205673759\n",
      "leader Task 2 average acc: 91.54072978098235\n",
      "leader Task 3 average acc: 70.45526909173434\n",
      "leader Task 4 average acc: 49.05672286319161\n",
      "leader Task 5 average acc: 46.72358696252839\n"
     ]
    }
   ],
   "source": [
    "avg_acc_history,leader_avg_acc_history = train(task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torchvision\\nimport os\\nfrom os import path\\nimport copy\\nimport numpy as np\\nimport torch.utils.data as data\\nfrom torchvision import transforms\\nfrom collections import OrderedDict\\n\\nbatch_size = 128\\nrepeat = 10\\nepoches = 1\\nalpha = 4\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\nclass CacheClassLabel(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that has a quick access to all labels of data.\\n    \"\"\"\\n    def __init__(self, dataset):\\n        super(CacheClassLabel, self).__init__()\\n        self.dataset = dataset\\n        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\\n        print(dataset.root)\\n        label_cache_filename = dataset.root + \\'/\\' +\\'_\\'+str(len(dataset))+\\'.pth\\'\\n        if path.exists(label_cache_filename):\\n            self.labels = torch.load(label_cache_filename)\\n        else:\\n            for i, data in enumerate(dataset):\\n                self.labels[i] = data[1]\\n            torch.save(self.labels, label_cache_filename)\\n        self.number_classes = len(torch.unique(self.labels))\\n\\n    def __len__(self):\\n        return len(self.dataset)\\n\\n    def __getitem__(self, index):\\n        img,target = self.dataset[index]\\n        return img, target\\n    \\nclass AppendName(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that also return the name of the dataset/task\\n    \"\"\"\\n    def __init__(self, dataset, name, first_class_ind=0):\\n        super(AppendName,self).__init__()\\n        self.dataset = dataset\\n        self.name = name\\n        self.first_class_ind = first_class_ind  # For remapping the class index\\n\\n    def __len__(self):\\n        return len(self.dataset)\\n\\n    def __getitem__(self, index):\\n        img,target = self.dataset[index]\\n        target = target + self.first_class_ind\\n        return img, target, self.name\\n    \\nclass Subclass(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\\n    \"\"\"\\n    def __init__(self, dataset, class_list, remap=True):\\n        super(Subclass,self).__init__()\\n        assert isinstance(dataset, CacheClassLabel), \\'dataset must be wrapped by CacheClassLabel\\'\\n        self.dataset = dataset\\n        self.class_list = class_list\\n        self.remap = remap\\n        self.indices = []\\n        for c in class_list:\\n            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\\n        if remap:\\n            self.class_mapping = {c: i for i, c in enumerate(class_list)}\\n\\n    def __len__(self):\\n        return len(self.indices)\\n    def __getitem__(self, index):\\n        img,target = self.dataset[self.indices[index]]\\n        if self.remap:\\n            raw_target = target.item() if isinstance(target,torch.Tensor) else target\\n            target = self.class_mapping[raw_target]\\n        return img, target\\n\\ndef SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\\n    assert train_dataset.number_classes==val_dataset.number_classes,\\'Train/Val has different number of classes\\'\\n    num_classes =  train_dataset.number_classes\\n\\n    # Calculate the boundary index of classes for splits\\n    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\\n    split_boundaries = [0, first_split_sz]\\n    while split_boundaries[-1]<num_classes:\\n        split_boundaries.append(split_boundaries[-1]+other_split_sz)\\n    print(\\'split_boundaries:\\',split_boundaries)\\n    assert split_boundaries[-1]==num_classes,\\'Invalid split size\\'\\n\\n    # Assign classes to each splits\\n    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\\n    if not rand_split:\\n        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\\n    else:\\n        randseq = torch.randperm(num_classes)\\n        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\\n    print(class_lists)\\n\\n    # Generate the dicts of splits\\n    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\\n    train_dataset_splits = {}\\n    val_dataset_splits = {}\\n    task_output_space = {}\\n    for name,class_list in class_lists.items():\\n        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\\n        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\\n        task_output_space[name] = len(class_list)\\n\\n    return train_dataset_splits, val_dataset_splits, task_output_space\\n\\ndef MNIST(dataroot, train_aug=False):\\n    val_transform = transforms.Compose([\\n        transforms.Pad(2, fill=0, padding_mode=\\'constant\\'),\\n        transforms.ToTensor(),\\n        transforms.Normalize([0.5], [0.5]),\\n    ])\\n    train_transform = val_transform\\n    if train_aug:\\n        train_transform = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5], [0.5]),\\n        ])\\n\\n    train_dataset = torchvision.datasets.MNIST(\\n        root=dataroot,\\n        train=True,\\n        download=True,\\n        transform=train_transform\\n    )\\n    train_dataset = CacheClassLabel(train_dataset)\\n\\n    val_dataset = torchvision.datasets.MNIST(\\n        dataroot,\\n        train=False,\\n        transform=val_transform\\n    )\\n    val_dataset = CacheClassLabel(val_dataset)\\n\\n    return train_dataset, val_dataset\\n\\ntrain_dataset, val_dataset = MNIST(\\'./data\\', False)\\n\\ntrain_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\\n                                                                          first_split_sz=2,\\n                                                                          other_split_sz=2,\\n                                                                          rand_split=False,\\n                                                                          remap_class=False)\\n\\nclass MLP(nn.Module):\\n    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\\n        super(MLP, self).__init__()\\n        self.in_dim = in_channel*img_sz*img_sz\\n        self.linear = nn.Sequential(\\n            nn.Linear(self.in_dim, hidden_dim),\\n            nn.BatchNorm1d(hidden_dim),\\n            nn.ReLU(inplace=True),\\n            nn.Linear(hidden_dim, hidden_dim),\\n            nn.BatchNorm1d(hidden_dim),\\n            nn.ReLU(inplace=True),\\n        )\\n        self.last = nn.Linear(hidden_dim, out_dim)\\n\\n    def features(self, x):\\n        x = self.linear(x.view(-1,self.in_dim))\\n        return x\\n\\n    def logits(self, x):\\n        x = self.last(x)\\n        return x\\n\\n    def forward(self, x):\\n        x = self.features(x)\\n        x = self.logits(x)\\n        return x\\n\\ndef MLP400():\\n    return MLP(hidden_dim=400)\\n\\nclass AverageMeter(object):\\n    def __init__(self):\\n        self.reset()\\n\\n    def reset(self):\\n        self.val = 0\\n        self.avg = 0\\n        self.sum = 0\\n        self.count = 0\\n\\n    def update(self, val, n=1):\\n        self.val = val\\n        self.sum += val * n\\n        self.count += n\\n        self.avg = float(self.sum) / self.count\\n\\ndef accuracy(output, target):\\n    with torch.no_grad():\\n        _, predicted = torch.max(output.data, 1)\\n        batch_size = target.size(0)\\n        correct = (predicted == target).sum().item() * 100\\n    return correct / batch_size\\n\\ndef accumulate_acc(output, target, meter):\\n    acc = accuracy(output, target)\\n    meter.update(acc, len(target))\\n    return meter\\n\\ndef criterion_fn(criterion, preds, targets, valid_out_dim):\\n    if valid_out_dim != 0:\\n        pred = preds[:,:valid_out_dim]\\n    loss = criterion(pred, targets)\\n    return loss\\n\\ndef train_on_task(model, train_loader, optimizer, criterion, \\n                  valid_out_dim, best_model_wts, task_num, task_names):\\n    leader = MLP400().to(device)\\n    best_loss = float(\\'inf\\')\\n    if (best_model_wts):\\n        leader.load_state_dict(best_model_wts)\\n\\n    for epoch in range(epoches):\\n        train_acc = AverageMeter()\\n        batch_num = 0\\n        for images, labels, _ in train_loader:\\n            images, labels = images.to(device), labels.to(device)\\n\\n            with torch.no_grad():\\n                leader_outputs = leader(images)\\n\\n            model.train()\\n            follower_outputs = model(images)\\n\\n            # reg_loss = 0\\n            # for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\\n                # reg_loss += torch.norm(follower_para - lead_para, p = 2)\\n            \\n            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\\n            loss = c_loss + alpha * torch.mean((follower_outputs - leader_outputs) ** 2)\\n\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n\\n            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\\n            \\n            model.eval()\\n            with torch.no_grad():\\n                val_loss = AverageMeter()\\n\\n                for task in range(task_num + 1):\\n                    val_name = task_names[task]\\n                    val_data = val_dataset_splits[val_name]\\n                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\\n\\n                    for i, (input, target, _) in enumerate(val_loader):\\n                        input, target = input.to(device), target.to(device)\\n                        output = model(input)\\n                        loss_v = criterion(output, target).item()\\n\\n                        val_loss.update(loss_v, len(target))\\n\\n                    if val_loss.avg < best_loss:\\n                        best_loss = val_loss.avg\\n                        best_model_wts = copy.deepcopy(model.state_dict())\\n                        leader.load_state_dict(best_model_wts) \\n            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss_v:.4f}\")\\n            batch_num += 1\\n    return best_model_wts, best_loss\\n\\ndef train(task_names):\\n    acc_table = OrderedDict()\\n    valid_out_dim = 0\\n\\n    model = MLP400().to(device)\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\\n\\n    best_model_wts = None\\n    for i in range(len(task_names)):\\n        valid_out_dim += 2\\n        train_name = task_names[i]\\n        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\\n        \\n        print(f\\'=====Task: {train_name}=====\\')\\n        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\\n    \\n        acc_table[train_name] = OrderedDict()\\n\\n        for j in range(i+1):\\n            val_name = task_names[j]\\n            val_data = val_dataset_splits[val_name]\\n            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\\n            model.eval()\\n            val_acc = AverageMeter()\\n            with torch.no_grad():\\n                for i, (input, target, _) in enumerate(val_loader):\\n                    input, target = input.to(device), target.to(device)\\n                    output = model(input)\\n                    val_acc = accumulate_acc(output, target, val_acc)\\n\\n            acc_table[val_name][train_name] = val_acc.avg\\n\\n        print(acc_table)\\n\\n    avg_acc_history = [0] * len(task_names)\\n    for i in range(len(task_names)):\\n        train_name = task_names[i]\\n        cls_acc_sum = 0\\n        for j in range(i + 1):\\n            val_name = task_names[j]\\n            cls_acc_sum += acc_table[val_name][train_name]\\n\\n        avg_acc_history[i] = cls_acc_sum / (i + 1)\\n        print(\\'Task\\', train_name, \\'average acc:\\', avg_acc_history[i])\\n    \\n    return avg_acc_history\\n\\ntask_names = sorted(list(task_output_space.keys()), key=int)\\nprint(\\'Task order:\\',task_names)\\n\\navg_acc_history = train(task_names)\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        print(dataset.root)\n",
    "        label_cache_filename = dataset.root + '/' +'_'+str(len(dataset))+'.pth'\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target\n",
    "\n",
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space\n",
    "\n",
    "def MNIST(dataroot, train_aug=False):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = MNIST('./data', False)\n",
    "\n",
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count\n",
    "\n",
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size\n",
    "\n",
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter\n",
    "\n",
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss\n",
    "\n",
    "def train_on_task(model, train_loader, optimizer, criterion, \n",
    "                  valid_out_dim, best_model_wts, task_num, task_names):\n",
    "    leader = MLP400().to(device)\n",
    "    best_loss = float('inf')\n",
    "    if (best_model_wts):\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                leader_outputs = leader(images)\n",
    "\n",
    "            model.train()\n",
    "            follower_outputs = model(images)\n",
    "\n",
    "            # reg_loss = 0\n",
    "            # for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
    "                # reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\n",
    "            loss = c_loss + alpha * torch.mean((follower_outputs - leader_outputs) ** 2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = AverageMeter()\n",
    "\n",
    "                for task in range(task_num + 1):\n",
    "                    val_name = task_names[task]\n",
    "                    val_data = val_dataset_splits[val_name]\n",
    "                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    for i, (input, target, _) in enumerate(val_loader):\n",
    "                        input, target = input.to(device), target.to(device)\n",
    "                        output = model(input)\n",
    "                        loss_v = criterion(output, target).item()\n",
    "\n",
    "                        val_loss.update(loss_v, len(target))\n",
    "\n",
    "                    if val_loss.avg < best_loss:\n",
    "                        best_loss = val_loss.avg\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss_v:.4f}\")\n",
    "            batch_num += 1\n",
    "    return best_model_wts, best_loss\n",
    "\n",
    "def train(task_names):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\n",
    "    \n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "            model.eval()\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "        print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history\n",
    "\n",
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)\n",
    "\n",
    "avg_acc_history = train(task_names)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
