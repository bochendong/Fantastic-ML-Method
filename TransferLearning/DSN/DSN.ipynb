{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "lr = 0.01\n",
        "lambda_recon = 0.1  # Reconstruction loss weight\n",
        "lambda_sim = 0.1    # Similarity loss weight\n",
        "lambda_diff = 0.1   # Difference loss weight\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DSN (Domain Separation Networks)\n",
        "\n",
        "DSN is a domain adaptation method that separates shared and private features to learn domain-invariant representations.\n",
        "\n",
        "**Key Components:**\n",
        "1. **Shared Encoder (Es)**: Extracts domain-invariant features\n",
        "2. **Private Encoder (Ep)**: Extracts domain-specific features  \n",
        "3. **Reconstruction Decoder (D)**: Reconstructs images from shared + private features\n",
        "4. **Task Classifier (C)**: Classifies using shared features\n",
        "5. **Similarity Loss**: Ensures shared features are similar across domains\n",
        "6. **Difference Loss**: Ensures shared and private features are different\n",
        "\n",
        "**Training Objective:**\n",
        "- Minimize task classification loss on source domain\n",
        "- Minimize reconstruction loss (shared + private â†’ original image)\n",
        "- Maximize similarity between shared features from different domains\n",
        "- Maximize difference between shared and private features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shared Encoder (Es) - extracts domain-invariant features\n",
        "class SharedEncoder(nn.Module):\n",
        "    \"\"\"Shared encoder for domain-invariant features\"\"\"\n",
        "    def __init__(self, input_channels=1):\n",
        "        super(SharedEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Private Encoder (Ep) - extracts domain-specific features\n",
        "class PrivateEncoder(nn.Module):\n",
        "    \"\"\"Private encoder for domain-specific features\"\"\"\n",
        "    def __init__(self, input_channels=1):\n",
        "        super(PrivateEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconstruction Decoder (D) - reconstructs image from shared + private features\n",
        "class ReconstructionDecoder(nn.Module):\n",
        "    \"\"\"Decoder to reconstruct images from shared and private features\"\"\"\n",
        "    def __init__(self, output_channels=1):\n",
        "        super(ReconstructionDecoder, self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, output_channels, kernel_size=5, padding=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, shared_feat, private_feat):\n",
        "        # Concatenate shared and private features\n",
        "        combined = torch.cat([shared_feat, private_feat], dim=1)\n",
        "        return self.decoder(combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task Classifier (C) - classifies using shared features\n",
        "class TaskClassifier(nn.Module):\n",
        "    \"\"\"Task classifier using shared features\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(TaskClassifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((4, 4)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, 100),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(100, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, shared_feat):\n",
        "        return self.classifier(shared_feat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete DSN Model\n",
        "class DSN(nn.Module):\n",
        "    \"\"\"Domain Separation Network\"\"\"\n",
        "    def __init__(self, num_classes=10, input_channels=1, output_channels=1):\n",
        "        super(DSN, self).__init__()\n",
        "        self.shared_encoder = SharedEncoder(input_channels)\n",
        "        self.private_encoder = PrivateEncoder(input_channels)\n",
        "        self.decoder = ReconstructionDecoder(output_channels)\n",
        "        self.classifier = TaskClassifier(num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encode features\n",
        "        shared_feat = self.shared_encoder(x)\n",
        "        private_feat = self.private_encoder(x)\n",
        "        \n",
        "        # Reconstruct image\n",
        "        reconstructed = self.decoder(shared_feat, private_feat)\n",
        "        \n",
        "        # Classify using shared features\n",
        "        class_output = self.classifier(shared_feat)\n",
        "        \n",
        "        return shared_feat, private_feat, reconstructed, class_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "def similarity_loss(shared_s, shared_t):\n",
        "    \"\"\"Encourage shared features from source and target to be similar\"\"\"\n",
        "    # L2 distance between shared features\n",
        "    return F.mse_loss(shared_s, shared_t)\n",
        "\n",
        "def difference_loss(shared, private):\n",
        "    \"\"\"Encourage shared and private features to be different\"\"\"\n",
        "    # Cosine similarity (we want it to be low, so we maximize 1 - similarity)\n",
        "    shared_flat = shared.view(shared.size(0), -1)\n",
        "    private_flat = private.view(private.size(0), -1)\n",
        "    \n",
        "    # Normalize\n",
        "    shared_norm = F.normalize(shared_flat, p=2, dim=1)\n",
        "    private_norm = F.normalize(private_flat, p=2, dim=1)\n",
        "    \n",
        "    # Cosine similarity\n",
        "    cosine_sim = (shared_norm * private_norm).sum(dim=1).mean()\n",
        "    \n",
        "    # We want to maximize difference, so minimize similarity\n",
        "    return cosine_sim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Source domain: MNIST\n",
        "source_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Target domain: MNIST test set (for demonstration)\n",
        "target_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f'Source dataset size: {len(source_dataset)}')\n",
        "print(f'Target dataset size: {len(target_dataset)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = DSN(num_classes=10, input_channels=1, output_channels=1).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "# Loss functions\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_recon = nn.MSELoss()\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, source_loader, target_loader, optimizer, epoch, num_epochs):\n",
        "    model.train()\n",
        "    \n",
        "    source_iter = iter(source_loader)\n",
        "    target_iter = iter(target_loader)\n",
        "    \n",
        "    total_class_loss = 0\n",
        "    total_recon_loss = 0\n",
        "    total_sim_loss = 0\n",
        "    total_diff_loss = 0\n",
        "    correct_class = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    min_len = min(len(source_loader), len(target_loader))\n",
        "    \n",
        "    for batch_idx in range(min_len):\n",
        "        # Get source batch\n",
        "        try:\n",
        "            source_data, source_labels = next(source_iter)\n",
        "        except StopIteration:\n",
        "            source_iter = iter(source_loader)\n",
        "            source_data, source_labels = next(source_iter)\n",
        "        \n",
        "        # Get target batch\n",
        "        try:\n",
        "            target_data, _ = next(target_iter)\n",
        "        except StopIteration:\n",
        "            target_iter = iter(target_loader)\n",
        "            target_data, _ = next(target_iter)\n",
        "        \n",
        "        # Move to device\n",
        "        source_data = source_data.to(device)\n",
        "        source_labels = source_labels.to(device)\n",
        "        target_data = target_data.to(device)\n",
        "        \n",
        "        # Forward pass on source\n",
        "        shared_s, private_s, recon_s, class_s = model(source_data)\n",
        "        \n",
        "        # Forward pass on target\n",
        "        shared_t, private_t, recon_t, class_t = model(target_data)\n",
        "        \n",
        "        # Classification loss (only on source)\n",
        "        class_loss = criterion_class(class_s, source_labels)\n",
        "        \n",
        "        # Reconstruction loss (on both domains)\n",
        "        recon_loss = (criterion_recon(recon_s, source_data) + \n",
        "                     criterion_recon(recon_t, target_data)) / 2\n",
        "        \n",
        "        # Similarity loss (encourage shared features to be similar)\n",
        "        sim_loss = similarity_loss(\n",
        "            shared_s.view(shared_s.size(0), -1).mean(dim=0),\n",
        "            shared_t.view(shared_t.size(0), -1).mean(dim=0)\n",
        "        )\n",
        "        \n",
        "        # Difference loss (encourage shared and private to be different)\n",
        "        diff_loss_s = difference_loss(shared_s, private_s)\n",
        "        diff_loss_t = difference_loss(shared_t, private_t)\n",
        "        diff_loss = (diff_loss_s + diff_loss_t) / 2\n",
        "        \n",
        "        # Total loss\n",
        "        loss = (class_loss + \n",
        "                lambda_recon * recon_loss - \n",
        "                lambda_sim * sim_loss + \n",
        "                lambda_diff * diff_loss)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        total_class_loss += class_loss.item()\n",
        "        total_recon_loss += recon_loss.item()\n",
        "        total_sim_loss += sim_loss.item()\n",
        "        total_diff_loss += diff_loss.item()\n",
        "        _, predicted = class_s.max(1)\n",
        "        correct_class += predicted.eq(source_labels).sum().item()\n",
        "        total_samples += source_labels.size(0)\n",
        "        \n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{min_len}], '\n",
        "                  f'Class: {class_loss.item():.4f}, Recon: {recon_loss.item():.4f}, '\n",
        "                  f'Sim: {sim_loss.item():.4f}, Diff: {diff_loss.item():.4f}, '\n",
        "                  f'Acc: {100.*correct_class/total_samples:.2f}%')\n",
        "    \n",
        "    avg_class_loss = total_class_loss / min_len\n",
        "    avg_recon_loss = total_recon_loss / min_len\n",
        "    avg_sim_loss = total_sim_loss / min_len\n",
        "    avg_diff_loss = total_diff_loss / min_len\n",
        "    accuracy = 100. * correct_class / total_samples\n",
        "    \n",
        "    return avg_class_loss, avg_recon_loss, avg_sim_loss, avg_diff_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    class_loss, recon_loss, sim_loss, diff_loss, accuracy = train_epoch(\n",
        "        model, source_loader, target_loader, optimizer, epoch, num_epochs\n",
        "    )\n",
        "    train_losses.append(class_loss)\n",
        "    train_accs.append(accuracy)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - Class: {class_loss:.4f}, '\n",
        "          f'Recon: {recon_loss:.4f}, Sim: {sim_loss:.4f}, Diff: {diff_loss:.4f}, '\n",
        "          f'Accuracy: {accuracy:.2f}%')\n",
        "    print('-' * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Classification Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, data_loader, domain_name='Target'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            _, _, _, class_output = model(data)\n",
        "            _, predicted = class_output.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'{domain_name} Domain Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on source and target\n",
        "source_acc = evaluate(model, source_loader, 'Source')\n",
        "target_acc = evaluate(model, target_loader, 'Target')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize reconstructions\n",
        "def visualize_reconstructions(model, data_loader, num_samples=8):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        data, labels = next(iter(data_loader))\n",
        "        data = data[:num_samples].to(device)\n",
        "        \n",
        "        shared, private, recon, _ = model(data)\n",
        "        \n",
        "        # Denormalize for visualization\n",
        "        data_vis = (data + 1) / 2\n",
        "        recon_vis = (recon + 1) / 2\n",
        "        \n",
        "        # Plot\n",
        "        fig, axes = plt.subplots(2, num_samples, figsize=(15, 4))\n",
        "        for i in range(num_samples):\n",
        "            axes[0, i].imshow(data_vis[i].cpu().squeeze(), cmap='gray')\n",
        "            axes[0, i].axis('off')\n",
        "            axes[0, i].set_title('Original')\n",
        "            \n",
        "            axes[1, i].imshow(recon_vis[i].cpu().squeeze(), cmap='gray')\n",
        "            axes[1, i].axis('off')\n",
        "            axes[1, i].set_title('Reconstructed')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "visualize_reconstructions(model, source_loader)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
