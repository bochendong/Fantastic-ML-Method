{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 4\n",
    "pre_heat_batches_base = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        print(dataset.root)\n",
    "        label_cache_filename = dataset.root + '/' +'_'+str(len(dataset))+'.pth'\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        '''\n",
    "        :param dataset: (CacheClassLabel)\n",
    "        :param class_list: (list) A list of integers\n",
    "        :param remap: (bool) Ex: remap class [2,4,6 ...] to [0,1,2 ...]\n",
    "        '''\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST(dataroot, train_aug=False):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n",
      "./data\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = MNIST('./data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_boundaries: [0, 2, 4, 6, 8, 10]\n",
      "{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_task(model, train_loader, optimizer, criterion, \n",
    "                  valid_out_dim, best_model_wts, task_num, task_names):\n",
    "    leader = MLP400().to(device)\n",
    "    best_loss = float('inf')\n",
    "    best_pre_heat_loss = float('inf')\n",
    "    with torch.no_grad(): \n",
    "        if (best_model_wts):\n",
    "            leader.load_state_dict(best_model_wts)\n",
    "        else:\n",
    "            leader.load_state_dict(model.state_dict())\n",
    "    pre_heat_batches = int(pre_heat_batches_base)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Epoch: [ {epoch} / {epoches} ]')\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            model.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                leader_outputs = leader(images)\n",
    "\n",
    "            follower_outputs = model(images)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\n",
    "            loss = c_loss + alpha * torch.mean(torch.abs(follower_outputs - leader_outputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = AverageMeter()\n",
    "\n",
    "                for task in range(task_num + 1):\n",
    "                    val_name = task_names[task]\n",
    "                    val_data = val_dataset_splits[val_name]\n",
    "                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    for i, (input, target, _) in enumerate(val_loader):\n",
    "                        input, target = input.to(device), target.to(device)\n",
    "                        output = model(input)\n",
    "                        loss_v = criterion(output, target).item()\n",
    "\n",
    "                        val_loss.update(loss_v, len(target))\n",
    "\n",
    "                print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss:.4f}\", end = \" \")\n",
    "                \n",
    "                if(pre_heat_batches > 0):\n",
    "                    if (val_loss.avg < best_pre_heat_loss):\n",
    "                        best_pre_heat_loss = val_loss.avg\n",
    "                        best_pre_heat_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        print(\"best_pre_heat_loss selected\")\n",
    "                    else:\n",
    "                        print()\n",
    "                elif (pre_heat_batches == 0):\n",
    "                    best_model_wts = best_pre_heat_model_wts\n",
    "                    best_loss = best_pre_heat_loss\n",
    "                    print(f\"Preheat End: Leader changed with val acc {best_loss: .4f}\")\n",
    "                    leader.load_state_dict(best_model_wts) \n",
    "                else:\n",
    "                    if val_loss.avg < best_loss:\n",
    "                        best_loss = val_loss.avg\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        print(f\"Leader changed with val acc {best_loss: .4f}\")\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "                    else:\n",
    "                        print()\n",
    "            pre_heat_batches -= 1\n",
    "            batch_num += 1\n",
    "    with torch.no_grad():  \n",
    "        model.load_state_dict(best_model_wts) \n",
    "        \n",
    "    return best_model_wts, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(acc_table, model, train_name, task_names, task_index):\n",
    "    acc_table[train_name] = OrderedDict()\n",
    "\n",
    "    for j in range(task_index+1):\n",
    "        val_name = task_names[j]\n",
    "        val_data = val_dataset_splits[val_name]\n",
    "        val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        model.eval()\n",
    "        val_acc = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "        acc_table[val_name][train_name] = val_acc.avg\n",
    "    \n",
    "    return acc_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task_names):\n",
    "    leader_acc_table = OrderedDict()\n",
    "    follower_acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    best_loss = float('inf')\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\n",
    "    \n",
    "        follower_acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        leader = MLP400().to(device)\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "        eval(follower_acc_table, model, train_name, task_names, i)\n",
    "        eval(leader_acc_table, leader, train_name, task_names, i)\n",
    "\n",
    "        print(follower_acc_table)\n",
    "        print(leader_acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += follower_acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('follower Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    leader_avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += leader_acc_table[val_name][train_name]\n",
    "\n",
    "        leader_avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('leader Task', train_name, 'average acc:', leader_avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history, leader_avg_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task order: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Task: 1=====\n",
      "Epoch: [ 0 / 1 ]\n",
      "batch_num: 0, c_loss:0.7865, val_loss: 1.4107, loss:0.7865 best_pre_heat_loss selected\n",
      "batch_num: 1, c_loss:0.1910, val_loss: 1.9850, loss:1.3494 \n",
      "batch_num: 2, c_loss:0.1940, val_loss: 1.9774, loss:1.4754 \n",
      "batch_num: 3, c_loss:0.1925, val_loss: 1.9766, loss:1.3551 \n",
      "batch_num: 4, c_loss:0.2361, val_loss: 1.9346, loss:1.2183 \n",
      "batch_num: 5, c_loss:0.2799, val_loss: 1.9034, loss:1.1513 \n",
      "batch_num: 6, c_loss:0.3741, val_loss: 1.8271, loss:1.1485 \n",
      "batch_num: 7, c_loss:0.4632, val_loss: 1.7563, loss:1.1338 \n",
      "batch_num: 8, c_loss:0.5859, val_loss: 1.6371, loss:1.1554 \n",
      "batch_num: 9, c_loss:0.6083, val_loss: 1.6044, loss:1.1634 \n",
      "batch_num: 10, c_loss:0.5816, val_loss: 1.6086, loss:1.1016 \n",
      "batch_num: 11, c_loss:0.5327, val_loss: 1.6264, loss:1.1011 \n",
      "batch_num: 12, c_loss:0.4867, val_loss: 1.6413, loss:1.0875 \n",
      "batch_num: 13, c_loss:0.4414, val_loss: 1.6623, loss:1.1150 \n",
      "batch_num: 14, c_loss:0.3987, val_loss: 1.6880, loss:1.0747 \n",
      "batch_num: 15, c_loss:0.3958, val_loss: 1.6783, loss:1.0738 \n",
      "batch_num: 16, c_loss:0.4067, val_loss: 1.6571, loss:1.0787 \n",
      "batch_num: 17, c_loss:0.4047, val_loss: 1.6467, loss:1.0931 \n",
      "batch_num: 18, c_loss:0.4102, val_loss: 1.6347, loss:1.1099 \n",
      "batch_num: 19, c_loss:0.4250, val_loss: 1.6169, loss:1.0601 \n",
      "batch_num: 20, c_loss:0.4479, val_loss: 1.5928, loss:1.0749 \n",
      "batch_num: 21, c_loss:0.4451, val_loss: 1.5994, loss:1.0385 \n",
      "batch_num: 22, c_loss:0.4659, val_loss: 1.5787, loss:1.0605 \n",
      "batch_num: 23, c_loss:0.4633, val_loss: 1.5781, loss:1.0569 \n",
      "batch_num: 24, c_loss:0.4969, val_loss: 1.5381, loss:1.0490 \n",
      "batch_num: 25, c_loss:0.5078, val_loss: 1.5228, loss:1.0439 \n",
      "batch_num: 26, c_loss:0.5128, val_loss: 1.5053, loss:1.0344 \n",
      "batch_num: 27, c_loss:0.5109, val_loss: 1.4841, loss:1.0468 \n",
      "batch_num: 28, c_loss:0.4851, val_loss: 1.4812, loss:1.0177 \n",
      "batch_num: 29, c_loss:0.4686, val_loss: 1.4738, loss:1.0270 \n",
      "batch_num: 30, c_loss:0.4675, val_loss: 1.4605, loss:1.0660 Preheat End: Leader changed with val acc  1.4107\n",
      "batch_num: 31, c_loss:0.4843, val_loss: 1.3443, loss:1.4373 Leader changed with val acc  1.3443\n",
      "batch_num: 32, c_loss:0.3888, val_loss: 1.3174, loss:0.3888 Leader changed with val acc  1.3174\n",
      "batch_num: 33, c_loss:0.3193, val_loss: 1.2521, loss:0.3193 Leader changed with val acc  1.2521\n",
      "batch_num: 34, c_loss:0.2501, val_loss: 1.1838, loss:0.2501 Leader changed with val acc  1.1838\n",
      "batch_num: 35, c_loss:0.1913, val_loss: 1.1089, loss:0.1913 Leader changed with val acc  1.1089\n",
      "batch_num: 36, c_loss:0.1685, val_loss: 1.0076, loss:0.1685 Leader changed with val acc  1.0076\n",
      "batch_num: 37, c_loss:0.1394, val_loss: 0.9200, loss:0.1394 Leader changed with val acc  0.9200\n",
      "batch_num: 38, c_loss:0.1088, val_loss: 0.8459, loss:0.1088 Leader changed with val acc  0.8459\n",
      "batch_num: 39, c_loss:0.0734, val_loss: 0.7931, loss:0.0734 Leader changed with val acc  0.7931\n",
      "batch_num: 40, c_loss:0.0668, val_loss: 0.7211, loss:0.0668 Leader changed with val acc  0.7211\n",
      "batch_num: 41, c_loss:0.0665, val_loss: 0.6583, loss:0.0665 Leader changed with val acc  0.6583\n",
      "batch_num: 42, c_loss:0.0582, val_loss: 0.6108, loss:0.0582 Leader changed with val acc  0.6108\n",
      "batch_num: 43, c_loss:0.0508, val_loss: 0.5689, loss:0.0508 Leader changed with val acc  0.5689\n",
      "batch_num: 44, c_loss:0.0427, val_loss: 0.5322, loss:0.0427 Leader changed with val acc  0.5322\n",
      "batch_num: 45, c_loss:0.0473, val_loss: 0.4919, loss:0.0473 Leader changed with val acc  0.4919\n",
      "batch_num: 46, c_loss:0.0328, val_loss: 0.4771, loss:0.0328 Leader changed with val acc  0.4771\n",
      "batch_num: 47, c_loss:0.0329, val_loss: 0.4517, loss:0.0329 Leader changed with val acc  0.4517\n",
      "batch_num: 48, c_loss:0.0459, val_loss: 0.4209, loss:0.0459 Leader changed with val acc  0.4209\n",
      "batch_num: 49, c_loss:0.0242, val_loss: 0.4236, loss:0.0242 \n",
      "batch_num: 50, c_loss:0.0209, val_loss: 0.4471, loss:0.0804 \n",
      "batch_num: 51, c_loss:0.0171, val_loss: 0.4770, loss:0.1699 \n",
      "batch_num: 52, c_loss:0.0684, val_loss: 0.4386, loss:0.1795 \n",
      "batch_num: 53, c_loss:0.0199, val_loss: 0.4941, loss:0.1492 \n",
      "batch_num: 54, c_loss:0.0223, val_loss: 0.4906, loss:0.1496 \n",
      "batch_num: 55, c_loss:0.0243, val_loss: 0.4903, loss:0.1422 \n",
      "batch_num: 56, c_loss:0.0308, val_loss: 0.4969, loss:0.1640 \n",
      "batch_num: 57, c_loss:0.0301, val_loss: 0.5122, loss:0.1712 \n",
      "batch_num: 58, c_loss:0.0224, val_loss: 0.5287, loss:0.1512 \n",
      "batch_num: 59, c_loss:0.0410, val_loss: 0.5215, loss:0.1796 \n",
      "batch_num: 60, c_loss:0.0253, val_loss: 0.5487, loss:0.1721 \n",
      "batch_num: 61, c_loss:0.0229, val_loss: 0.5605, loss:0.1471 \n",
      "batch_num: 62, c_loss:0.0436, val_loss: 0.5329, loss:0.2052 \n",
      "batch_num: 63, c_loss:0.0263, val_loss: 0.5493, loss:0.1777 \n",
      "batch_num: 64, c_loss:0.0240, val_loss: 0.5480, loss:0.1568 \n",
      "batch_num: 65, c_loss:0.0418, val_loss: 0.5275, loss:0.1840 \n",
      "batch_num: 66, c_loss:0.0468, val_loss: 0.5217, loss:0.1980 \n",
      "batch_num: 67, c_loss:0.0288, val_loss: 0.5348, loss:0.1891 \n",
      "batch_num: 68, c_loss:0.0243, val_loss: 0.5432, loss:0.1777 \n",
      "batch_num: 69, c_loss:0.0267, val_loss: 0.5396, loss:0.1873 \n",
      "batch_num: 70, c_loss:0.0209, val_loss: 0.5398, loss:0.1985 \n",
      "batch_num: 71, c_loss:0.0195, val_loss: 0.5330, loss:0.1701 \n",
      "batch_num: 72, c_loss:0.0193, val_loss: 0.5388, loss:0.1848 \n",
      "batch_num: 73, c_loss:0.0217, val_loss: 0.5583, loss:0.1844 \n",
      "batch_num: 74, c_loss:0.0199, val_loss: 0.5750, loss:0.1660 \n",
      "batch_num: 75, c_loss:0.0339, val_loss: 0.5628, loss:0.1966 \n",
      "batch_num: 76, c_loss:0.0202, val_loss: 0.5621, loss:0.1745 \n",
      "batch_num: 77, c_loss:0.0221, val_loss: 0.5628, loss:0.1784 \n",
      "batch_num: 78, c_loss:0.0274, val_loss: 0.5603, loss:0.1829 \n",
      "batch_num: 79, c_loss:0.0227, val_loss: 0.5662, loss:0.1834 \n",
      "batch_num: 80, c_loss:0.0195, val_loss: 0.5715, loss:0.1719 \n",
      "batch_num: 81, c_loss:0.0263, val_loss: 0.5744, loss:0.1857 \n",
      "batch_num: 82, c_loss:0.0266, val_loss: 0.5936, loss:0.1978 \n",
      "batch_num: 83, c_loss:0.0254, val_loss: 0.6004, loss:0.1857 \n",
      "batch_num: 84, c_loss:0.0230, val_loss: 0.5981, loss:0.1809 \n",
      "batch_num: 85, c_loss:0.0209, val_loss: 0.5938, loss:0.1809 \n",
      "batch_num: 86, c_loss:0.0222, val_loss: 0.5918, loss:0.1795 \n",
      "batch_num: 87, c_loss:0.0298, val_loss: 0.5890, loss:0.1884 \n",
      "batch_num: 88, c_loss:0.0321, val_loss: 0.5833, loss:0.1894 \n",
      "batch_num: 89, c_loss:0.0200, val_loss: 0.5925, loss:0.1697 \n",
      "batch_num: 90, c_loss:0.0338, val_loss: 0.5805, loss:0.1931 \n",
      "batch_num: 91, c_loss:0.0284, val_loss: 0.5913, loss:0.1861 \n",
      "batch_num: 92, c_loss:0.0443, val_loss: 0.5901, loss:0.2219 \n",
      "batch_num: 93, c_loss:0.0254, val_loss: 0.6191, loss:0.1914 \n",
      "batch_num: 94, c_loss:0.0275, val_loss: 0.6104, loss:0.1983 \n",
      "batch_num: 95, c_loss:0.0219, val_loss: 0.6069, loss:0.1867 \n",
      "batch_num: 96, c_loss:0.0253, val_loss: 0.6044, loss:0.1984 \n",
      "batch_num: 97, c_loss:0.0300, val_loss: 0.6078, loss:0.1942 \n",
      "batch_num: 98, c_loss:0.0354, val_loss: 0.6159, loss:0.1993 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052)]))])\n",
      "=====Task: 2=====\n",
      "Epoch: [ 0 / 1 ]\n",
      "batch_num: 0, c_loss:1.9407, val_loss:-0.4845, loss:1.9407 best_pre_heat_loss selected\n",
      "batch_num: 1, c_loss:1.8465, val_loss:-0.4201, loss:1.9775 \n",
      "batch_num: 2, c_loss:1.7280, val_loss:-0.3344, loss:1.9471 \n",
      "batch_num: 3, c_loss:1.6011, val_loss:-0.2425, loss:1.9317 \n",
      "batch_num: 4, c_loss:1.4837, val_loss:-0.1507, loss:1.9077 \n",
      "batch_num: 5, c_loss:1.3800, val_loss:-0.0725, loss:1.8813 \n",
      "batch_num: 6, c_loss:1.2422, val_loss: 0.0460, loss:1.8436 \n",
      "batch_num: 7, c_loss:1.1174, val_loss: 0.1485, loss:1.7970 \n",
      "batch_num: 8, c_loss:1.0670, val_loss: 0.1783, loss:1.8265 \n",
      "batch_num: 9, c_loss:0.9345, val_loss: 0.3028, loss:1.7964 \n",
      "batch_num: 10, c_loss:0.8903, val_loss: 0.3485, loss:1.7956 \n",
      "batch_num: 11, c_loss:0.7942, val_loss: 0.4452, loss:1.7651 \n",
      "batch_num: 12, c_loss:0.7797, val_loss: 0.4523, loss:1.7790 \n",
      "batch_num: 13, c_loss:0.7529, val_loss: 0.4771, loss:1.8112 \n",
      "batch_num: 14, c_loss:0.7225, val_loss: 0.5041, loss:1.8147 \n",
      "batch_num: 15, c_loss:0.6514, val_loss: 0.5673, loss:1.7582 \n",
      "batch_num: 16, c_loss:0.6788, val_loss: 0.5257, loss:1.7987 \n",
      "batch_num: 17, c_loss:0.6810, val_loss: 0.5081, loss:1.8203 \n",
      "batch_num: 18, c_loss:0.6584, val_loss: 0.5140, loss:1.8242 \n",
      "batch_num: 19, c_loss:0.6571, val_loss: 0.4929, loss:1.8203 \n",
      "batch_num: 20, c_loss:0.6140, val_loss: 0.5182, loss:1.7758 \n",
      "batch_num: 21, c_loss:0.6655, val_loss: 0.4419, loss:1.7918 \n",
      "batch_num: 22, c_loss:0.6857, val_loss: 0.3986, loss:1.8399 \n",
      "batch_num: 23, c_loss:0.6225, val_loss: 0.4373, loss:1.7598 \n",
      "batch_num: 24, c_loss:0.6547, val_loss: 0.3856, loss:1.7692 \n",
      "batch_num: 25, c_loss:0.6363, val_loss: 0.3914, loss:1.7709 \n",
      "batch_num: 26, c_loss:0.7340, val_loss: 0.2863, loss:1.8425 \n",
      "batch_num: 27, c_loss:0.6461, val_loss: 0.3689, loss:1.7694 \n",
      "batch_num: 28, c_loss:0.7230, val_loss: 0.2918, loss:1.8212 \n",
      "batch_num: 29, c_loss:0.6713, val_loss: 0.3406, loss:1.7707 \n",
      "batch_num: 30, c_loss:0.6965, val_loss: 0.3054, loss:1.7782 Preheat End: Leader changed with val acc -0.4845\n",
      "batch_num: 31, c_loss:0.6945, val_loss: 0.3019, loss:1.7203 \n",
      "batch_num: 32, c_loss:0.6888, val_loss: 0.3051, loss:1.7382 \n",
      "batch_num: 33, c_loss:0.6512, val_loss: 0.3348, loss:1.6936 \n",
      "batch_num: 34, c_loss:0.7147, val_loss: 0.2643, loss:1.7474 \n",
      "batch_num: 35, c_loss:0.6730, val_loss: 0.3011, loss:1.7594 \n",
      "batch_num: 36, c_loss:0.6298, val_loss: 0.3515, loss:1.7104 \n",
      "batch_num: 37, c_loss:0.6314, val_loss: 0.3562, loss:1.7117 \n",
      "batch_num: 38, c_loss:0.6374, val_loss: 0.3526, loss:1.6857 \n",
      "batch_num: 39, c_loss:0.6730, val_loss: 0.3216, loss:1.7373 \n",
      "batch_num: 40, c_loss:0.7242, val_loss: 0.2803, loss:1.7596 \n",
      "batch_num: 41, c_loss:0.6767, val_loss: 0.3266, loss:1.7006 \n",
      "batch_num: 42, c_loss:0.6724, val_loss: 0.3313, loss:1.7060 \n",
      "batch_num: 43, c_loss:0.6893, val_loss: 0.3096, loss:1.6883 \n",
      "batch_num: 44, c_loss:0.7129, val_loss: 0.2868, loss:1.7394 \n",
      "batch_num: 45, c_loss:0.7075, val_loss: 0.2876, loss:1.7396 \n",
      "batch_num: 46, c_loss:0.6645, val_loss: 0.3306, loss:1.6922 \n",
      "batch_num: 47, c_loss:0.7071, val_loss: 0.2804, loss:1.7239 \n",
      "batch_num: 48, c_loss:0.6542, val_loss: 0.3283, loss:1.6826 \n",
      "batch_num: 49, c_loss:0.7008, val_loss: 0.2713, loss:1.7404 \n",
      "batch_num: 50, c_loss:0.6705, val_loss: 0.2918, loss:1.7105 \n",
      "batch_num: 51, c_loss:0.6871, val_loss: 0.2683, loss:1.7268 \n",
      "batch_num: 52, c_loss:0.6599, val_loss: 0.2953, loss:1.7040 \n",
      "batch_num: 53, c_loss:0.6727, val_loss: 0.2896, loss:1.7291 \n",
      "batch_num: 54, c_loss:0.6839, val_loss: 0.2868, loss:1.7347 \n",
      "batch_num: 55, c_loss:0.6552, val_loss: 0.3260, loss:1.7343 \n",
      "batch_num: 56, c_loss:0.6936, val_loss: 0.3013, loss:1.7280 \n",
      "batch_num: 57, c_loss:0.6376, val_loss: 0.3672, loss:1.6873 \n",
      "batch_num: 58, c_loss:0.6602, val_loss: 0.3489, loss:1.7075 \n",
      "batch_num: 59, c_loss:0.6573, val_loss: 0.3539, loss:1.7056 \n",
      "batch_num: 60, c_loss:0.6793, val_loss: 0.3314, loss:1.7162 \n",
      "batch_num: 61, c_loss:0.6815, val_loss: 0.3285, loss:1.7144 \n",
      "batch_num: 62, c_loss:0.6920, val_loss: 0.3204, loss:1.7204 \n",
      "batch_num: 63, c_loss:0.6351, val_loss: 0.3771, loss:1.6731 \n",
      "batch_num: 64, c_loss:0.6509, val_loss: 0.3596, loss:1.7035 \n",
      "batch_num: 65, c_loss:0.6695, val_loss: 0.3377, loss:1.7119 \n",
      "batch_num: 66, c_loss:0.7002, val_loss: 0.3037, loss:1.7306 \n",
      "batch_num: 67, c_loss:0.6935, val_loss: 0.3048, loss:1.7333 \n",
      "batch_num: 68, c_loss:0.6771, val_loss: 0.3208, loss:1.7315 \n",
      "batch_num: 69, c_loss:0.6939, val_loss: 0.3018, loss:1.7237 \n",
      "batch_num: 70, c_loss:0.6398, val_loss: 0.3540, loss:1.6848 \n",
      "batch_num: 71, c_loss:0.6890, val_loss: 0.3019, loss:1.7216 \n",
      "batch_num: 72, c_loss:0.7138, val_loss: 0.2729, loss:1.7341 \n",
      "batch_num: 73, c_loss:0.6766, val_loss: 0.3082, loss:1.6923 \n",
      "batch_num: 74, c_loss:0.6742, val_loss: 0.3069, loss:1.7031 \n",
      "batch_num: 75, c_loss:0.6581, val_loss: 0.3228, loss:1.7154 \n",
      "batch_num: 76, c_loss:0.7195, val_loss: 0.2593, loss:1.7533 \n",
      "batch_num: 77, c_loss:0.6703, val_loss: 0.3083, loss:1.6910 \n",
      "batch_num: 78, c_loss:0.6451, val_loss: 0.3296, loss:1.6943 \n",
      "batch_num: 79, c_loss:0.6371, val_loss: 0.3373, loss:1.6705 \n",
      "batch_num: 80, c_loss:0.6773, val_loss: 0.3025, loss:1.7165 \n",
      "batch_num: 81, c_loss:0.6407, val_loss: 0.3423, loss:1.6756 \n",
      "batch_num: 82, c_loss:0.6544, val_loss: 0.3328, loss:1.7086 \n",
      "batch_num: 83, c_loss:0.6577, val_loss: 0.3411, loss:1.7215 \n",
      "batch_num: 84, c_loss:0.6764, val_loss: 0.3351, loss:1.7293 \n",
      "batch_num: 85, c_loss:0.6616, val_loss: 0.3645, loss:1.7177 \n",
      "batch_num: 86, c_loss:0.6593, val_loss: 0.3781, loss:1.6906 \n",
      "batch_num: 87, c_loss:0.6428, val_loss: 0.3969, loss:1.6631 \n",
      "batch_num: 88, c_loss:0.6551, val_loss: 0.3844, loss:1.7045 \n",
      "batch_num: 89, c_loss:0.6328, val_loss: 0.4056, loss:1.6734 \n",
      "batch_num: 90, c_loss:0.6584, val_loss: 0.3792, loss:1.6889 \n",
      "batch_num: 91, c_loss:0.6670, val_loss: 0.3641, loss:1.6799 \n",
      "batch_num: 92, c_loss:0.6359, val_loss: 0.3909, loss:1.6702 \n",
      "batch_num: 93, c_loss:0.6058, val_loss: 0.4184, loss:1.6521 \n",
      "batch_num: 94, c_loss:0.6567, val_loss: 0.3689, loss:1.7190 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052)])), ('2', OrderedDict([('2', 2.3016650342801177)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052)])), ('2', OrderedDict([('2', 2.3016650342801177)]))])\n",
      "=====Task: 3=====\n",
      "Epoch: [ 0 / 1 ]\n",
      "batch_num: 0, c_loss:2.1243, val_loss:-0.3609, loss:2.1243 best_pre_heat_loss selected\n",
      "batch_num: 1, c_loss:2.0207, val_loss:-0.3002, loss:2.1640 \n",
      "batch_num: 2, c_loss:1.9218, val_loss:-0.2470, loss:2.1455 \n",
      "batch_num: 3, c_loss:1.7120, val_loss:-0.0781, loss:2.0403 \n",
      "batch_num: 4, c_loss:1.5999, val_loss: 0.0007, loss:2.0364 \n",
      "batch_num: 5, c_loss:1.5084, val_loss: 0.0616, loss:2.0193 \n",
      "batch_num: 6, c_loss:1.3226, val_loss: 0.2196, loss:1.9257 \n",
      "batch_num: 7, c_loss:1.1970, val_loss: 0.3212, loss:1.9178 \n",
      "batch_num: 8, c_loss:1.1520, val_loss: 0.3524, loss:1.9483 \n",
      "batch_num: 9, c_loss:1.0512, val_loss: 0.4401, loss:1.9137 \n",
      "batch_num: 10, c_loss:0.9344, val_loss: 0.5520, loss:1.8717 \n",
      "batch_num: 11, c_loss:0.8533, val_loss: 0.6395, loss:1.8739 \n",
      "batch_num: 12, c_loss:0.8311, val_loss: 0.6714, loss:1.8918 \n",
      "batch_num: 13, c_loss:0.7135, val_loss: 0.7995, loss:1.8360 \n",
      "batch_num: 14, c_loss:0.6715, val_loss: 0.8507, loss:1.8429 \n",
      "batch_num: 15, c_loss:0.6720, val_loss: 0.8553, loss:1.8896 \n",
      "batch_num: 16, c_loss:0.6782, val_loss: 0.8450, loss:1.9156 \n",
      "batch_num: 17, c_loss:0.5900, val_loss: 0.9261, loss:1.8490 \n",
      "batch_num: 18, c_loss:0.5922, val_loss: 0.9141, loss:1.8742 \n",
      "batch_num: 19, c_loss:0.5696, val_loss: 0.9271, loss:1.8748 \n",
      "batch_num: 20, c_loss:0.5584, val_loss: 0.9364, loss:1.8805 \n",
      "batch_num: 21, c_loss:0.5574, val_loss: 0.9342, loss:1.8530 \n",
      "batch_num: 22, c_loss:0.5846, val_loss: 0.9005, loss:1.8909 \n",
      "batch_num: 23, c_loss:0.5784, val_loss: 0.8938, loss:1.8698 \n",
      "batch_num: 24, c_loss:0.5543, val_loss: 0.9091, loss:1.8601 \n",
      "batch_num: 25, c_loss:0.5462, val_loss: 0.9141, loss:1.8613 \n",
      "batch_num: 26, c_loss:0.5781, val_loss: 0.8811, loss:1.8705 \n",
      "batch_num: 27, c_loss:0.5214, val_loss: 0.9303, loss:1.8465 \n",
      "batch_num: 28, c_loss:0.5580, val_loss: 0.8833, loss:1.8770 \n",
      "batch_num: 29, c_loss:0.5752, val_loss: 0.8523, loss:1.8736 \n",
      "batch_num: 30, c_loss:0.5744, val_loss: 0.8442, loss:1.8363 Preheat End: Leader changed with val acc -0.3609\n",
      "batch_num: 31, c_loss:0.5604, val_loss: 0.8436, loss:1.7735 \n",
      "batch_num: 32, c_loss:0.5887, val_loss: 0.8081, loss:1.8124 \n",
      "batch_num: 33, c_loss:0.6239, val_loss: 0.7693, loss:1.8282 \n",
      "batch_num: 34, c_loss:0.6077, val_loss: 0.7885, loss:1.8129 \n",
      "batch_num: 35, c_loss:0.5813, val_loss: 0.8222, loss:1.7859 \n",
      "batch_num: 36, c_loss:0.5822, val_loss: 0.8341, loss:1.7668 \n",
      "batch_num: 37, c_loss:0.6055, val_loss: 0.8110, loss:1.7611 \n",
      "batch_num: 38, c_loss:0.6303, val_loss: 0.7857, loss:1.8047 \n",
      "batch_num: 39, c_loss:0.5925, val_loss: 0.8220, loss:1.7578 \n",
      "batch_num: 40, c_loss:0.6380, val_loss: 0.7760, loss:1.7745 \n",
      "batch_num: 41, c_loss:0.6497, val_loss: 0.7649, loss:1.7750 \n",
      "batch_num: 42, c_loss:0.6096, val_loss: 0.8050, loss:1.7438 \n",
      "batch_num: 43, c_loss:0.6230, val_loss: 0.7902, loss:1.7595 \n",
      "batch_num: 44, c_loss:0.6492, val_loss: 0.7661, loss:1.7937 \n",
      "batch_num: 45, c_loss:0.6423, val_loss: 0.7727, loss:1.7753 \n",
      "batch_num: 46, c_loss:0.6185, val_loss: 0.7982, loss:1.7569 \n",
      "batch_num: 47, c_loss:0.6507, val_loss: 0.7690, loss:1.7807 \n",
      "batch_num: 48, c_loss:0.6417, val_loss: 0.7816, loss:1.7768 \n",
      "batch_num: 49, c_loss:0.6283, val_loss: 0.7949, loss:1.7646 \n",
      "batch_num: 50, c_loss:0.6266, val_loss: 0.7990, loss:1.7623 \n",
      "batch_num: 51, c_loss:0.6354, val_loss: 0.7875, loss:1.7520 \n",
      "batch_num: 52, c_loss:0.6280, val_loss: 0.7886, loss:1.7629 \n",
      "batch_num: 53, c_loss:0.6454, val_loss: 0.7655, loss:1.7606 \n",
      "batch_num: 54, c_loss:0.5946, val_loss: 0.8172, loss:1.7389 \n",
      "batch_num: 55, c_loss:0.6035, val_loss: 0.8077, loss:1.7414 \n",
      "batch_num: 56, c_loss:0.6359, val_loss: 0.7754, loss:1.7691 \n",
      "batch_num: 57, c_loss:0.5885, val_loss: 0.8178, loss:1.7201 \n",
      "batch_num: 58, c_loss:0.6623, val_loss: 0.7343, loss:1.7753 \n",
      "batch_num: 59, c_loss:0.6300, val_loss: 0.7599, loss:1.7765 \n",
      "batch_num: 60, c_loss:0.6420, val_loss: 0.7426, loss:1.7615 \n",
      "batch_num: 61, c_loss:0.6169, val_loss: 0.7718, loss:1.7547 \n",
      "batch_num: 62, c_loss:0.6274, val_loss: 0.7629, loss:1.7638 \n",
      "batch_num: 63, c_loss:0.6342, val_loss: 0.7615, loss:1.7838 \n",
      "batch_num: 64, c_loss:0.6297, val_loss: 0.7696, loss:1.7782 \n",
      "batch_num: 65, c_loss:0.5972, val_loss: 0.8040, loss:1.7482 \n",
      "batch_num: 66, c_loss:0.6360, val_loss: 0.7705, loss:1.7730 \n",
      "batch_num: 67, c_loss:0.6004, val_loss: 0.8072, loss:1.7568 \n",
      "batch_num: 68, c_loss:0.5973, val_loss: 0.8104, loss:1.7539 \n",
      "batch_num: 69, c_loss:0.6138, val_loss: 0.7954, loss:1.7703 \n",
      "batch_num: 70, c_loss:0.6249, val_loss: 0.7872, loss:1.7814 \n",
      "batch_num: 71, c_loss:0.6355, val_loss: 0.7835, loss:1.8160 \n",
      "batch_num: 72, c_loss:0.6277, val_loss: 0.7961, loss:1.7710 \n",
      "batch_num: 73, c_loss:0.5986, val_loss: 0.8319, loss:1.7582 \n",
      "batch_num: 74, c_loss:0.5769, val_loss: 0.8619, loss:1.7401 \n",
      "batch_num: 75, c_loss:0.6233, val_loss: 0.8174, loss:1.7692 \n",
      "batch_num: 76, c_loss:0.5994, val_loss: 0.8429, loss:1.7672 \n",
      "batch_num: 77, c_loss:0.6029, val_loss: 0.8420, loss:1.7594 \n",
      "batch_num: 78, c_loss:0.6030, val_loss: 0.8484, loss:1.7660 \n",
      "batch_num: 79, c_loss:0.5991, val_loss: 0.8566, loss:1.7576 \n",
      "batch_num: 80, c_loss:0.5791, val_loss: 0.8826, loss:1.7649 \n",
      "batch_num: 81, c_loss:0.5752, val_loss: 0.8874, loss:1.7391 \n",
      "batch_num: 82, c_loss:0.5831, val_loss: 0.8821, loss:1.7468 \n",
      "batch_num: 83, c_loss:0.6136, val_loss: 0.8507, loss:1.7466 \n",
      "batch_num: 84, c_loss:0.6130, val_loss: 0.8485, loss:1.7821 \n",
      "batch_num: 85, c_loss:0.6227, val_loss: 0.8310, loss:1.7762 \n",
      "batch_num: 86, c_loss:0.6066, val_loss: 0.8439, loss:1.7933 \n",
      "batch_num: 87, c_loss:0.5889, val_loss: 0.8632, loss:1.7665 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052), ('3', 99.62174940898345)])), ('2', OrderedDict([('2', 2.3016650342801177), ('3', 2.889324191968658)])), ('3', OrderedDict([('3', 1.1205976520811098)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052), ('3', 99.62174940898345)])), ('2', OrderedDict([('2', 2.3016650342801177), ('3', 2.889324191968658)])), ('3', OrderedDict([('3', 1.1205976520811098)]))])\n",
      "=====Task: 4=====\n",
      "Epoch: [ 0 / 1 ]\n",
      "batch_num: 0, c_loss:2.3400, val_loss:-0.4800, loss:2.3400 best_pre_heat_loss selected\n",
      "batch_num: 1, c_loss:2.1813, val_loss:-0.3573, loss:2.3483 \n",
      "batch_num: 2, c_loss:2.0308, val_loss:-0.2454, loss:2.3421 \n",
      "batch_num: 3, c_loss:1.8906, val_loss:-0.1336, loss:2.3129 \n",
      "batch_num: 4, c_loss:1.7508, val_loss:-0.0167, loss:2.2713 \n",
      "batch_num: 5, c_loss:1.6414, val_loss: 0.0734, loss:2.2536 \n",
      "batch_num: 6, c_loss:1.4628, val_loss: 0.2382, loss:2.1736 \n",
      "batch_num: 7, c_loss:1.3433, val_loss: 0.3501, loss:2.1115 \n",
      "batch_num: 8, c_loss:1.1167, val_loss: 0.5723, loss:2.0191 \n",
      "batch_num: 9, c_loss:1.0395, val_loss: 0.6495, loss:1.9990 \n",
      "batch_num: 10, c_loss:0.9359, val_loss: 0.7534, loss:2.0053 \n",
      "batch_num: 11, c_loss:0.8667, val_loss: 0.8230, loss:1.9946 \n",
      "batch_num: 12, c_loss:0.7638, val_loss: 0.9294, loss:1.9793 \n",
      "batch_num: 13, c_loss:0.7280, val_loss: 0.9716, loss:2.0575 \n",
      "batch_num: 14, c_loss:0.6085, val_loss: 1.0999, loss:2.0310 \n",
      "batch_num: 15, c_loss:0.6145, val_loss: 1.1150, loss:2.0159 \n",
      "batch_num: 16, c_loss:0.5558, val_loss: 1.1910, loss:2.0302 \n",
      "batch_num: 17, c_loss:0.5388, val_loss: 1.2272, loss:2.0128 \n",
      "batch_num: 18, c_loss:0.5516, val_loss: 1.2251, loss:2.0259 \n",
      "batch_num: 19, c_loss:0.5080, val_loss: 1.2713, loss:2.0235 \n",
      "batch_num: 20, c_loss:0.5018, val_loss: 1.2781, loss:2.0255 \n",
      "batch_num: 21, c_loss:0.4942, val_loss: 1.2798, loss:2.0439 \n",
      "batch_num: 22, c_loss:0.4540, val_loss: 1.3125, loss:2.0053 \n",
      "batch_num: 23, c_loss:0.4977, val_loss: 1.2635, loss:2.0228 \n",
      "batch_num: 24, c_loss:0.4902, val_loss: 1.2683, loss:2.0304 \n",
      "batch_num: 25, c_loss:0.4748, val_loss: 1.2813, loss:1.9770 \n",
      "batch_num: 26, c_loss:0.4355, val_loss: 1.3171, loss:1.9539 \n",
      "batch_num: 27, c_loss:0.4941, val_loss: 1.2473, loss:1.9989 \n",
      "batch_num: 28, c_loss:0.5064, val_loss: 1.2275, loss:2.0057 \n",
      "batch_num: 29, c_loss:0.5158, val_loss: 1.2141, loss:2.0087 \n",
      "batch_num: 30, c_loss:0.5266, val_loss: 1.2017, loss:1.9839 Preheat End: Leader changed with val acc -0.4800\n",
      "batch_num: 31, c_loss:0.5466, val_loss: 1.1822, loss:1.9291 \n",
      "batch_num: 32, c_loss:0.5720, val_loss: 1.1584, loss:1.9577 \n",
      "batch_num: 33, c_loss:0.5582, val_loss: 1.1697, loss:1.9261 \n",
      "batch_num: 34, c_loss:0.5605, val_loss: 1.1587, loss:1.9203 \n",
      "batch_num: 35, c_loss:0.5706, val_loss: 1.1348, loss:1.9222 \n",
      "batch_num: 36, c_loss:0.6091, val_loss: 1.0822, loss:1.9257 \n",
      "batch_num: 37, c_loss:0.5513, val_loss: 1.1323, loss:1.8935 \n",
      "batch_num: 38, c_loss:0.5476, val_loss: 1.1300, loss:1.8870 \n",
      "batch_num: 39, c_loss:0.5456, val_loss: 1.1334, loss:1.8825 \n",
      "batch_num: 40, c_loss:0.5620, val_loss: 1.1199, loss:1.8418 \n",
      "batch_num: 41, c_loss:0.6211, val_loss: 1.0655, loss:1.9275 \n",
      "batch_num: 42, c_loss:0.6103, val_loss: 1.0771, loss:1.8787 \n",
      "batch_num: 43, c_loss:0.6201, val_loss: 1.0666, loss:1.8722 \n",
      "batch_num: 44, c_loss:0.6208, val_loss: 1.0626, loss:1.8845 \n",
      "batch_num: 45, c_loss:0.6188, val_loss: 1.0634, loss:1.8622 \n",
      "batch_num: 46, c_loss:0.5897, val_loss: 1.0989, loss:1.8597 \n",
      "batch_num: 47, c_loss:0.6249, val_loss: 1.0684, loss:1.8876 \n",
      "batch_num: 48, c_loss:0.5962, val_loss: 1.0994, loss:1.8783 \n",
      "batch_num: 49, c_loss:0.5854, val_loss: 1.1124, loss:1.8920 \n",
      "batch_num: 50, c_loss:0.5922, val_loss: 1.1027, loss:1.8659 \n",
      "batch_num: 51, c_loss:0.6141, val_loss: 1.0788, loss:1.9233 \n",
      "batch_num: 52, c_loss:0.5840, val_loss: 1.1044, loss:1.8835 \n",
      "batch_num: 53, c_loss:0.6016, val_loss: 1.0815, loss:1.8937 \n",
      "batch_num: 54, c_loss:0.6126, val_loss: 1.0682, loss:1.8792 \n",
      "batch_num: 55, c_loss:0.5945, val_loss: 1.0837, loss:1.8822 \n",
      "batch_num: 56, c_loss:0.6052, val_loss: 1.0690, loss:1.8826 \n",
      "batch_num: 57, c_loss:0.5884, val_loss: 1.0861, loss:1.8672 \n",
      "batch_num: 58, c_loss:0.5826, val_loss: 1.0946, loss:1.8918 \n",
      "batch_num: 59, c_loss:0.5919, val_loss: 1.0853, loss:1.8700 \n",
      "batch_num: 60, c_loss:0.5656, val_loss: 1.1166, loss:1.8581 \n",
      "batch_num: 61, c_loss:0.5495, val_loss: 1.1379, loss:1.8656 \n",
      "batch_num: 62, c_loss:0.5629, val_loss: 1.1277, loss:1.8802 \n",
      "batch_num: 63, c_loss:0.5848, val_loss: 1.1068, loss:1.8988 \n",
      "batch_num: 64, c_loss:0.5531, val_loss: 1.1364, loss:1.8548 \n",
      "batch_num: 65, c_loss:0.5662, val_loss: 1.1199, loss:1.8615 \n",
      "batch_num: 66, c_loss:0.5938, val_loss: 1.0960, loss:1.9238 \n",
      "batch_num: 67, c_loss:0.5696, val_loss: 1.1229, loss:1.8374 \n",
      "batch_num: 68, c_loss:0.5609, val_loss: 1.1342, loss:1.8353 \n",
      "batch_num: 69, c_loss:0.5856, val_loss: 1.1122, loss:1.8469 \n",
      "batch_num: 70, c_loss:0.5808, val_loss: 1.1180, loss:1.8643 \n",
      "batch_num: 71, c_loss:0.5794, val_loss: 1.1219, loss:1.8629 \n",
      "batch_num: 72, c_loss:0.5877, val_loss: 1.1122, loss:1.8639 \n",
      "batch_num: 73, c_loss:0.5418, val_loss: 1.1577, loss:1.8461 \n",
      "batch_num: 74, c_loss:0.5350, val_loss: 1.1621, loss:1.8259 \n",
      "batch_num: 75, c_loss:0.5702, val_loss: 1.1233, loss:1.8464 \n",
      "batch_num: 76, c_loss:0.5491, val_loss: 1.1397, loss:1.8312 \n",
      "batch_num: 77, c_loss:0.5706, val_loss: 1.1157, loss:1.8460 \n",
      "batch_num: 78, c_loss:0.5968, val_loss: 1.0886, loss:1.8513 \n",
      "batch_num: 79, c_loss:0.5802, val_loss: 1.1065, loss:1.8405 \n",
      "batch_num: 80, c_loss:0.5925, val_loss: 1.0991, loss:1.8716 \n",
      "batch_num: 81, c_loss:0.6136, val_loss: 1.0813, loss:1.8532 \n",
      "batch_num: 82, c_loss:0.5914, val_loss: 1.1017, loss:1.8514 \n",
      "batch_num: 83, c_loss:0.5738, val_loss: 1.1166, loss:1.8308 \n",
      "batch_num: 84, c_loss:0.5704, val_loss: 1.1172, loss:1.8348 \n",
      "batch_num: 85, c_loss:0.5609, val_loss: 1.1290, loss:1.8376 \n",
      "batch_num: 86, c_loss:0.5560, val_loss: 1.1396, loss:1.8223 \n",
      "batch_num: 87, c_loss:0.5702, val_loss: 1.1331, loss:1.8507 \n",
      "batch_num: 88, c_loss:0.5715, val_loss: 1.1342, loss:1.8341 \n",
      "batch_num: 89, c_loss:0.5540, val_loss: 1.1559, loss:1.8321 \n",
      "batch_num: 90, c_loss:0.5872, val_loss: 1.1223, loss:1.8663 \n",
      "batch_num: 91, c_loss:0.5642, val_loss: 1.1446, loss:1.8550 \n",
      "batch_num: 92, c_loss:0.5685, val_loss: 1.1374, loss:1.8340 \n",
      "batch_num: 93, c_loss:0.5672, val_loss: 1.1363, loss:1.8294 \n",
      "batch_num: 94, c_loss:0.5798, val_loss: 1.1213, loss:1.8794 \n",
      "batch_num: 95, c_loss:0.6810, val_loss: 1.0218, loss:1.9829 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052), ('3', 99.62174940898345), ('4', 99.57446808510639)])), ('2', OrderedDict([('2', 2.3016650342801177), ('3', 2.889324191968658), ('4', 2.7913809990205682)])), ('3', OrderedDict([('3', 1.1205976520811098), ('4', 1.2273212379935965)])), ('4', OrderedDict([('4', 9.818731117824774)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052), ('3', 99.62174940898345), ('4', 99.57446808510639)])), ('2', OrderedDict([('2', 2.3016650342801177), ('3', 2.889324191968658), ('4', 2.7913809990205682)])), ('3', OrderedDict([('3', 1.1205976520811098), ('4', 1.2273212379935965)])), ('4', OrderedDict([('4', 9.818731117824774)]))])\n",
      "=====Task: 5=====\n",
      "Epoch: [ 0 / 1 ]\n",
      "batch_num: 0, c_loss:2.6525, val_loss:-0.6686, loss:2.6525 best_pre_heat_loss selected\n",
      "batch_num: 1, c_loss:2.5408, val_loss:-0.5823, loss:2.6783 \n",
      "batch_num: 2, c_loss:2.3605, val_loss:-0.4190, loss:2.5996 \n",
      "batch_num: 3, c_loss:2.2306, val_loss:-0.3010, loss:2.5519 \n",
      "batch_num: 4, c_loss:2.1437, val_loss:-0.2217, loss:2.5463 \n",
      "batch_num: 5, c_loss:1.9829, val_loss:-0.0693, loss:2.4792 \n",
      "batch_num: 6, c_loss:1.7909, val_loss: 0.1119, loss:2.3963 \n",
      "batch_num: 7, c_loss:1.6481, val_loss: 0.2515, loss:2.3521 \n",
      "batch_num: 8, c_loss:1.5713, val_loss: 0.3323, loss:2.3610 \n",
      "batch_num: 9, c_loss:1.3341, val_loss: 0.5844, loss:2.2451 \n",
      "batch_num: 10, c_loss:1.1600, val_loss: 0.7838, loss:2.1867 \n",
      "batch_num: 11, c_loss:1.1391, val_loss: 0.8346, loss:2.2079 \n",
      "batch_num: 12, c_loss:1.0097, val_loss: 0.9955, loss:2.1903 \n",
      "batch_num: 13, c_loss:0.9770, val_loss: 1.0609, loss:2.1946 \n",
      "batch_num: 14, c_loss:0.8953, val_loss: 1.1731, loss:2.2055 \n",
      "batch_num: 15, c_loss:0.8167, val_loss: 1.2788, loss:2.2079 \n",
      "batch_num: 16, c_loss:0.7421, val_loss: 1.3810, loss:2.1638 \n",
      "batch_num: 17, c_loss:0.7335, val_loss: 1.4070, loss:2.1928 \n",
      "batch_num: 18, c_loss:0.6931, val_loss: 1.4586, loss:2.2144 \n",
      "batch_num: 19, c_loss:0.6430, val_loss: 1.5111, loss:2.1763 \n",
      "batch_num: 20, c_loss:0.6236, val_loss: 1.5329, loss:2.1845 \n",
      "batch_num: 21, c_loss:0.6276, val_loss: 1.5350, loss:2.1719 \n",
      "batch_num: 22, c_loss:0.6607, val_loss: 1.5031, loss:2.2269 \n",
      "batch_num: 23, c_loss:0.6330, val_loss: 1.5369, loss:2.2152 \n",
      "batch_num: 24, c_loss:0.6044, val_loss: 1.5633, loss:2.1813 \n",
      "batch_num: 25, c_loss:0.5964, val_loss: 1.5585, loss:2.1977 \n",
      "batch_num: 26, c_loss:0.5888, val_loss: 1.5477, loss:2.1818 \n",
      "batch_num: 27, c_loss:0.6046, val_loss: 1.5110, loss:2.1817 \n",
      "batch_num: 28, c_loss:0.5610, val_loss: 1.5442, loss:2.1064 \n",
      "batch_num: 29, c_loss:0.6356, val_loss: 1.4588, loss:2.1647 \n",
      "batch_num: 30, c_loss:0.5846, val_loss: 1.4967, loss:2.1482 Preheat End: Leader changed with val acc -0.6686\n",
      "batch_num: 31, c_loss:0.6380, val_loss: 1.4325, loss:2.0987 \n",
      "batch_num: 32, c_loss:0.6644, val_loss: 1.3943, loss:2.1123 \n",
      "batch_num: 33, c_loss:0.6525, val_loss: 1.3980, loss:2.0969 \n",
      "batch_num: 34, c_loss:0.6443, val_loss: 1.3943, loss:2.1017 \n",
      "batch_num: 35, c_loss:0.6753, val_loss: 1.3541, loss:2.1022 \n",
      "batch_num: 36, c_loss:0.6528, val_loss: 1.3731, loss:2.0849 \n",
      "batch_num: 37, c_loss:0.6834, val_loss: 1.3384, loss:2.0833 \n",
      "batch_num: 38, c_loss:0.7405, val_loss: 1.2782, loss:2.1317 \n",
      "batch_num: 39, c_loss:0.7411, val_loss: 1.2745, loss:2.1501 \n",
      "batch_num: 40, c_loss:0.7147, val_loss: 1.2948, loss:2.1035 \n",
      "batch_num: 41, c_loss:0.7482, val_loss: 1.2561, loss:2.1229 \n",
      "batch_num: 42, c_loss:0.7245, val_loss: 1.2739, loss:2.0958 \n",
      "batch_num: 43, c_loss:0.7044, val_loss: 1.2887, loss:2.0586 \n",
      "batch_num: 44, c_loss:0.7589, val_loss: 1.2331, loss:2.0913 \n",
      "batch_num: 45, c_loss:0.7215, val_loss: 1.2721, loss:2.0691 \n",
      "batch_num: 46, c_loss:0.7088, val_loss: 1.2903, loss:2.0651 \n",
      "batch_num: 47, c_loss:0.8166, val_loss: 1.1928, loss:2.1755 \n",
      "batch_num: 48, c_loss:0.6466, val_loss: 1.3657, loss:2.0608 \n",
      "batch_num: 49, c_loss:0.7040, val_loss: 1.3128, loss:2.0791 \n",
      "batch_num: 50, c_loss:0.6912, val_loss: 1.3302, loss:2.0366 \n",
      "batch_num: 51, c_loss:0.6875, val_loss: 1.3341, loss:2.0418 \n",
      "batch_num: 52, c_loss:0.6820, val_loss: 1.3391, loss:2.0689 \n",
      "batch_num: 53, c_loss:0.6974, val_loss: 1.3226, loss:2.0928 \n",
      "batch_num: 54, c_loss:0.7123, val_loss: 1.3083, loss:2.0810 \n",
      "batch_num: 55, c_loss:0.7016, val_loss: 1.3223, loss:2.0929 \n",
      "batch_num: 56, c_loss:0.6892, val_loss: 1.3377, loss:2.0526 \n",
      "batch_num: 57, c_loss:0.7050, val_loss: 1.3258, loss:2.0965 \n",
      "batch_num: 58, c_loss:0.7018, val_loss: 1.3331, loss:2.1037 \n",
      "batch_num: 59, c_loss:0.6932, val_loss: 1.3410, loss:2.0839 \n",
      "batch_num: 60, c_loss:0.6883, val_loss: 1.3448, loss:2.0602 \n",
      "batch_num: 61, c_loss:0.6447, val_loss: 1.3921, loss:2.0301 \n",
      "batch_num: 62, c_loss:0.6629, val_loss: 1.3775, loss:2.0533 \n",
      "batch_num: 63, c_loss:0.7054, val_loss: 1.3393, loss:2.0902 \n",
      "batch_num: 64, c_loss:0.6635, val_loss: 1.3839, loss:2.0692 \n",
      "batch_num: 65, c_loss:0.6595, val_loss: 1.3909, loss:2.0660 \n",
      "batch_num: 66, c_loss:0.6533, val_loss: 1.4026, loss:2.0544 \n",
      "batch_num: 67, c_loss:0.6949, val_loss: 1.3645, loss:2.0729 \n",
      "batch_num: 68, c_loss:0.6709, val_loss: 1.3961, loss:2.0524 \n",
      "batch_num: 69, c_loss:0.6883, val_loss: 1.3879, loss:2.0733 \n",
      "batch_num: 70, c_loss:0.7006, val_loss: 1.3809, loss:2.1062 \n",
      "batch_num: 71, c_loss:0.6445, val_loss: 1.4392, loss:2.0409 \n",
      "batch_num: 72, c_loss:0.6493, val_loss: 1.4337, loss:2.0233 \n",
      "batch_num: 73, c_loss:0.6574, val_loss: 1.4227, loss:2.0657 \n",
      "batch_num: 74, c_loss:0.6413, val_loss: 1.4343, loss:2.0291 \n",
      "batch_num: 75, c_loss:0.6448, val_loss: 1.4232, loss:2.0143 \n",
      "batch_num: 76, c_loss:0.6409, val_loss: 1.4224, loss:2.0270 \n",
      "batch_num: 77, c_loss:0.7316, val_loss: 1.3264, loss:2.1197 \n",
      "batch_num: 78, c_loss:0.6390, val_loss: 1.4097, loss:2.0119 \n",
      "batch_num: 79, c_loss:0.6615, val_loss: 1.3793, loss:2.0533 \n",
      "batch_num: 80, c_loss:0.6750, val_loss: 1.3646, loss:2.0752 \n",
      "batch_num: 81, c_loss:0.6602, val_loss: 1.3818, loss:2.0324 \n",
      "batch_num: 82, c_loss:0.6312, val_loss: 1.4115, loss:2.0327 \n",
      "batch_num: 83, c_loss:0.7222, val_loss: 1.3197, loss:2.0918 \n",
      "batch_num: 84, c_loss:0.6380, val_loss: 1.4052, loss:2.0164 \n",
      "batch_num: 85, c_loss:0.6300, val_loss: 1.4117, loss:2.0215 \n",
      "batch_num: 86, c_loss:0.6637, val_loss: 1.3763, loss:2.0744 \n",
      "batch_num: 87, c_loss:0.6300, val_loss: 1.4100, loss:2.0220 \n",
      "batch_num: 88, c_loss:0.7013, val_loss: 1.3394, loss:2.0613 \n",
      "batch_num: 89, c_loss:0.6231, val_loss: 1.4183, loss:1.9966 \n",
      "batch_num: 90, c_loss:0.6785, val_loss: 1.3574, loss:2.0710 \n",
      "batch_num: 91, c_loss:0.6356, val_loss: 1.3968, loss:2.0157 \n",
      "batch_num: 92, c_loss:0.7640, val_loss: 1.2651, loss:2.1706 \n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052), ('3', 99.62174940898345), ('4', 99.57446808510639), ('5', 99.52718676122932)])), ('2', OrderedDict([('2', 2.3016650342801177), ('3', 2.889324191968658), ('4', 2.7913809990205682), ('5', 2.742409402546523)])), ('3', OrderedDict([('3', 1.1205976520811098), ('4', 1.2273212379935965), ('5', 1.5474919957310567)])), ('4', OrderedDict([('4', 9.818731117824774), ('5', 9.365558912386707)])), ('5', OrderedDict([('5', 2.471003530005043)]))])\n",
      "OrderedDict([('1', OrderedDict([('1', 99.66903073286052), ('2', 99.66903073286052), ('3', 99.62174940898345), ('4', 99.57446808510639), ('5', 99.52718676122932)])), ('2', OrderedDict([('2', 2.3016650342801177), ('3', 2.889324191968658), ('4', 2.7913809990205682), ('5', 2.742409402546523)])), ('3', OrderedDict([('3', 1.1205976520811098), ('4', 1.2273212379935965), ('5', 1.5474919957310567)])), ('4', OrderedDict([('4', 9.818731117824774), ('5', 9.365558912386707)])), ('5', OrderedDict([('5', 2.471003530005043)]))])\n",
      "follower Task 1 average acc: 99.66903073286052\n",
      "follower Task 2 average acc: 50.98534788357032\n",
      "follower Task 3 average acc: 34.54389041767774\n",
      "follower Task 4 average acc: 28.35297535998633\n",
      "follower Task 5 average acc: 23.13073012037973\n",
      "leader Task 1 average acc: 99.66903073286052\n",
      "leader Task 2 average acc: 50.98534788357032\n",
      "leader Task 3 average acc: 34.54389041767774\n",
      "leader Task 4 average acc: 28.35297535998633\n",
      "leader Task 5 average acc: 23.13073012037973\n"
     ]
    }
   ],
   "source": [
    "avg_acc_history,leader_avg_acc_history = train(task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torchvision\\nimport os\\nfrom os import path\\nimport copy\\nimport numpy as np\\nimport torch.utils.data as data\\nfrom torchvision import transforms\\nfrom collections import OrderedDict\\n\\nbatch_size = 128\\nrepeat = 10\\nepoches = 1\\nalpha = 4\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\nclass CacheClassLabel(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that has a quick access to all labels of data.\\n    \"\"\"\\n    def __init__(self, dataset):\\n        super(CacheClassLabel, self).__init__()\\n        self.dataset = dataset\\n        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\\n        print(dataset.root)\\n        label_cache_filename = dataset.root + \\'/\\' +\\'_\\'+str(len(dataset))+\\'.pth\\'\\n        if path.exists(label_cache_filename):\\n            self.labels = torch.load(label_cache_filename)\\n        else:\\n            for i, data in enumerate(dataset):\\n                self.labels[i] = data[1]\\n            torch.save(self.labels, label_cache_filename)\\n        self.number_classes = len(torch.unique(self.labels))\\n\\n    def __len__(self):\\n        return len(self.dataset)\\n\\n    def __getitem__(self, index):\\n        img,target = self.dataset[index]\\n        return img, target\\n    \\nclass AppendName(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that also return the name of the dataset/task\\n    \"\"\"\\n    def __init__(self, dataset, name, first_class_ind=0):\\n        super(AppendName,self).__init__()\\n        self.dataset = dataset\\n        self.name = name\\n        self.first_class_ind = first_class_ind  # For remapping the class index\\n\\n    def __len__(self):\\n        return len(self.dataset)\\n\\n    def __getitem__(self, index):\\n        img,target = self.dataset[index]\\n        target = target + self.first_class_ind\\n        return img, target, self.name\\n    \\nclass Subclass(data.Dataset):\\n    \"\"\"\\n    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\\n    \"\"\"\\n    def __init__(self, dataset, class_list, remap=True):\\n        super(Subclass,self).__init__()\\n        assert isinstance(dataset, CacheClassLabel), \\'dataset must be wrapped by CacheClassLabel\\'\\n        self.dataset = dataset\\n        self.class_list = class_list\\n        self.remap = remap\\n        self.indices = []\\n        for c in class_list:\\n            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\\n        if remap:\\n            self.class_mapping = {c: i for i, c in enumerate(class_list)}\\n\\n    def __len__(self):\\n        return len(self.indices)\\n    def __getitem__(self, index):\\n        img,target = self.dataset[self.indices[index]]\\n        if self.remap:\\n            raw_target = target.item() if isinstance(target,torch.Tensor) else target\\n            target = self.class_mapping[raw_target]\\n        return img, target\\n\\ndef SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\\n    assert train_dataset.number_classes==val_dataset.number_classes,\\'Train/Val has different number of classes\\'\\n    num_classes =  train_dataset.number_classes\\n\\n    # Calculate the boundary index of classes for splits\\n    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\\n    split_boundaries = [0, first_split_sz]\\n    while split_boundaries[-1]<num_classes:\\n        split_boundaries.append(split_boundaries[-1]+other_split_sz)\\n    print(\\'split_boundaries:\\',split_boundaries)\\n    assert split_boundaries[-1]==num_classes,\\'Invalid split size\\'\\n\\n    # Assign classes to each splits\\n    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\\n    if not rand_split:\\n        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\\n    else:\\n        randseq = torch.randperm(num_classes)\\n        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\\n    print(class_lists)\\n\\n    # Generate the dicts of splits\\n    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\\n    train_dataset_splits = {}\\n    val_dataset_splits = {}\\n    task_output_space = {}\\n    for name,class_list in class_lists.items():\\n        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\\n        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\\n        task_output_space[name] = len(class_list)\\n\\n    return train_dataset_splits, val_dataset_splits, task_output_space\\n\\ndef MNIST(dataroot, train_aug=False):\\n    val_transform = transforms.Compose([\\n        transforms.Pad(2, fill=0, padding_mode=\\'constant\\'),\\n        transforms.ToTensor(),\\n        transforms.Normalize([0.5], [0.5]),\\n    ])\\n    train_transform = val_transform\\n    if train_aug:\\n        train_transform = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5], [0.5]),\\n        ])\\n\\n    train_dataset = torchvision.datasets.MNIST(\\n        root=dataroot,\\n        train=True,\\n        download=True,\\n        transform=train_transform\\n    )\\n    train_dataset = CacheClassLabel(train_dataset)\\n\\n    val_dataset = torchvision.datasets.MNIST(\\n        dataroot,\\n        train=False,\\n        transform=val_transform\\n    )\\n    val_dataset = CacheClassLabel(val_dataset)\\n\\n    return train_dataset, val_dataset\\n\\ntrain_dataset, val_dataset = MNIST(\\'./data\\', False)\\n\\ntrain_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\\n                                                                          first_split_sz=2,\\n                                                                          other_split_sz=2,\\n                                                                          rand_split=False,\\n                                                                          remap_class=False)\\n\\nclass MLP(nn.Module):\\n    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\\n        super(MLP, self).__init__()\\n        self.in_dim = in_channel*img_sz*img_sz\\n        self.linear = nn.Sequential(\\n            nn.Linear(self.in_dim, hidden_dim),\\n            nn.BatchNorm1d(hidden_dim),\\n            nn.ReLU(inplace=True),\\n            nn.Linear(hidden_dim, hidden_dim),\\n            nn.BatchNorm1d(hidden_dim),\\n            nn.ReLU(inplace=True),\\n        )\\n        self.last = nn.Linear(hidden_dim, out_dim)\\n\\n    def features(self, x):\\n        x = self.linear(x.view(-1,self.in_dim))\\n        return x\\n\\n    def logits(self, x):\\n        x = self.last(x)\\n        return x\\n\\n    def forward(self, x):\\n        x = self.features(x)\\n        x = self.logits(x)\\n        return x\\n\\ndef MLP400():\\n    return MLP(hidden_dim=400)\\n\\nclass AverageMeter(object):\\n    def __init__(self):\\n        self.reset()\\n\\n    def reset(self):\\n        self.val = 0\\n        self.avg = 0\\n        self.sum = 0\\n        self.count = 0\\n\\n    def update(self, val, n=1):\\n        self.val = val\\n        self.sum += val * n\\n        self.count += n\\n        self.avg = float(self.sum) / self.count\\n\\ndef accuracy(output, target):\\n    with torch.no_grad():\\n        _, predicted = torch.max(output.data, 1)\\n        batch_size = target.size(0)\\n        correct = (predicted == target).sum().item() * 100\\n    return correct / batch_size\\n\\ndef accumulate_acc(output, target, meter):\\n    acc = accuracy(output, target)\\n    meter.update(acc, len(target))\\n    return meter\\n\\ndef criterion_fn(criterion, preds, targets, valid_out_dim):\\n    if valid_out_dim != 0:\\n        pred = preds[:,:valid_out_dim]\\n    loss = criterion(pred, targets)\\n    return loss\\n\\ndef train_on_task(model, train_loader, optimizer, criterion, \\n                  valid_out_dim, best_model_wts, task_num, task_names):\\n    leader = MLP400().to(device)\\n    best_loss = float(\\'inf\\')\\n    if (best_model_wts):\\n        leader.load_state_dict(best_model_wts)\\n\\n    for epoch in range(epoches):\\n        train_acc = AverageMeter()\\n        batch_num = 0\\n        for images, labels, _ in train_loader:\\n            images, labels = images.to(device), labels.to(device)\\n\\n            with torch.no_grad():\\n                leader_outputs = leader(images)\\n\\n            model.train()\\n            follower_outputs = model(images)\\n\\n            # reg_loss = 0\\n            # for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\\n                # reg_loss += torch.norm(follower_para - lead_para, p = 2)\\n            \\n            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\\n            loss = c_loss + alpha * torch.mean((follower_outputs - leader_outputs) ** 2)\\n\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n\\n            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\\n            \\n            model.eval()\\n            with torch.no_grad():\\n                val_loss = AverageMeter()\\n\\n                for task in range(task_num + 1):\\n                    val_name = task_names[task]\\n                    val_data = val_dataset_splits[val_name]\\n                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\\n\\n                    for i, (input, target, _) in enumerate(val_loader):\\n                        input, target = input.to(device), target.to(device)\\n                        output = model(input)\\n                        loss_v = criterion(output, target).item()\\n\\n                        val_loss.update(loss_v, len(target))\\n\\n                    if val_loss.avg < best_loss:\\n                        best_loss = val_loss.avg\\n                        best_model_wts = copy.deepcopy(model.state_dict())\\n                        leader.load_state_dict(best_model_wts) \\n            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss_v:.4f}\")\\n            batch_num += 1\\n    return best_model_wts, best_loss\\n\\ndef train(task_names):\\n    acc_table = OrderedDict()\\n    valid_out_dim = 0\\n\\n    model = MLP400().to(device)\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\\n\\n    best_model_wts = None\\n    for i in range(len(task_names)):\\n        valid_out_dim += 2\\n        train_name = task_names[i]\\n        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\\n        \\n        print(f\\'=====Task: {train_name}=====\\')\\n        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\\n    \\n        acc_table[train_name] = OrderedDict()\\n\\n        for j in range(i+1):\\n            val_name = task_names[j]\\n            val_data = val_dataset_splits[val_name]\\n            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\\n            model.eval()\\n            val_acc = AverageMeter()\\n            with torch.no_grad():\\n                for i, (input, target, _) in enumerate(val_loader):\\n                    input, target = input.to(device), target.to(device)\\n                    output = model(input)\\n                    val_acc = accumulate_acc(output, target, val_acc)\\n\\n            acc_table[val_name][train_name] = val_acc.avg\\n\\n        print(acc_table)\\n\\n    avg_acc_history = [0] * len(task_names)\\n    for i in range(len(task_names)):\\n        train_name = task_names[i]\\n        cls_acc_sum = 0\\n        for j in range(i + 1):\\n            val_name = task_names[j]\\n            cls_acc_sum += acc_table[val_name][train_name]\\n\\n        avg_acc_history[i] = cls_acc_sum / (i + 1)\\n        print(\\'Task\\', train_name, \\'average acc:\\', avg_acc_history[i])\\n    \\n    return avg_acc_history\\n\\ntask_names = sorted(list(task_output_space.keys()), key=int)\\nprint(\\'Task order:\\',task_names)\\n\\navg_acc_history = train(task_names)\\n\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        print(dataset.root)\n",
    "        label_cache_filename = dataset.root + '/' +'_'+str(len(dataset))+'.pth'\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target\n",
    "\n",
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space\n",
    "\n",
    "def MNIST(dataroot, train_aug=False):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = MNIST('./data', False)\n",
    "\n",
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count\n",
    "\n",
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size\n",
    "\n",
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter\n",
    "\n",
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss\n",
    "\n",
    "def train_on_task(model, train_loader, optimizer, criterion, \n",
    "                  valid_out_dim, best_model_wts, task_num, task_names):\n",
    "    leader = MLP400().to(device)\n",
    "    best_loss = float('inf')\n",
    "    if (best_model_wts):\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                leader_outputs = leader(images)\n",
    "\n",
    "            model.train()\n",
    "            follower_outputs = model(images)\n",
    "\n",
    "            # reg_loss = 0\n",
    "            # for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
    "                # reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, follower_outputs, labels, valid_out_dim)\n",
    "            loss = c_loss + alpha * torch.mean((follower_outputs - leader_outputs) ** 2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = accumulate_acc(follower_outputs, labels, train_acc)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = AverageMeter()\n",
    "\n",
    "                for task in range(task_num + 1):\n",
    "                    val_name = task_names[task]\n",
    "                    val_data = val_dataset_splits[val_name]\n",
    "                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    for i, (input, target, _) in enumerate(val_loader):\n",
    "                        input, target = input.to(device), target.to(device)\n",
    "                        output = model(input)\n",
    "                        loss_v = criterion(output, target).item()\n",
    "\n",
    "                        val_loss.update(loss_v, len(target))\n",
    "\n",
    "                    if val_loss.avg < best_loss:\n",
    "                        best_loss = val_loss.avg\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss_v:.4f}\")\n",
    "            batch_num += 1\n",
    "    return best_model_wts, best_loss\n",
    "\n",
    "def train(task_names):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, i, task_names)\n",
    "    \n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "            model.eval()\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "        print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history\n",
    "\n",
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)\n",
    "\n",
    "avg_acc_history = train(task_names)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
