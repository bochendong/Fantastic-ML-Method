{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "lr = 0.01\n",
        "lambda_domain = 0.1  # Domain adaptation weight\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DANN (Domain Adversarial Neural Network)\n",
        "\n",
        "DANN is a domain adaptation method that uses adversarial training to learn domain-invariant features.\n",
        "\n",
        "**Key Components:**\n",
        "1. **Feature Extractor (Gf)**: Extracts features from input images\n",
        "2. **Label Classifier (Gy)**: Classifies the task labels\n",
        "3. **Domain Discriminator (Gd)**: Distinguishes between source and target domains\n",
        "4. **Gradient Reversal Layer (GRL)**: Reverses gradients during backpropagation to enable adversarial training\n",
        "\n",
        "**Training Objective:**\n",
        "- Minimize label classification loss on source domain\n",
        "- Maximize domain discrimination loss (via GRL) to learn domain-invariant features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Reversal Layer\n",
        "class GradientReversalLayer(torch.autograd.Function):\n",
        "    \"\"\"Gradient Reversal Layer for adversarial domain adaptation\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_param):\n",
        "        ctx.lambda_param = lambda_param\n",
        "        return x.view_as(x)\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.lambda_param, None\n",
        "\n",
        "class GRL(nn.Module):\n",
        "    def __init__(self, lambda_param=1.0):\n",
        "        super(GRL, self).__init__()\n",
        "        self.lambda_param = lambda_param\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return GradientReversalLayer.apply(x, self.lambda_param)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Extractor (Gf)\n",
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Feature extractor network\"\"\"\n",
        "    def __init__(self, input_channels=1):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=5),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=5),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label Classifier (Gy)\n",
        "class LabelClassifier(nn.Module):\n",
        "    \"\"\"Task label classifier\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LabelClassifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 100),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(100, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Domain Discriminator (Gd)\n",
        "class DomainDiscriminator(nn.Module):\n",
        "    \"\"\"Domain discriminator to distinguish source and target domains\"\"\"\n",
        "    def __init__(self):\n",
        "        super(DomainDiscriminator, self).__init__()\n",
        "        self.grl = GRL(lambda_param=lambda_domain)\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 100),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(100, 2)  # Binary classification: source or target\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.grl(x)\n",
        "        return self.discriminator(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete DANN Model\n",
        "class DANN(nn.Module):\n",
        "    \"\"\"Domain Adversarial Neural Network\"\"\"\n",
        "    def __init__(self, num_classes=10, input_channels=1):\n",
        "        super(DANN, self).__init__()\n",
        "        self.feature_extractor = FeatureExtractor(input_channels)\n",
        "        self.label_classifier = LabelClassifier(num_classes)\n",
        "        self.domain_discriminator = DomainDiscriminator()\n",
        "    \n",
        "    def forward(self, x, alpha=1.0):\n",
        "        # Extract features\n",
        "        features = self.feature_extractor(x)\n",
        "        \n",
        "        # Classify labels\n",
        "        class_output = self.label_classifier(features)\n",
        "        \n",
        "        # Discriminate domains (with gradient reversal)\n",
        "        domain_output = self.domain_discriminator(features)\n",
        "        \n",
        "        return class_output, domain_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare datasets (using MNIST as source, MNIST-M as target for demonstration)\n",
        "# In practice, you would use different domains (e.g., real photos vs. synthetic images)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Source domain: MNIST\n",
        "source_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# For demonstration, we'll use a subset of MNIST as \"target\" domain\n",
        "# In real applications, target would be a different dataset (e.g., MNIST-M, SVHN)\n",
        "target_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f'Source dataset size: {len(source_dataset)}')\n",
        "print(f'Target dataset size: {len(target_dataset)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model (MNIST is 1 channel)\n",
        "model = DANN(num_classes=10, input_channels=1).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "# Loss functions\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_domain = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, source_loader, target_loader, optimizer, epoch, num_epochs):\n",
        "    model.train()\n",
        "    \n",
        "    # Create iterators\n",
        "    source_iter = iter(source_loader)\n",
        "    target_iter = iter(target_loader)\n",
        "    \n",
        "    total_class_loss = 0\n",
        "    total_domain_loss = 0\n",
        "    correct_class = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    # Process batches\n",
        "    min_len = min(len(source_loader), len(target_loader))\n",
        "    \n",
        "    for batch_idx in range(min_len):\n",
        "        # Get source batch\n",
        "        try:\n",
        "            source_data, source_labels = next(source_iter)\n",
        "        except StopIteration:\n",
        "            source_iter = iter(source_loader)\n",
        "            source_data, source_labels = next(source_iter)\n",
        "        \n",
        "        # Get target batch\n",
        "        try:\n",
        "            target_data, _ = next(target_iter)\n",
        "        except StopIteration:\n",
        "            target_iter = iter(target_loader)\n",
        "            target_data, _ = next(target_iter)\n",
        "        \n",
        "        # Move to device\n",
        "        source_data = source_data.to(device)\n",
        "        source_labels = source_labels.to(device)\n",
        "        target_data = target_data.to(device)\n",
        "        \n",
        "        # Create domain labels: 0 for source, 1 for target\n",
        "        source_domain_labels = torch.zeros(source_data.size(0), dtype=torch.long).to(device)\n",
        "        target_domain_labels = torch.ones(target_data.size(0), dtype=torch.long).to(device)\n",
        "        \n",
        "        # Combine source and target\n",
        "        combined_data = torch.cat([source_data, target_data], dim=0)\n",
        "        combined_domain_labels = torch.cat([source_domain_labels, target_domain_labels], dim=0)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        class_output, domain_output = model(combined_data)\n",
        "        \n",
        "        # Split outputs\n",
        "        source_class_output = class_output[:source_data.size(0)]\n",
        "        source_domain_output = domain_output[:source_data.size(0)]\n",
        "        target_domain_output = domain_output[source_data.size(0):]\n",
        "        \n",
        "        # Classification loss (only on source domain)\n",
        "        class_loss = criterion_class(source_class_output, source_labels)\n",
        "        \n",
        "        # Domain loss (on both domains)\n",
        "        domain_loss = (criterion_domain(source_domain_output, source_domain_labels) + \n",
        "                      criterion_domain(target_domain_output, target_domain_labels)) / 2\n",
        "        \n",
        "        # Total loss\n",
        "        loss = class_loss + lambda_domain * domain_loss\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        total_class_loss += class_loss.item()\n",
        "        total_domain_loss += domain_loss.item()\n",
        "        _, predicted = source_class_output.max(1)\n",
        "        correct_class += predicted.eq(source_labels).sum().item()\n",
        "        total_samples += source_labels.size(0)\n",
        "        \n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{min_len}], '\n",
        "                  f'Class Loss: {class_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}, '\n",
        "                  f'Acc: {100.*correct_class/total_samples:.2f}%')\n",
        "    \n",
        "    avg_class_loss = total_class_loss / min_len\n",
        "    avg_domain_loss = total_domain_loss / min_len\n",
        "    accuracy = 100. * correct_class / total_samples\n",
        "    \n",
        "    return avg_class_loss, avg_domain_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    class_loss, domain_loss, accuracy = train_epoch(\n",
        "        model, source_loader, target_loader, optimizer, epoch, num_epochs\n",
        "    )\n",
        "    train_losses.append(class_loss)\n",
        "    train_accs.append(accuracy)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - Class Loss: {class_loss:.4f}, '\n",
        "          f'Domain Loss: {domain_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "    print('-' * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Classification Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation on target domain\n",
        "def evaluate(model, data_loader, domain_name='Target'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            class_output, _ = model(data)\n",
        "            _, predicted = class_output.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'{domain_name} Domain Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on source and target\n",
        "source_acc = evaluate(model, source_loader, 'Source')\n",
        "target_acc = evaluate(model, target_loader, 'Target')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
