{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import copy\n",
    "import math\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Split training set for training and validation\n",
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = len(train_set) - train_size\n",
    "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
    "\n",
    "# DataLoader for validation set\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "train_loader =  DataLoader(train_set, batch_size=256, shuffle=False, drop_last = True)\n",
    "test_loader =  DataLoader(test_set, batch_size=256, shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, label in testloader:\n",
    "        images, label = images.to(device), label.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoches, criterion, optimizer):\n",
    "    best_model_wts = None\n",
    "    best_loss = float('inf')\n",
    "    batch_num = 0\n",
    "    warm_up_batch = 3\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        if (best_model_wts):\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        prev_loss = float('inf')\n",
    "        for epoch in range(epoches):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    outputs = model(val_inputs)\n",
    "                    batch_loss = criterion(outputs, val_labels)\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            print(f\"Batch: {batch_num}, epoch: {epoch}, Train Loss: {loss.item()}, Val Loss: {val_loss}\")\n",
    "            if (prev_loss < val_loss and warm_up_batch < batch_num):\n",
    "                break\n",
    "            prev_loss = val_loss\n",
    "        with torch.no_grad():\n",
    "            test_acc = test(model, test_loader)\n",
    "            \n",
    "        print(f\"epoch: {epoch}, Test Acc: {test_acc}\")\n",
    "\n",
    "        batch_num += 1\n",
    "\n",
    "    return best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, epoch: 0, Train Loss: 2.300583839416504, Val Loss: 89.79979133605957\n",
      "Batch: 0, epoch: 1, Train Loss: 2.2727200984954834, Val Loss: 89.8000717163086\n",
      "Batch: 0, epoch: 2, Train Loss: 2.244594097137451, Val Loss: 89.79906725883484\n",
      "Batch: 0, epoch: 3, Train Loss: 2.2109744548797607, Val Loss: 89.79881739616394\n",
      "Batch: 0, epoch: 4, Train Loss: 2.187385320663452, Val Loss: 89.80052065849304\n",
      "Batch: 0, epoch: 5, Train Loss: 2.1453003883361816, Val Loss: 89.80090165138245\n",
      "Batch: 0, epoch: 6, Train Loss: 2.1085588932037354, Val Loss: 89.7977991104126\n",
      "Batch: 0, epoch: 7, Train Loss: 2.0575807094573975, Val Loss: 89.79571390151978\n",
      "Batch: 0, epoch: 8, Train Loss: 1.9971327781677246, Val Loss: 89.79208707809448\n",
      "Batch: 0, epoch: 9, Train Loss: 1.9513640403747559, Val Loss: 89.78596496582031\n",
      "Batch: 0, epoch: 10, Train Loss: 1.890354037284851, Val Loss: 89.77544331550598\n",
      "Batch: 0, epoch: 11, Train Loss: 1.8344842195510864, Val Loss: 89.76152038574219\n",
      "Batch: 0, epoch: 12, Train Loss: 1.7836235761642456, Val Loss: 89.74940037727356\n",
      "Batch: 0, epoch: 13, Train Loss: 1.7334939241409302, Val Loss: 89.73694610595703\n",
      "Batch: 0, epoch: 14, Train Loss: 1.7067369222640991, Val Loss: 89.73199439048767\n",
      "Batch: 0, epoch: 15, Train Loss: 1.675659418106079, Val Loss: 89.72382545471191\n",
      "Batch: 0, epoch: 16, Train Loss: 1.644167423248291, Val Loss: 89.70573592185974\n",
      "Batch: 0, epoch: 17, Train Loss: 1.619888186454773, Val Loss: 89.66067934036255\n",
      "Batch: 0, epoch: 18, Train Loss: 1.5964041948318481, Val Loss: 89.60523271560669\n",
      "Batch: 0, epoch: 19, Train Loss: 1.5732042789459229, Val Loss: 89.51657271385193\n",
      "epoch: 19, Test Acc: 0.11738782051282051\n",
      "Batch: 1, epoch: 0, Train Loss: 2.0910139083862305, Val Loss: 89.33890509605408\n",
      "Batch: 1, epoch: 1, Train Loss: 1.9319424629211426, Val Loss: 89.12067818641663\n",
      "Batch: 1, epoch: 2, Train Loss: 1.8028148412704468, Val Loss: 88.88727569580078\n",
      "Batch: 1, epoch: 3, Train Loss: 1.7274895906448364, Val Loss: 89.00806927680969\n",
      "Batch: 1, epoch: 4, Train Loss: 1.6678987741470337, Val Loss: 89.13103723526001\n",
      "Batch: 1, epoch: 5, Train Loss: 1.6228160858154297, Val Loss: 88.98298978805542\n",
      "Batch: 1, epoch: 6, Train Loss: 1.591198205947876, Val Loss: 88.69694304466248\n",
      "Batch: 1, epoch: 7, Train Loss: 1.5622769594192505, Val Loss: 88.48027658462524\n",
      "Batch: 1, epoch: 8, Train Loss: 1.5375525951385498, Val Loss: 88.37360191345215\n",
      "Batch: 1, epoch: 9, Train Loss: 1.5168884992599487, Val Loss: 88.34171152114868\n",
      "Batch: 1, epoch: 10, Train Loss: 1.505112886428833, Val Loss: 88.41983079910278\n",
      "Batch: 1, epoch: 11, Train Loss: 1.4946082830429077, Val Loss: 88.48380875587463\n",
      "Batch: 1, epoch: 12, Train Loss: 1.4885106086730957, Val Loss: 88.2987654209137\n",
      "Batch: 1, epoch: 13, Train Loss: 1.4816718101501465, Val Loss: 87.9096245765686\n",
      "Batch: 1, epoch: 14, Train Loss: 1.477455496788025, Val Loss: 87.41241097450256\n",
      "Batch: 1, epoch: 15, Train Loss: 1.4757989645004272, Val Loss: 86.94656586647034\n",
      "Batch: 1, epoch: 16, Train Loss: 1.4747984409332275, Val Loss: 86.55573725700378\n",
      "Batch: 1, epoch: 17, Train Loss: 1.472708821296692, Val Loss: 86.1991024017334\n",
      "Batch: 1, epoch: 18, Train Loss: 1.4698957204818726, Val Loss: 85.87635445594788\n",
      "Batch: 1, epoch: 19, Train Loss: 1.4701049327850342, Val Loss: 85.5269410610199\n",
      "epoch: 19, Test Acc: 0.27123397435897434\n",
      "Batch: 2, epoch: 0, Train Loss: 2.0552453994750977, Val Loss: 84.52156972885132\n",
      "Batch: 2, epoch: 1, Train Loss: 1.9144866466522217, Val Loss: 83.72923970222473\n",
      "Batch: 2, epoch: 2, Train Loss: 1.8145264387130737, Val Loss: 83.36229300498962\n",
      "Batch: 2, epoch: 3, Train Loss: 1.764601230621338, Val Loss: 83.11811876296997\n",
      "Batch: 2, epoch: 4, Train Loss: 1.717673897743225, Val Loss: 82.79178714752197\n",
      "Batch: 2, epoch: 5, Train Loss: 1.6612052917480469, Val Loss: 82.79469394683838\n",
      "Batch: 2, epoch: 6, Train Loss: 1.6237843036651611, Val Loss: 83.14476609230042\n",
      "Batch: 2, epoch: 7, Train Loss: 1.5940121412277222, Val Loss: 83.48219680786133\n",
      "Batch: 2, epoch: 8, Train Loss: 1.579274296760559, Val Loss: 83.85617399215698\n",
      "Batch: 2, epoch: 9, Train Loss: 1.5645705461502075, Val Loss: 84.35060548782349\n",
      "Batch: 2, epoch: 10, Train Loss: 1.5556038618087769, Val Loss: 84.91775178909302\n",
      "Batch: 2, epoch: 11, Train Loss: 1.5469342470169067, Val Loss: 85.35461044311523\n",
      "Batch: 2, epoch: 12, Train Loss: 1.5340582132339478, Val Loss: 85.75372838973999\n",
      "Batch: 2, epoch: 13, Train Loss: 1.521598219871521, Val Loss: 85.89270734786987\n",
      "Batch: 2, epoch: 14, Train Loss: 1.5109238624572754, Val Loss: 85.94244623184204\n",
      "Batch: 2, epoch: 15, Train Loss: 1.508086085319519, Val Loss: 86.0789475440979\n",
      "Batch: 2, epoch: 16, Train Loss: 1.4982916116714478, Val Loss: 85.97245287895203\n",
      "Batch: 2, epoch: 17, Train Loss: 1.496671199798584, Val Loss: 85.57739925384521\n",
      "Batch: 2, epoch: 18, Train Loss: 1.4925060272216797, Val Loss: 84.94770288467407\n",
      "Batch: 2, epoch: 19, Train Loss: 1.4855488538742065, Val Loss: 84.30835437774658\n",
      "epoch: 19, Test Acc: 0.2987780448717949\n",
      "Batch: 3, epoch: 0, Train Loss: 2.0721569061279297, Val Loss: 82.51460814476013\n",
      "Batch: 3, epoch: 1, Train Loss: 1.9262027740478516, Val Loss: 82.74165725708008\n",
      "Batch: 3, epoch: 2, Train Loss: 1.8367582559585571, Val Loss: 82.90229368209839\n",
      "Batch: 3, epoch: 3, Train Loss: 1.766525149345398, Val Loss: 83.45197534561157\n",
      "Batch: 3, epoch: 4, Train Loss: 1.7116109132766724, Val Loss: 84.32349967956543\n",
      "Batch: 3, epoch: 5, Train Loss: 1.666748046875, Val Loss: 84.73871755599976\n",
      "Batch: 3, epoch: 6, Train Loss: 1.6281406879425049, Val Loss: 84.44949793815613\n",
      "Batch: 3, epoch: 7, Train Loss: 1.587310791015625, Val Loss: 84.10030961036682\n",
      "Batch: 3, epoch: 8, Train Loss: 1.5654891729354858, Val Loss: 83.74628973007202\n",
      "Batch: 3, epoch: 9, Train Loss: 1.5552598237991333, Val Loss: 83.61978673934937\n",
      "Batch: 3, epoch: 10, Train Loss: 1.542697548866272, Val Loss: 83.88083338737488\n",
      "Batch: 3, epoch: 11, Train Loss: 1.5260837078094482, Val Loss: 84.24227714538574\n",
      "Batch: 3, epoch: 12, Train Loss: 1.5176939964294434, Val Loss: 84.4677426815033\n",
      "Batch: 3, epoch: 13, Train Loss: 1.5095205307006836, Val Loss: 84.32725214958191\n",
      "Batch: 3, epoch: 14, Train Loss: 1.5049762725830078, Val Loss: 84.0481927394867\n",
      "Batch: 3, epoch: 15, Train Loss: 1.494705080986023, Val Loss: 83.71757578849792\n",
      "Batch: 3, epoch: 16, Train Loss: 1.4854092597961426, Val Loss: 83.52957272529602\n",
      "Batch: 3, epoch: 17, Train Loss: 1.4778904914855957, Val Loss: 83.43722534179688\n",
      "Batch: 3, epoch: 18, Train Loss: 1.4761139154434204, Val Loss: 83.40248036384583\n",
      "Batch: 3, epoch: 19, Train Loss: 1.4744633436203003, Val Loss: 83.34655809402466\n",
      "epoch: 19, Test Acc: 0.33543669871794873\n",
      "Batch: 4, epoch: 0, Train Loss: 2.0693724155426025, Val Loss: 82.15730619430542\n",
      "Batch: 4, epoch: 1, Train Loss: 1.946155071258545, Val Loss: 82.25234913825989\n",
      "epoch: 1, Test Acc: 0.3577724358974359\n",
      "Batch: 5, epoch: 0, Train Loss: 2.0981523990631104, Val Loss: 81.69717645645142\n",
      "Batch: 5, epoch: 1, Train Loss: 1.9569562673568726, Val Loss: 81.50263166427612\n",
      "Batch: 5, epoch: 2, Train Loss: 1.8696789741516113, Val Loss: 81.7658052444458\n",
      "epoch: 2, Test Acc: 0.38221153846153844\n",
      "Batch: 6, epoch: 0, Train Loss: 2.014338970184326, Val Loss: 80.94974708557129\n",
      "Batch: 6, epoch: 1, Train Loss: 1.9108150005340576, Val Loss: 81.594078540802\n",
      "epoch: 1, Test Acc: 0.3733974358974359\n",
      "Batch: 7, epoch: 0, Train Loss: 2.009906053543091, Val Loss: 81.45599174499512\n",
      "Batch: 7, epoch: 1, Train Loss: 1.9010237455368042, Val Loss: 81.89871716499329\n",
      "epoch: 1, Test Acc: 0.3580729166666667\n",
      "Batch: 8, epoch: 0, Train Loss: 2.0614240169525146, Val Loss: 80.18966221809387\n",
      "Batch: 8, epoch: 1, Train Loss: 1.9471969604492188, Val Loss: 80.70263361930847\n",
      "epoch: 1, Test Acc: 0.38912259615384615\n",
      "Batch: 9, epoch: 0, Train Loss: 1.9801044464111328, Val Loss: 80.22731947898865\n",
      "Batch: 9, epoch: 1, Train Loss: 1.9188979864120483, Val Loss: 81.63715648651123\n",
      "epoch: 1, Test Acc: 0.36338141025641024\n",
      "Batch: 10, epoch: 0, Train Loss: 2.01522159576416, Val Loss: 80.32074999809265\n",
      "Batch: 10, epoch: 1, Train Loss: 1.892350435256958, Val Loss: 81.95375895500183\n",
      "epoch: 1, Test Acc: 0.35106169871794873\n",
      "Batch: 11, epoch: 0, Train Loss: 2.0137038230895996, Val Loss: 80.85574102401733\n",
      "Batch: 11, epoch: 1, Train Loss: 1.9051660299301147, Val Loss: 82.1238443851471\n",
      "epoch: 1, Test Acc: 0.3506610576923077\n",
      "Batch: 12, epoch: 0, Train Loss: 2.004488945007324, Val Loss: 80.61338210105896\n",
      "Batch: 12, epoch: 1, Train Loss: 1.8899606466293335, Val Loss: 82.19569730758667\n",
      "epoch: 1, Test Acc: 0.3469551282051282\n",
      "Batch: 13, epoch: 0, Train Loss: 1.9968560934066772, Val Loss: 80.15378475189209\n",
      "Batch: 13, epoch: 1, Train Loss: 1.8715026378631592, Val Loss: 81.94639611244202\n",
      "epoch: 1, Test Acc: 0.36768830128205127\n",
      "Batch: 14, epoch: 0, Train Loss: 2.006397247314453, Val Loss: 81.07021474838257\n",
      "Batch: 14, epoch: 1, Train Loss: 1.909934401512146, Val Loss: 81.41218876838684\n",
      "epoch: 1, Test Acc: 0.3776041666666667\n",
      "Batch: 15, epoch: 0, Train Loss: 2.0013699531555176, Val Loss: 79.75600957870483\n",
      "Batch: 15, epoch: 1, Train Loss: 1.9069786071777344, Val Loss: 80.67792320251465\n",
      "epoch: 1, Test Acc: 0.3965344551282051\n",
      "Batch: 16, epoch: 0, Train Loss: 1.9745795726776123, Val Loss: 80.28217196464539\n",
      "Batch: 16, epoch: 1, Train Loss: 1.8839452266693115, Val Loss: 80.78944897651672\n",
      "epoch: 1, Test Acc: 0.39142628205128205\n",
      "Batch: 17, epoch: 0, Train Loss: 2.0082130432128906, Val Loss: 80.01104068756104\n",
      "Batch: 17, epoch: 1, Train Loss: 1.9118516445159912, Val Loss: 80.114187002182\n",
      "epoch: 1, Test Acc: 0.4113581730769231\n",
      "Batch: 18, epoch: 0, Train Loss: 2.004695415496826, Val Loss: 79.32419443130493\n",
      "Batch: 18, epoch: 1, Train Loss: 1.9100337028503418, Val Loss: 80.34135746955872\n",
      "epoch: 1, Test Acc: 0.40254407051282054\n",
      "Batch: 19, epoch: 0, Train Loss: 1.9627989530563354, Val Loss: 80.00591802597046\n",
      "Batch: 19, epoch: 1, Train Loss: 1.869920253753662, Val Loss: 82.25984048843384\n",
      "epoch: 1, Test Acc: 0.3444511217948718\n",
      "Batch: 20, epoch: 0, Train Loss: 1.973752498626709, Val Loss: 79.8772075176239\n",
      "Batch: 20, epoch: 1, Train Loss: 1.869409441947937, Val Loss: 82.06941986083984\n",
      "epoch: 1, Test Acc: 0.3486578525641026\n",
      "Batch: 21, epoch: 0, Train Loss: 1.9655389785766602, Val Loss: 79.07291269302368\n",
      "Batch: 21, epoch: 1, Train Loss: 1.8561971187591553, Val Loss: 81.2770504951477\n",
      "epoch: 1, Test Acc: 0.3773036858974359\n",
      "Batch: 22, epoch: 0, Train Loss: 1.9312487840652466, Val Loss: 80.3430757522583\n",
      "Batch: 22, epoch: 1, Train Loss: 1.8376542329788208, Val Loss: 81.73598551750183\n",
      "epoch: 1, Test Acc: 0.36087740384615385\n",
      "Batch: 23, epoch: 0, Train Loss: 1.961881160736084, Val Loss: 78.71236729621887\n",
      "Batch: 23, epoch: 1, Train Loss: 1.8742079734802246, Val Loss: 80.0855872631073\n",
      "epoch: 1, Test Acc: 0.4128605769230769\n",
      "Batch: 24, epoch: 0, Train Loss: 1.9725205898284912, Val Loss: 79.71707391738892\n",
      "Batch: 24, epoch: 1, Train Loss: 1.8500642776489258, Val Loss: 80.81813669204712\n",
      "epoch: 1, Test Acc: 0.3841145833333333\n",
      "Batch: 25, epoch: 0, Train Loss: 2.0111069679260254, Val Loss: 79.82304465770721\n",
      "Batch: 25, epoch: 1, Train Loss: 1.898284673690796, Val Loss: 81.88919568061829\n",
      "epoch: 1, Test Acc: 0.3489583333333333\n",
      "Batch: 26, epoch: 0, Train Loss: 1.9478015899658203, Val Loss: 80.48977243900299\n",
      "Batch: 26, epoch: 1, Train Loss: 1.8716754913330078, Val Loss: 81.61882472038269\n",
      "epoch: 1, Test Acc: 0.36147836538461536\n",
      "Batch: 27, epoch: 0, Train Loss: 2.0106282234191895, Val Loss: 79.58170855045319\n",
      "Batch: 27, epoch: 1, Train Loss: 1.9060897827148438, Val Loss: 81.01450252532959\n",
      "epoch: 1, Test Acc: 0.37900641025641024\n",
      "Batch: 28, epoch: 0, Train Loss: 1.9605051279067993, Val Loss: 79.86478912830353\n",
      "Batch: 28, epoch: 1, Train Loss: 1.8707776069641113, Val Loss: 81.5914077758789\n",
      "epoch: 1, Test Acc: 0.3607772435897436\n",
      "Batch: 29, epoch: 0, Train Loss: 1.9702608585357666, Val Loss: 80.14312827587128\n",
      "Batch: 29, epoch: 1, Train Loss: 1.8767917156219482, Val Loss: 81.58545899391174\n",
      "epoch: 1, Test Acc: 0.3573717948717949\n",
      "Batch: 30, epoch: 0, Train Loss: 1.966002345085144, Val Loss: 80.1651281118393\n",
      "Batch: 30, epoch: 1, Train Loss: 1.8532170057296753, Val Loss: 80.8001480102539\n",
      "epoch: 1, Test Acc: 0.3838141025641026\n",
      "Batch: 31, epoch: 0, Train Loss: 1.9513964653015137, Val Loss: 79.86781179904938\n",
      "Batch: 31, epoch: 1, Train Loss: 1.858651876449585, Val Loss: 81.54447078704834\n",
      "epoch: 1, Test Acc: 0.3620793269230769\n",
      "Batch: 32, epoch: 0, Train Loss: 1.9543843269348145, Val Loss: 79.69749677181244\n",
      "Batch: 32, epoch: 1, Train Loss: 1.8571853637695312, Val Loss: 81.0733232498169\n",
      "epoch: 1, Test Acc: 0.374599358974359\n",
      "Batch: 33, epoch: 0, Train Loss: 2.0027828216552734, Val Loss: 78.83186388015747\n",
      "Batch: 33, epoch: 1, Train Loss: 1.9026765823364258, Val Loss: 80.04847073554993\n",
      "epoch: 1, Test Acc: 0.41125801282051283\n",
      "Batch: 34, epoch: 0, Train Loss: 1.955763816833496, Val Loss: 78.74491059780121\n",
      "Batch: 34, epoch: 1, Train Loss: 1.8354580402374268, Val Loss: 79.49675643444061\n",
      "epoch: 1, Test Acc: 0.433994391025641\n",
      "Batch: 35, epoch: 0, Train Loss: 1.9427286386489868, Val Loss: 79.35007083415985\n",
      "Batch: 35, epoch: 1, Train Loss: 1.8311935663223267, Val Loss: 80.43864822387695\n",
      "epoch: 1, Test Acc: 0.39042467948717946\n",
      "Batch: 36, epoch: 0, Train Loss: 1.9281164407730103, Val Loss: 79.33961498737335\n",
      "Batch: 36, epoch: 1, Train Loss: 1.8380757570266724, Val Loss: 81.26575803756714\n",
      "epoch: 1, Test Acc: 0.3733974358974359\n",
      "Batch: 37, epoch: 0, Train Loss: 1.9388664960861206, Val Loss: 78.87062859535217\n",
      "Batch: 37, epoch: 1, Train Loss: 1.826912522315979, Val Loss: 79.93497848510742\n",
      "epoch: 1, Test Acc: 0.41235977564102566\n",
      "Batch: 38, epoch: 0, Train Loss: 1.94025719165802, Val Loss: 78.57819271087646\n",
      "Batch: 38, epoch: 1, Train Loss: 1.8317651748657227, Val Loss: 80.19346952438354\n",
      "epoch: 1, Test Acc: 0.4049479166666667\n",
      "Batch: 39, epoch: 0, Train Loss: 1.9858202934265137, Val Loss: 79.54008626937866\n",
      "Batch: 39, epoch: 1, Train Loss: 1.9024296998977661, Val Loss: 80.78671312332153\n",
      "epoch: 1, Test Acc: 0.39052483974358976\n",
      "Batch: 40, epoch: 0, Train Loss: 1.9411046504974365, Val Loss: 78.87158203125\n",
      "Batch: 40, epoch: 1, Train Loss: 1.8326199054718018, Val Loss: 79.4112560749054\n",
      "epoch: 1, Test Acc: 0.4244791666666667\n",
      "Batch: 41, epoch: 0, Train Loss: 1.9627503156661987, Val Loss: 78.32565724849701\n",
      "Batch: 41, epoch: 1, Train Loss: 1.8667101860046387, Val Loss: 80.39496719837189\n",
      "epoch: 1, Test Acc: 0.39042467948717946\n",
      "Batch: 42, epoch: 0, Train Loss: 1.9096307754516602, Val Loss: 80.61977791786194\n",
      "Batch: 42, epoch: 1, Train Loss: 1.8229120969772339, Val Loss: 82.5047836303711\n",
      "epoch: 1, Test Acc: 0.3401442307692308\n",
      "Batch: 43, epoch: 0, Train Loss: 1.9171912670135498, Val Loss: 79.72970926761627\n",
      "Batch: 43, epoch: 1, Train Loss: 1.7887612581253052, Val Loss: 80.92809927463531\n",
      "epoch: 1, Test Acc: 0.38782051282051283\n",
      "Batch: 44, epoch: 0, Train Loss: 1.9287866353988647, Val Loss: 79.38561010360718\n",
      "Batch: 44, epoch: 1, Train Loss: 1.8569716215133667, Val Loss: 80.4628928899765\n",
      "epoch: 1, Test Acc: 0.39853766025641024\n",
      "Batch: 45, epoch: 0, Train Loss: 1.9560084342956543, Val Loss: 78.53968131542206\n",
      "Batch: 45, epoch: 1, Train Loss: 1.845106840133667, Val Loss: 79.37198626995087\n",
      "epoch: 1, Test Acc: 0.4247796474358974\n",
      "Batch: 46, epoch: 0, Train Loss: 1.963736653327942, Val Loss: 78.5041035413742\n",
      "Batch: 46, epoch: 1, Train Loss: 1.8246166706085205, Val Loss: 79.59185564517975\n",
      "epoch: 1, Test Acc: 0.42077323717948717\n",
      "Batch: 47, epoch: 0, Train Loss: 1.914018154144287, Val Loss: 78.16423654556274\n",
      "Batch: 47, epoch: 1, Train Loss: 1.8422410488128662, Val Loss: 79.23446786403656\n",
      "epoch: 1, Test Acc: 0.4316907051282051\n",
      "Batch: 48, epoch: 0, Train Loss: 1.914747953414917, Val Loss: 78.3053982257843\n",
      "Batch: 48, epoch: 1, Train Loss: 1.8196322917938232, Val Loss: 78.95620346069336\n",
      "epoch: 1, Test Acc: 0.4384014423076923\n",
      "Batch: 49, epoch: 0, Train Loss: 1.93822181224823, Val Loss: 77.3740907907486\n",
      "Batch: 49, epoch: 1, Train Loss: 1.861925721168518, Val Loss: 77.58087646961212\n",
      "epoch: 1, Test Acc: 0.480068108974359\n",
      "Batch: 50, epoch: 0, Train Loss: 1.908691167831421, Val Loss: 77.49340057373047\n",
      "Batch: 50, epoch: 1, Train Loss: 1.8186554908752441, Val Loss: 78.40130925178528\n",
      "epoch: 1, Test Acc: 0.45322516025641024\n",
      "Batch: 51, epoch: 0, Train Loss: 1.940368413925171, Val Loss: 77.29022562503815\n",
      "Batch: 51, epoch: 1, Train Loss: 1.824280023574829, Val Loss: 77.9031811952591\n",
      "epoch: 1, Test Acc: 0.473056891025641\n",
      "Batch: 52, epoch: 0, Train Loss: 1.9083491563796997, Val Loss: 77.47784471511841\n",
      "Batch: 52, epoch: 1, Train Loss: 1.7994961738586426, Val Loss: 78.85274875164032\n",
      "epoch: 1, Test Acc: 0.44581330128205127\n",
      "Batch: 53, epoch: 0, Train Loss: 1.9321722984313965, Val Loss: 77.12936580181122\n",
      "Batch: 53, epoch: 1, Train Loss: 1.804811954498291, Val Loss: 78.9963059425354\n",
      "epoch: 1, Test Acc: 0.43689903846153844\n",
      "Batch: 54, epoch: 0, Train Loss: 1.9473958015441895, Val Loss: 78.18269228935242\n",
      "Batch: 54, epoch: 1, Train Loss: 1.8511046171188354, Val Loss: 78.81543338298798\n",
      "epoch: 1, Test Acc: 0.4417067307692308\n",
      "Batch: 55, epoch: 0, Train Loss: 1.9384777545928955, Val Loss: 76.7982177734375\n",
      "Batch: 55, epoch: 1, Train Loss: 1.8493760824203491, Val Loss: 77.82006895542145\n",
      "epoch: 1, Test Acc: 0.4712540064102564\n",
      "Batch: 56, epoch: 0, Train Loss: 1.982662558555603, Val Loss: 77.8427482843399\n",
      "Batch: 56, epoch: 1, Train Loss: 1.8677054643630981, Val Loss: 79.61334931850433\n",
      "epoch: 1, Test Acc: 0.42267628205128205\n",
      "Batch: 57, epoch: 0, Train Loss: 1.888700246810913, Val Loss: 76.56962084770203\n",
      "Batch: 57, epoch: 1, Train Loss: 1.8187907934188843, Val Loss: 76.89462852478027\n",
      "epoch: 1, Test Acc: 0.5032051282051282\n",
      "Batch: 58, epoch: 0, Train Loss: 1.9234157800674438, Val Loss: 77.31537914276123\n",
      "Batch: 58, epoch: 1, Train Loss: 1.8373862504959106, Val Loss: 79.59086585044861\n",
      "epoch: 1, Test Acc: 0.4244791666666667\n",
      "Batch: 59, epoch: 0, Train Loss: 1.911032795906067, Val Loss: 77.8506201505661\n",
      "Batch: 59, epoch: 1, Train Loss: 1.8285350799560547, Val Loss: 79.57581031322479\n",
      "epoch: 1, Test Acc: 0.425380608974359\n",
      "Batch: 60, epoch: 0, Train Loss: 1.9116307497024536, Val Loss: 77.00957345962524\n",
      "Batch: 60, epoch: 1, Train Loss: 1.8238379955291748, Val Loss: 78.84645783901215\n",
      "epoch: 1, Test Acc: 0.44290865384615385\n",
      "Batch: 61, epoch: 0, Train Loss: 1.8716928958892822, Val Loss: 77.37215268611908\n",
      "Batch: 61, epoch: 1, Train Loss: 1.7837090492248535, Val Loss: 79.07606744766235\n",
      "epoch: 1, Test Acc: 0.43529647435897434\n",
      "Batch: 62, epoch: 0, Train Loss: 1.9218246936798096, Val Loss: 77.28691375255585\n",
      "Batch: 62, epoch: 1, Train Loss: 1.7843587398529053, Val Loss: 79.276620388031\n",
      "epoch: 1, Test Acc: 0.4316907051282051\n",
      "Batch: 63, epoch: 0, Train Loss: 1.904706358909607, Val Loss: 77.84931743144989\n",
      "Batch: 63, epoch: 1, Train Loss: 1.8038486242294312, Val Loss: 79.54651927947998\n",
      "epoch: 1, Test Acc: 0.4241786858974359\n",
      "Batch: 64, epoch: 0, Train Loss: 1.9140880107879639, Val Loss: 77.03722536563873\n",
      "Batch: 64, epoch: 1, Train Loss: 1.7806540727615356, Val Loss: 79.09678554534912\n",
      "epoch: 1, Test Acc: 0.4388020833333333\n",
      "Batch: 65, epoch: 0, Train Loss: 1.9161863327026367, Val Loss: 77.08264529705048\n",
      "Batch: 65, epoch: 1, Train Loss: 1.7948930263519287, Val Loss: 79.49143159389496\n",
      "epoch: 1, Test Acc: 0.42127403846153844\n",
      "Batch: 66, epoch: 0, Train Loss: 1.8989133834838867, Val Loss: 77.26417791843414\n",
      "Batch: 66, epoch: 1, Train Loss: 1.7798763513565063, Val Loss: 80.02222347259521\n",
      "epoch: 1, Test Acc: 0.40514823717948717\n",
      "Batch: 67, epoch: 0, Train Loss: 1.9637593030929565, Val Loss: 78.13922214508057\n",
      "Batch: 67, epoch: 1, Train Loss: 1.8749077320098877, Val Loss: 80.51776194572449\n",
      "epoch: 1, Test Acc: 0.39563301282051283\n",
      "Batch: 68, epoch: 0, Train Loss: 1.8961784839630127, Val Loss: 77.39389324188232\n",
      "Batch: 68, epoch: 1, Train Loss: 1.767651081085205, Val Loss: 79.8790054321289\n",
      "epoch: 1, Test Acc: 0.41025641025641024\n",
      "Batch: 69, epoch: 0, Train Loss: 1.9203696250915527, Val Loss: 77.05473065376282\n",
      "Batch: 69, epoch: 1, Train Loss: 1.8361608982086182, Val Loss: 78.9063560962677\n",
      "epoch: 1, Test Acc: 0.43760016025641024\n",
      "Batch: 70, epoch: 0, Train Loss: 1.927951693534851, Val Loss: 77.10833656787872\n",
      "Batch: 70, epoch: 1, Train Loss: 1.8285356760025024, Val Loss: 78.70157551765442\n",
      "epoch: 1, Test Acc: 0.44360977564102566\n",
      "Batch: 71, epoch: 0, Train Loss: 1.8988361358642578, Val Loss: 77.04722607135773\n",
      "Batch: 71, epoch: 1, Train Loss: 1.787102460861206, Val Loss: 79.25736129283905\n",
      "epoch: 1, Test Acc: 0.43028846153846156\n",
      "Batch: 72, epoch: 0, Train Loss: 1.9305469989776611, Val Loss: 77.48439812660217\n",
      "Batch: 72, epoch: 1, Train Loss: 1.8373299837112427, Val Loss: 79.65722012519836\n",
      "epoch: 1, Test Acc: 0.4159655448717949\n",
      "Batch: 73, epoch: 0, Train Loss: 1.918728232383728, Val Loss: 77.26489472389221\n",
      "Batch: 73, epoch: 1, Train Loss: 1.8059473037719727, Val Loss: 79.18073725700378\n",
      "epoch: 1, Test Acc: 0.4273838141025641\n",
      "Batch: 74, epoch: 0, Train Loss: 1.9440125226974487, Val Loss: 78.00301504135132\n",
      "Batch: 74, epoch: 1, Train Loss: 1.8371716737747192, Val Loss: 82.08770751953125\n",
      "epoch: 1, Test Acc: 0.34845753205128205\n",
      "Batch: 75, epoch: 0, Train Loss: 1.8904533386230469, Val Loss: 79.03243780136108\n",
      "Batch: 75, epoch: 1, Train Loss: 1.7975058555603027, Val Loss: 83.00027060508728\n",
      "epoch: 1, Test Acc: 0.32181490384615385\n",
      "Batch: 76, epoch: 0, Train Loss: 1.9164434671401978, Val Loss: 78.06710684299469\n",
      "Batch: 76, epoch: 1, Train Loss: 1.844266414642334, Val Loss: 80.26624321937561\n",
      "epoch: 1, Test Acc: 0.3965344551282051\n",
      "Batch: 77, epoch: 0, Train Loss: 1.9078254699707031, Val Loss: 75.97926771640778\n",
      "Batch: 77, epoch: 1, Train Loss: 1.8327666521072388, Val Loss: 76.98524272441864\n",
      "epoch: 1, Test Acc: 0.4866786858974359\n",
      "Batch: 78, epoch: 0, Train Loss: 1.9356577396392822, Val Loss: 77.21424400806427\n",
      "Batch: 78, epoch: 1, Train Loss: 1.8385460376739502, Val Loss: 79.76652324199677\n",
      "epoch: 1, Test Acc: 0.4124599358974359\n",
      "Batch: 79, epoch: 0, Train Loss: 1.8999135494232178, Val Loss: 77.13590550422668\n",
      "Batch: 79, epoch: 1, Train Loss: 1.7939728498458862, Val Loss: 79.53331983089447\n",
      "epoch: 1, Test Acc: 0.4166666666666667\n",
      "Batch: 80, epoch: 0, Train Loss: 1.913000464439392, Val Loss: 77.64369297027588\n",
      "Batch: 80, epoch: 1, Train Loss: 1.7836211919784546, Val Loss: 80.27275013923645\n",
      "epoch: 1, Test Acc: 0.39513221153846156\n",
      "Batch: 81, epoch: 0, Train Loss: 1.93229079246521, Val Loss: 77.68581712245941\n",
      "Batch: 81, epoch: 1, Train Loss: 1.827799916267395, Val Loss: 80.16904640197754\n",
      "epoch: 1, Test Acc: 0.39633413461538464\n",
      "Batch: 82, epoch: 0, Train Loss: 1.963687539100647, Val Loss: 77.00563395023346\n",
      "Batch: 82, epoch: 1, Train Loss: 1.8666458129882812, Val Loss: 79.35520327091217\n",
      "epoch: 1, Test Acc: 0.4211738782051282\n",
      "Batch: 83, epoch: 0, Train Loss: 1.9671516418457031, Val Loss: 77.57093143463135\n",
      "Batch: 83, epoch: 1, Train Loss: 1.8641550540924072, Val Loss: 80.1193333864212\n",
      "epoch: 1, Test Acc: 0.39833733974358976\n",
      "Batch: 84, epoch: 0, Train Loss: 1.9271091222763062, Val Loss: 77.31249439716339\n",
      "Batch: 84, epoch: 1, Train Loss: 1.8239163160324097, Val Loss: 79.53879368305206\n",
      "epoch: 1, Test Acc: 0.4166666666666667\n",
      "Batch: 85, epoch: 0, Train Loss: 1.8656584024429321, Val Loss: 77.5546680688858\n",
      "Batch: 85, epoch: 1, Train Loss: 1.7638987302780151, Val Loss: 80.21182584762573\n",
      "epoch: 1, Test Acc: 0.38952323717948717\n",
      "Batch: 86, epoch: 0, Train Loss: 1.9065569639205933, Val Loss: 76.53293871879578\n",
      "Batch: 86, epoch: 1, Train Loss: 1.792739748954773, Val Loss: 78.15634512901306\n",
      "epoch: 1, Test Acc: 0.45693108974358976\n",
      "Batch: 87, epoch: 0, Train Loss: 1.9172303676605225, Val Loss: 77.51978433132172\n",
      "Batch: 87, epoch: 1, Train Loss: 1.7926849126815796, Val Loss: 79.45396173000336\n",
      "epoch: 1, Test Acc: 0.41856971153846156\n",
      "Batch: 88, epoch: 0, Train Loss: 1.8997255563735962, Val Loss: 76.57260167598724\n",
      "Batch: 88, epoch: 1, Train Loss: 1.8017113208770752, Val Loss: 78.38195145130157\n",
      "epoch: 1, Test Acc: 0.45032051282051283\n",
      "Batch: 89, epoch: 0, Train Loss: 1.9194402694702148, Val Loss: 76.38625407218933\n",
      "Batch: 89, epoch: 1, Train Loss: 1.8378489017486572, Val Loss: 78.19302320480347\n",
      "epoch: 1, Test Acc: 0.4511217948717949\n",
      "Batch: 90, epoch: 0, Train Loss: 1.9355645179748535, Val Loss: 76.42859590053558\n",
      "Batch: 90, epoch: 1, Train Loss: 1.8388925790786743, Val Loss: 78.13097155094147\n",
      "epoch: 1, Test Acc: 0.4586338141025641\n",
      "Batch: 91, epoch: 0, Train Loss: 1.882454752922058, Val Loss: 76.16118669509888\n",
      "Batch: 91, epoch: 1, Train Loss: 1.8041386604309082, Val Loss: 77.27893781661987\n",
      "epoch: 1, Test Acc: 0.48768028846153844\n",
      "Batch: 92, epoch: 0, Train Loss: 1.9506686925888062, Val Loss: 76.0728999376297\n",
      "Batch: 92, epoch: 1, Train Loss: 1.8631503582000732, Val Loss: 77.94142258167267\n",
      "epoch: 1, Test Acc: 0.47095352564102566\n",
      "Batch: 93, epoch: 0, Train Loss: 1.9189131259918213, Val Loss: 76.27827060222626\n",
      "Batch: 93, epoch: 1, Train Loss: 1.8232784271240234, Val Loss: 78.1733968257904\n",
      "epoch: 1, Test Acc: 0.4561298076923077\n",
      "Batch: 94, epoch: 0, Train Loss: 1.8952820301055908, Val Loss: 75.87453866004944\n",
      "Batch: 94, epoch: 1, Train Loss: 1.793717861175537, Val Loss: 77.62614917755127\n",
      "epoch: 1, Test Acc: 0.4701522435897436\n",
      "Batch: 95, epoch: 0, Train Loss: 1.9359626770019531, Val Loss: 77.55922520160675\n",
      "Batch: 95, epoch: 1, Train Loss: 1.8651251792907715, Val Loss: 78.46298801898956\n",
      "epoch: 1, Test Acc: 0.45072115384615385\n",
      "Batch: 96, epoch: 0, Train Loss: 1.9012991189956665, Val Loss: 76.06180477142334\n",
      "Batch: 96, epoch: 1, Train Loss: 1.8246055841445923, Val Loss: 76.91712582111359\n",
      "epoch: 1, Test Acc: 0.4980969551282051\n",
      "Batch: 97, epoch: 0, Train Loss: 1.9530651569366455, Val Loss: 75.50533103942871\n",
      "Batch: 97, epoch: 1, Train Loss: 1.8534904718399048, Val Loss: 77.36031651496887\n",
      "epoch: 1, Test Acc: 0.4816706730769231\n",
      "Batch: 98, epoch: 0, Train Loss: 1.8967775106430054, Val Loss: 77.06922650337219\n",
      "Batch: 98, epoch: 1, Train Loss: 1.8030321598052979, Val Loss: 78.92276644706726\n",
      "epoch: 1, Test Acc: 0.437099358974359\n",
      "Batch: 99, epoch: 0, Train Loss: 1.8499258756637573, Val Loss: 77.2982531785965\n",
      "Batch: 99, epoch: 1, Train Loss: 1.7570122480392456, Val Loss: 79.80161821842194\n",
      "epoch: 1, Test Acc: 0.4126602564102564\n",
      "Batch: 100, epoch: 0, Train Loss: 1.9023350477218628, Val Loss: 77.19848310947418\n",
      "Batch: 100, epoch: 1, Train Loss: 1.809495449066162, Val Loss: 78.66124391555786\n",
      "epoch: 1, Test Acc: 0.4456129807692308\n",
      "Batch: 101, epoch: 0, Train Loss: 1.9158697128295898, Val Loss: 76.6795529127121\n",
      "Batch: 101, epoch: 1, Train Loss: 1.839917778968811, Val Loss: 78.58810126781464\n",
      "epoch: 1, Test Acc: 0.4517227564102564\n",
      "Batch: 102, epoch: 0, Train Loss: 1.8493330478668213, Val Loss: 76.85917568206787\n",
      "Batch: 102, epoch: 1, Train Loss: 1.7459325790405273, Val Loss: 79.7167717218399\n",
      "epoch: 1, Test Acc: 0.42207532051282054\n",
      "Batch: 103, epoch: 0, Train Loss: 1.8983734846115112, Val Loss: 77.24267423152924\n",
      "Batch: 103, epoch: 1, Train Loss: 1.768777847290039, Val Loss: 80.02653396129608\n",
      "epoch: 1, Test Acc: 0.40795272435897434\n",
      "Batch: 104, epoch: 0, Train Loss: 1.9222970008850098, Val Loss: 77.11573374271393\n",
      "Batch: 104, epoch: 1, Train Loss: 1.835252285003662, Val Loss: 78.19075345993042\n",
      "epoch: 1, Test Acc: 0.4680488782051282\n",
      "Batch: 105, epoch: 0, Train Loss: 1.8757606744766235, Val Loss: 75.40131402015686\n",
      "Batch: 105, epoch: 1, Train Loss: 1.7815287113189697, Val Loss: 76.16125071048737\n",
      "epoch: 1, Test Acc: 0.5191306089743589\n",
      "Batch: 106, epoch: 0, Train Loss: 1.9093400239944458, Val Loss: 75.95450472831726\n",
      "Batch: 106, epoch: 1, Train Loss: 1.8113408088684082, Val Loss: 76.9906896352768\n",
      "epoch: 1, Test Acc: 0.4967948717948718\n",
      "Batch: 107, epoch: 0, Train Loss: 1.893822193145752, Val Loss: 75.79277181625366\n",
      "Batch: 107, epoch: 1, Train Loss: 1.809401273727417, Val Loss: 77.04595828056335\n",
      "epoch: 1, Test Acc: 0.4894831730769231\n",
      "Batch: 108, epoch: 0, Train Loss: 1.856263279914856, Val Loss: 75.14784073829651\n",
      "Batch: 108, epoch: 1, Train Loss: 1.78400719165802, Val Loss: 76.41147899627686\n",
      "epoch: 1, Test Acc: 0.5119190705128205\n",
      "Batch: 109, epoch: 0, Train Loss: 1.8848317861557007, Val Loss: 76.40676319599152\n",
      "Batch: 109, epoch: 1, Train Loss: 1.7762960195541382, Val Loss: 77.81192398071289\n",
      "epoch: 1, Test Acc: 0.46043669871794873\n",
      "Batch: 110, epoch: 0, Train Loss: 1.8684204816818237, Val Loss: 76.6658136844635\n",
      "Batch: 110, epoch: 1, Train Loss: 1.7845429182052612, Val Loss: 78.70137333869934\n",
      "epoch: 1, Test Acc: 0.4364983974358974\n",
      "Batch: 111, epoch: 0, Train Loss: 1.8986709117889404, Val Loss: 76.18777859210968\n",
      "Batch: 111, epoch: 1, Train Loss: 1.793569564819336, Val Loss: 78.04371201992035\n",
      "epoch: 1, Test Acc: 0.4596354166666667\n",
      "Batch: 112, epoch: 0, Train Loss: 1.8396555185317993, Val Loss: 75.25414669513702\n",
      "Batch: 112, epoch: 1, Train Loss: 1.7655038833618164, Val Loss: 76.92810416221619\n",
      "epoch: 1, Test Acc: 0.49919871794871795\n",
      "Batch: 113, epoch: 0, Train Loss: 1.8913434743881226, Val Loss: 75.56125509738922\n",
      "Batch: 113, epoch: 1, Train Loss: 1.7753108739852905, Val Loss: 78.58174252510071\n",
      "epoch: 1, Test Acc: 0.44310897435897434\n",
      "Batch: 114, epoch: 0, Train Loss: 1.8786042928695679, Val Loss: 76.5827226638794\n",
      "Batch: 114, epoch: 1, Train Loss: 1.7787119150161743, Val Loss: 79.29736483097076\n",
      "epoch: 1, Test Acc: 0.4267828525641026\n",
      "Batch: 115, epoch: 0, Train Loss: 1.8860291242599487, Val Loss: 75.94293284416199\n",
      "Batch: 115, epoch: 1, Train Loss: 1.7786579132080078, Val Loss: 78.12496936321259\n",
      "epoch: 1, Test Acc: 0.4561298076923077\n",
      "Batch: 116, epoch: 0, Train Loss: 1.8846378326416016, Val Loss: 76.82367753982544\n",
      "Batch: 116, epoch: 1, Train Loss: 1.778237223625183, Val Loss: 81.40412497520447\n",
      "epoch: 1, Test Acc: 0.36608573717948717\n",
      "Batch: 117, epoch: 0, Train Loss: 1.9019290208816528, Val Loss: 76.98066008090973\n",
      "Batch: 117, epoch: 1, Train Loss: 1.8031705617904663, Val Loss: 80.75477004051208\n",
      "epoch: 1, Test Acc: 0.3844150641025641\n",
      "Batch: 118, epoch: 0, Train Loss: 1.8483208417892456, Val Loss: 76.8860251903534\n",
      "Batch: 118, epoch: 1, Train Loss: 1.7690541744232178, Val Loss: 79.91614377498627\n",
      "epoch: 1, Test Acc: 0.4053485576923077\n",
      "Batch: 119, epoch: 0, Train Loss: 1.8736374378204346, Val Loss: 75.76408088207245\n",
      "Batch: 119, epoch: 1, Train Loss: 1.7804065942764282, Val Loss: 77.19923865795135\n",
      "epoch: 1, Test Acc: 0.4869791666666667\n",
      "Batch: 120, epoch: 0, Train Loss: 1.8853429555892944, Val Loss: 75.76922833919525\n",
      "Batch: 120, epoch: 1, Train Loss: 1.7842291593551636, Val Loss: 77.192058801651\n",
      "epoch: 1, Test Acc: 0.48227163461538464\n",
      "Batch: 121, epoch: 0, Train Loss: 1.8758383989334106, Val Loss: 75.3336933851242\n",
      "Batch: 121, epoch: 1, Train Loss: 1.755568265914917, Val Loss: 76.15947151184082\n",
      "epoch: 1, Test Acc: 0.5120192307692307\n",
      "Batch: 122, epoch: 0, Train Loss: 1.8812676668167114, Val Loss: 75.51947689056396\n",
      "Batch: 122, epoch: 1, Train Loss: 1.7756427526474, Val Loss: 76.48145568370819\n",
      "epoch: 1, Test Acc: 0.5009014423076923\n",
      "Batch: 123, epoch: 0, Train Loss: 1.8859020471572876, Val Loss: 75.84069609642029\n",
      "Batch: 123, epoch: 1, Train Loss: 1.7760967016220093, Val Loss: 77.57552099227905\n",
      "epoch: 1, Test Acc: 0.4712540064102564\n",
      "Batch: 124, epoch: 0, Train Loss: 1.8921983242034912, Val Loss: 76.61951780319214\n",
      "Batch: 124, epoch: 1, Train Loss: 1.7967219352722168, Val Loss: 77.8117949962616\n",
      "epoch: 1, Test Acc: 0.46654647435897434\n",
      "Batch: 125, epoch: 0, Train Loss: 1.9146350622177124, Val Loss: 75.44932460784912\n",
      "Batch: 125, epoch: 1, Train Loss: 1.809417963027954, Val Loss: 76.47472429275513\n",
      "epoch: 1, Test Acc: 0.5061097756410257\n",
      "Batch: 126, epoch: 0, Train Loss: 1.9180123805999756, Val Loss: 75.83502733707428\n",
      "Batch: 126, epoch: 1, Train Loss: 1.8263095617294312, Val Loss: 78.5718309879303\n",
      "epoch: 1, Test Acc: 0.4485176282051282\n",
      "Batch: 127, epoch: 0, Train Loss: 1.8848483562469482, Val Loss: 76.29419481754303\n",
      "Batch: 127, epoch: 1, Train Loss: 1.777915358543396, Val Loss: 78.35315954685211\n",
      "epoch: 1, Test Acc: 0.4622395833333333\n",
      "Batch: 128, epoch: 0, Train Loss: 1.875138282775879, Val Loss: 75.93564665317535\n",
      "Batch: 128, epoch: 1, Train Loss: 1.773579478263855, Val Loss: 78.75477755069733\n",
      "epoch: 1, Test Acc: 0.44641426282051283\n",
      "Batch: 129, epoch: 0, Train Loss: 1.9185879230499268, Val Loss: 76.86328852176666\n",
      "Batch: 129, epoch: 1, Train Loss: 1.788851261138916, Val Loss: 80.02780425548553\n",
      "epoch: 1, Test Acc: 0.40935496794871795\n",
      "Batch: 130, epoch: 0, Train Loss: 1.8961962461471558, Val Loss: 76.08497989177704\n",
      "Batch: 130, epoch: 1, Train Loss: 1.809591293334961, Val Loss: 77.81672716140747\n",
      "epoch: 1, Test Acc: 0.46534455128205127\n",
      "Batch: 131, epoch: 0, Train Loss: 1.9058749675750732, Val Loss: 76.64954829216003\n",
      "Batch: 131, epoch: 1, Train Loss: 1.8056261539459229, Val Loss: 78.17794680595398\n",
      "epoch: 1, Test Acc: 0.4551282051282051\n",
      "Batch: 132, epoch: 0, Train Loss: 1.9009826183319092, Val Loss: 74.88870143890381\n",
      "Batch: 132, epoch: 1, Train Loss: 1.788588285446167, Val Loss: 75.15910589694977\n",
      "epoch: 1, Test Acc: 0.5439703525641025\n",
      "Batch: 133, epoch: 0, Train Loss: 1.850587248802185, Val Loss: 74.52306807041168\n",
      "Batch: 133, epoch: 1, Train Loss: 1.784938931465149, Val Loss: 75.39425551891327\n",
      "epoch: 1, Test Acc: 0.5430689102564102\n",
      "Batch: 134, epoch: 0, Train Loss: 1.9128730297088623, Val Loss: 76.02805519104004\n",
      "Batch: 134, epoch: 1, Train Loss: 1.8231310844421387, Val Loss: 78.61333692073822\n",
      "epoch: 1, Test Acc: 0.4501201923076923\n",
      "Batch: 135, epoch: 0, Train Loss: 1.9199800491333008, Val Loss: 75.556560754776\n",
      "Batch: 135, epoch: 1, Train Loss: 1.836435079574585, Val Loss: 76.2422468662262\n",
      "epoch: 1, Test Acc: 0.51953125\n",
      "Batch: 136, epoch: 0, Train Loss: 1.9039126634597778, Val Loss: 74.64891934394836\n",
      "Batch: 136, epoch: 1, Train Loss: 1.7833654880523682, Val Loss: 75.88477528095245\n",
      "epoch: 1, Test Acc: 0.5259415064102564\n",
      "Batch: 137, epoch: 0, Train Loss: 1.8996609449386597, Val Loss: 74.55275464057922\n",
      "Batch: 137, epoch: 1, Train Loss: 1.792039394378662, Val Loss: 76.48190689086914\n",
      "epoch: 1, Test Acc: 0.5084134615384616\n",
      "Batch: 138, epoch: 0, Train Loss: 1.8806335926055908, Val Loss: 75.59370255470276\n",
      "Batch: 138, epoch: 1, Train Loss: 1.7901257276535034, Val Loss: 77.90456354618073\n",
      "epoch: 1, Test Acc: 0.461338141025641\n",
      "Batch: 139, epoch: 0, Train Loss: 1.89224374294281, Val Loss: 74.43918907642365\n",
      "Batch: 139, epoch: 1, Train Loss: 1.79951012134552, Val Loss: 75.75377440452576\n",
      "epoch: 1, Test Acc: 0.5308493589743589\n",
      "Batch: 140, epoch: 0, Train Loss: 1.8795979022979736, Val Loss: 75.08288431167603\n",
      "Batch: 140, epoch: 1, Train Loss: 1.7836469411849976, Val Loss: 76.92229270935059\n",
      "epoch: 1, Test Acc: 0.49659455128205127\n",
      "Batch: 141, epoch: 0, Train Loss: 1.885388970375061, Val Loss: 74.57189202308655\n",
      "Batch: 141, epoch: 1, Train Loss: 1.792392611503601, Val Loss: 75.66392111778259\n",
      "epoch: 1, Test Acc: 0.5342548076923077\n",
      "Batch: 142, epoch: 0, Train Loss: 1.9053215980529785, Val Loss: 74.86834275722504\n",
      "Batch: 142, epoch: 1, Train Loss: 1.8275411128997803, Val Loss: 76.582146525383\n",
      "epoch: 1, Test Acc: 0.5112179487179487\n",
      "Batch: 143, epoch: 0, Train Loss: 1.8905746936798096, Val Loss: 74.77869355678558\n",
      "Batch: 143, epoch: 1, Train Loss: 1.8016386032104492, Val Loss: 76.32958471775055\n",
      "epoch: 1, Test Acc: 0.5104166666666666\n",
      "Batch: 144, epoch: 0, Train Loss: 1.8959556818008423, Val Loss: 74.85618925094604\n",
      "Batch: 144, epoch: 1, Train Loss: 1.7862987518310547, Val Loss: 76.2312341928482\n",
      "epoch: 1, Test Acc: 0.5143229166666666\n",
      "Batch: 145, epoch: 0, Train Loss: 1.8932160139083862, Val Loss: 75.72101473808289\n",
      "Batch: 145, epoch: 1, Train Loss: 1.7926005125045776, Val Loss: 78.02234411239624\n",
      "epoch: 1, Test Acc: 0.46424278846153844\n",
      "Batch: 146, epoch: 0, Train Loss: 1.859345555305481, Val Loss: 76.3431921005249\n",
      "Batch: 146, epoch: 1, Train Loss: 1.7416805028915405, Val Loss: 78.93045794963837\n",
      "epoch: 1, Test Acc: 0.4400040064102564\n",
      "Batch: 147, epoch: 0, Train Loss: 1.8581514358520508, Val Loss: 75.14925456047058\n",
      "Batch: 147, epoch: 1, Train Loss: 1.7840946912765503, Val Loss: 76.56561601161957\n",
      "epoch: 1, Test Acc: 0.5065104166666666\n",
      "Batch: 148, epoch: 0, Train Loss: 1.8892744779586792, Val Loss: 77.00788414478302\n",
      "Batch: 148, epoch: 1, Train Loss: 1.7797930240631104, Val Loss: 80.01859200000763\n",
      "epoch: 1, Test Acc: 0.41486378205128205\n",
      "Batch: 149, epoch: 0, Train Loss: 1.9138435125350952, Val Loss: 76.8331139087677\n",
      "Batch: 149, epoch: 1, Train Loss: 1.7905542850494385, Val Loss: 80.08664357662201\n",
      "epoch: 1, Test Acc: 0.40905448717948717\n",
      "Batch: 150, epoch: 0, Train Loss: 1.8614203929901123, Val Loss: 76.88646709918976\n",
      "Batch: 150, epoch: 1, Train Loss: 1.7461705207824707, Val Loss: 80.35818433761597\n",
      "epoch: 1, Test Acc: 0.3991386217948718\n",
      "Batch: 151, epoch: 0, Train Loss: 1.883689045906067, Val Loss: 75.09925019741058\n",
      "Batch: 151, epoch: 1, Train Loss: 1.7784323692321777, Val Loss: 77.19597911834717\n",
      "epoch: 1, Test Acc: 0.48497596153846156\n",
      "Batch: 152, epoch: 0, Train Loss: 1.8913952112197876, Val Loss: 74.98846650123596\n",
      "Batch: 152, epoch: 1, Train Loss: 1.7944486141204834, Val Loss: 78.0278708934784\n",
      "epoch: 1, Test Acc: 0.46484375\n",
      "Batch: 153, epoch: 0, Train Loss: 1.8656036853790283, Val Loss: 75.2479954957962\n",
      "Batch: 153, epoch: 1, Train Loss: 1.7727950811386108, Val Loss: 76.65606188774109\n",
      "epoch: 1, Test Acc: 0.5019030448717948\n",
      "Batch: 154, epoch: 0, Train Loss: 1.8509559631347656, Val Loss: 74.71474242210388\n",
      "Batch: 154, epoch: 1, Train Loss: 1.7533307075500488, Val Loss: 76.63527846336365\n",
      "epoch: 1, Test Acc: 0.49589342948717946\n",
      "Batch: 155, epoch: 0, Train Loss: 1.893781065940857, Val Loss: 75.3667072057724\n",
      "Batch: 155, epoch: 1, Train Loss: 1.8091390132904053, Val Loss: 77.79243957996368\n",
      "epoch: 1, Test Acc: 0.4657451923076923\n"
     ]
    }
   ],
   "source": [
    "model = VGG().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "best_model_wts = train(model, 20, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.save(model.state_dict(), 'cifar-10-baseline.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG().to(device)\n",
    "model.load_state_dict(torch.load('cifar-10-baseline.pth'))\n",
    "\n",
    "test_acc = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5663060897435898"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
