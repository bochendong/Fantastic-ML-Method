{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 0.01\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        label_cache_filename = path.join(dataset.root, str(type(dataset))+'_'+str(len(dataset))+'.pth')\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        '''\n",
    "        :param dataset: (CacheClassLabel)\n",
    "        :param class_list: (list) A list of integers\n",
    "        :param remap: (bool) Ex: remap class [2,4,6 ...] to [0,1,2 ...]\n",
    "        '''\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST(dataroot, train_aug=False):\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = MNIST('data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_boundaries: [0, 2, 4, 6, 8, 10]\n",
      "{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_task(model, train_loader, val_loader, optimizer, criterion, valid_out_dim, best_model_wts, best_loss):\n",
    "    leader = MLP400().to(device)\n",
    "    if (best_model_wts):\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            model.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            reg_loss = 0\n",
    "            for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
    "                reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, outputs, labels, valid_out_dim)\n",
    "            loss = c_loss + 5 * reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = accumulate_acc(outputs, labels, train_acc)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_loss += criterion(output, target).item()\n",
    "\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss: .4f}, loss:{loss:.4f}\")\n",
    "    return best_model_wts, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task_names):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    best_loss = float('inf')\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name],\n",
    "                                                            batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset_splits[train_name],\n",
    "                                                        batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, val_loader, optimizer, \n",
    "                                       criterion, valid_out_dim, best_model_wts, best_loss)\n",
    "    \n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False,)\n",
    "            print(f\"val_name{val_name}:\")\n",
    "            model.eval()\n",
    "            val_tester = 0\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "                    val_tester += val_acc.val\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "        print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task order: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Task: 1=====\n",
      "batch_num: 0, c_loss:0.7679, val_loss: 37.7261, loss:184.2126\n",
      "batch_num: 0, c_loss:0.2657, val_loss: 36.3874, loss:0.2657\n",
      "batch_num: 0, c_loss:0.1132, val_loss: 35.0087, loss:0.1132\n",
      "batch_num: 0, c_loss:0.0833, val_loss: 33.6017, loss:0.0833\n",
      "batch_num: 0, c_loss:0.0589, val_loss: 32.1603, loss:0.0589\n",
      "batch_num: 0, c_loss:0.0385, val_loss: 30.6586, loss:0.0385\n",
      "batch_num: 0, c_loss:0.0358, val_loss: 29.1066, loss:0.0358\n",
      "batch_num: 0, c_loss:0.0227, val_loss: 27.5230, loss:0.0227\n",
      "batch_num: 0, c_loss:0.0215, val_loss: 25.9493, loss:0.0215\n",
      "batch_num: 0, c_loss:0.0223, val_loss: 24.3476, loss:0.0223\n",
      "batch_num: 0, c_loss:0.0177, val_loss: 22.8052, loss:0.0177\n",
      "batch_num: 0, c_loss:0.0296, val_loss: 21.3114, loss:0.0296\n",
      "batch_num: 0, c_loss:0.0283, val_loss: 19.9393, loss:0.0283\n",
      "batch_num: 0, c_loss:0.0097, val_loss: 18.6321, loss:0.0097\n",
      "batch_num: 0, c_loss:0.0230, val_loss: 17.3990, loss:0.0230\n",
      "batch_num: 0, c_loss:0.0164, val_loss: 16.2351, loss:0.0164\n",
      "batch_num: 0, c_loss:0.0142, val_loss: 15.1791, loss:0.0142\n",
      "batch_num: 0, c_loss:0.0111, val_loss: 14.1654, loss:0.0111\n",
      "batch_num: 0, c_loss:0.0071, val_loss: 13.2787, loss:0.0071\n",
      "batch_num: 0, c_loss:0.0058, val_loss: 12.4811, loss:0.0058\n",
      "batch_num: 0, c_loss:0.0199, val_loss: 11.7555, loss:0.0199\n",
      "batch_num: 0, c_loss:0.0115, val_loss: 11.0853, loss:0.0115\n",
      "batch_num: 0, c_loss:0.0048, val_loss: 10.4742, loss:0.0048\n",
      "batch_num: 0, c_loss:0.0058, val_loss: 10.0036, loss:0.0058\n",
      "batch_num: 0, c_loss:0.0052, val_loss: 9.5672, loss:0.0052\n",
      "batch_num: 0, c_loss:0.0148, val_loss: 9.2230, loss:0.0148\n",
      "batch_num: 0, c_loss:0.0043, val_loss: 8.8593, loss:0.0043\n",
      "batch_num: 0, c_loss:0.0058, val_loss: 8.5055, loss:0.0058\n",
      "batch_num: 0, c_loss:0.0034, val_loss: 8.2117, loss:0.0034\n",
      "batch_num: 0, c_loss:0.0062, val_loss: 7.9401, loss:0.0062\n",
      "batch_num: 0, c_loss:0.0055, val_loss: 7.7427, loss:0.0055\n",
      "batch_num: 0, c_loss:0.0057, val_loss: 7.5119, loss:0.0057\n",
      "batch_num: 0, c_loss:0.0118, val_loss: 7.3200, loss:0.0118\n",
      "batch_num: 0, c_loss:0.0047, val_loss: 7.1691, loss:0.0047\n",
      "batch_num: 0, c_loss:0.0034, val_loss: 7.0063, loss:0.0034\n",
      "batch_num: 0, c_loss:0.0044, val_loss: 6.8935, loss:0.0044\n",
      "batch_num: 0, c_loss:0.0042, val_loss: 6.7772, loss:0.0042\n",
      "batch_num: 0, c_loss:0.0328, val_loss: 6.6555, loss:0.0328\n",
      "batch_num: 0, c_loss:0.0028, val_loss: 6.5240, loss:0.0028\n",
      "batch_num: 0, c_loss:0.0034, val_loss: 6.3834, loss:0.0034\n",
      "batch_num: 0, c_loss:0.0024, val_loss: 6.2784, loss:0.0024\n",
      "batch_num: 0, c_loss:0.0057, val_loss: 6.1724, loss:0.0057\n",
      "batch_num: 0, c_loss:0.0472, val_loss: 5.9990, loss:0.0472\n",
      "batch_num: 0, c_loss:0.0019, val_loss: 5.8932, loss:0.0019\n",
      "batch_num: 0, c_loss:0.0028, val_loss: 5.7919, loss:0.0028\n",
      "batch_num: 0, c_loss:0.0033, val_loss: 5.7313, loss:0.0033\n",
      "batch_num: 0, c_loss:0.0029, val_loss: 5.6489, loss:0.0029\n",
      "batch_num: 0, c_loss:0.0042, val_loss: 5.5439, loss:0.0042\n",
      "batch_num: 0, c_loss:0.0056, val_loss: 5.5026, loss:0.0056\n",
      "batch_num: 0, c_loss:0.0100, val_loss: 5.4436, loss:0.0100\n",
      "batch_num: 0, c_loss:0.0040, val_loss: 5.3835, loss:0.0040\n",
      "batch_num: 0, c_loss:0.0020, val_loss: 5.3871, loss:0.0020\n",
      "batch_num: 0, c_loss:0.0030, val_loss: 5.3763, loss:0.0030\n",
      "batch_num: 0, c_loss:0.0024, val_loss: 5.3709, loss:0.0024\n",
      "batch_num: 0, c_loss:0.0149, val_loss: 5.8547, loss:0.1158\n",
      "batch_num: 0, c_loss:0.0024, val_loss: 5.8460, loss:1.0817\n",
      "batch_num: 0, c_loss:0.0266, val_loss: 5.5141, loss:0.8936\n",
      "batch_num: 0, c_loss:0.0021, val_loss: 5.3150, loss:0.7612\n",
      "batch_num: 0, c_loss:0.0149, val_loss: 5.3910, loss:0.7552\n",
      "batch_num: 0, c_loss:0.0025, val_loss: 5.5230, loss:0.0025\n",
      "batch_num: 0, c_loss:0.0017, val_loss: 5.5825, loss:0.0017\n",
      "batch_num: 0, c_loss:0.0030, val_loss: 5.4673, loss:0.6433\n",
      "batch_num: 0, c_loss:0.0057, val_loss: 5.3723, loss:0.5004\n",
      "batch_num: 0, c_loss:0.0058, val_loss: 5.5452, loss:0.4494\n",
      "batch_num: 0, c_loss:0.0015, val_loss: 5.6727, loss:0.4614\n",
      "batch_num: 0, c_loss:0.0032, val_loss: 5.6049, loss:0.4275\n",
      "batch_num: 0, c_loss:0.0029, val_loss: 5.5628, loss:0.4025\n",
      "batch_num: 0, c_loss:0.0022, val_loss: 5.5273, loss:0.4019\n",
      "batch_num: 0, c_loss:0.0021, val_loss: 5.5238, loss:0.3612\n",
      "batch_num: 0, c_loss:0.0181, val_loss: 5.5690, loss:0.3814\n",
      "batch_num: 0, c_loss:0.0054, val_loss: 5.5803, loss:0.3588\n",
      "batch_num: 0, c_loss:0.0102, val_loss: 5.5021, loss:0.3403\n",
      "batch_num: 0, c_loss:0.0023, val_loss: 5.5273, loss:0.3147\n",
      "batch_num: 0, c_loss:0.0037, val_loss: 5.5388, loss:0.3145\n",
      "batch_num: 0, c_loss:0.0027, val_loss: 5.4668, loss:0.3005\n",
      "batch_num: 0, c_loss:0.0020, val_loss: 5.4598, loss:0.2732\n",
      "batch_num: 0, c_loss:0.0064, val_loss: 5.4668, loss:0.2979\n",
      "batch_num: 0, c_loss:0.0018, val_loss: 5.5069, loss:0.2639\n",
      "batch_num: 0, c_loss:0.0026, val_loss: 5.5414, loss:0.2723\n",
      "batch_num: 0, c_loss:0.0022, val_loss: 5.5330, loss:0.2499\n",
      "batch_num: 0, c_loss:0.0023, val_loss: 5.5201, loss:0.2561\n",
      "batch_num: 0, c_loss:0.0031, val_loss: 5.5217, loss:0.2296\n",
      "batch_num: 0, c_loss:0.0028, val_loss: 5.5183, loss:0.2545\n",
      "batch_num: 0, c_loss:0.0023, val_loss: 5.4891, loss:0.2064\n",
      "batch_num: 0, c_loss:0.0016, val_loss: 5.5012, loss:0.2565\n",
      "batch_num: 0, c_loss:0.0039, val_loss: 5.5353, loss:0.2102\n",
      "batch_num: 0, c_loss:0.0019, val_loss: 5.5338, loss:0.2405\n",
      "batch_num: 0, c_loss:0.0015, val_loss: 5.5271, loss:0.2059\n",
      "batch_num: 0, c_loss:0.0023, val_loss: 5.5292, loss:0.2306\n",
      "batch_num: 0, c_loss:0.0050, val_loss: 5.5167, loss:0.1986\n",
      "batch_num: 0, c_loss:0.0123, val_loss: 5.5087, loss:0.2335\n",
      "batch_num: 0, c_loss:0.0019, val_loss: 5.4883, loss:0.2015\n",
      "batch_num: 0, c_loss:0.0274, val_loss: 5.5147, loss:0.2384\n",
      "batch_num: 0, c_loss:0.0077, val_loss: 5.5155, loss:0.2173\n",
      "batch_num: 0, c_loss:0.0132, val_loss: 5.4875, loss:0.2007\n",
      "batch_num: 0, c_loss:0.0061, val_loss: 5.4782, loss:0.2105\n",
      "batch_num: 0, c_loss:0.0019, val_loss: 5.4646, loss:0.1768\n",
      "batch_num: 0, c_loss:0.0055, val_loss: 5.4912, loss:0.2021\n",
      "batch_num: 0, c_loss:0.0029, val_loss: 5.4846, loss:0.1721\n",
      "val_name1:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.8581560283688)]))])\n",
      "=====Task: 2=====\n",
      "batch_num: 0, c_loss:2.3645, val_loss: 40.1002, loss:2.5550\n",
      "batch_num: 0, c_loss:2.3334, val_loss: 39.4449, loss:2.6587\n",
      "batch_num: 0, c_loss:2.2574, val_loss: 39.3949, loss:2.6805\n",
      "batch_num: 0, c_loss:2.1863, val_loss: 39.4669, loss:2.6375\n",
      "batch_num: 0, c_loss:2.2255, val_loss: 39.3040, loss:2.6881\n",
      "batch_num: 0, c_loss:2.1578, val_loss: 39.0481, loss:2.6408\n",
      "batch_num: 0, c_loss:2.1714, val_loss: 38.6763, loss:2.6943\n",
      "batch_num: 0, c_loss:2.0964, val_loss: 38.0829, loss:2.6711\n",
      "batch_num: 0, c_loss:2.0176, val_loss: 37.3441, loss:2.6606\n",
      "batch_num: 0, c_loss:1.8557, val_loss: 36.7412, loss:2.5738\n",
      "batch_num: 0, c_loss:1.7416, val_loss: 36.2494, loss:2.5223\n",
      "batch_num: 0, c_loss:1.7303, val_loss: 35.8718, loss:2.5627\n",
      "batch_num: 0, c_loss:1.6924, val_loss: 35.5054, loss:2.5555\n",
      "batch_num: 0, c_loss:1.6449, val_loss: 35.0998, loss:2.5382\n",
      "batch_num: 0, c_loss:1.6298, val_loss: 34.6976, loss:2.5639\n",
      "batch_num: 0, c_loss:1.5116, val_loss: 34.1769, loss:2.4861\n",
      "batch_num: 0, c_loss:1.5260, val_loss: 33.6579, loss:2.5449\n",
      "batch_num: 0, c_loss:1.4351, val_loss: 33.2924, loss:2.4969\n",
      "batch_num: 0, c_loss:1.3665, val_loss: 33.0278, loss:2.4512\n",
      "batch_num: 0, c_loss:1.4020, val_loss: 32.7788, loss:2.5109\n",
      "batch_num: 0, c_loss:1.3146, val_loss: 32.6162, loss:2.4389\n",
      "batch_num: 0, c_loss:1.3606, val_loss: 32.3693, loss:2.5064\n",
      "batch_num: 0, c_loss:1.2913, val_loss: 32.1985, loss:2.4510\n",
      "batch_num: 0, c_loss:1.3079, val_loss: 31.9897, loss:2.4776\n",
      "batch_num: 0, c_loss:1.3059, val_loss: 31.8646, loss:2.4930\n",
      "batch_num: 0, c_loss:1.2872, val_loss: 31.8493, loss:2.4850\n",
      "batch_num: 0, c_loss:1.2958, val_loss: 31.8704, loss:2.4906\n",
      "batch_num: 0, c_loss:1.2247, val_loss: 32.0189, loss:2.4185\n",
      "batch_num: 0, c_loss:1.2889, val_loss: 32.1652, loss:2.4745\n",
      "batch_num: 0, c_loss:1.2882, val_loss: 32.1724, loss:2.4656\n",
      "batch_num: 0, c_loss:1.3770, val_loss: 32.2237, loss:2.5454\n",
      "batch_num: 0, c_loss:1.3722, val_loss: 32.1334, loss:2.5370\n",
      "batch_num: 0, c_loss:1.3306, val_loss: 32.0703, loss:2.4942\n",
      "batch_num: 0, c_loss:1.3588, val_loss: 32.1193, loss:2.5224\n",
      "batch_num: 0, c_loss:1.3438, val_loss: 32.2181, loss:2.5023\n",
      "batch_num: 0, c_loss:1.3388, val_loss: 32.4162, loss:2.4893\n",
      "batch_num: 0, c_loss:1.2459, val_loss: 32.6212, loss:2.3870\n",
      "batch_num: 0, c_loss:1.2991, val_loss: 32.7984, loss:2.4282\n",
      "batch_num: 0, c_loss:1.3282, val_loss: 32.9326, loss:2.4414\n",
      "batch_num: 0, c_loss:1.4387, val_loss: 32.8910, loss:2.5456\n",
      "batch_num: 0, c_loss:1.3707, val_loss: 32.9162, loss:2.4779\n",
      "batch_num: 0, c_loss:1.3620, val_loss: 32.9635, loss:2.4696\n",
      "batch_num: 0, c_loss:1.3554, val_loss: 33.2064, loss:2.4560\n",
      "batch_num: 0, c_loss:1.3411, val_loss: 33.4781, loss:2.4237\n",
      "batch_num: 0, c_loss:1.3551, val_loss: 33.7297, loss:2.4256\n",
      "batch_num: 0, c_loss:1.4259, val_loss: 33.7472, loss:2.4847\n",
      "batch_num: 0, c_loss:1.4141, val_loss: 33.7120, loss:2.4677\n",
      "batch_num: 0, c_loss:1.3588, val_loss: 33.8621, loss:2.4089\n",
      "batch_num: 0, c_loss:1.4214, val_loss: 33.9922, loss:2.4721\n",
      "batch_num: 0, c_loss:1.4165, val_loss: 34.1986, loss:2.4590\n",
      "batch_num: 0, c_loss:1.4610, val_loss: 34.2767, loss:2.4969\n",
      "batch_num: 0, c_loss:1.4722, val_loss: 34.3471, loss:2.4969\n",
      "batch_num: 0, c_loss:1.4525, val_loss: 34.3857, loss:2.4743\n",
      "batch_num: 0, c_loss:1.4336, val_loss: 34.4184, loss:2.4499\n",
      "batch_num: 0, c_loss:1.4186, val_loss: 34.4594, loss:2.4337\n",
      "batch_num: 0, c_loss:1.4174, val_loss: 34.4557, loss:2.4308\n",
      "batch_num: 0, c_loss:1.4445, val_loss: 34.4594, loss:2.4546\n",
      "batch_num: 0, c_loss:1.4770, val_loss: 34.4666, loss:2.4787\n",
      "batch_num: 0, c_loss:1.4966, val_loss: 34.4468, loss:2.5010\n",
      "batch_num: 0, c_loss:1.4382, val_loss: 34.4220, loss:2.4353\n",
      "batch_num: 0, c_loss:1.4913, val_loss: 34.4228, loss:2.4893\n",
      "batch_num: 0, c_loss:1.4138, val_loss: 34.4260, loss:2.4069\n",
      "batch_num: 0, c_loss:1.4491, val_loss: 34.4355, loss:2.4447\n",
      "batch_num: 0, c_loss:1.4860, val_loss: 34.4346, loss:2.4784\n",
      "batch_num: 0, c_loss:1.4908, val_loss: 34.3732, loss:2.4881\n",
      "batch_num: 0, c_loss:1.4534, val_loss: 34.3438, loss:2.4496\n",
      "batch_num: 0, c_loss:1.5238, val_loss: 34.2851, loss:2.5243\n",
      "batch_num: 0, c_loss:1.4987, val_loss: 34.2316, loss:2.4939\n",
      "batch_num: 0, c_loss:1.5209, val_loss: 34.0906, loss:2.5195\n",
      "batch_num: 0, c_loss:1.4349, val_loss: 34.0176, loss:2.4354\n",
      "batch_num: 0, c_loss:1.4143, val_loss: 33.8971, loss:2.4268\n",
      "batch_num: 0, c_loss:1.3851, val_loss: 33.8328, loss:2.4018\n",
      "batch_num: 0, c_loss:1.4232, val_loss: 33.7508, loss:2.4465\n",
      "batch_num: 0, c_loss:1.4183, val_loss: 33.8370, loss:2.4385\n",
      "batch_num: 0, c_loss:1.4191, val_loss: 33.8786, loss:2.4430\n",
      "batch_num: 0, c_loss:1.4077, val_loss: 33.8324, loss:2.4302\n",
      "batch_num: 0, c_loss:1.3975, val_loss: 33.6219, loss:2.4311\n",
      "batch_num: 0, c_loss:1.4203, val_loss: 33.5878, loss:2.4542\n",
      "batch_num: 0, c_loss:1.3548, val_loss: 33.6252, loss:2.3965\n",
      "batch_num: 0, c_loss:1.3988, val_loss: 33.5992, loss:2.4362\n",
      "batch_num: 0, c_loss:1.4262, val_loss: 33.5444, loss:2.4719\n",
      "batch_num: 0, c_loss:1.4261, val_loss: 33.6283, loss:2.4709\n",
      "batch_num: 0, c_loss:1.3573, val_loss: 33.7113, loss:2.4026\n",
      "batch_num: 0, c_loss:1.4196, val_loss: 33.6983, loss:2.4592\n",
      "batch_num: 0, c_loss:1.3674, val_loss: 33.6597, loss:2.4086\n",
      "batch_num: 0, c_loss:1.4600, val_loss: 33.6627, loss:2.4966\n",
      "batch_num: 0, c_loss:1.3750, val_loss: 33.7293, loss:2.4105\n",
      "batch_num: 0, c_loss:1.4286, val_loss: 33.8079, loss:2.4583\n",
      "batch_num: 0, c_loss:1.3810, val_loss: 33.8014, loss:2.4093\n",
      "batch_num: 0, c_loss:1.4276, val_loss: 33.8260, loss:2.4572\n",
      "batch_num: 0, c_loss:1.3990, val_loss: 33.7502, loss:2.4250\n",
      "batch_num: 0, c_loss:1.3923, val_loss: 33.8554, loss:2.4250\n",
      "batch_num: 0, c_loss:1.4639, val_loss: 33.9058, loss:2.4896\n",
      "batch_num: 0, c_loss:1.3439, val_loss: 34.0077, loss:2.3674\n",
      "batch_num: 0, c_loss:1.4681, val_loss: 34.1026, loss:2.4844\n",
      "val_name1:\n",
      "val_name2:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.8581560283688), ('2', 99.71631205673759)])), ('2', OrderedDict([('2', 23.457394711067582)]))])\n",
      "=====Task: 3=====\n",
      "batch_num: 0, c_loss:2.2502, val_loss: 38.6260, loss:3.2750\n",
      "batch_num: 0, c_loss:2.1545, val_loss: 37.1072, loss:3.1745\n",
      "batch_num: 0, c_loss:2.0835, val_loss: 35.8330, loss:3.1003\n",
      "batch_num: 0, c_loss:2.0536, val_loss: 34.4230, loss:3.0473\n",
      "batch_num: 0, c_loss:1.9727, val_loss: 33.1745, loss:2.9323\n",
      "batch_num: 0, c_loss:1.9043, val_loss: 32.1768, loss:2.8172\n",
      "batch_num: 0, c_loss:1.8134, val_loss: 31.1636, loss:2.7161\n",
      "batch_num: 0, c_loss:1.6837, val_loss: 30.0194, loss:2.6076\n",
      "batch_num: 0, c_loss:1.6645, val_loss: 28.7620, loss:2.6332\n",
      "batch_num: 0, c_loss:1.5810, val_loss: 27.7172, loss:2.6169\n",
      "batch_num: 0, c_loss:1.4612, val_loss: 27.0800, loss:2.5667\n",
      "batch_num: 0, c_loss:1.4329, val_loss: 26.6905, loss:2.5936\n",
      "batch_num: 0, c_loss:1.4744, val_loss: 26.4327, loss:2.6753\n",
      "batch_num: 0, c_loss:1.4269, val_loss: 26.1753, loss:2.6481\n",
      "batch_num: 0, c_loss:1.4229, val_loss: 25.8733, loss:2.6696\n",
      "batch_num: 0, c_loss:1.3921, val_loss: 25.5448, loss:2.6622\n",
      "batch_num: 0, c_loss:1.3670, val_loss: 25.2270, loss:2.6631\n",
      "batch_num: 0, c_loss:1.3372, val_loss: 25.0111, loss:2.6509\n",
      "batch_num: 0, c_loss:1.3216, val_loss: 24.9696, loss:2.6568\n",
      "batch_num: 0, c_loss:1.3200, val_loss: 24.9736, loss:2.6557\n",
      "batch_num: 0, c_loss:1.3074, val_loss: 25.0644, loss:2.6438\n",
      "batch_num: 0, c_loss:1.3049, val_loss: 25.1919, loss:2.6271\n",
      "batch_num: 0, c_loss:1.3011, val_loss: 25.2509, loss:2.6200\n",
      "batch_num: 0, c_loss:1.2884, val_loss: 25.4302, loss:2.5961\n",
      "batch_num: 0, c_loss:1.3146, val_loss: 25.5770, loss:2.6223\n",
      "batch_num: 0, c_loss:1.2838, val_loss: 25.8442, loss:2.5727\n",
      "batch_num: 0, c_loss:1.3570, val_loss: 26.0659, loss:2.6387\n",
      "batch_num: 0, c_loss:1.3494, val_loss: 26.4114, loss:2.6047\n",
      "batch_num: 0, c_loss:1.3135, val_loss: 26.7419, loss:2.5508\n",
      "batch_num: 0, c_loss:1.3635, val_loss: 27.0477, loss:2.5713\n",
      "batch_num: 0, c_loss:1.4030, val_loss: 27.3417, loss:2.5937\n",
      "batch_num: 0, c_loss:1.4112, val_loss: 27.6145, loss:2.5785\n",
      "batch_num: 0, c_loss:1.4848, val_loss: 27.8308, loss:2.6351\n",
      "batch_num: 0, c_loss:1.4573, val_loss: 28.1398, loss:2.5857\n",
      "batch_num: 0, c_loss:1.4873, val_loss: 28.3634, loss:2.5930\n",
      "batch_num: 0, c_loss:1.5448, val_loss: 28.5569, loss:2.6298\n",
      "batch_num: 0, c_loss:1.5045, val_loss: 28.8431, loss:2.5666\n",
      "batch_num: 0, c_loss:1.5627, val_loss: 29.0937, loss:2.6092\n",
      "batch_num: 0, c_loss:1.5322, val_loss: 29.2851, loss:2.5582\n",
      "batch_num: 0, c_loss:1.5698, val_loss: 29.3576, loss:2.5849\n",
      "batch_num: 0, c_loss:1.5647, val_loss: 29.4484, loss:2.5603\n",
      "batch_num: 0, c_loss:1.5705, val_loss: 29.6221, loss:2.5556\n",
      "batch_num: 0, c_loss:1.5987, val_loss: 29.7417, loss:2.5714\n",
      "batch_num: 0, c_loss:1.6207, val_loss: 29.8124, loss:2.5876\n",
      "batch_num: 0, c_loss:1.6018, val_loss: 29.9377, loss:2.5558\n",
      "batch_num: 0, c_loss:1.6320, val_loss: 30.0191, loss:2.5778\n",
      "batch_num: 0, c_loss:1.6736, val_loss: 30.1832, loss:2.6109\n",
      "batch_num: 0, c_loss:1.6538, val_loss: 30.3357, loss:2.5820\n",
      "batch_num: 0, c_loss:1.6392, val_loss: 30.3932, loss:2.5635\n",
      "batch_num: 0, c_loss:1.6849, val_loss: 30.2884, loss:2.6167\n",
      "batch_num: 0, c_loss:1.6168, val_loss: 30.2840, loss:2.5460\n",
      "batch_num: 0, c_loss:1.6476, val_loss: 30.3544, loss:2.5814\n",
      "batch_num: 0, c_loss:1.6284, val_loss: 30.4755, loss:2.5577\n",
      "batch_num: 0, c_loss:1.6106, val_loss: 30.4647, loss:2.5401\n",
      "batch_num: 0, c_loss:1.5955, val_loss: 30.5159, loss:2.5192\n",
      "batch_num: 0, c_loss:1.6706, val_loss: 30.5510, loss:2.5936\n",
      "batch_num: 0, c_loss:1.6495, val_loss: 30.5195, loss:2.5733\n",
      "batch_num: 0, c_loss:1.5814, val_loss: 30.4521, loss:2.5090\n",
      "batch_num: 0, c_loss:1.5675, val_loss: 30.4195, loss:2.4957\n",
      "batch_num: 0, c_loss:1.6736, val_loss: 30.3674, loss:2.6087\n",
      "batch_num: 0, c_loss:1.6074, val_loss: 30.3620, loss:2.5435\n",
      "batch_num: 0, c_loss:1.6342, val_loss: 30.4091, loss:2.5682\n",
      "batch_num: 0, c_loss:1.7107, val_loss: 30.3293, loss:2.6434\n",
      "batch_num: 0, c_loss:1.6725, val_loss: 30.2000, loss:2.6093\n",
      "batch_num: 0, c_loss:1.6789, val_loss: 30.1410, loss:2.6167\n",
      "batch_num: 0, c_loss:1.6567, val_loss: 30.0547, loss:2.6017\n",
      "batch_num: 0, c_loss:1.6528, val_loss: 29.9448, loss:2.5953\n",
      "batch_num: 0, c_loss:1.6100, val_loss: 29.7624, loss:2.5689\n",
      "batch_num: 0, c_loss:1.5681, val_loss: 29.6621, loss:2.5316\n",
      "batch_num: 0, c_loss:1.5927, val_loss: 29.5594, loss:2.5713\n",
      "batch_num: 0, c_loss:1.5785, val_loss: 29.5908, loss:2.5529\n",
      "batch_num: 0, c_loss:1.5700, val_loss: 29.6214, loss:2.5559\n",
      "batch_num: 0, c_loss:1.5903, val_loss: 29.6256, loss:2.5702\n",
      "batch_num: 0, c_loss:1.5994, val_loss: 29.5773, loss:2.5800\n",
      "batch_num: 0, c_loss:1.5557, val_loss: 29.5222, loss:2.5288\n",
      "batch_num: 0, c_loss:1.5630, val_loss: 29.4741, loss:2.5459\n",
      "batch_num: 0, c_loss:1.6073, val_loss: 29.4404, loss:2.5923\n",
      "batch_num: 0, c_loss:1.6078, val_loss: 29.3530, loss:2.5966\n",
      "batch_num: 0, c_loss:1.6224, val_loss: 29.3462, loss:2.6105\n",
      "batch_num: 0, c_loss:1.6208, val_loss: 29.3414, loss:2.6129\n",
      "batch_num: 0, c_loss:1.5693, val_loss: 29.2556, loss:2.5611\n",
      "batch_num: 0, c_loss:1.5941, val_loss: 29.1250, loss:2.5944\n",
      "batch_num: 0, c_loss:1.5705, val_loss: 29.1484, loss:2.5710\n",
      "batch_num: 0, c_loss:1.5660, val_loss: 29.1638, loss:2.5713\n",
      "batch_num: 0, c_loss:1.5336, val_loss: 29.1464, loss:2.5367\n",
      "batch_num: 0, c_loss:1.5351, val_loss: 29.2174, loss:2.5398\n",
      "batch_num: 0, c_loss:1.5049, val_loss: 29.2883, loss:2.5055\n",
      "batch_num: 0, c_loss:1.5505, val_loss: 29.2116, loss:2.5500\n",
      "val_name1:\n",
      "val_name2:\n",
      "val_name3:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.8581560283688), ('2', 99.71631205673759), ('3', 99.57446808510639)])), ('2', OrderedDict([('2', 23.457394711067582), ('3', 0.04897159647404505)])), ('3', OrderedDict([('3', 27.054429028815367)]))])\n",
      "=====Task: 4=====\n",
      "batch_num: 0, c_loss:2.4256, val_loss: 40.5431, loss:3.4273\n",
      "batch_num: 0, c_loss:2.3523, val_loss: 38.4882, loss:3.3561\n",
      "batch_num: 0, c_loss:2.1780, val_loss: 36.6432, loss:3.1887\n",
      "batch_num: 0, c_loss:2.1583, val_loss: 34.7004, loss:3.1520\n",
      "batch_num: 0, c_loss:2.0217, val_loss: 32.8530, loss:3.0013\n",
      "batch_num: 0, c_loss:1.9220, val_loss: 31.3791, loss:2.8832\n",
      "batch_num: 0, c_loss:1.7885, val_loss: 30.2465, loss:2.7498\n",
      "batch_num: 0, c_loss:1.7314, val_loss: 29.1494, loss:2.7086\n",
      "batch_num: 0, c_loss:1.7018, val_loss: 27.8265, loss:2.7302\n",
      "batch_num: 0, c_loss:1.6206, val_loss: 26.3409, loss:2.7202\n",
      "batch_num: 0, c_loss:1.4898, val_loss: 25.1039, loss:2.6775\n",
      "batch_num: 0, c_loss:1.4447, val_loss: 24.2144, loss:2.7168\n",
      "batch_num: 0, c_loss:1.3983, val_loss: 23.5161, loss:2.7381\n",
      "batch_num: 0, c_loss:1.3550, val_loss: 23.0331, loss:2.7452\n",
      "batch_num: 0, c_loss:1.3288, val_loss: 22.6921, loss:2.7499\n",
      "batch_num: 0, c_loss:1.3093, val_loss: 22.3915, loss:2.7538\n",
      "batch_num: 0, c_loss:1.2560, val_loss: 21.9637, loss:2.7247\n",
      "batch_num: 0, c_loss:1.2466, val_loss: 21.5411, loss:2.7417\n",
      "batch_num: 0, c_loss:1.2166, val_loss: 21.2256, loss:2.7411\n",
      "batch_num: 0, c_loss:1.1984, val_loss: 21.0993, loss:2.7351\n",
      "batch_num: 0, c_loss:1.1863, val_loss: 21.1146, loss:2.7310\n",
      "batch_num: 0, c_loss:1.1674, val_loss: 21.3169, loss:2.7031\n",
      "batch_num: 0, c_loss:1.2318, val_loss: 21.5294, loss:2.7530\n",
      "batch_num: 0, c_loss:1.1999, val_loss: 21.6804, loss:2.7026\n",
      "batch_num: 0, c_loss:1.2434, val_loss: 21.7699, loss:2.7403\n",
      "batch_num: 0, c_loss:1.2151, val_loss: 21.9669, loss:2.6980\n",
      "batch_num: 0, c_loss:1.2286, val_loss: 22.1888, loss:2.7069\n",
      "batch_num: 0, c_loss:1.2498, val_loss: 22.6421, loss:2.6982\n",
      "batch_num: 0, c_loss:1.3000, val_loss: 23.1430, loss:2.7265\n",
      "batch_num: 0, c_loss:1.2630, val_loss: 23.5580, loss:2.6488\n",
      "batch_num: 0, c_loss:1.3057, val_loss: 23.8573, loss:2.6774\n",
      "batch_num: 0, c_loss:1.2541, val_loss: 24.2141, loss:2.5955\n",
      "batch_num: 0, c_loss:1.3099, val_loss: 24.5978, loss:2.6389\n",
      "batch_num: 0, c_loss:1.4050, val_loss: 25.0499, loss:2.6970\n",
      "batch_num: 0, c_loss:1.3740, val_loss: 25.5306, loss:2.6431\n",
      "batch_num: 0, c_loss:1.3935, val_loss: 26.0635, loss:2.6221\n",
      "batch_num: 0, c_loss:1.4527, val_loss: 26.3996, loss:2.6577\n",
      "batch_num: 0, c_loss:1.4794, val_loss: 26.6905, loss:2.6602\n",
      "batch_num: 0, c_loss:1.5070, val_loss: 26.8867, loss:2.6635\n",
      "batch_num: 0, c_loss:1.5240, val_loss: 27.1334, loss:2.6641\n",
      "batch_num: 0, c_loss:1.5318, val_loss: 27.2729, loss:2.6565\n",
      "batch_num: 0, c_loss:1.5116, val_loss: 27.4610, loss:2.6252\n",
      "batch_num: 0, c_loss:1.5577, val_loss: 27.6131, loss:2.6585\n",
      "batch_num: 0, c_loss:1.6001, val_loss: 27.7186, loss:2.6878\n",
      "batch_num: 0, c_loss:1.6100, val_loss: 27.8688, loss:2.6891\n",
      "batch_num: 0, c_loss:1.5957, val_loss: 28.0055, loss:2.6681\n",
      "batch_num: 0, c_loss:1.5829, val_loss: 28.0322, loss:2.6489\n",
      "batch_num: 0, c_loss:1.6083, val_loss: 28.0429, loss:2.6724\n",
      "batch_num: 0, c_loss:1.5353, val_loss: 28.0804, loss:2.5972\n",
      "batch_num: 0, c_loss:1.5990, val_loss: 28.1047, loss:2.6628\n",
      "batch_num: 0, c_loss:1.5561, val_loss: 28.1701, loss:2.6172\n",
      "batch_num: 0, c_loss:1.5519, val_loss: 28.2661, loss:2.6113\n",
      "batch_num: 0, c_loss:1.6345, val_loss: 28.2944, loss:2.6912\n",
      "batch_num: 0, c_loss:1.6234, val_loss: 28.2878, loss:2.6789\n",
      "batch_num: 0, c_loss:1.5957, val_loss: 28.2324, loss:2.6491\n",
      "batch_num: 0, c_loss:1.5648, val_loss: 28.1768, loss:2.6213\n",
      "batch_num: 0, c_loss:1.5548, val_loss: 28.1147, loss:2.6174\n",
      "batch_num: 0, c_loss:1.6104, val_loss: 27.9715, loss:2.6784\n",
      "batch_num: 0, c_loss:1.5826, val_loss: 28.0024, loss:2.6515\n",
      "batch_num: 0, c_loss:1.5416, val_loss: 27.9007, loss:2.6184\n",
      "batch_num: 0, c_loss:1.6100, val_loss: 27.8754, loss:2.6802\n",
      "batch_num: 0, c_loss:1.5444, val_loss: 27.8330, loss:2.6207\n",
      "batch_num: 0, c_loss:1.5741, val_loss: 27.7987, loss:2.6502\n",
      "batch_num: 0, c_loss:1.5871, val_loss: 27.6282, loss:2.6743\n",
      "batch_num: 0, c_loss:1.5703, val_loss: 27.4736, loss:2.6612\n",
      "batch_num: 0, c_loss:1.5953, val_loss: 27.4155, loss:2.6933\n",
      "batch_num: 0, c_loss:1.5256, val_loss: 27.3742, loss:2.6230\n",
      "batch_num: 0, c_loss:1.5375, val_loss: 27.2170, loss:2.6462\n",
      "batch_num: 0, c_loss:1.5599, val_loss: 27.1983, loss:2.6665\n",
      "batch_num: 0, c_loss:1.5700, val_loss: 27.1165, loss:2.6885\n",
      "batch_num: 0, c_loss:1.5074, val_loss: 27.0254, loss:2.6235\n",
      "batch_num: 0, c_loss:1.5644, val_loss: 26.9676, loss:2.6925\n",
      "batch_num: 0, c_loss:1.5316, val_loss: 26.9936, loss:2.6519\n",
      "batch_num: 0, c_loss:1.5488, val_loss: 26.9495, loss:2.6819\n",
      "batch_num: 0, c_loss:1.5423, val_loss: 26.9912, loss:2.6695\n",
      "batch_num: 0, c_loss:1.5574, val_loss: 26.8925, loss:2.6902\n",
      "batch_num: 0, c_loss:1.5327, val_loss: 26.8616, loss:2.6627\n",
      "batch_num: 0, c_loss:1.5403, val_loss: 26.7501, loss:2.6771\n",
      "batch_num: 0, c_loss:1.5020, val_loss: 26.6543, loss:2.6416\n",
      "batch_num: 0, c_loss:1.4894, val_loss: 26.5897, loss:2.6348\n",
      "batch_num: 0, c_loss:1.5305, val_loss: 26.6414, loss:2.6764\n",
      "batch_num: 0, c_loss:1.4665, val_loss: 26.6608, loss:2.6090\n",
      "batch_num: 0, c_loss:1.4891, val_loss: 26.6744, loss:2.6301\n",
      "batch_num: 0, c_loss:1.5392, val_loss: 26.7003, loss:2.6799\n",
      "batch_num: 0, c_loss:1.5055, val_loss: 26.8053, loss:2.6446\n",
      "batch_num: 0, c_loss:1.5394, val_loss: 26.7567, loss:2.6773\n",
      "batch_num: 0, c_loss:1.5164, val_loss: 26.7649, loss:2.6527\n",
      "batch_num: 0, c_loss:1.5508, val_loss: 26.8245, loss:2.6845\n",
      "batch_num: 0, c_loss:1.5049, val_loss: 26.8576, loss:2.6364\n",
      "batch_num: 0, c_loss:1.5418, val_loss: 26.8878, loss:2.6712\n",
      "batch_num: 0, c_loss:1.5411, val_loss: 26.9210, loss:2.6688\n",
      "batch_num: 0, c_loss:1.5097, val_loss: 26.8726, loss:2.6381\n",
      "batch_num: 0, c_loss:1.4980, val_loss: 26.7742, loss:2.6264\n",
      "batch_num: 0, c_loss:1.5141, val_loss: 26.7722, loss:2.6464\n",
      "batch_num: 0, c_loss:1.5105, val_loss: 26.7573, loss:2.6438\n",
      "batch_num: 0, c_loss:1.5529, val_loss: 26.9917, loss:2.6873\n",
      "val_name1:\n",
      "val_name2:\n",
      "val_name3:\n",
      "val_name4:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.8581560283688), ('2', 99.71631205673759), ('3', 99.57446808510639), ('4', 99.57446808510639)])), ('2', OrderedDict([('2', 23.457394711067582), ('3', 0.04897159647404505), ('4', 0.0)])), ('3', OrderedDict([('3', 27.054429028815367), ('4', 0.0)])), ('4', OrderedDict([('4', 37.8147029204431)]))])\n",
      "=====Task: 5=====\n",
      "batch_num: 0, c_loss:2.8001, val_loss: 42.6834, loss:3.9554\n",
      "batch_num: 0, c_loss:2.7966, val_loss: 40.7727, loss:3.9180\n",
      "batch_num: 0, c_loss:2.5895, val_loss: 38.6917, loss:3.6876\n",
      "batch_num: 0, c_loss:2.5139, val_loss: 36.5122, loss:3.5787\n",
      "batch_num: 0, c_loss:2.3500, val_loss: 34.4270, loss:3.3877\n",
      "batch_num: 0, c_loss:2.1308, val_loss: 32.7449, loss:3.1457\n",
      "batch_num: 0, c_loss:2.0788, val_loss: 31.2209, loss:3.1181\n",
      "batch_num: 0, c_loss:1.9356, val_loss: 29.6097, loss:3.0236\n",
      "batch_num: 0, c_loss:1.8802, val_loss: 28.1710, loss:3.0452\n",
      "batch_num: 0, c_loss:1.7616, val_loss: 26.9302, loss:3.0142\n",
      "batch_num: 0, c_loss:1.6234, val_loss: 26.0107, loss:2.9655\n",
      "batch_num: 0, c_loss:1.6390, val_loss: 25.3398, loss:3.0484\n",
      "batch_num: 0, c_loss:1.5880, val_loss: 24.7884, loss:3.0504\n",
      "batch_num: 0, c_loss:1.5362, val_loss: 24.2799, loss:3.0480\n",
      "batch_num: 0, c_loss:1.5240, val_loss: 23.8000, loss:3.0767\n",
      "batch_num: 0, c_loss:1.4944, val_loss: 23.3549, loss:3.0869\n",
      "batch_num: 0, c_loss:1.4869, val_loss: 22.9760, loss:3.1135\n",
      "batch_num: 0, c_loss:1.4319, val_loss: 22.8629, loss:3.0748\n",
      "batch_num: 0, c_loss:1.4575, val_loss: 22.9038, loss:3.1042\n",
      "batch_num: 0, c_loss:1.3998, val_loss: 23.0408, loss:3.0396\n",
      "batch_num: 0, c_loss:1.4732, val_loss: 23.1058, loss:3.1037\n",
      "batch_num: 0, c_loss:1.4885, val_loss: 23.0933, loss:3.1137\n",
      "batch_num: 0, c_loss:1.4523, val_loss: 23.1388, loss:3.0696\n",
      "batch_num: 0, c_loss:1.3871, val_loss: 23.3736, loss:2.9896\n",
      "batch_num: 0, c_loss:1.4878, val_loss: 23.7211, loss:3.0661\n",
      "batch_num: 0, c_loss:1.4942, val_loss: 24.1024, loss:3.0419\n",
      "batch_num: 0, c_loss:1.5005, val_loss: 24.4678, loss:3.0238\n",
      "batch_num: 0, c_loss:1.5580, val_loss: 24.8951, loss:3.0437\n",
      "batch_num: 0, c_loss:1.5309, val_loss: 25.2165, loss:2.9994\n",
      "batch_num: 0, c_loss:1.6337, val_loss: 25.5190, loss:3.0630\n",
      "batch_num: 0, c_loss:1.5958, val_loss: 25.9113, loss:3.0086\n",
      "batch_num: 0, c_loss:1.6158, val_loss: 26.4490, loss:2.9863\n",
      "batch_num: 0, c_loss:1.6707, val_loss: 27.0035, loss:3.0180\n",
      "batch_num: 0, c_loss:1.7107, val_loss: 27.4446, loss:3.0134\n",
      "batch_num: 0, c_loss:1.6930, val_loss: 27.6976, loss:2.9766\n",
      "batch_num: 0, c_loss:1.6989, val_loss: 28.0011, loss:2.9550\n",
      "batch_num: 0, c_loss:1.7470, val_loss: 28.3231, loss:2.9796\n",
      "batch_num: 0, c_loss:1.7107, val_loss: 28.6229, loss:2.9136\n",
      "batch_num: 0, c_loss:1.7673, val_loss: 28.8691, loss:2.9512\n",
      "batch_num: 0, c_loss:1.8598, val_loss: 29.1564, loss:3.0206\n",
      "batch_num: 0, c_loss:1.8471, val_loss: 29.5100, loss:2.9897\n",
      "batch_num: 0, c_loss:1.9293, val_loss: 29.7697, loss:3.0503\n",
      "batch_num: 0, c_loss:1.8965, val_loss: 30.0154, loss:3.0005\n",
      "batch_num: 0, c_loss:1.8499, val_loss: 30.1153, loss:2.9425\n",
      "batch_num: 0, c_loss:1.9626, val_loss: 30.2151, loss:3.0393\n",
      "batch_num: 0, c_loss:1.9434, val_loss: 30.4235, loss:3.0069\n",
      "batch_num: 0, c_loss:1.9931, val_loss: 30.5779, loss:3.0418\n",
      "batch_num: 0, c_loss:1.9522, val_loss: 30.6657, loss:2.9964\n",
      "batch_num: 0, c_loss:1.9550, val_loss: 30.7386, loss:2.9934\n",
      "batch_num: 0, c_loss:1.9367, val_loss: 30.7841, loss:2.9779\n",
      "batch_num: 0, c_loss:1.9207, val_loss: 30.7842, loss:2.9584\n",
      "batch_num: 0, c_loss:2.0067, val_loss: 30.7567, loss:3.0490\n",
      "batch_num: 0, c_loss:1.9400, val_loss: 30.8882, loss:2.9752\n",
      "batch_num: 0, c_loss:1.9802, val_loss: 30.8217, loss:3.0166\n",
      "batch_num: 0, c_loss:1.9314, val_loss: 30.5995, loss:2.9712\n",
      "batch_num: 0, c_loss:1.9128, val_loss: 30.4270, loss:2.9679\n",
      "batch_num: 0, c_loss:1.9471, val_loss: 30.3102, loss:3.0060\n",
      "batch_num: 0, c_loss:1.9175, val_loss: 30.2939, loss:2.9844\n",
      "batch_num: 0, c_loss:1.9776, val_loss: 30.2898, loss:3.0440\n",
      "batch_num: 0, c_loss:1.9364, val_loss: 30.1357, loss:3.0111\n",
      "batch_num: 0, c_loss:1.9193, val_loss: 29.9040, loss:2.9966\n",
      "batch_num: 0, c_loss:1.8961, val_loss: 29.8063, loss:2.9888\n",
      "batch_num: 0, c_loss:1.8932, val_loss: 29.7481, loss:2.9882\n",
      "batch_num: 0, c_loss:1.8692, val_loss: 29.7386, loss:2.9738\n",
      "batch_num: 0, c_loss:1.9211, val_loss: 29.7045, loss:3.0206\n",
      "batch_num: 0, c_loss:1.8241, val_loss: 29.5836, loss:2.9312\n",
      "batch_num: 0, c_loss:1.8859, val_loss: 29.4201, loss:2.9919\n",
      "batch_num: 0, c_loss:1.8630, val_loss: 29.3096, loss:2.9836\n",
      "batch_num: 0, c_loss:1.8785, val_loss: 29.3581, loss:3.0026\n",
      "batch_num: 0, c_loss:1.8394, val_loss: 29.3538, loss:2.9669\n",
      "batch_num: 0, c_loss:1.7988, val_loss: 29.3801, loss:2.9168\n",
      "batch_num: 0, c_loss:1.8831, val_loss: 29.2605, loss:3.0098\n",
      "batch_num: 0, c_loss:1.8395, val_loss: 29.1063, loss:2.9666\n",
      "batch_num: 0, c_loss:1.8148, val_loss: 29.0893, loss:2.9488\n",
      "batch_num: 0, c_loss:1.8428, val_loss: 29.1551, loss:2.9742\n",
      "batch_num: 0, c_loss:1.8140, val_loss: 29.1957, loss:2.9447\n",
      "batch_num: 0, c_loss:1.8410, val_loss: 29.3067, loss:2.9649\n",
      "batch_num: 0, c_loss:1.8645, val_loss: 29.2649, loss:2.9870\n",
      "batch_num: 0, c_loss:1.8644, val_loss: 29.2134, loss:2.9834\n",
      "batch_num: 0, c_loss:1.8764, val_loss: 29.1692, loss:3.0027\n",
      "batch_num: 0, c_loss:1.8340, val_loss: 29.1413, loss:2.9628\n",
      "batch_num: 0, c_loss:1.8684, val_loss: 29.1617, loss:2.9984\n",
      "batch_num: 0, c_loss:1.8331, val_loss: 29.3027, loss:2.9564\n",
      "batch_num: 0, c_loss:1.8383, val_loss: 29.4417, loss:2.9577\n",
      "batch_num: 0, c_loss:1.8403, val_loss: 29.4450, loss:2.9539\n",
      "batch_num: 0, c_loss:1.8325, val_loss: 29.3680, loss:2.9488\n",
      "batch_num: 0, c_loss:1.8601, val_loss: 29.3935, loss:2.9779\n",
      "batch_num: 0, c_loss:1.8566, val_loss: 29.4860, loss:2.9715\n",
      "batch_num: 0, c_loss:1.9090, val_loss: 29.5744, loss:3.0209\n",
      "batch_num: 0, c_loss:1.8489, val_loss: 29.6713, loss:2.9506\n",
      "batch_num: 0, c_loss:1.8903, val_loss: 29.5643, loss:2.9954\n",
      "batch_num: 0, c_loss:1.9002, val_loss: 29.6071, loss:3.0040\n",
      "batch_num: 0, c_loss:1.7373, val_loss: 29.6826, loss:2.8434\n",
      "val_name1:\n",
      "val_name2:\n",
      "val_name3:\n",
      "val_name4:\n",
      "val_name5:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.8581560283688), ('2', 99.71631205673759), ('3', 99.57446808510639), ('4', 99.57446808510639), ('5', 99.71631205673759)])), ('2', OrderedDict([('2', 23.457394711067582), ('3', 0.04897159647404505), ('4', 0.0), ('5', 0.0979431929480901)])), ('3', OrderedDict([('3', 27.054429028815367), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 37.8147029204431), ('5', 0.050352467270896276)])), ('5', OrderedDict([('5', 28.946041351487644)]))])\n",
      "Task 1 average acc: 99.8581560283688\n",
      "Task 2 average acc: 61.586853383902586\n",
      "Task 3 average acc: 42.2259562367986\n",
      "Task 4 average acc: 34.34729275138737\n",
      "Task 5 average acc: 25.762129813688848\n"
     ]
    }
   ],
   "source": [
    "avg_acc_history = train(task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"avg_final_acc = np.zeros(repeat)\\n\\nfor r in range (repeat):\\n    acc_table = OrderedDict()\\n    valid_out_dim = 0\\n    for i in range(len(task_names)):\\n        valid_out_dim += 2\\n        train_name = task_names[i]\\n        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name],\\n                                                            batch_size=batch_size, shuffle=True)\\n        val_loader = torch.utils.data.DataLoader(val_dataset_splits[train_name],\\n                                                        batch_size=batch_size, shuffle=False)\\n        # Train\\n        for epoch in range(4):\\n            train_acc = AverageMeter()\\n            for (input, target, task) in train_loader:\\n                agent.train()\\n                input, target = input.to(device), target.to(device)\\n\\n                output = agent(input)\\n                loss = criterion_fn(criterion, output, target, valid_out_dim)\\n\\n                optimizer.zero_grad()\\n                loss.backward()\\n                optimizer.step()\\n\\n                train_acc = accumulate_acc(output, target, train_acc)\\n\\n        # Eval\\n        acc_table[train_name] = OrderedDict()\\n\\n        for j in range(i+1):\\n            val_name = task_names[j]\\n            val_data = val_dataset_splits[val_name]\\n            val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False,)\\n\\n            agent.eval()\\n            val_acc = AverageMeter()\\n            with torch.no_grad():\\n                for i, (input, target, task) in enumerate(val_loader):\\n                    input, target = input.to(device), target.to(device)\\n                    output = agent(input)\\n                    val_acc = accumulate_acc(output, target, val_acc)\\n\\n            acc_table[val_name][train_name] = val_acc.avg\\n\\n    print(acc_table)\\n\\n    avg_acc_history = [0] * len(task_names)\\n    for i in range(len(task_names)):\\n        train_name = task_names[i]\\n        cls_acc_sum = 0\\n        for j in range(i + 1):\\n            val_name = task_names[j]\\n            cls_acc_sum += acc_table[val_name][train_name]\\n\\n        avg_acc_history[i] = cls_acc_sum / (i + 1)\\n        print('Task', train_name, 'average acc:', avg_acc_history[i])\\n    \\n    avg_final_acc[r] = avg_acc_history[-1]\\n    print('===Summary of experiment repeats:',r+1,'/',repeat,'===')\\n    print(avg_final_acc)\\n    print('mean:', avg_final_acc.mean(), 'std:', avg_final_acc.std())\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''avg_final_acc = np.zeros(repeat)\n",
    "\n",
    "for r in range (repeat):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name],\n",
    "                                                            batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset_splits[train_name],\n",
    "                                                        batch_size=batch_size, shuffle=False)\n",
    "        # Train\n",
    "        for epoch in range(4):\n",
    "            train_acc = AverageMeter()\n",
    "            for (input, target, task) in train_loader:\n",
    "                agent.train()\n",
    "                input, target = input.to(device), target.to(device)\n",
    "\n",
    "                output = agent(input)\n",
    "                loss = criterion_fn(criterion, output, target, valid_out_dim)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_acc = accumulate_acc(output, target, train_acc)\n",
    "\n",
    "        # Eval\n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False,)\n",
    "\n",
    "            agent.eval()\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, task) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = agent(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "    print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    avg_final_acc[r] = avg_acc_history[-1]\n",
    "    print('===Summary of experiment repeats:',r+1,'/',repeat,'===')\n",
    "    print(avg_final_acc)\n",
    "    print('mean:', avg_final_acc.mean(), 'std:', avg_final_acc.std())'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
