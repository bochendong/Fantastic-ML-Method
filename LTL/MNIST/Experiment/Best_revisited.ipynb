{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_settings = {\n",
    "    \"Baseline\": {\"epoches\": 4, \"lr\": 0.001, \"l2\": None, \"loss_type\": None},\n",
    "    \"Baseline\": {\"epoches\": 4, \"lr\": 0.001, \"l2\": 1e-4, \"loss_type\": None},\n",
    "    \"Distribution\": {\"epoches\": 4, \"lr\": 0.001, \"l2\": None, \"loss_type\": \"distribution\"},\n",
    "    \"FollowLeader\": {\"epoches\": 4, \"lr\": 0.001, \"l2\": None, \"loss_type\": \"follow_leader\"},\n",
    "    \"Interupt\": {\"epoches\": 4, \"lr\": 0.001, \"l2\": None, \"loss_type\": \"Interupt\"},\n",
    "    \"Interupt\": {\"epoches\": 4, \"lr\": 0.0005, \"l2\": None, \"loss_type\": \"Interupt\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_dataloader(task_num):\n",
    "    task_dir = {0: [0, 1], 1: [2, 3], 2: [4, 5], 3: [6, 7], 4:[8, 9]}\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=tf)\n",
    "\n",
    "    indices = [i for i, (_, label) in enumerate(train_dataset) if label in task_dir[task_num]]\n",
    "    dataset = Subset(train_dataset, indices)\n",
    "    task_train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=tf, download=True)\n",
    "    indices = [i for i, (_, label) in enumerate(test_dataset) if label in task_dir[task_num]]\n",
    "    dataset = Subset(test_dataset, indices)\n",
    "    task_test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return task_train_loader, task_test_loader\n",
    "\n",
    "def load_all_data():\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=tf, download=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=tf, download=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.75 * param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Enhance(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=28, hidden_dim=400):\n",
    "        super(MLP_Enhance, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.linear:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                init.constant_(layer.bias, 0)\n",
    "        last_linear = self.last[0]\n",
    "        init.xavier_normal_(last_linear.weight)\n",
    "        init.constant_(last_linear.bias, 0)\n",
    "\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_all_data()\n",
    "task_dataloaders = {}\n",
    "for task in range(0, 5):\n",
    "    train_dl, test_ld = task_dataloader(task)\n",
    "    task_dataloaders[task] = (train_dl, test_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_history(all_accuracies):\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    for i, task_accuracies in enumerate(all_accuracies):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        for task, accs in task_accuracies.items():\n",
    "            plt.plot(accs,'-', label=f'Task {task}')\n",
    "        plt.title(f'Accuracy per Mini-Batch for Task {i}')\n",
    "        plt.xlabel('Mini-Batch Number')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.ylim(-5, 105)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_task_acc_history(all_accuracies, save_name):\n",
    "    for task in [0, 1, 2, 3, 4]:\n",
    "        if (task == 0):\n",
    "            task_history = []\n",
    "        else:\n",
    "            task_history =  [0] * (50 * task)\n",
    "        for i, task_accuracies in enumerate (all_accuracies):\n",
    "            if (i >= task):\n",
    "                task_history.extend(task_accuracies[task])\n",
    "                    \n",
    "        plt.plot(task_history, '-', label=f'Task {task}')\n",
    "\n",
    "    plt.title(f'Accuracy per Mini-Batch for base line')\n",
    "    plt.xlabel('Mini-Batch Number')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(-5, 105)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, task_num, criterion, epoches = 4, lr = 0.001, l2 = None, loss_type = None):\n",
    "    follower = MLP_Enhance()\n",
    "    follower = follower.to(device)\n",
    "    follower.load_state_dict(model.state_dict())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if l2 is not None:\n",
    "        optimizer_F = torch.optim.Adam(follower.parameters(), lr)\n",
    "    else:\n",
    "        optimizer_F = torch.optim.Adam(follower.parameters(), lr, weight_decay = l2)\n",
    "        \n",
    "    train_loader = task_dataloaders[task_num][0]\n",
    "\n",
    "    task_accuracies = {task: [] for task in range(task_num + 1)}\n",
    "\n",
    "    valid_out_dim = task_num * 2 + 2\n",
    "    logging.info(f\"##########Task {task_num}##########\")\n",
    "    for e in range(epoches):\n",
    "        logging.info(f\"Epoch {e}\")\n",
    "        batch_num = 0\n",
    "        for images, labels in train_loader:\n",
    "            follower.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = follower(images)\n",
    "\n",
    "            reg_loss = 0\n",
    "            if (loss_type == \"distribution\"):\n",
    "                except_mean = 0.1\n",
    "                for task in range (10):\n",
    "                    task_start = task\n",
    "                    task_end = (task + 1)\n",
    "\n",
    "                    fake_image = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "                    fake_output = follower(fake_image)\n",
    "\n",
    "                    real_mean = torch.mean(torch.abs(fake_output[:,task_start:task_end]))\n",
    "                    reg_loss += torch.abs(except_mean - real_mean)\n",
    "            elif (loss_type == \"follow_leader\"):\n",
    "                with torch.no_grad():\n",
    "                    leader_output = model(images)\n",
    "                if (task_num != 0):\n",
    "                    reg_loss =  2 * torch.mean(torch.abs(leader_output[:,:valid_out_dim - 2] - outputs[:,:valid_out_dim - 2]))\n",
    "            elif (loss_type == \"interupt\"):\n",
    "                for _ in range (4):\n",
    "                    fake_image = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        leader_fake_output = model(fake_image)\n",
    "                    \n",
    "                    fake_outputs = follower(images)\n",
    "\n",
    "                    if (task_num != 0):\n",
    "                        reg_loss += torch.mean(torch.abs(leader_fake_output - fake_outputs))\n",
    "\n",
    "            loss = criterion(outputs[:,:valid_out_dim], labels) + reg_loss\n",
    "            optimizer_F.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_F.step()\n",
    "\n",
    "            if(batch_num % 10 == 0):\n",
    "                avg_acc = 0\n",
    "                log_message = []\n",
    "                for task in range(task_num + 1):\n",
    "                    acc = cal_acc(follower, task_dataloaders[task][0], device)\n",
    "                    avg_acc += acc\n",
    "                    task_accuracies[task].append(acc * 100)\n",
    "                    log_message.append(f\"Batch num: {batch_num}, Task {task} acc: {acc * 100:.4f}\")\n",
    "\n",
    "                logging.info(', '.join(log_message) + f\", Task avg acc:{avg_acc*100/(task_num + 1):.4f}\")\n",
    "\n",
    "            batch_num += 1\n",
    "\n",
    "        adjust_learning_rate(optimizer_F)\n",
    "    \n",
    "    return follower, task_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(task_name, epochs, lr, l2):\n",
    "    log_filename = f\"{task_name}_{epochs}_lr={lr}_l2={l2}.log\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        handlers=[\n",
    "                            logging.FileHandler(log_filename),\n",
    "                            logging.StreamHandler()\n",
    "                        ],\n",
    "                        force=True)\n",
    "    \n",
    "def train_split_task(Task_Name):\n",
    "    model = MLP_Enhance()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    settings = train_settings[Task_Name]\n",
    "    epochs, lr, l2, loss_type = settings['epochs'], settings['lr'], settings['l2'], settings['loss_type']\n",
    "\n",
    "    if os.path.exists(f\"{Task_Name}_{epochs}_lr={lr}_l2={l2}.log\") == False:\n",
    "        setup_logging(Task_Name, epochs, lr, l2)\n",
    "        all_accuracies = []\n",
    "        for task in [0, 1, 2, 3, 4]:\n",
    "            model, task_accuracies = train(model, task, criterion, epoches = epochs, lr = lr, l2 = l2, loss_type = loss_type)\n",
    "            all_accuracies.append(task_accuracies)\n",
    "\n",
    "    plot_task_acc_history(all_accuracies, f\"{Task_Name}_{epochs}_{lr}_{l2}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 22:52:51,167 - INFO - ##########Task 0##########\n",
      "2024-05-25 22:52:51,181 - INFO - Epoch 0\n",
      "2024-05-25 22:52:54,022 - INFO - Batch num: 0, Task 0 acc: 3.6990, Task avg acc:3.6990\n",
      "2024-05-25 22:52:56,214 - INFO - Batch num: 10, Task 0 acc: 98.1904, Task avg acc:98.1904\n",
      "2024-05-25 22:52:58,938 - INFO - Batch num: 20, Task 0 acc: 99.6572, Task avg acc:99.6572\n",
      "2024-05-25 22:53:01,332 - INFO - Batch num: 30, Task 0 acc: 99.7768, Task avg acc:99.7768\n",
      "2024-05-25 22:53:03,742 - INFO - Batch num: 40, Task 0 acc: 99.8087, Task avg acc:99.8087\n",
      "2024-05-25 22:53:05,882 - INFO - Batch num: 50, Task 0 acc: 99.8565, Task avg acc:99.8565\n",
      "2024-05-25 22:53:08,172 - INFO - Batch num: 60, Task 0 acc: 99.8884, Task avg acc:99.8884\n",
      "2024-05-25 22:53:10,156 - INFO - Batch num: 70, Task 0 acc: 99.9123, Task avg acc:99.9123\n",
      "2024-05-25 22:53:12,820 - INFO - Batch num: 80, Task 0 acc: 99.9203, Task avg acc:99.9203\n",
      "2024-05-25 22:53:15,358 - INFO - Batch num: 90, Task 0 acc: 99.9123, Task avg acc:99.9123\n",
      "2024-05-25 22:53:15,559 - INFO - Epoch 1\n",
      "2024-05-25 22:53:17,488 - INFO - Batch num: 0, Task 0 acc: 99.9203, Task avg acc:99.9203\n",
      "2024-05-25 22:53:19,620 - INFO - Batch num: 10, Task 0 acc: 99.9362, Task avg acc:99.9362\n",
      "2024-05-25 22:53:22,071 - INFO - Batch num: 20, Task 0 acc: 99.9283, Task avg acc:99.9283\n",
      "2024-05-25 22:53:24,765 - INFO - Batch num: 30, Task 0 acc: 99.9362, Task avg acc:99.9362\n",
      "2024-05-25 22:53:26,949 - INFO - Batch num: 40, Task 0 acc: 99.9442, Task avg acc:99.9442\n",
      "2024-05-25 22:53:29,371 - INFO - Batch num: 50, Task 0 acc: 99.9442, Task avg acc:99.9442\n",
      "2024-05-25 22:53:32,485 - INFO - Batch num: 60, Task 0 acc: 99.9522, Task avg acc:99.9522\n",
      "2024-05-25 22:53:35,701 - INFO - Batch num: 70, Task 0 acc: 99.9522, Task avg acc:99.9522\n",
      "2024-05-25 22:53:38,431 - INFO - Batch num: 80, Task 0 acc: 99.9601, Task avg acc:99.9601\n",
      "2024-05-25 22:53:41,165 - INFO - Batch num: 90, Task 0 acc: 99.9522, Task avg acc:99.9522\n",
      "2024-05-25 22:53:41,357 - INFO - Epoch 2\n",
      "2024-05-25 22:53:43,798 - INFO - Batch num: 0, Task 0 acc: 99.9522, Task avg acc:99.9522\n",
      "2024-05-25 22:53:47,479 - INFO - Batch num: 10, Task 0 acc: 99.9522, Task avg acc:99.9522\n",
      "2024-05-25 22:53:50,633 - INFO - Batch num: 20, Task 0 acc: 99.9601, Task avg acc:99.9601\n",
      "2024-05-25 22:53:53,314 - INFO - Batch num: 30, Task 0 acc: 99.9601, Task avg acc:99.9601\n",
      "2024-05-25 22:53:55,406 - INFO - Batch num: 40, Task 0 acc: 99.9522, Task avg acc:99.9522\n",
      "2024-05-25 22:53:57,490 - INFO - Batch num: 50, Task 0 acc: 99.9442, Task avg acc:99.9442\n",
      "2024-05-25 22:53:59,773 - INFO - Batch num: 60, Task 0 acc: 99.9601, Task avg acc:99.9601\n",
      "2024-05-25 22:54:01,996 - INFO - Batch num: 70, Task 0 acc: 99.9601, Task avg acc:99.9601\n",
      "2024-05-25 22:54:04,062 - INFO - Batch num: 80, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:06,147 - INFO - Batch num: 90, Task 0 acc: 99.9601, Task avg acc:99.9601\n",
      "2024-05-25 22:54:06,329 - INFO - Epoch 3\n",
      "2024-05-25 22:54:08,117 - INFO - Batch num: 0, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:10,122 - INFO - Batch num: 10, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:12,121 - INFO - Batch num: 20, Task 0 acc: 99.9601, Task avg acc:99.9601\n",
      "2024-05-25 22:54:14,133 - INFO - Batch num: 30, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:16,138 - INFO - Batch num: 40, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:18,555 - INFO - Batch num: 50, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:20,780 - INFO - Batch num: 60, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:22,827 - INFO - Batch num: 70, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:24,784 - INFO - Batch num: 80, Task 0 acc: 99.9681, Task avg acc:99.9681\n",
      "2024-05-25 22:54:26,728 - INFO - Batch num: 90, Task 0 acc: 99.9761, Task avg acc:99.9761\n",
      "2024-05-25 22:54:26,920 - INFO - ##########Task 1##########\n",
      "2024-05-25 22:54:26,920 - INFO - Epoch 0\n",
      "2024-05-25 22:54:30,252 - INFO - Batch num: 0, Task 0 acc: 99.9761, Batch num: 0, Task 1 acc: 0.0000, Task avg acc:49.9880\n",
      "2024-05-25 22:54:34,465 - INFO - Batch num: 10, Task 0 acc: 99.9601, Batch num: 10, Task 1 acc: 10.5219, Task avg acc:55.2410\n",
      "2024-05-25 22:54:38,458 - INFO - Batch num: 20, Task 0 acc: 98.8600, Batch num: 20, Task 1 acc: 63.3311, Task avg acc:81.0956\n",
      "2024-05-25 22:54:42,106 - INFO - Batch num: 30, Task 0 acc: 45.8705, Batch num: 30, Task 1 acc: 91.9548, Task avg acc:68.9127\n",
      "2024-05-25 22:54:45,677 - INFO - Batch num: 40, Task 0 acc: 2.1763, Batch num: 40, Task 1 acc: 96.0938, Task avg acc:49.1350\n",
      "2024-05-25 22:54:49,152 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 96.7670, Task avg acc:48.3835\n",
      "2024-05-25 22:54:53,268 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 97.1659, Task avg acc:48.5829\n",
      "2024-05-25 22:54:57,634 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 97.5066, Task avg acc:48.7533\n",
      "2024-05-25 22:55:01,631 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 97.8308, Task avg acc:48.9154\n",
      "2024-05-25 22:55:06,376 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 98.0884, Task avg acc:49.0442\n",
      "2024-05-25 22:55:06,474 - INFO - Epoch 1\n",
      "2024-05-25 22:55:10,386 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 98.1300, Task avg acc:49.0650\n",
      "2024-05-25 22:55:14,434 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 98.2380, Task avg acc:49.1190\n",
      "2024-05-25 22:55:18,552 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 98.4043, Task avg acc:49.2021\n",
      "2024-05-25 22:55:22,703 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 98.3959, Task avg acc:49.1980\n",
      "2024-05-25 22:55:27,726 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 98.4541, Task avg acc:49.2271\n",
      "2024-05-25 22:55:31,965 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 98.5040, Task avg acc:49.2520\n",
      "2024-05-25 22:55:35,755 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 98.6120, Task avg acc:49.3060\n",
      "2024-05-25 22:55:40,030 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 98.7118, Task avg acc:49.3559\n",
      "2024-05-25 22:55:43,854 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 98.7866, Task avg acc:49.3933\n",
      "2024-05-25 22:55:47,552 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 98.7616, Task avg acc:49.3808\n",
      "2024-05-25 22:55:47,634 - INFO - Epoch 2\n",
      "2024-05-25 22:55:51,379 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 98.7201, Task avg acc:49.3600\n",
      "2024-05-25 22:55:55,000 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 98.7783, Task avg acc:49.3891\n",
      "2024-05-25 22:55:58,520 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 98.8697, Task avg acc:49.4348\n",
      "2024-05-25 22:56:02,069 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 98.8697, Task avg acc:49.4348\n",
      "2024-05-25 22:56:05,755 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 98.9029, Task avg acc:49.4515\n",
      "2024-05-25 22:56:09,396 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 98.9362, Task avg acc:49.4681\n",
      "2024-05-25 22:56:12,918 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 98.8863, Task avg acc:49.4432\n",
      "2024-05-25 22:56:16,427 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 99.0193, Task avg acc:49.5096\n",
      "2024-05-25 22:56:19,967 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 99.0691, Task avg acc:49.5346\n",
      "2024-05-25 22:56:23,826 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 99.0691, Task avg acc:49.5346\n",
      "2024-05-25 22:56:23,924 - INFO - Epoch 3\n",
      "2024-05-25 22:56:27,780 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 99.0775, Task avg acc:49.5387\n",
      "2024-05-25 22:56:31,347 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 99.0193, Task avg acc:49.5096\n",
      "2024-05-25 22:56:34,914 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 99.1523, Task avg acc:49.5761\n",
      "2024-05-25 22:56:38,421 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 99.1772, Task avg acc:49.5886\n",
      "2024-05-25 22:56:41,995 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 99.1855, Task avg acc:49.5928\n",
      "2024-05-25 22:56:45,621 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 99.2188, Task avg acc:49.6094\n",
      "2024-05-25 22:56:49,100 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 99.1772, Task avg acc:49.5886\n",
      "2024-05-25 22:56:52,860 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 99.2354, Task avg acc:49.6177\n",
      "2024-05-25 22:56:57,184 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 99.2603, Task avg acc:49.6302\n",
      "2024-05-25 22:57:01,131 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 99.2603, Task avg acc:49.6302\n",
      "2024-05-25 22:57:01,241 - INFO - ##########Task 2##########\n",
      "2024-05-25 22:57:01,242 - INFO - Epoch 0\n",
      "2024-05-25 22:57:06,247 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 99.1439, Batch num: 0, Task 2 acc: 0.0000, Task avg acc:33.0480\n",
      "2024-05-25 22:57:11,308 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 98.5622, Batch num: 10, Task 2 acc: 0.0000, Task avg acc:32.8541\n",
      "2024-05-25 22:57:16,497 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 96.8251, Batch num: 20, Task 2 acc: 9.8060, Task avg acc:35.5437\n",
      "2024-05-25 22:57:21,612 - INFO - Batch num: 30, Task 0 acc: 0.0957, Batch num: 30, Task 1 acc: 76.9531, Batch num: 30, Task 2 acc: 51.0237, Task avg acc:42.6908\n",
      "2024-05-25 22:57:26,764 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 30.3358, Batch num: 40, Task 2 acc: 79.8132, Task avg acc:36.7163\n",
      "2024-05-25 22:57:31,966 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 3.4574, Batch num: 50, Task 2 acc: 98.4555, Task avg acc:33.9710\n",
      "2024-05-25 22:57:37,094 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.2909, Batch num: 60, Task 2 acc: 99.0751, Task avg acc:33.1220\n",
      "2024-05-25 22:57:42,404 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0332, Batch num: 70, Task 2 acc: 99.2636, Task avg acc:33.0990\n",
      "2024-05-25 22:57:47,598 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0249, Batch num: 80, Task 2 acc: 99.3534, Task avg acc:33.1261\n",
      "2024-05-25 22:57:47,752 - INFO - Epoch 1\n",
      "2024-05-25 22:57:52,689 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0083, Batch num: 0, Task 2 acc: 99.4163, Task avg acc:33.1415\n",
      "2024-05-25 22:57:57,841 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.0249, Batch num: 10, Task 2 acc: 99.4432, Task avg acc:33.1561\n",
      "2024-05-25 22:58:03,360 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.0332, Batch num: 20, Task 2 acc: 99.4971, Task avg acc:33.1768\n",
      "2024-05-25 22:58:10,258 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0416, Batch num: 30, Task 2 acc: 99.5151, Task avg acc:33.1855\n",
      "2024-05-25 22:58:15,877 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0083, Batch num: 40, Task 2 acc: 99.5690, Task avg acc:33.1924\n",
      "2024-05-25 22:58:22,471 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0416, Batch num: 50, Task 2 acc: 99.6408, Task avg acc:33.2275\n",
      "2024-05-25 22:58:30,457 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.0748, Batch num: 60, Task 2 acc: 99.6588, Task avg acc:33.2445\n",
      "2024-05-25 22:58:36,928 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0665, Batch num: 70, Task 2 acc: 99.6857, Task avg acc:33.2507\n",
      "2024-05-25 22:58:44,374 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0665, Batch num: 80, Task 2 acc: 99.7037, Task avg acc:33.2567\n",
      "2024-05-25 22:58:44,550 - INFO - Epoch 2\n",
      "2024-05-25 22:58:49,721 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0332, Batch num: 0, Task 2 acc: 99.6857, Task avg acc:33.2396\n",
      "2024-05-25 22:58:56,006 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.0499, Batch num: 10, Task 2 acc: 99.6857, Task avg acc:33.2452\n",
      "2024-05-25 22:59:02,121 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.0416, Batch num: 20, Task 2 acc: 99.6677, Task avg acc:33.2364\n",
      "2024-05-25 22:59:08,417 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0249, Batch num: 30, Task 2 acc: 99.6588, Task avg acc:33.2279\n",
      "2024-05-25 22:59:16,110 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0000, Batch num: 40, Task 2 acc: 99.7126, Task avg acc:33.2375\n",
      "2024-05-25 22:59:22,128 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0083, Batch num: 50, Task 2 acc: 99.7306, Task avg acc:33.2463\n",
      "2024-05-25 22:59:28,205 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.0083, Batch num: 60, Task 2 acc: 99.7665, Task avg acc:33.2583\n",
      "2024-05-25 22:59:34,210 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0083, Batch num: 70, Task 2 acc: 99.7755, Task avg acc:33.2613\n",
      "2024-05-25 22:59:40,635 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0083, Batch num: 80, Task 2 acc: 99.7755, Task avg acc:33.2613\n",
      "2024-05-25 22:59:40,829 - INFO - Epoch 3\n",
      "2024-05-25 22:59:46,584 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0166, Batch num: 0, Task 2 acc: 99.7575, Task avg acc:33.2581\n",
      "2024-05-25 22:59:52,126 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.0166, Batch num: 10, Task 2 acc: 99.7575, Task avg acc:33.2581\n",
      "2024-05-25 22:59:57,758 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.0249, Batch num: 20, Task 2 acc: 99.7665, Task avg acc:33.2638\n",
      "2024-05-25 23:00:02,464 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0166, Batch num: 30, Task 2 acc: 99.7935, Task avg acc:33.2700\n",
      "2024-05-25 23:00:07,843 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0166, Batch num: 40, Task 2 acc: 99.8024, Task avg acc:33.2730\n",
      "2024-05-25 23:00:14,513 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0249, Batch num: 50, Task 2 acc: 99.8114, Task avg acc:33.2788\n",
      "2024-05-25 23:00:20,269 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.0166, Batch num: 60, Task 2 acc: 99.8024, Task avg acc:33.2730\n",
      "2024-05-25 23:00:26,285 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0249, Batch num: 70, Task 2 acc: 99.8114, Task avg acc:33.2788\n",
      "2024-05-25 23:00:31,668 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0249, Batch num: 80, Task 2 acc: 99.7935, Task avg acc:33.2728\n",
      "2024-05-25 23:00:31,871 - INFO - ##########Task 3##########\n",
      "2024-05-25 23:00:31,872 - INFO - Epoch 0\n",
      "2024-05-25 23:00:39,079 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0249, Batch num: 0, Task 2 acc: 99.8114, Batch num: 0, Task 3 acc: 0.0000, Task avg acc:24.9591\n",
      "2024-05-25 23:00:46,272 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.7729, Batch num: 10, Task 2 acc: 99.6049, Batch num: 10, Task 3 acc: 7.9359, Task avg acc:27.0784\n",
      "2024-05-25 23:00:53,823 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.4987, Batch num: 20, Task 2 acc: 89.6462, Batch num: 20, Task 3 acc: 47.7138, Task avg acc:34.4647\n",
      "2024-05-25 23:01:01,055 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0083, Batch num: 30, Task 2 acc: 60.8836, Batch num: 30, Task 3 acc: 50.8964, Task avg acc:27.9471\n",
      "2024-05-25 23:01:07,125 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0000, Batch num: 40, Task 2 acc: 42.4659, Batch num: 40, Task 3 acc: 51.2418, Task avg acc:23.4269\n",
      "2024-05-25 23:01:14,904 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0000, Batch num: 50, Task 2 acc: 31.4206, Batch num: 50, Task 3 acc: 51.5132, Task avg acc:20.7334\n",
      "2024-05-25 23:01:21,694 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.0000, Batch num: 60, Task 2 acc: 12.2575, Batch num: 60, Task 3 acc: 98.6760, Task avg acc:27.7334\n",
      "2024-05-25 23:01:27,666 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0000, Batch num: 70, Task 2 acc: 0.9788, Batch num: 70, Task 3 acc: 99.6382, Task avg acc:25.1542\n",
      "2024-05-25 23:01:34,749 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0000, Batch num: 80, Task 2 acc: 0.3143, Batch num: 80, Task 3 acc: 99.7533, Task avg acc:25.0169\n",
      "2024-05-25 23:01:42,169 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 0.0000, Batch num: 90, Task 2 acc: 0.1616, Batch num: 90, Task 3 acc: 99.7451, Task avg acc:24.9767\n",
      "2024-05-25 23:01:42,265 - INFO - Epoch 1\n",
      "2024-05-25 23:01:49,183 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0000, Batch num: 0, Task 2 acc: 0.1257, Batch num: 0, Task 3 acc: 99.7451, Task avg acc:24.9677\n",
      "2024-05-25 23:01:56,289 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.0000, Batch num: 10, Task 2 acc: 0.0629, Batch num: 10, Task 3 acc: 99.7944, Task avg acc:24.9643\n",
      "2024-05-25 23:02:02,392 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.0000, Batch num: 20, Task 2 acc: 0.0629, Batch num: 20, Task 3 acc: 99.8355, Task avg acc:24.9746\n",
      "2024-05-25 23:02:09,188 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0000, Batch num: 30, Task 2 acc: 0.0718, Batch num: 30, Task 3 acc: 99.8355, Task avg acc:24.9768\n",
      "2024-05-25 23:02:14,956 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0000, Batch num: 40, Task 2 acc: 0.0359, Batch num: 40, Task 3 acc: 99.8438, Task avg acc:24.9699\n",
      "2024-05-25 23:02:21,103 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0000, Batch num: 50, Task 2 acc: 0.0449, Batch num: 50, Task 3 acc: 99.8766, Task avg acc:24.9804\n",
      "2024-05-25 23:02:26,922 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.0000, Batch num: 60, Task 2 acc: 0.0539, Batch num: 60, Task 3 acc: 99.8849, Task avg acc:24.9847\n",
      "2024-05-25 23:02:32,894 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0000, Batch num: 70, Task 2 acc: 0.0539, Batch num: 70, Task 3 acc: 99.9095, Task avg acc:24.9909\n",
      "2024-05-25 23:02:38,866 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0000, Batch num: 80, Task 2 acc: 0.0449, Batch num: 80, Task 3 acc: 99.9178, Task avg acc:24.9907\n",
      "2024-05-25 23:02:44,826 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 0.0000, Batch num: 90, Task 2 acc: 0.0539, Batch num: 90, Task 3 acc: 99.9178, Task avg acc:24.9929\n",
      "2024-05-25 23:02:44,920 - INFO - Epoch 2\n",
      "2024-05-25 23:02:51,156 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0000, Batch num: 0, Task 2 acc: 0.0359, Batch num: 0, Task 3 acc: 99.9260, Task avg acc:24.9905\n",
      "2024-05-25 23:02:57,084 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.0000, Batch num: 10, Task 2 acc: 0.0269, Batch num: 10, Task 3 acc: 99.9342, Task avg acc:24.9903\n",
      "2024-05-25 23:03:05,031 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.0000, Batch num: 20, Task 2 acc: 0.0269, Batch num: 20, Task 3 acc: 99.9342, Task avg acc:24.9903\n",
      "2024-05-25 23:03:11,634 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0000, Batch num: 30, Task 2 acc: 0.0539, Batch num: 30, Task 3 acc: 99.9260, Task avg acc:24.9950\n",
      "2024-05-25 23:03:20,141 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0000, Batch num: 40, Task 2 acc: 0.0539, Batch num: 40, Task 3 acc: 99.9260, Task avg acc:24.9950\n",
      "2024-05-25 23:03:27,994 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0000, Batch num: 50, Task 2 acc: 0.0449, Batch num: 50, Task 3 acc: 99.9342, Task avg acc:24.9948\n",
      "2024-05-25 23:03:35,611 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.0000, Batch num: 60, Task 2 acc: 0.0629, Batch num: 60, Task 3 acc: 99.9424, Task avg acc:25.0013\n",
      "2024-05-25 23:03:45,869 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0000, Batch num: 70, Task 2 acc: 0.0269, Batch num: 70, Task 3 acc: 99.9507, Task avg acc:24.9944\n",
      "2024-05-25 23:03:54,274 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0000, Batch num: 80, Task 2 acc: 0.0449, Batch num: 80, Task 3 acc: 99.9507, Task avg acc:24.9989\n",
      "2024-05-25 23:04:00,985 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 0.0000, Batch num: 90, Task 2 acc: 0.0449, Batch num: 90, Task 3 acc: 99.9424, Task avg acc:24.9968\n",
      "2024-05-25 23:04:01,087 - INFO - Epoch 3\n",
      "2024-05-25 23:04:07,315 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0000, Batch num: 0, Task 2 acc: 0.0449, Batch num: 0, Task 3 acc: 99.9424, Task avg acc:24.9968\n",
      "2024-05-25 23:04:13,949 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.0000, Batch num: 10, Task 2 acc: 0.0269, Batch num: 10, Task 3 acc: 99.9507, Task avg acc:24.9944\n",
      "2024-05-25 23:04:21,767 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.0000, Batch num: 20, Task 2 acc: 0.0269, Batch num: 20, Task 3 acc: 99.9507, Task avg acc:24.9944\n",
      "2024-05-25 23:04:29,624 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0000, Batch num: 30, Task 2 acc: 0.0269, Batch num: 30, Task 3 acc: 99.9507, Task avg acc:24.9944\n",
      "2024-05-25 23:04:38,171 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0000, Batch num: 40, Task 2 acc: 0.0180, Batch num: 40, Task 3 acc: 99.9507, Task avg acc:24.9922\n",
      "2024-05-25 23:04:45,822 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0000, Batch num: 50, Task 2 acc: 0.0269, Batch num: 50, Task 3 acc: 99.9507, Task avg acc:24.9944\n",
      "2024-05-25 23:04:55,735 - INFO - Batch num: 60, Task 0 acc: 0.0000, Batch num: 60, Task 1 acc: 0.0000, Batch num: 60, Task 2 acc: 0.0449, Batch num: 60, Task 3 acc: 99.9507, Task avg acc:24.9989\n",
      "2024-05-25 23:05:04,961 - INFO - Batch num: 70, Task 0 acc: 0.0000, Batch num: 70, Task 1 acc: 0.0000, Batch num: 70, Task 2 acc: 0.0359, Batch num: 70, Task 3 acc: 99.9507, Task avg acc:24.9966\n",
      "2024-05-25 23:05:12,191 - INFO - Batch num: 80, Task 0 acc: 0.0000, Batch num: 80, Task 1 acc: 0.0000, Batch num: 80, Task 2 acc: 0.0180, Batch num: 80, Task 3 acc: 99.9507, Task avg acc:24.9922\n",
      "2024-05-25 23:05:19,527 - INFO - Batch num: 90, Task 0 acc: 0.0000, Batch num: 90, Task 1 acc: 0.0000, Batch num: 90, Task 2 acc: 0.0180, Batch num: 90, Task 3 acc: 99.9507, Task avg acc:24.9922\n",
      "2024-05-25 23:05:19,687 - INFO - ##########Task 4##########\n",
      "2024-05-25 23:05:19,688 - INFO - Epoch 0\n",
      "2024-05-25 23:05:30,200 - INFO - Batch num: 0, Task 0 acc: 0.0000, Batch num: 0, Task 1 acc: 0.0000, Batch num: 0, Task 2 acc: 0.0449, Batch num: 0, Task 3 acc: 99.9260, Batch num: 0, Task 4 acc: 0.0000, Task avg acc:19.9942\n",
      "2024-05-25 23:05:39,686 - INFO - Batch num: 10, Task 0 acc: 0.0000, Batch num: 10, Task 1 acc: 0.0000, Batch num: 10, Task 2 acc: 2.1193, Batch num: 10, Task 3 acc: 99.0543, Batch num: 10, Task 4 acc: 0.2632, Task avg acc:20.2874\n",
      "2024-05-25 23:05:49,508 - INFO - Batch num: 20, Task 0 acc: 0.0000, Batch num: 20, Task 1 acc: 0.0083, Batch num: 20, Task 2 acc: 5.6932, Batch num: 20, Task 3 acc: 92.8618, Batch num: 20, Task 4 acc: 36.2772, Task avg acc:26.9681\n",
      "2024-05-25 23:05:57,650 - INFO - Batch num: 30, Task 0 acc: 0.0000, Batch num: 30, Task 1 acc: 0.0000, Batch num: 30, Task 2 acc: 8.9889, Batch num: 30, Task 3 acc: 50.0000, Batch num: 30, Task 4 acc: 49.5414, Task avg acc:21.7061\n",
      "2024-05-25 23:06:06,740 - INFO - Batch num: 40, Task 0 acc: 0.0000, Batch num: 40, Task 1 acc: 0.0000, Batch num: 40, Task 2 acc: 2.8466, Batch num: 40, Task 3 acc: 31.4474, Batch num: 40, Task 4 acc: 74.4905, Task avg acc:21.7569\n",
      "2024-05-25 23:06:17,546 - INFO - Batch num: 50, Task 0 acc: 0.0000, Batch num: 50, Task 1 acc: 0.0000, Batch num: 50, Task 2 acc: 0.0180, Batch num: 50, Task 3 acc: 0.9786, Batch num: 50, Task 4 acc: 94.8539, Task avg acc:19.1701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m train_settings\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_split_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [112]\u001b[0m, in \u001b[0;36mtrain_split_task\u001b[0;34m(Task_Name)\u001b[0m\n\u001b[1;32m     19\u001b[0m     all_accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[0;32m---> 21\u001b[0m         model, task_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         all_accuracies\u001b[38;5;241m.\u001b[39mappend(task_accuracies)\n\u001b[1;32m     24\u001b[0m plot_task_acc_history(all_accuracies, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTask_Name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, task_num, criterion, epoches, lr, l2, loss_type)\u001b[0m\n\u001b[1;32m     61\u001b[0m log_message \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(task_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43mcal_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfollower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_dataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     avg_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n\u001b[1;32m     65\u001b[0m     task_accuracies[task]\u001b[38;5;241m.\u001b[39mappend(acc \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36mcal_acc\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:269\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py:953\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    951\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    952\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key in train_settings.keys():\n",
    "    train_split_task(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
