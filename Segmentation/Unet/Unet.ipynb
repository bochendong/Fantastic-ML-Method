{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "\n",
        "U-Net is a fully convolutional network architecture designed for semantic segmentation tasks, particularly in biomedical image analysis.\n",
        "\n",
        "**Key Features:**\n",
        "1. **Encoder-Decoder Structure**: Contracting path (encoder) to capture context, expanding path (decoder) to enable precise localization\n",
        "2. **Skip Connections**: Concatenate feature maps from encoder to decoder to preserve fine-grained details\n",
        "3. **Symmetric Architecture**: The network has a U-shaped architecture with symmetric encoder and decoder paths\n",
        "\n",
        "**Architecture:**\n",
        "- **Encoder (Contracting Path)**: Repeated application of two 3x3 convolutions, each followed by ReLU and 2x2 max pooling\n",
        "- **Decoder (Expanding Path)**: Up-convolution (transposed convolution) followed by concatenation with corresponding feature map from encoder, then two 3x3 convolutions\n",
        "- **Final Layer**: 1x1 convolution to map feature maps to desired number of classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Double Convolution Block\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Down (Encoder) Block\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Up (Decoder) Block\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super(Up, self).__init__()\n",
        "        \n",
        "        # If bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "    \n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # Input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        \n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        \n",
        "        # Concatenate along channel dimension (skip connection)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete U-Net Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels=1, n_classes=1, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "        \n",
        "        # Encoder (Contracting Path)\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        \n",
        "        # Decoder (Expanding Path)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic dataset for demonstration\n",
        "class SyntheticSegmentationDataset(Dataset):\n",
        "    \"\"\"Synthetic dataset for segmentation demonstration\"\"\"\n",
        "    def __init__(self, num_samples=1000, img_size=128):\n",
        "        self.num_samples = num_samples\n",
        "        self.img_size = img_size\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Create a simple synthetic image with geometric shapes\n",
        "        img = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n",
        "        mask = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n",
        "        \n",
        "        # Add random circles\n",
        "        num_circles = np.random.randint(1, 4)\n",
        "        for _ in range(num_circles):\n",
        "            center_x = np.random.randint(20, self.img_size - 20)\n",
        "            center_y = np.random.randint(20, self.img_size - 20)\n",
        "            radius = np.random.randint(10, 30)\n",
        "            \n",
        "            y, x = np.ogrid[:self.img_size, :self.img_size]\n",
        "            dist_from_center = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
        "            circle = dist_from_center <= radius\n",
        "            \n",
        "            img[circle] = np.random.uniform(0.3, 1.0)\n",
        "            mask[circle] = 1.0\n",
        "        \n",
        "        # Add noise\n",
        "        noise = np.random.normal(0, 0.1, (self.img_size, self.img_size))\n",
        "        img = np.clip(img + noise, 0, 1)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        img = torch.from_numpy(img).unsqueeze(0)  # Add channel dimension\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
        "        \n",
        "        return img, mask\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SyntheticSegmentationDataset(num_samples=800, img_size=128)\n",
        "val_dataset = SyntheticSegmentationDataset(num_samples=200, img_size=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Training samples: {len(train_dataset)}')\n",
        "print(f'Validation samples: {len(val_dataset)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = UNet(n_channels=1, n_classes=1, bilinear=True).to(device)\n",
        "\n",
        "# Loss function (Dice Loss + BCE Loss for binary segmentation)\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "    \n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        # Flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        # Binary Cross Entropy\n",
        "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
        "        \n",
        "        # Dice Loss\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_loss = 1 - (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "        \n",
        "        # Combine\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "        return Dice_BCE\n",
        "\n",
        "criterion = DiceBCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "print(model)\n",
        "print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if (batch_idx + 1) % 20 == 0:\n",
        "            print(f'Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "    \n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            masks_bool = masks > 0.5\n",
        "            correct_pixels += (preds == masks_bool).sum().item()\n",
        "            total_pixels += masks.numel()\n",
        "    \n",
        "    accuracy = 100. * correct_pixels / total_pixels\n",
        "    return running_loss / len(val_loader), accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
        "    print('-' * 50)\n",
        "    \n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(val_accuracies, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "def visualize_predictions(model, data_loader, num_samples=4, device='cpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        images, masks = next(iter(data_loader))\n",
        "        images = images[:num_samples].to(device)\n",
        "        masks = masks[:num_samples].to(device)\n",
        "        \n",
        "        outputs = model(images)\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        \n",
        "        fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            # Original image\n",
        "            axes[i, 0].imshow(images[i].cpu().squeeze(), cmap='gray')\n",
        "            axes[i, 0].set_title('Input Image')\n",
        "            axes[i, 0].axis('off')\n",
        "            \n",
        "            # Ground truth mask\n",
        "            axes[i, 1].imshow(masks[i].cpu().squeeze(), cmap='gray')\n",
        "            axes[i, 1].set_title('Ground Truth')\n",
        "            axes[i, 1].axis('off')\n",
        "            \n",
        "            # Prediction\n",
        "            axes[i, 2].imshow(preds[i].cpu().squeeze(), cmap='gray')\n",
        "            axes[i, 2].set_title('Prediction')\n",
        "            axes[i, 2].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "visualize_predictions(model, val_loader, num_samples=4, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate IoU (Intersection over Union) metric\n",
        "def calculate_iou(pred, target, threshold=0.5):\n",
        "    \"\"\"Calculate IoU for binary segmentation\"\"\"\n",
        "    pred = (torch.sigmoid(pred) > threshold).float()\n",
        "    target = target.float()\n",
        "    \n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() - intersection\n",
        "    \n",
        "    if union == 0:\n",
        "        return 1.0  # Both are empty\n",
        "    \n",
        "    iou = intersection / union\n",
        "    return iou.item()\n",
        "\n",
        "# Evaluate IoU on validation set\n",
        "model.eval()\n",
        "ious = []\n",
        "with torch.no_grad():\n",
        "    for images, masks in val_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        outputs = model(images)\n",
        "        \n",
        "        for i in range(outputs.size(0)):\n",
        "            iou = calculate_iou(outputs[i], masks[i])\n",
        "            ious.append(iou)\n",
        "\n",
        "mean_iou = np.mean(ious)\n",
        "print(f'Mean IoU on validation set: {mean_iou:.4f}')\n",
        "print(f'IoU range: [{np.min(ious):.4f}, {np.max(ious):.4f}]')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
