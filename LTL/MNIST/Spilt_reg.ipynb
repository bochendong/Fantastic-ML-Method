{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "repeat = 10\n",
    "epoches = 1\n",
    "alpha = 0.01\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheClassLabel(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that has a quick access to all labels of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super(CacheClassLabel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.labels = torch.LongTensor(len(dataset)).fill_(-1)\n",
    "        label_cache_filename = path.join(dataset.root, str(type(dataset))+'_'+str(len(dataset))+'.pth')\n",
    "        if path.exists(label_cache_filename):\n",
    "            self.labels = torch.load(label_cache_filename)\n",
    "        else:\n",
    "            for i, data in enumerate(dataset):\n",
    "                self.labels[i] = data[1]\n",
    "            torch.save(self.labels, label_cache_filename)\n",
    "        self.number_classes = len(torch.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target\n",
    "    \n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name, first_class_ind=0):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "        self.first_class_ind = first_class_ind  # For remapping the class index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        target = target + self.first_class_ind\n",
    "        return img, target, self.name\n",
    "    \n",
    "class Subclass(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that return the task name and remove the offset of labels (Let the labels start from 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_list, remap=True):\n",
    "        '''\n",
    "        :param dataset: (CacheClassLabel)\n",
    "        :param class_list: (list) A list of integers\n",
    "        :param remap: (bool) Ex: remap class [2,4,6 ...] to [0,1,2 ...]\n",
    "        '''\n",
    "        super(Subclass,self).__init__()\n",
    "        assert isinstance(dataset, CacheClassLabel), 'dataset must be wrapped by CacheClassLabel'\n",
    "        self.dataset = dataset\n",
    "        self.class_list = class_list\n",
    "        self.remap = remap\n",
    "        self.indices = []\n",
    "        for c in class_list:\n",
    "            self.indices.extend((dataset.labels==c).nonzero().flatten().tolist())\n",
    "        if remap:\n",
    "            self.class_mapping = {c: i for i, c in enumerate(class_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[self.indices[index]]\n",
    "        if self.remap:\n",
    "            raw_target = target.item() if isinstance(target,torch.Tensor) else target\n",
    "            target = self.class_mapping[raw_target]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitGen(train_dataset, val_dataset, first_split_sz=2, other_split_sz=2, rand_split=False, remap_class=False):\n",
    "    assert train_dataset.number_classes==val_dataset.number_classes,'Train/Val has different number of classes'\n",
    "    num_classes =  train_dataset.number_classes\n",
    "\n",
    "    # Calculate the boundary index of classes for splits\n",
    "    # Ex: [0,2,4,6,8,10] or [0,50,60,70,80,90,100]\n",
    "    split_boundaries = [0, first_split_sz]\n",
    "    while split_boundaries[-1]<num_classes:\n",
    "        split_boundaries.append(split_boundaries[-1]+other_split_sz)\n",
    "    print('split_boundaries:',split_boundaries)\n",
    "    assert split_boundaries[-1]==num_classes,'Invalid split size'\n",
    "\n",
    "    # Assign classes to each splits\n",
    "    # Create the dict: {split_name1:[2,6,7], split_name2:[0,3,9], ...}\n",
    "    if not rand_split:\n",
    "        class_lists = {str(i):list(range(split_boundaries[i-1],split_boundaries[i])) for i in range(1,len(split_boundaries))}\n",
    "    else:\n",
    "        randseq = torch.randperm(num_classes)\n",
    "        class_lists = {str(i):randseq[list(range(split_boundaries[i-1],split_boundaries[i]))].tolist() for i in range(1,len(split_boundaries))}\n",
    "    print(class_lists)\n",
    "\n",
    "    # Generate the dicts of splits\n",
    "    # Ex: {split_name1:dataset_split1, split_name2:dataset_split2, ...}\n",
    "    train_dataset_splits = {}\n",
    "    val_dataset_splits = {}\n",
    "    task_output_space = {}\n",
    "    for name,class_list in class_lists.items():\n",
    "        train_dataset_splits[name] = AppendName(Subclass(train_dataset, class_list, remap_class), name)\n",
    "        val_dataset_splits[name] = AppendName(Subclass(val_dataset, class_list, remap_class), name)\n",
    "        task_output_space[name] = len(class_list)\n",
    "\n",
    "    return train_dataset_splits, val_dataset_splits, task_output_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST(dataroot, train_aug=False):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Pad(2, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    train_transform = val_transform\n",
    "    if train_aug:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_dataset = CacheClassLabel(train_dataset)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        dataroot,\n",
    "        train=False,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    val_dataset = CacheClassLabel(val_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = MNIST('data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_boundaries: [0, 2, 4, 6, 8, 10]\n",
      "{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_splits, val_dataset_splits, task_output_space = SplitGen(train_dataset, val_dataset,\n",
    "                                                                          first_split_sz=2,\n",
    "                                                                          other_split_sz=2,\n",
    "                                                                          rand_split=False,\n",
    "                                                                          remap_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, out_dim=10, in_channel=1, img_sz=32, hidden_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_channel*img_sz*img_sz\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.last = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.linear(x.view(-1,self.in_dim))\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP400():\n",
    "    return MLP(hidden_dim=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_size = target.size(0)\n",
    "        correct = (predicted == target).sum().item() * 100\n",
    "    return correct / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_acc(output, target, meter):\n",
    "    acc = accuracy(output, target)\n",
    "    meter.update(acc, len(target))\n",
    "    return meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_fn(criterion, preds, targets, valid_out_dim):\n",
    "    if valid_out_dim != 0:\n",
    "        pred = preds[:,:valid_out_dim]\n",
    "    loss = criterion(pred, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_task(model, train_loader, optimizer, criterion, \n",
    "                  valid_out_dim, best_model_wts, best_loss, task_num, task_names):\n",
    "    leader = MLP400().to(device)\n",
    "    if (best_model_wts):\n",
    "        leader.load_state_dict(best_model_wts)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        train_acc = AverageMeter()\n",
    "        batch_num = 0\n",
    "        for images, labels, _ in train_loader:\n",
    "            model.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            reg_loss = 0\n",
    "            for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
    "                reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
    "            \n",
    "            c_loss = criterion_fn(criterion, outputs, labels, valid_out_dim)\n",
    "            loss = c_loss + 5 * reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = accumulate_acc(outputs, labels, train_acc)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = AverageMeter()\n",
    "\n",
    "                for task in range(task_num + 1):\n",
    "                    val_name = task_names[task]\n",
    "                    val_data = val_dataset_splits[val_name]\n",
    "                    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    for i, (input, target, _) in enumerate(val_loader):\n",
    "                        input, target = input.to(device), target.to(device)\n",
    "                        output = model(input)\n",
    "                        loss = criterion(output, target).item()\n",
    "\n",
    "                        val_loss.update(loss, len(target))\n",
    "\n",
    "                    if val_loss.avg < best_loss:\n",
    "                        best_loss = val_loss.avg\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        leader.load_state_dict(best_model_wts) \n",
    "            print(f\"batch_num: {batch_num}, c_loss:{c_loss.item():.4f}, val_loss:{val_loss.avg: .4f}, loss:{loss:.4f}\")\n",
    "    return best_model_wts, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task_names):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "\n",
    "    model = MLP400().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "    best_model_wts = None\n",
    "    best_loss = float('inf')\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name], batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f'=====Task: {train_name}=====')\n",
    "        best_model_wts, best_loss = train_on_task(model, train_loader, optimizer, criterion, valid_out_dim, best_model_wts, best_loss, i, task_names)\n",
    "    \n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "            model.eval()\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, _) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = model(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "        print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    return avg_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task order: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "task_names = sorted(list(task_output_space.keys()), key=int)\n",
    "print('Task order:',task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Task: 1=====\n",
      "batch_num: 0, c_loss:0.7457, val_loss: 2.2492, loss:2.2799\n",
      "batch_num: 0, c_loss:0.3090, val_loss: 2.1597, loss:2.1512\n",
      "batch_num: 0, c_loss:0.1448, val_loss: 2.0643, loss:2.0230\n",
      "batch_num: 0, c_loss:0.0816, val_loss: 1.9663, loss:1.8981\n",
      "batch_num: 0, c_loss:0.0644, val_loss: 1.8712, loss:1.7748\n",
      "batch_num: 0, c_loss:0.0495, val_loss: 1.7770, loss:1.6600\n",
      "batch_num: 0, c_loss:0.0477, val_loss: 1.6845, loss:1.5532\n",
      "batch_num: 0, c_loss:0.0347, val_loss: 1.5941, loss:1.4521\n",
      "batch_num: 0, c_loss:0.0181, val_loss: 1.5052, loss:1.3543\n",
      "batch_num: 0, c_loss:0.0210, val_loss: 1.4181, loss:1.2602\n",
      "batch_num: 0, c_loss:0.0187, val_loss: 1.3311, loss:1.1771\n",
      "batch_num: 0, c_loss:0.0165, val_loss: 1.2499, loss:1.0941\n",
      "batch_num: 0, c_loss:0.0312, val_loss: 1.1748, loss:1.0136\n",
      "batch_num: 0, c_loss:0.0129, val_loss: 1.1004, loss:0.9402\n",
      "batch_num: 0, c_loss:0.0204, val_loss: 1.0268, loss:0.8822\n",
      "batch_num: 0, c_loss:0.0100, val_loss: 0.9599, loss:0.8242\n",
      "batch_num: 0, c_loss:0.0203, val_loss: 0.8954, loss:0.7884\n",
      "batch_num: 0, c_loss:0.0067, val_loss: 0.8399, loss:0.7394\n",
      "batch_num: 0, c_loss:0.0075, val_loss: 0.7868, loss:0.6954\n",
      "batch_num: 0, c_loss:0.0086, val_loss: 0.7364, loss:0.6532\n",
      "batch_num: 0, c_loss:0.0106, val_loss: 0.6966, loss:0.6118\n",
      "batch_num: 0, c_loss:0.0082, val_loss: 0.6573, loss:0.5826\n",
      "batch_num: 0, c_loss:0.0109, val_loss: 0.6173, loss:0.5754\n",
      "batch_num: 0, c_loss:0.0119, val_loss: 0.5851, loss:0.5475\n",
      "batch_num: 0, c_loss:0.0082, val_loss: 0.5552, loss:0.5306\n",
      "batch_num: 0, c_loss:0.0067, val_loss: 0.5303, loss:0.5153\n",
      "batch_num: 0, c_loss:0.0081, val_loss: 0.5055, loss:0.5063\n",
      "batch_num: 0, c_loss:0.0264, val_loss: 0.4862, loss:0.4740\n",
      "batch_num: 0, c_loss:0.0074, val_loss: 0.4691, loss:0.4614\n",
      "batch_num: 0, c_loss:0.0111, val_loss: 0.4516, loss:0.4622\n",
      "batch_num: 0, c_loss:0.0043, val_loss: 0.4366, loss:0.4457\n",
      "batch_num: 0, c_loss:0.0046, val_loss: 0.4246, loss:0.4453\n",
      "batch_num: 0, c_loss:0.0069, val_loss: 0.4130, loss:0.4484\n",
      "batch_num: 0, c_loss:0.0041, val_loss: 0.4030, loss:0.4425\n",
      "batch_num: 0, c_loss:0.0063, val_loss: 0.3932, loss:0.4450\n",
      "batch_num: 0, c_loss:0.0285, val_loss: 0.3826, loss:0.4412\n",
      "batch_num: 0, c_loss:0.0040, val_loss: 0.3734, loss:0.4328\n",
      "batch_num: 0, c_loss:0.0043, val_loss: 0.3669, loss:0.4356\n",
      "batch_num: 0, c_loss:0.0044, val_loss: 0.3591, loss:0.4353\n",
      "batch_num: 0, c_loss:0.0037, val_loss: 0.3521, loss:0.4364\n",
      "batch_num: 0, c_loss:0.0027, val_loss: 0.3469, loss:0.4301\n",
      "batch_num: 0, c_loss:0.0097, val_loss: 0.3427, loss:0.4308\n",
      "batch_num: 0, c_loss:0.0060, val_loss: 0.3389, loss:0.4347\n",
      "batch_num: 0, c_loss:0.0090, val_loss: 0.3354, loss:0.4325\n",
      "batch_num: 0, c_loss:0.0032, val_loss: 0.3299, loss:0.4184\n",
      "batch_num: 0, c_loss:0.0032, val_loss: 0.3281, loss:0.4114\n",
      "batch_num: 0, c_loss:0.0092, val_loss: 0.3260, loss:0.4210\n",
      "batch_num: 0, c_loss:0.0038, val_loss: 0.3247, loss:0.4265\n",
      "batch_num: 0, c_loss:0.0036, val_loss: 0.3238, loss:0.4325\n",
      "batch_num: 0, c_loss:0.0066, val_loss: 0.3242, loss:0.4426\n",
      "batch_num: 0, c_loss:0.0038, val_loss: 0.3358, loss:0.4342\n",
      "batch_num: 0, c_loss:0.0049, val_loss: 0.3353, loss:0.4329\n",
      "batch_num: 0, c_loss:0.0030, val_loss: 0.3250, loss:0.4356\n",
      "batch_num: 0, c_loss:0.0035, val_loss: 0.3198, loss:0.4350\n",
      "batch_num: 0, c_loss:0.0026, val_loss: 0.3171, loss:0.4451\n",
      "batch_num: 0, c_loss:0.0038, val_loss: 0.3159, loss:0.4574\n",
      "batch_num: 0, c_loss:0.0235, val_loss: 0.3165, loss:0.4636\n",
      "batch_num: 0, c_loss:0.0021, val_loss: 0.3145, loss:0.4513\n",
      "batch_num: 0, c_loss:0.0069, val_loss: 0.3137, loss:0.4406\n",
      "batch_num: 0, c_loss:0.0021, val_loss: 0.3135, loss:0.4348\n",
      "batch_num: 0, c_loss:0.0031, val_loss: 0.3129, loss:0.4279\n",
      "batch_num: 0, c_loss:0.0031, val_loss: 0.3115, loss:0.4185\n",
      "batch_num: 0, c_loss:0.0023, val_loss: 0.3082, loss:0.4040\n",
      "batch_num: 0, c_loss:0.0022, val_loss: 0.3071, loss:0.3985\n",
      "batch_num: 0, c_loss:0.0025, val_loss: 0.3051, loss:0.3922\n",
      "batch_num: 0, c_loss:0.0015, val_loss: 0.3054, loss:0.3861\n",
      "batch_num: 0, c_loss:0.0020, val_loss: 0.3050, loss:0.3904\n",
      "batch_num: 0, c_loss:0.0061, val_loss: 0.3036, loss:0.3851\n",
      "batch_num: 0, c_loss:0.0021, val_loss: 0.3041, loss:0.3895\n",
      "batch_num: 0, c_loss:0.0028, val_loss: 0.3044, loss:0.3940\n",
      "batch_num: 0, c_loss:0.0043, val_loss: 0.3069, loss:0.3968\n",
      "batch_num: 0, c_loss:0.0035, val_loss: 0.3067, loss:0.3931\n",
      "batch_num: 0, c_loss:0.0032, val_loss: 0.3065, loss:0.3952\n",
      "batch_num: 0, c_loss:0.0169, val_loss: 0.3053, loss:0.4016\n",
      "batch_num: 0, c_loss:0.0028, val_loss: 0.3042, loss:0.4019\n",
      "batch_num: 0, c_loss:0.0031, val_loss: 0.3031, loss:0.3879\n",
      "batch_num: 0, c_loss:0.0055, val_loss: 0.3054, loss:0.3919\n",
      "batch_num: 0, c_loss:0.0090, val_loss: 0.3030, loss:0.3869\n",
      "batch_num: 0, c_loss:0.0024, val_loss: 0.3016, loss:0.3767\n",
      "batch_num: 0, c_loss:0.0214, val_loss: 0.3004, loss:0.3665\n",
      "batch_num: 0, c_loss:0.0022, val_loss: 0.2990, loss:0.3641\n",
      "batch_num: 0, c_loss:0.0022, val_loss: 0.2990, loss:0.3653\n",
      "batch_num: 0, c_loss:0.0234, val_loss: 0.2993, loss:0.3659\n",
      "batch_num: 0, c_loss:0.0121, val_loss: 0.2982, loss:0.3560\n",
      "batch_num: 0, c_loss:0.0051, val_loss: 0.2987, loss:0.3605\n",
      "batch_num: 0, c_loss:0.0062, val_loss: 0.2980, loss:0.3588\n",
      "batch_num: 0, c_loss:0.0027, val_loss: 0.2968, loss:0.3605\n",
      "batch_num: 0, c_loss:0.0027, val_loss: 0.2976, loss:0.3677\n",
      "batch_num: 0, c_loss:0.0076, val_loss: 0.2980, loss:0.3679\n",
      "batch_num: 0, c_loss:0.0028, val_loss: 0.2983, loss:0.3624\n",
      "batch_num: 0, c_loss:0.0032, val_loss: 0.2989, loss:0.3607\n",
      "batch_num: 0, c_loss:0.0032, val_loss: 0.2991, loss:0.3590\n",
      "batch_num: 0, c_loss:0.0022, val_loss: 0.2973, loss:0.3626\n",
      "batch_num: 0, c_loss:0.0138, val_loss: 0.2982, loss:0.3713\n",
      "batch_num: 0, c_loss:0.0021, val_loss: 0.2988, loss:0.3737\n",
      "batch_num: 0, c_loss:0.0201, val_loss: 0.2982, loss:0.3762\n",
      "batch_num: 0, c_loss:0.0020, val_loss: 0.2988, loss:0.3709\n",
      "batch_num: 0, c_loss:0.0023, val_loss: 0.2997, loss:0.3796\n",
      "batch_num: 0, c_loss:0.0025, val_loss: 0.3002, loss:0.3885\n",
      "val_name1:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.90543735224587)]))])\n",
      "=====Task: 2=====\n",
      "batch_num: 0, c_loss:2.3625, val_loss: 1.3976, loss:2.5651\n",
      "batch_num: 0, c_loss:2.2340, val_loss: 1.3663, loss:2.4962\n",
      "batch_num: 0, c_loss:2.1358, val_loss: 1.3489, loss:2.4575\n",
      "batch_num: 0, c_loss:2.1150, val_loss: 1.3058, loss:2.3503\n",
      "batch_num: 0, c_loss:1.8632, val_loss: 1.2670, loss:2.2701\n",
      "batch_num: 0, c_loss:1.7962, val_loss: 1.2433, loss:2.2396\n",
      "batch_num: 0, c_loss:1.7453, val_loss: 1.2294, loss:2.2223\n",
      "batch_num: 0, c_loss:1.7036, val_loss: 1.2210, loss:2.1890\n",
      "batch_num: 0, c_loss:1.6426, val_loss: 1.2122, loss:2.1485\n",
      "batch_num: 0, c_loss:1.5392, val_loss: 1.1936, loss:2.1038\n",
      "batch_num: 0, c_loss:1.4308, val_loss: 1.1692, loss:2.0573\n",
      "batch_num: 0, c_loss:1.4075, val_loss: 1.1504, loss:2.0193\n",
      "batch_num: 0, c_loss:1.2625, val_loss: 1.1384, loss:1.9769\n",
      "batch_num: 0, c_loss:1.2053, val_loss: 1.1286, loss:1.9487\n",
      "batch_num: 0, c_loss:1.2445, val_loss: 1.1266, loss:1.9374\n",
      "batch_num: 0, c_loss:1.1972, val_loss: 1.1242, loss:1.9390\n",
      "batch_num: 0, c_loss:1.2029, val_loss: 1.1194, loss:1.9437\n",
      "batch_num: 0, c_loss:1.2143, val_loss: 1.1156, loss:1.9244\n",
      "batch_num: 0, c_loss:1.1734, val_loss: 1.1071, loss:1.9026\n",
      "batch_num: 0, c_loss:1.1393, val_loss: 1.0974, loss:1.8778\n",
      "batch_num: 0, c_loss:1.0397, val_loss: 1.0873, loss:1.8646\n",
      "batch_num: 0, c_loss:1.0552, val_loss: 1.0844, loss:1.8673\n",
      "batch_num: 0, c_loss:1.0752, val_loss: 1.0838, loss:1.8781\n",
      "batch_num: 0, c_loss:1.0822, val_loss: 1.0779, loss:1.8827\n",
      "batch_num: 0, c_loss:1.1478, val_loss: 1.0714, loss:1.8780\n",
      "batch_num: 0, c_loss:1.1241, val_loss: 1.0643, loss:1.8655\n",
      "batch_num: 0, c_loss:1.1124, val_loss: 1.0590, loss:1.8692\n",
      "batch_num: 0, c_loss:1.1790, val_loss: 1.0587, loss:1.8760\n",
      "batch_num: 0, c_loss:1.2160, val_loss: 1.0596, loss:1.8835\n",
      "batch_num: 0, c_loss:1.1589, val_loss: 1.0562, loss:1.8812\n",
      "batch_num: 0, c_loss:1.1959, val_loss: 1.0541, loss:1.8940\n",
      "batch_num: 0, c_loss:1.2730, val_loss: 1.0554, loss:1.9070\n",
      "batch_num: 0, c_loss:1.2556, val_loss: 1.0391, loss:1.8724\n",
      "batch_num: 0, c_loss:1.1677, val_loss: 1.0253, loss:1.8464\n",
      "batch_num: 0, c_loss:1.1243, val_loss: 1.0261, loss:1.8470\n",
      "batch_num: 0, c_loss:1.1189, val_loss: 1.0320, loss:1.8546\n",
      "batch_num: 0, c_loss:1.1483, val_loss: 1.0375, loss:1.8596\n",
      "batch_num: 0, c_loss:1.1944, val_loss: 1.0369, loss:1.8595\n",
      "batch_num: 0, c_loss:1.1392, val_loss: 1.0331, loss:1.8523\n",
      "batch_num: 0, c_loss:1.1498, val_loss: 1.0305, loss:1.8440\n",
      "batch_num: 0, c_loss:1.1741, val_loss: 1.0314, loss:1.8455\n",
      "batch_num: 0, c_loss:1.1051, val_loss: 1.0319, loss:1.8580\n",
      "batch_num: 0, c_loss:1.1271, val_loss: 1.0346, loss:1.8704\n",
      "batch_num: 0, c_loss:1.2371, val_loss: 1.0360, loss:1.8787\n",
      "batch_num: 0, c_loss:1.2222, val_loss: 1.0339, loss:1.8785\n",
      "batch_num: 0, c_loss:1.1556, val_loss: 1.0296, loss:1.8697\n",
      "batch_num: 0, c_loss:1.1924, val_loss: 1.0117, loss:1.8135\n",
      "batch_num: 0, c_loss:1.1610, val_loss: 0.9992, loss:1.7804\n",
      "batch_num: 0, c_loss:1.0342, val_loss: 0.9995, loss:1.7881\n",
      "batch_num: 0, c_loss:1.0679, val_loss: 1.0090, loss:1.8103\n",
      "batch_num: 0, c_loss:1.1117, val_loss: 1.0153, loss:1.8269\n",
      "batch_num: 0, c_loss:1.0984, val_loss: 1.0153, loss:1.8335\n",
      "batch_num: 0, c_loss:1.0925, val_loss: 1.0110, loss:1.8110\n",
      "batch_num: 0, c_loss:1.0768, val_loss: 1.0049, loss:1.8004\n",
      "batch_num: 0, c_loss:1.1199, val_loss: 1.0053, loss:1.7921\n",
      "batch_num: 0, c_loss:1.0881, val_loss: 1.0085, loss:1.8023\n",
      "batch_num: 0, c_loss:1.0644, val_loss: 1.0092, loss:1.8137\n",
      "batch_num: 0, c_loss:1.0785, val_loss: 1.0074, loss:1.8126\n",
      "batch_num: 0, c_loss:1.1281, val_loss: 1.0078, loss:1.8287\n",
      "batch_num: 0, c_loss:1.1192, val_loss: 1.0059, loss:1.8283\n",
      "batch_num: 0, c_loss:1.1493, val_loss: 0.9909, loss:1.7864\n",
      "batch_num: 0, c_loss:1.1362, val_loss: 0.9816, loss:1.7525\n",
      "batch_num: 0, c_loss:1.0581, val_loss: 0.9807, loss:1.7491\n",
      "batch_num: 0, c_loss:1.0444, val_loss: 0.9898, loss:1.7728\n",
      "batch_num: 0, c_loss:1.1058, val_loss: 0.9999, loss:1.7961\n",
      "batch_num: 0, c_loss:1.0533, val_loss: 1.0037, loss:1.7988\n",
      "batch_num: 0, c_loss:1.0958, val_loss: 1.0034, loss:1.7874\n",
      "batch_num: 0, c_loss:1.1454, val_loss: 0.9992, loss:1.7753\n",
      "batch_num: 0, c_loss:1.0192, val_loss: 0.9924, loss:1.7642\n",
      "batch_num: 0, c_loss:1.0272, val_loss: 0.9891, loss:1.7622\n",
      "batch_num: 0, c_loss:1.1221, val_loss: 0.9954, loss:1.7802\n",
      "batch_num: 0, c_loss:1.0508, val_loss: 1.0002, loss:1.8069\n",
      "batch_num: 0, c_loss:1.1238, val_loss: 1.0040, loss:1.8121\n",
      "batch_num: 0, c_loss:1.1087, val_loss: 1.0010, loss:1.7964\n",
      "batch_num: 0, c_loss:1.1528, val_loss: 0.9989, loss:1.7915\n",
      "batch_num: 0, c_loss:1.0810, val_loss: 0.9980, loss:1.7911\n",
      "batch_num: 0, c_loss:1.1065, val_loss: 0.9996, loss:1.7905\n",
      "batch_num: 0, c_loss:1.1078, val_loss: 0.9989, loss:1.7920\n",
      "batch_num: 0, c_loss:1.0909, val_loss: 0.9997, loss:1.7856\n",
      "batch_num: 0, c_loss:1.0791, val_loss: 1.0009, loss:1.7838\n",
      "batch_num: 0, c_loss:1.0604, val_loss: 0.9995, loss:1.7868\n",
      "batch_num: 0, c_loss:1.0931, val_loss: 1.0002, loss:1.7861\n",
      "batch_num: 0, c_loss:1.0900, val_loss: 1.0005, loss:1.7880\n",
      "batch_num: 0, c_loss:1.1338, val_loss: 1.0026, loss:1.7884\n",
      "batch_num: 0, c_loss:1.1264, val_loss: 1.0044, loss:1.7943\n",
      "batch_num: 0, c_loss:1.1457, val_loss: 1.0064, loss:1.8078\n",
      "batch_num: 0, c_loss:1.1329, val_loss: 1.0053, loss:1.8066\n",
      "batch_num: 0, c_loss:1.0419, val_loss: 1.0028, loss:1.7952\n",
      "batch_num: 0, c_loss:1.0963, val_loss: 1.0023, loss:1.7905\n",
      "batch_num: 0, c_loss:1.0762, val_loss: 1.0012, loss:1.7993\n",
      "batch_num: 0, c_loss:1.1194, val_loss: 1.0039, loss:1.7944\n",
      "batch_num: 0, c_loss:1.1272, val_loss: 1.0039, loss:1.7936\n",
      "batch_num: 0, c_loss:1.1034, val_loss: 1.0029, loss:1.7894\n",
      "batch_num: 0, c_loss:1.0150, val_loss: 0.9980, loss:1.7810\n",
      "batch_num: 0, c_loss:1.0087, val_loss: 0.9959, loss:1.7733\n",
      "val_name1:\n",
      "val_name2:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.90543735224587), ('2', 99.62174940898345)])), ('2', OrderedDict([('2', 44.613124387855045)]))])\n",
      "=====Task: 3=====\n",
      "batch_num: 0, c_loss:2.3150, val_loss: 1.4751, loss:2.5300\n",
      "batch_num: 0, c_loss:2.2117, val_loss: 1.4640, loss:2.4764\n",
      "batch_num: 0, c_loss:2.1345, val_loss: 1.4603, loss:2.4553\n",
      "batch_num: 0, c_loss:2.1419, val_loss: 1.4573, loss:2.4354\n",
      "batch_num: 0, c_loss:2.0823, val_loss: 1.4555, loss:2.4021\n",
      "batch_num: 0, c_loss:2.0092, val_loss: 1.4541, loss:2.3646\n",
      "batch_num: 0, c_loss:1.9769, val_loss: 1.4498, loss:2.3215\n",
      "batch_num: 0, c_loss:1.9608, val_loss: 1.4410, loss:2.2912\n",
      "batch_num: 0, c_loss:1.8905, val_loss: 1.4376, loss:2.2684\n",
      "batch_num: 0, c_loss:1.8462, val_loss: 1.4379, loss:2.2410\n",
      "batch_num: 0, c_loss:1.8281, val_loss: 1.4390, loss:2.2133\n",
      "batch_num: 0, c_loss:1.7639, val_loss: 1.4382, loss:2.1819\n",
      "batch_num: 0, c_loss:1.7345, val_loss: 1.4387, loss:2.1513\n",
      "batch_num: 0, c_loss:1.6736, val_loss: 1.4397, loss:2.1433\n",
      "batch_num: 0, c_loss:1.6663, val_loss: 1.4376, loss:2.1293\n",
      "batch_num: 0, c_loss:1.6414, val_loss: 1.4380, loss:2.1083\n",
      "batch_num: 0, c_loss:1.6469, val_loss: 1.4382, loss:2.0916\n",
      "batch_num: 0, c_loss:1.5880, val_loss: 1.4388, loss:2.0951\n",
      "batch_num: 0, c_loss:1.5865, val_loss: 1.4412, loss:2.0962\n",
      "batch_num: 0, c_loss:1.5850, val_loss: 1.4394, loss:2.0846\n",
      "batch_num: 0, c_loss:1.5934, val_loss: 1.4423, loss:2.0844\n",
      "batch_num: 0, c_loss:1.5524, val_loss: 1.4430, loss:2.0986\n",
      "batch_num: 0, c_loss:1.5779, val_loss: 1.4425, loss:2.1127\n",
      "batch_num: 0, c_loss:1.5657, val_loss: 1.4428, loss:2.1023\n",
      "batch_num: 0, c_loss:1.5232, val_loss: 1.4422, loss:2.0945\n",
      "batch_num: 0, c_loss:1.5361, val_loss: 1.4426, loss:2.0902\n",
      "batch_num: 0, c_loss:1.5652, val_loss: 1.4436, loss:2.1040\n",
      "batch_num: 0, c_loss:1.5476, val_loss: 1.4428, loss:2.1039\n",
      "batch_num: 0, c_loss:1.5978, val_loss: 1.4432, loss:2.1005\n",
      "batch_num: 0, c_loss:1.5802, val_loss: 1.4427, loss:2.0958\n",
      "batch_num: 0, c_loss:1.5621, val_loss: 1.4392, loss:2.0838\n",
      "batch_num: 0, c_loss:1.6024, val_loss: 1.4387, loss:2.0783\n",
      "batch_num: 0, c_loss:1.5765, val_loss: 1.4401, loss:2.0723\n",
      "batch_num: 0, c_loss:1.6159, val_loss: 1.4418, loss:2.0754\n",
      "batch_num: 0, c_loss:1.6102, val_loss: 1.4420, loss:2.0896\n",
      "batch_num: 0, c_loss:1.6259, val_loss: 1.4398, loss:2.0889\n",
      "batch_num: 0, c_loss:1.6312, val_loss: 1.4366, loss:2.0760\n",
      "batch_num: 0, c_loss:1.6227, val_loss: 1.4372, loss:2.0788\n",
      "batch_num: 0, c_loss:1.6289, val_loss: 1.4379, loss:2.0824\n",
      "batch_num: 0, c_loss:1.6129, val_loss: 1.4363, loss:2.0999\n",
      "batch_num: 0, c_loss:1.6028, val_loss: 1.4354, loss:2.1046\n",
      "batch_num: 0, c_loss:1.6734, val_loss: 1.4361, loss:2.1104\n",
      "batch_num: 0, c_loss:1.6578, val_loss: 1.4350, loss:2.1131\n",
      "batch_num: 0, c_loss:1.6909, val_loss: 1.4327, loss:2.1336\n",
      "batch_num: 0, c_loss:1.6550, val_loss: 1.4313, loss:2.1627\n",
      "batch_num: 0, c_loss:1.6859, val_loss: 1.4311, loss:2.1815\n",
      "batch_num: 0, c_loss:1.6947, val_loss: 1.4322, loss:2.1922\n",
      "batch_num: 0, c_loss:1.6875, val_loss: 1.4331, loss:2.1967\n",
      "batch_num: 0, c_loss:1.6924, val_loss: 1.4310, loss:2.1926\n",
      "batch_num: 0, c_loss:1.7609, val_loss: 1.4305, loss:2.1930\n",
      "batch_num: 0, c_loss:1.7297, val_loss: 1.4323, loss:2.1931\n",
      "batch_num: 0, c_loss:1.7142, val_loss: 1.4342, loss:2.2055\n",
      "batch_num: 0, c_loss:1.6921, val_loss: 1.4344, loss:2.2101\n",
      "batch_num: 0, c_loss:1.6807, val_loss: 1.4344, loss:2.2067\n",
      "batch_num: 0, c_loss:1.6782, val_loss: 1.4323, loss:2.1958\n",
      "batch_num: 0, c_loss:1.7393, val_loss: 1.4338, loss:2.1840\n",
      "batch_num: 0, c_loss:1.7406, val_loss: 1.4351, loss:2.1775\n",
      "batch_num: 0, c_loss:1.7043, val_loss: 1.4334, loss:2.1883\n",
      "batch_num: 0, c_loss:1.6976, val_loss: 1.4339, loss:2.1837\n",
      "batch_num: 0, c_loss:1.7127, val_loss: 1.4354, loss:2.1733\n",
      "batch_num: 0, c_loss:1.6959, val_loss: 1.4342, loss:2.1584\n",
      "batch_num: 0, c_loss:1.7565, val_loss: 1.4338, loss:2.1517\n",
      "batch_num: 0, c_loss:1.6922, val_loss: 1.4345, loss:2.1318\n",
      "batch_num: 0, c_loss:1.7542, val_loss: 1.4349, loss:2.1141\n",
      "batch_num: 0, c_loss:1.7232, val_loss: 1.4346, loss:2.1037\n",
      "batch_num: 0, c_loss:1.7026, val_loss: 1.4353, loss:2.1016\n",
      "batch_num: 0, c_loss:1.6929, val_loss: 1.4371, loss:2.1113\n",
      "batch_num: 0, c_loss:1.6661, val_loss: 1.4348, loss:2.1151\n",
      "batch_num: 0, c_loss:1.6965, val_loss: 1.4343, loss:2.1157\n",
      "batch_num: 0, c_loss:1.7112, val_loss: 1.4329, loss:2.1168\n",
      "batch_num: 0, c_loss:1.7162, val_loss: 1.4333, loss:2.1232\n",
      "batch_num: 0, c_loss:1.6865, val_loss: 1.4330, loss:2.1455\n",
      "batch_num: 0, c_loss:1.6771, val_loss: 1.4331, loss:2.1675\n",
      "batch_num: 0, c_loss:1.6887, val_loss: 1.4308, loss:2.1721\n",
      "batch_num: 0, c_loss:1.6757, val_loss: 1.4306, loss:2.1789\n",
      "batch_num: 0, c_loss:1.7260, val_loss: 1.4296, loss:2.1861\n",
      "batch_num: 0, c_loss:1.6920, val_loss: 1.4297, loss:2.1959\n",
      "batch_num: 0, c_loss:1.7226, val_loss: 1.4327, loss:2.2011\n",
      "batch_num: 0, c_loss:1.7263, val_loss: 1.4335, loss:2.2022\n",
      "batch_num: 0, c_loss:1.6724, val_loss: 1.4321, loss:2.2059\n",
      "batch_num: 0, c_loss:1.6995, val_loss: 1.4322, loss:2.1758\n",
      "batch_num: 0, c_loss:1.7080, val_loss: 1.4338, loss:2.1507\n",
      "batch_num: 0, c_loss:1.7158, val_loss: 1.4337, loss:2.1458\n",
      "batch_num: 0, c_loss:1.7032, val_loss: 1.4362, loss:2.1438\n",
      "batch_num: 0, c_loss:1.6403, val_loss: 1.4364, loss:2.1453\n",
      "batch_num: 0, c_loss:1.6918, val_loss: 1.4370, loss:2.1385\n",
      "batch_num: 0, c_loss:1.6595, val_loss: 1.4373, loss:2.1289\n",
      "batch_num: 0, c_loss:1.6878, val_loss: 1.4383, loss:2.1241\n",
      "val_name1:\n",
      "val_name2:\n",
      "val_name3:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.90543735224587), ('2', 99.62174940898345), ('3', 99.47990543735224)])), ('2', OrderedDict([('2', 44.613124387855045), ('3', 36.14103819784525)])), ('3', OrderedDict([('3', 20.064034151547492)]))])\n",
      "=====Task: 4=====\n",
      "batch_num: 0, c_loss:2.3852, val_loss: 1.7199, loss:2.5319\n",
      "batch_num: 0, c_loss:2.2234, val_loss: 1.7083, loss:2.3895\n",
      "batch_num: 0, c_loss:2.1245, val_loss: 1.7049, loss:2.2710\n",
      "batch_num: 0, c_loss:1.9910, val_loss: 1.7058, loss:2.1464\n",
      "batch_num: 0, c_loss:1.9042, val_loss: 1.7187, loss:2.0533\n",
      "batch_num: 0, c_loss:1.8211, val_loss: 1.7426, loss:2.0260\n",
      "batch_num: 0, c_loss:1.7852, val_loss: 1.7679, loss:1.9800\n",
      "batch_num: 0, c_loss:1.7348, val_loss: 1.7919, loss:1.9069\n",
      "batch_num: 0, c_loss:1.6359, val_loss: 1.8062, loss:1.8021\n",
      "batch_num: 0, c_loss:1.5707, val_loss: 1.8168, loss:1.7182\n",
      "batch_num: 0, c_loss:1.4445, val_loss: 1.8269, loss:1.6561\n",
      "batch_num: 0, c_loss:1.4510, val_loss: 1.8402, loss:1.6285\n",
      "batch_num: 0, c_loss:1.4271, val_loss: 1.8537, loss:1.6196\n",
      "batch_num: 0, c_loss:1.4284, val_loss: 1.8656, loss:1.6125\n",
      "batch_num: 0, c_loss:1.4024, val_loss: 1.8731, loss:1.5897\n",
      "batch_num: 0, c_loss:1.4081, val_loss: 1.8752, loss:1.5603\n",
      "batch_num: 0, c_loss:1.3551, val_loss: 1.8726, loss:1.5219\n",
      "batch_num: 0, c_loss:1.3997, val_loss: 1.8665, loss:1.5035\n",
      "batch_num: 0, c_loss:1.3490, val_loss: 1.8620, loss:1.4931\n",
      "batch_num: 0, c_loss:1.3231, val_loss: 1.8616, loss:1.5030\n",
      "batch_num: 0, c_loss:1.3253, val_loss: 1.8570, loss:1.5297\n",
      "batch_num: 0, c_loss:1.3764, val_loss: 1.8517, loss:1.5553\n",
      "batch_num: 0, c_loss:1.3478, val_loss: 1.8491, loss:1.5870\n",
      "batch_num: 0, c_loss:1.4098, val_loss: 1.8425, loss:1.6229\n",
      "batch_num: 0, c_loss:1.4287, val_loss: 1.8318, loss:1.6380\n",
      "batch_num: 0, c_loss:1.3792, val_loss: 1.8240, loss:1.6530\n",
      "batch_num: 0, c_loss:1.4123, val_loss: 1.8200, loss:1.6765\n",
      "batch_num: 0, c_loss:1.4433, val_loss: 1.8161, loss:1.7212\n",
      "batch_num: 0, c_loss:1.4910, val_loss: 1.8133, loss:1.7676\n",
      "batch_num: 0, c_loss:1.5223, val_loss: 1.8084, loss:1.8070\n",
      "batch_num: 0, c_loss:1.4996, val_loss: 1.8029, loss:1.8167\n",
      "batch_num: 0, c_loss:1.5687, val_loss: 1.7964, loss:1.8201\n",
      "batch_num: 0, c_loss:1.5714, val_loss: 1.7923, loss:1.8422\n",
      "batch_num: 0, c_loss:1.5815, val_loss: 1.7860, loss:1.8724\n",
      "batch_num: 0, c_loss:1.6312, val_loss: 1.7807, loss:1.8951\n",
      "batch_num: 0, c_loss:1.6241, val_loss: 1.7816, loss:1.9209\n",
      "batch_num: 0, c_loss:1.6385, val_loss: 1.7832, loss:1.9487\n",
      "batch_num: 0, c_loss:1.6630, val_loss: 1.7839, loss:1.9839\n",
      "batch_num: 0, c_loss:1.7229, val_loss: 1.7821, loss:2.0111\n",
      "batch_num: 0, c_loss:1.7019, val_loss: 1.7787, loss:2.0144\n",
      "batch_num: 0, c_loss:1.7200, val_loss: 1.7776, loss:2.0166\n",
      "batch_num: 0, c_loss:1.7457, val_loss: 1.7779, loss:2.0414\n",
      "batch_num: 0, c_loss:1.7596, val_loss: 1.7775, loss:2.0637\n",
      "batch_num: 0, c_loss:1.7611, val_loss: 1.7801, loss:2.0741\n",
      "batch_num: 0, c_loss:1.7622, val_loss: 1.7828, loss:2.0914\n",
      "batch_num: 0, c_loss:1.7186, val_loss: 1.7823, loss:2.1025\n",
      "batch_num: 0, c_loss:1.7994, val_loss: 1.7780, loss:2.0971\n",
      "batch_num: 0, c_loss:1.7560, val_loss: 1.7748, loss:2.0866\n",
      "batch_num: 0, c_loss:1.7895, val_loss: 1.7714, loss:2.0862\n",
      "batch_num: 0, c_loss:1.7699, val_loss: 1.7727, loss:2.0938\n",
      "batch_num: 0, c_loss:1.7737, val_loss: 1.7750, loss:2.1053\n",
      "batch_num: 0, c_loss:1.8316, val_loss: 1.7748, loss:2.1053\n",
      "batch_num: 0, c_loss:1.7769, val_loss: 1.7758, loss:2.1065\n",
      "batch_num: 0, c_loss:1.7780, val_loss: 1.7784, loss:2.1243\n",
      "batch_num: 0, c_loss:1.8087, val_loss: 1.7785, loss:2.1317\n",
      "batch_num: 0, c_loss:1.8047, val_loss: 1.7791, loss:2.1311\n",
      "batch_num: 0, c_loss:1.8182, val_loss: 1.7823, loss:2.1342\n",
      "batch_num: 0, c_loss:1.7690, val_loss: 1.7826, loss:2.1238\n",
      "batch_num: 0, c_loss:1.8049, val_loss: 1.7805, loss:2.1119\n",
      "batch_num: 0, c_loss:1.7857, val_loss: 1.7791, loss:2.0947\n",
      "batch_num: 0, c_loss:1.7444, val_loss: 1.7798, loss:2.0883\n",
      "batch_num: 0, c_loss:1.8021, val_loss: 1.7780, loss:2.0595\n",
      "batch_num: 0, c_loss:1.7909, val_loss: 1.7737, loss:2.0232\n",
      "batch_num: 0, c_loss:1.8110, val_loss: 1.7698, loss:1.9978\n",
      "batch_num: 0, c_loss:1.7497, val_loss: 1.7690, loss:1.9738\n",
      "batch_num: 0, c_loss:1.7075, val_loss: 1.7687, loss:1.9689\n",
      "batch_num: 0, c_loss:1.7624, val_loss: 1.7708, loss:1.9571\n",
      "batch_num: 0, c_loss:1.7256, val_loss: 1.7736, loss:1.9515\n",
      "batch_num: 0, c_loss:1.7385, val_loss: 1.7774, loss:1.9719\n",
      "batch_num: 0, c_loss:1.7419, val_loss: 1.7790, loss:1.9929\n",
      "batch_num: 0, c_loss:1.7331, val_loss: 1.7780, loss:1.9787\n",
      "batch_num: 0, c_loss:1.6698, val_loss: 1.7832, loss:1.9826\n",
      "batch_num: 0, c_loss:1.6898, val_loss: 1.7883, loss:2.0076\n",
      "batch_num: 0, c_loss:1.7170, val_loss: 1.7937, loss:2.0283\n",
      "batch_num: 0, c_loss:1.6675, val_loss: 1.7971, loss:2.0428\n",
      "batch_num: 0, c_loss:1.7524, val_loss: 1.7981, loss:2.0553\n",
      "batch_num: 0, c_loss:1.7385, val_loss: 1.7953, loss:2.0359\n",
      "batch_num: 0, c_loss:1.7062, val_loss: 1.7922, loss:2.0089\n",
      "batch_num: 0, c_loss:1.7114, val_loss: 1.7911, loss:2.0027\n",
      "batch_num: 0, c_loss:1.7059, val_loss: 1.7891, loss:1.9867\n",
      "batch_num: 0, c_loss:1.7030, val_loss: 1.7854, loss:1.9649\n",
      "batch_num: 0, c_loss:1.7099, val_loss: 1.7838, loss:1.9608\n",
      "batch_num: 0, c_loss:1.7003, val_loss: 1.7825, loss:1.9482\n",
      "batch_num: 0, c_loss:1.6992, val_loss: 1.7808, loss:1.9366\n",
      "batch_num: 0, c_loss:1.7077, val_loss: 1.7788, loss:1.9270\n",
      "batch_num: 0, c_loss:1.6876, val_loss: 1.7787, loss:1.9289\n",
      "batch_num: 0, c_loss:1.7052, val_loss: 1.7781, loss:1.9305\n",
      "batch_num: 0, c_loss:1.7091, val_loss: 1.7796, loss:1.9456\n",
      "batch_num: 0, c_loss:1.6744, val_loss: 1.7830, loss:1.9718\n",
      "batch_num: 0, c_loss:1.6884, val_loss: 1.7840, loss:1.9723\n",
      "batch_num: 0, c_loss:1.7188, val_loss: 1.7854, loss:1.9753\n",
      "batch_num: 0, c_loss:1.7087, val_loss: 1.7857, loss:1.9908\n",
      "batch_num: 0, c_loss:1.6906, val_loss: 1.7865, loss:1.9973\n",
      "batch_num: 0, c_loss:1.6569, val_loss: 1.7868, loss:1.9931\n",
      "batch_num: 0, c_loss:1.7160, val_loss: 1.7869, loss:1.9935\n",
      "batch_num: 0, c_loss:1.6971, val_loss: 1.7893, loss:1.9765\n",
      "val_name1:\n",
      "val_name2:\n",
      "val_name3:\n",
      "val_name4:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.90543735224587), ('2', 99.62174940898345), ('3', 99.47990543735224), ('4', 97.73049645390071)])), ('2', OrderedDict([('2', 44.613124387855045), ('3', 36.14103819784525), ('4', 21.00881488736533)])), ('3', OrderedDict([('3', 20.064034151547492), ('4', 0.0)])), ('4', OrderedDict([('4', 30.866062437059416)]))])\n",
      "=====Task: 5=====\n",
      "batch_num: 0, c_loss:2.5965, val_loss: 1.9219, loss:2.2139\n",
      "batch_num: 0, c_loss:2.4877, val_loss: 1.9100, loss:2.1071\n",
      "batch_num: 0, c_loss:2.3594, val_loss: 1.8994, loss:2.0127\n",
      "batch_num: 0, c_loss:2.2103, val_loss: 1.8937, loss:1.9331\n",
      "batch_num: 0, c_loss:2.1022, val_loss: 1.8982, loss:1.8550\n",
      "batch_num: 0, c_loss:1.9737, val_loss: 1.9148, loss:1.7834\n",
      "batch_num: 0, c_loss:1.8902, val_loss: 1.9376, loss:1.7024\n",
      "batch_num: 0, c_loss:1.8299, val_loss: 1.9623, loss:1.6331\n",
      "batch_num: 0, c_loss:1.7038, val_loss: 1.9866, loss:1.5611\n",
      "batch_num: 0, c_loss:1.6186, val_loss: 2.0056, loss:1.4955\n",
      "batch_num: 0, c_loss:1.6033, val_loss: 2.0234, loss:1.4525\n",
      "batch_num: 0, c_loss:1.5710, val_loss: 2.0366, loss:1.4297\n",
      "batch_num: 0, c_loss:1.5003, val_loss: 2.0465, loss:1.4287\n",
      "batch_num: 0, c_loss:1.4924, val_loss: 2.0527, loss:1.4292\n",
      "batch_num: 0, c_loss:1.5106, val_loss: 2.0566, loss:1.4117\n",
      "batch_num: 0, c_loss:1.4417, val_loss: 2.0570, loss:1.3983\n",
      "batch_num: 0, c_loss:1.5044, val_loss: 2.0519, loss:1.3972\n",
      "batch_num: 0, c_loss:1.4208, val_loss: 2.0463, loss:1.3909\n",
      "batch_num: 0, c_loss:1.4561, val_loss: 2.0394, loss:1.4012\n",
      "batch_num: 0, c_loss:1.5001, val_loss: 2.0328, loss:1.4371\n",
      "batch_num: 0, c_loss:1.4464, val_loss: 2.0257, loss:1.4560\n",
      "batch_num: 0, c_loss:1.5029, val_loss: 2.0168, loss:1.4649\n",
      "batch_num: 0, c_loss:1.4894, val_loss: 2.0057, loss:1.4594\n",
      "batch_num: 0, c_loss:1.4748, val_loss: 1.9955, loss:1.4715\n",
      "batch_num: 0, c_loss:1.5350, val_loss: 1.9863, loss:1.5020\n",
      "batch_num: 0, c_loss:1.5684, val_loss: 1.9776, loss:1.5207\n",
      "batch_num: 0, c_loss:1.5927, val_loss: 1.9693, loss:1.5416\n",
      "batch_num: 0, c_loss:1.6671, val_loss: 1.9607, loss:1.5605\n",
      "batch_num: 0, c_loss:1.6466, val_loss: 1.9536, loss:1.5806\n",
      "batch_num: 0, c_loss:1.7245, val_loss: 1.9437, loss:1.6215\n",
      "batch_num: 0, c_loss:1.7210, val_loss: 1.9363, loss:1.6679\n",
      "batch_num: 0, c_loss:1.7500, val_loss: 1.9314, loss:1.7094\n",
      "batch_num: 0, c_loss:1.7841, val_loss: 1.9270, loss:1.7317\n",
      "batch_num: 0, c_loss:1.7738, val_loss: 1.9246, loss:1.7683\n",
      "batch_num: 0, c_loss:1.7995, val_loss: 1.9235, loss:1.8073\n",
      "batch_num: 0, c_loss:1.8120, val_loss: 1.9208, loss:1.8288\n",
      "batch_num: 0, c_loss:1.8348, val_loss: 1.9212, loss:1.8363\n",
      "batch_num: 0, c_loss:1.8819, val_loss: 1.9194, loss:1.8384\n",
      "batch_num: 0, c_loss:1.8585, val_loss: 1.9182, loss:1.8319\n",
      "batch_num: 0, c_loss:1.9300, val_loss: 1.9189, loss:1.8420\n",
      "batch_num: 0, c_loss:1.9032, val_loss: 1.9205, loss:1.8319\n",
      "batch_num: 0, c_loss:1.9316, val_loss: 1.9206, loss:1.8172\n",
      "batch_num: 0, c_loss:1.9348, val_loss: 1.9216, loss:1.8203\n",
      "batch_num: 0, c_loss:1.9333, val_loss: 1.9223, loss:1.8153\n",
      "batch_num: 0, c_loss:1.9094, val_loss: 1.9253, loss:1.7987\n",
      "batch_num: 0, c_loss:1.9708, val_loss: 1.9249, loss:1.7888\n",
      "batch_num: 0, c_loss:1.9479, val_loss: 1.9244, loss:1.7887\n",
      "batch_num: 0, c_loss:1.9750, val_loss: 1.9235, loss:1.7892\n",
      "batch_num: 0, c_loss:1.9491, val_loss: 1.9233, loss:1.8031\n",
      "batch_num: 0, c_loss:1.9625, val_loss: 1.9247, loss:1.8190\n",
      "batch_num: 0, c_loss:1.9625, val_loss: 1.9228, loss:1.8250\n",
      "batch_num: 0, c_loss:1.9606, val_loss: 1.9228, loss:1.8337\n",
      "batch_num: 0, c_loss:1.9543, val_loss: 1.9224, loss:1.8297\n",
      "batch_num: 0, c_loss:1.9597, val_loss: 1.9230, loss:1.8333\n",
      "batch_num: 0, c_loss:1.9664, val_loss: 1.9229, loss:1.8402\n",
      "batch_num: 0, c_loss:1.9381, val_loss: 1.9224, loss:1.8462\n",
      "batch_num: 0, c_loss:1.9632, val_loss: 1.9221, loss:1.8453\n",
      "batch_num: 0, c_loss:1.9216, val_loss: 1.9221, loss:1.8443\n",
      "batch_num: 0, c_loss:1.9487, val_loss: 1.9230, loss:1.8597\n",
      "batch_num: 0, c_loss:1.9469, val_loss: 1.9246, loss:1.8600\n",
      "batch_num: 0, c_loss:1.9730, val_loss: 1.9263, loss:1.8653\n",
      "batch_num: 0, c_loss:1.9919, val_loss: 1.9258, loss:1.8676\n",
      "batch_num: 0, c_loss:1.9983, val_loss: 1.9241, loss:1.8710\n",
      "batch_num: 0, c_loss:1.9884, val_loss: 1.9231, loss:1.8981\n",
      "batch_num: 0, c_loss:1.9511, val_loss: 1.9217, loss:1.9091\n",
      "batch_num: 0, c_loss:1.9306, val_loss: 1.9198, loss:1.8996\n",
      "batch_num: 0, c_loss:1.9118, val_loss: 1.9195, loss:1.8718\n",
      "batch_num: 0, c_loss:1.9381, val_loss: 1.9208, loss:1.8465\n",
      "batch_num: 0, c_loss:1.8920, val_loss: 1.9210, loss:1.8295\n",
      "batch_num: 0, c_loss:1.9614, val_loss: 1.9245, loss:1.8169\n",
      "batch_num: 0, c_loss:1.9392, val_loss: 1.9260, loss:1.8020\n",
      "batch_num: 0, c_loss:1.8979, val_loss: 1.9249, loss:1.7824\n",
      "batch_num: 0, c_loss:1.8861, val_loss: 1.9228, loss:1.7708\n",
      "batch_num: 0, c_loss:1.8697, val_loss: 1.9238, loss:1.7666\n",
      "batch_num: 0, c_loss:1.8512, val_loss: 1.9259, loss:1.7651\n",
      "batch_num: 0, c_loss:1.9192, val_loss: 1.9252, loss:1.7576\n",
      "batch_num: 0, c_loss:1.8883, val_loss: 1.9236, loss:1.7694\n",
      "batch_num: 0, c_loss:1.8600, val_loss: 1.9234, loss:1.7747\n",
      "batch_num: 0, c_loss:1.8545, val_loss: 1.9244, loss:1.7728\n",
      "batch_num: 0, c_loss:1.8706, val_loss: 1.9236, loss:1.7749\n",
      "batch_num: 0, c_loss:1.8935, val_loss: 1.9234, loss:1.7954\n",
      "batch_num: 0, c_loss:1.8816, val_loss: 1.9211, loss:1.8149\n",
      "batch_num: 0, c_loss:1.8655, val_loss: 1.9213, loss:1.8255\n",
      "batch_num: 0, c_loss:1.8607, val_loss: 1.9217, loss:1.8092\n",
      "batch_num: 0, c_loss:1.8876, val_loss: 1.9213, loss:1.8075\n",
      "batch_num: 0, c_loss:1.8780, val_loss: 1.9210, loss:1.8048\n",
      "batch_num: 0, c_loss:1.9416, val_loss: 1.9242, loss:1.7953\n",
      "batch_num: 0, c_loss:1.9072, val_loss: 1.9248, loss:1.7984\n",
      "batch_num: 0, c_loss:1.9021, val_loss: 1.9248, loss:1.7915\n",
      "batch_num: 0, c_loss:1.8619, val_loss: 1.9255, loss:1.7759\n",
      "batch_num: 0, c_loss:1.9125, val_loss: 1.9247, loss:1.7672\n",
      "batch_num: 0, c_loss:1.8748, val_loss: 1.9237, loss:1.7719\n",
      "batch_num: 0, c_loss:1.9312, val_loss: 1.9215, loss:1.7842\n",
      "val_name1:\n",
      "val_name2:\n",
      "val_name3:\n",
      "val_name4:\n",
      "val_name5:\n",
      "OrderedDict([('1', OrderedDict([('1', 99.90543735224587), ('2', 99.62174940898345), ('3', 99.47990543735224), ('4', 97.73049645390071), ('5', 99.57446808510639)])), ('2', OrderedDict([('2', 44.613124387855045), ('3', 36.14103819784525), ('4', 21.00881488736533), ('5', 28.59941234084231)])), ('3', OrderedDict([('3', 20.064034151547492), ('4', 0.0), ('5', 0.0)])), ('4', OrderedDict([('4', 30.866062437059416), ('5', 0.10070493454179255)])), ('5', OrderedDict([('5', 29.601613716591025)]))])\n",
      "Task 1 average acc: 99.90543735224587\n",
      "Task 2 average acc: 72.11743689841924\n",
      "Task 3 average acc: 51.89499259558166\n",
      "Task 4 average acc: 37.401343444581364\n",
      "Task 5 average acc: 31.575239815416307\n"
     ]
    }
   ],
   "source": [
    "avg_acc_history = train(task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"avg_final_acc = np.zeros(repeat)\\n\\nfor r in range (repeat):\\n    acc_table = OrderedDict()\\n    valid_out_dim = 0\\n    for i in range(len(task_names)):\\n        valid_out_dim += 2\\n        train_name = task_names[i]\\n        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name],\\n                                                            batch_size=batch_size, shuffle=True)\\n        val_loader = torch.utils.data.DataLoader(val_dataset_splits[train_name],\\n                                                        batch_size=batch_size, shuffle=False)\\n        # Train\\n        for epoch in range(4):\\n            train_acc = AverageMeter()\\n            for (input, target, task) in train_loader:\\n                agent.train()\\n                input, target = input.to(device), target.to(device)\\n\\n                output = agent(input)\\n                loss = criterion_fn(criterion, output, target, valid_out_dim)\\n\\n                optimizer.zero_grad()\\n                loss.backward()\\n                optimizer.step()\\n\\n                train_acc = accumulate_acc(output, target, train_acc)\\n\\n        # Eval\\n        acc_table[train_name] = OrderedDict()\\n\\n        for j in range(i+1):\\n            val_name = task_names[j]\\n            val_data = val_dataset_splits[val_name]\\n            val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False,)\\n\\n            agent.eval()\\n            val_acc = AverageMeter()\\n            with torch.no_grad():\\n                for i, (input, target, task) in enumerate(val_loader):\\n                    input, target = input.to(device), target.to(device)\\n                    output = agent(input)\\n                    val_acc = accumulate_acc(output, target, val_acc)\\n\\n            acc_table[val_name][train_name] = val_acc.avg\\n\\n    print(acc_table)\\n\\n    avg_acc_history = [0] * len(task_names)\\n    for i in range(len(task_names)):\\n        train_name = task_names[i]\\n        cls_acc_sum = 0\\n        for j in range(i + 1):\\n            val_name = task_names[j]\\n            cls_acc_sum += acc_table[val_name][train_name]\\n\\n        avg_acc_history[i] = cls_acc_sum / (i + 1)\\n        print('Task', train_name, 'average acc:', avg_acc_history[i])\\n    \\n    avg_final_acc[r] = avg_acc_history[-1]\\n    print('===Summary of experiment repeats:',r+1,'/',repeat,'===')\\n    print(avg_final_acc)\\n    print('mean:', avg_final_acc.mean(), 'std:', avg_final_acc.std())\""
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''avg_final_acc = np.zeros(repeat)\n",
    "\n",
    "for r in range (repeat):\n",
    "    acc_table = OrderedDict()\n",
    "    valid_out_dim = 0\n",
    "    for i in range(len(task_names)):\n",
    "        valid_out_dim += 2\n",
    "        train_name = task_names[i]\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name],\n",
    "                                                            batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset_splits[train_name],\n",
    "                                                        batch_size=batch_size, shuffle=False)\n",
    "        # Train\n",
    "        for epoch in range(4):\n",
    "            train_acc = AverageMeter()\n",
    "            for (input, target, task) in train_loader:\n",
    "                agent.train()\n",
    "                input, target = input.to(device), target.to(device)\n",
    "\n",
    "                output = agent(input)\n",
    "                loss = criterion_fn(criterion, output, target, valid_out_dim)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_acc = accumulate_acc(output, target, train_acc)\n",
    "\n",
    "        # Eval\n",
    "        acc_table[train_name] = OrderedDict()\n",
    "\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            val_data = val_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False,)\n",
    "\n",
    "            agent.eval()\n",
    "            val_acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for i, (input, target, task) in enumerate(val_loader):\n",
    "                    input, target = input.to(device), target.to(device)\n",
    "                    output = agent(input)\n",
    "                    val_acc = accumulate_acc(output, target, val_acc)\n",
    "\n",
    "            acc_table[val_name][train_name] = val_acc.avg\n",
    "\n",
    "    print(acc_table)\n",
    "\n",
    "    avg_acc_history = [0] * len(task_names)\n",
    "    for i in range(len(task_names)):\n",
    "        train_name = task_names[i]\n",
    "        cls_acc_sum = 0\n",
    "        for j in range(i + 1):\n",
    "            val_name = task_names[j]\n",
    "            cls_acc_sum += acc_table[val_name][train_name]\n",
    "\n",
    "        avg_acc_history[i] = cls_acc_sum / (i + 1)\n",
    "        print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "    \n",
    "    avg_final_acc[r] = avg_acc_history[-1]\n",
    "    print('===Summary of experiment repeats:',r+1,'/',repeat,'===')\n",
    "    print(avg_final_acc)\n",
    "    print('mean:', avg_final_acc.mean(), 'std:', avg_final_acc.std())'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
