{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OT8NfOcbkbzR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.backends.cudnn as cudnn\n",
        "import copy\n",
        "from torch.autograd import Function\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-DsLK_NnkbzR"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 256\n",
        "alpha = 0.005\n",
        "DANN_EPOCHES = 10\n",
        "DANN_TRAINING_BATCH = 40\n",
        "dann_path = 'dann.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzl9AkwhkbzR",
        "outputId": "ceec5923-7085-42c8-a3af-c3034d429f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Split training set for training and validation\n",
        "train_size = int(0.8 * len(train_set))\n",
        "val_size = len(train_set) - train_size\n",
        "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "# DataLoader for validation set\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last = True)\n",
        "train_loader =  DataLoader(train_set, batch_size=batch_size, shuffle=False, drop_last = True)\n",
        "test_loader =  DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BPHNhlPkbzR"
      },
      "source": [
        "# DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0CWhtn0jkbzS"
      },
      "outputs": [],
      "source": [
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "    \n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1), \n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Classifier_Small(nn.Module):\n",
        "    def __init__(self, out_dim = 10):\n",
        "        super(Classifier_Small, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "             nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4 * 4 * 256, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, 4 * 4 * 256)  # Adjusted for the added depth\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dXz9YIu4kbzS"
      },
      "outputs": [],
      "source": [
        "class DANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DANN, self).__init__()\n",
        "        self.autoencoder = ConvAutoencoder()\n",
        "        self.classifier = Classifier_Small(out_dim = 10)\n",
        "        self.domain_classifier = Classifier_Small(out_dim = 2)\n",
        "\n",
        "    def forward(self, input_data, alpha):\n",
        "        feature = self.autoencoder(input_data)\n",
        "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
        "        class_output = self.classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "        \n",
        "        return class_output, domain_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p38-3bPBkbzS"
      },
      "outputs": [],
      "source": [
        "def train_DANN(train_loader, model, criterion, optimizer, epoches):\n",
        "    model.train()\n",
        "    src_domain_label = torch.ones(batch_size).long().to(device)\n",
        "    tgt_domain_label = torch.zeros(batch_size).long().to(device)\n",
        "\n",
        "    for e in range(epoches):\n",
        "        data_target_iter = iter(train_loader)\n",
        "        correct_source_domain, correct_tgt_domain = 0, 0\n",
        "        total = 0\n",
        "        for i in range(DANN_TRAINING_BATCH):\n",
        "            # Src\n",
        "            optimizer.zero_grad()\n",
        "            source, source_label = next(data_target_iter)\n",
        "            total += source.size(0)\n",
        "\n",
        "            source, source_label = source.to(device), source_label.to(device)\n",
        "\n",
        "            class_output, domain_output = model(source, alpha)\n",
        "\n",
        "            loss_s_label = criterion(class_output, source_label)\n",
        "            loss_s_domain = criterion(domain_output, src_domain_label)\n",
        "\n",
        "            _, predicted = torch.max(class_output.data, 1)\n",
        "            correct_source_domain += predicted.eq(source_label.data).cpu().sum().item()\n",
        "\n",
        "            # Tgt\n",
        "            target, target_label  = next(data_target_iter)\n",
        "            target, target_label = target.to(device), target_label.to(device)\n",
        "\n",
        "            class_output, domain_output = model(target, alpha)\n",
        "\n",
        "            loss_t_label = criterion(class_output, target_label)\n",
        "            loss_t_domain = criterion(domain_output, tgt_domain_label)\n",
        "\n",
        "            _, predicted = torch.max(class_output.data, 1)\n",
        "            correct_tgt_domain += predicted.eq(target_label.data).cpu().sum().item()\n",
        "\n",
        "            loss = loss_s_label + loss_s_domain + loss_t_domain + loss_t_label\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if ((e + 1) % 5 == 0):\n",
        "            print(f\"Epoch: [{e}/{epoches}]: source correct: {correct_source_domain/total}, target correct: {correct_tgt_domain/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ffFuN6kbzS",
        "outputId": "88381c15-b074-4449-d32d-8b714644a87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from file.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(dann_path):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    dann = DANN().to(device)\n",
        "    optimizer = optim.Adam(dann.parameters(), lr=0.001)\n",
        "\n",
        "    train_DANN(train_loader, dann, criterion, optimizer, DANN_EPOCHES)\n",
        "    with torch.no_grad():\n",
        "        torch.save(dann.state_dict(), dann_path)\n",
        "else:\n",
        "    dann = DANN().to(device)\n",
        "    dann.load_state_dict(torch.load(dann_path))\n",
        "    print(\"Loaded model from file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyMOw2MnkbzS"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, out_dim = 10):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, out_dim),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, 512)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, label in testloader:\n",
        "        images, label = images.to(device), label.to(device)\n",
        "        features = dann.autoencoder(images)\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, epoches, criterion, optimizer):\n",
        "    best_model_wts = None\n",
        "    leader = VGG().to(device)\n",
        "    best_loss = float('inf')\n",
        "    batch_num = 0\n",
        "    warm_up_batch = 3\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        print(f\"Batch: {batch_num}:\")\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        if (best_model_wts):\n",
        "            model.load_state_dict(best_model_wts)\n",
        "        for epoch in range(epoches):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                features = dann.autoencoder(inputs)\n",
        "            outputs = model(features)\n",
        "\n",
        "            classification_loss = criterion(outputs, labels)\n",
        "            if (warm_up_batch < batch_num):\n",
        "                reg_loss = 0\n",
        "                for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
        "                    reg_loss += torch.norm(follower_para - lead_para, p = 2)\n",
        "                \n",
        "                loss = classification_loss + reg_loss\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for val_inputs, val_labels in val_loader:\n",
        "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "                    features = dann.autoencoder(val_inputs)\n",
        "                    outputs = model(features)\n",
        "                    batch_loss = criterion(outputs, val_labels)\n",
        "                    val_loss += batch_loss.item()\n",
        "\n",
        "                if val_loss + (0.5 / (batch_num + 1.)) < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    leader.load_state_dict(best_model_wts)\n",
        "\n",
        "            print(f\"epoch: {epoch}, classification_loss: {classification_loss.item()}, Val Loss: {val_loss}, loss : {loss.item()}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_acc = test(model, test_loader)\n",
        "                \n",
        "        print(f\"Batch: {batch_num}, Test Acc: {test_acc}\")\n",
        "\n",
        "        batch_num += 1\n",
        "\n",
        "    return best_model_wts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################################################\n",
            "Batch: 0:\n",
            "epoch: 0, classification_loss: 2.3022351264953613, Val Loss: 89.79447531700134, loss : 2.3022351264953613\n",
            "epoch: 1, classification_loss: 2.263889789581299, Val Loss: 89.78946781158447, loss : 2.263889789581299\n",
            "epoch: 2, classification_loss: 2.2149581909179688, Val Loss: 89.78631091117859, loss : 2.2149581909179688\n",
            "epoch: 3, classification_loss: 2.167422294616699, Val Loss: 89.78484916687012, loss : 2.167422294616699\n",
            "epoch: 4, classification_loss: 2.1177639961242676, Val Loss: 89.77968502044678, loss : 2.1177639961242676\n",
            "epoch: 5, classification_loss: 2.0634968280792236, Val Loss: 89.773681640625, loss : 2.0634968280792236\n",
            "epoch: 6, classification_loss: 2.0152313709259033, Val Loss: 89.76514482498169, loss : 2.0152313709259033\n",
            "epoch: 7, classification_loss: 1.956165075302124, Val Loss: 89.76054334640503, loss : 1.956165075302124\n",
            "epoch: 8, classification_loss: 1.900434970855713, Val Loss: 89.76124906539917, loss : 1.900434970855713\n",
            "epoch: 9, classification_loss: 1.8524805307388306, Val Loss: 89.76278018951416, loss : 1.8524805307388306\n",
            "epoch: 10, classification_loss: 1.7999069690704346, Val Loss: 89.75933575630188, loss : 1.7999069690704346\n",
            "epoch: 11, classification_loss: 1.743151068687439, Val Loss: 89.7568142414093, loss : 1.743151068687439\n",
            "epoch: 12, classification_loss: 1.6981741189956665, Val Loss: 89.74715042114258, loss : 1.6981741189956665\n",
            "epoch: 13, classification_loss: 1.654845952987671, Val Loss: 89.73942375183105, loss : 1.654845952987671\n",
            "epoch: 14, classification_loss: 1.6164077520370483, Val Loss: 89.70170545578003, loss : 1.6164077520370483\n",
            "epoch: 15, classification_loss: 1.5854634046554565, Val Loss: 89.65524435043335, loss : 1.5854634046554565\n",
            "epoch: 16, classification_loss: 1.5613818168640137, Val Loss: 89.62695407867432, loss : 1.5613818168640137\n",
            "epoch: 17, classification_loss: 1.5515565872192383, Val Loss: 89.60179281234741, loss : 1.5515565872192383\n",
            "epoch: 18, classification_loss: 1.533514142036438, Val Loss: 89.46904492378235, loss : 1.533514142036438\n",
            "epoch: 19, classification_loss: 1.524276614189148, Val Loss: 89.26471376419067, loss : 1.524276614189148\n",
            "Batch: 0, Test Acc: 0.10136217948717949\n",
            "Batch: 1:\n",
            "epoch: 0, classification_loss: 1.9464069604873657, Val Loss: 89.33610224723816, loss : 1.9464069604873657\n",
            "epoch: 1, classification_loss: 1.8261523246765137, Val Loss: 89.21236729621887, loss : 1.8261523246765137\n",
            "epoch: 2, classification_loss: 1.7198721170425415, Val Loss: 88.80813813209534, loss : 1.7198721170425415\n",
            "epoch: 3, classification_loss: 1.6678097248077393, Val Loss: 88.36018419265747, loss : 1.6678097248077393\n",
            "epoch: 4, classification_loss: 1.6271023750305176, Val Loss: 87.89563703536987, loss : 1.6271023750305176\n",
            "epoch: 5, classification_loss: 1.5969212055206299, Val Loss: 87.18883895874023, loss : 1.5969212055206299\n",
            "epoch: 6, classification_loss: 1.571220874786377, Val Loss: 86.32099223136902, loss : 1.571220874786377\n",
            "epoch: 7, classification_loss: 1.5553622245788574, Val Loss: 85.66857862472534, loss : 1.5553622245788574\n",
            "epoch: 8, classification_loss: 1.5405985116958618, Val Loss: 85.2018530368805, loss : 1.5405985116958618\n",
            "epoch: 9, classification_loss: 1.5315418243408203, Val Loss: 84.9831030368805, loss : 1.5315418243408203\n",
            "epoch: 10, classification_loss: 1.517512559890747, Val Loss: 84.68838453292847, loss : 1.517512559890747\n",
            "epoch: 11, classification_loss: 1.5074259042739868, Val Loss: 84.24454808235168, loss : 1.5074259042739868\n",
            "epoch: 12, classification_loss: 1.5015003681182861, Val Loss: 83.51260566711426, loss : 1.5015003681182861\n",
            "epoch: 13, classification_loss: 1.4962986707687378, Val Loss: 82.67229127883911, loss : 1.4962986707687378\n",
            "epoch: 14, classification_loss: 1.4913593530654907, Val Loss: 81.93995332717896, loss : 1.4913593530654907\n",
            "epoch: 15, classification_loss: 1.4876654148101807, Val Loss: 81.38375568389893, loss : 1.4876654148101807\n",
            "epoch: 16, classification_loss: 1.4841264486312866, Val Loss: 80.9163932800293, loss : 1.4841264486312866\n",
            "epoch: 17, classification_loss: 1.4810898303985596, Val Loss: 80.56389570236206, loss : 1.4810898303985596\n",
            "epoch: 18, classification_loss: 1.4779711961746216, Val Loss: 80.22222638130188, loss : 1.4779711961746216\n",
            "epoch: 19, classification_loss: 1.4754486083984375, Val Loss: 79.89274311065674, loss : 1.4754486083984375\n",
            "Batch: 1, Test Acc: 0.4133613782051282\n",
            "Batch: 2:\n",
            "epoch: 0, classification_loss: 1.9070817232131958, Val Loss: 78.97448527812958, loss : 1.9070817232131958\n",
            "epoch: 1, classification_loss: 1.7712169885635376, Val Loss: 78.29867792129517, loss : 1.7712169885635376\n",
            "epoch: 2, classification_loss: 1.6919147968292236, Val Loss: 77.8714051246643, loss : 1.6919147968292236\n",
            "epoch: 3, classification_loss: 1.6488139629364014, Val Loss: 78.33942568302155, loss : 1.6488139629364014\n",
            "epoch: 4, classification_loss: 1.6134988069534302, Val Loss: 78.68838572502136, loss : 1.6134988069534302\n",
            "epoch: 5, classification_loss: 1.5836306810379028, Val Loss: 79.47908091545105, loss : 1.5836306810379028\n",
            "epoch: 6, classification_loss: 1.557165503501892, Val Loss: 80.67702722549438, loss : 1.557165503501892\n",
            "epoch: 7, classification_loss: 1.539548397064209, Val Loss: 81.75269556045532, loss : 1.539548397064209\n",
            "epoch: 8, classification_loss: 1.5279550552368164, Val Loss: 82.47399926185608, loss : 1.5279550552368164\n",
            "epoch: 9, classification_loss: 1.5194816589355469, Val Loss: 82.55457305908203, loss : 1.5194816589355469\n",
            "epoch: 10, classification_loss: 1.5129235982894897, Val Loss: 82.59862780570984, loss : 1.5129235982894897\n",
            "epoch: 11, classification_loss: 1.5047237873077393, Val Loss: 82.52177548408508, loss : 1.5047237873077393\n",
            "epoch: 12, classification_loss: 1.4993144273757935, Val Loss: 82.31784057617188, loss : 1.4993144273757935\n",
            "epoch: 13, classification_loss: 1.4931213855743408, Val Loss: 81.60770416259766, loss : 1.4931213855743408\n",
            "epoch: 14, classification_loss: 1.486476182937622, Val Loss: 80.6447982788086, loss : 1.486476182937622\n",
            "epoch: 15, classification_loss: 1.4852386713027954, Val Loss: 79.75991892814636, loss : 1.4852386713027954\n",
            "epoch: 16, classification_loss: 1.4831337928771973, Val Loss: 79.08064615726471, loss : 1.4831337928771973\n",
            "epoch: 17, classification_loss: 1.4823975563049316, Val Loss: 78.57166230678558, loss : 1.4823975563049316\n",
            "epoch: 18, classification_loss: 1.4809211492538452, Val Loss: 78.13033699989319, loss : 1.4809211492538452\n",
            "epoch: 19, classification_loss: 1.477947473526001, Val Loss: 77.81340253353119, loss : 1.477947473526001\n",
            "Batch: 2, Test Acc: 0.43599759615384615\n",
            "Batch: 3:\n",
            "epoch: 0, classification_loss: 1.888672113418579, Val Loss: 76.8785492181778, loss : 1.888672113418579\n",
            "epoch: 1, classification_loss: 1.7704647779464722, Val Loss: 75.96493816375732, loss : 1.7704647779464722\n",
            "epoch: 2, classification_loss: 1.692180871963501, Val Loss: 75.83462142944336, loss : 1.692180871963501\n",
            "epoch: 3, classification_loss: 1.6335967779159546, Val Loss: 76.38327991962433, loss : 1.6335967779159546\n",
            "epoch: 4, classification_loss: 1.6079063415527344, Val Loss: 77.04165315628052, loss : 1.6079063415527344\n",
            "epoch: 5, classification_loss: 1.5886369943618774, Val Loss: 77.48273813724518, loss : 1.5886369943618774\n",
            "epoch: 6, classification_loss: 1.5694801807403564, Val Loss: 77.8866456747055, loss : 1.5694801807403564\n",
            "epoch: 7, classification_loss: 1.5535780191421509, Val Loss: 78.27416610717773, loss : 1.5535780191421509\n",
            "epoch: 8, classification_loss: 1.5425366163253784, Val Loss: 78.79329836368561, loss : 1.5425366163253784\n",
            "epoch: 9, classification_loss: 1.532942295074463, Val Loss: 79.00046908855438, loss : 1.532942295074463\n",
            "epoch: 10, classification_loss: 1.514507532119751, Val Loss: 79.31352174282074, loss : 1.514507532119751\n",
            "epoch: 11, classification_loss: 1.5049996376037598, Val Loss: 79.61344289779663, loss : 1.5049996376037598\n",
            "epoch: 12, classification_loss: 1.5028549432754517, Val Loss: 79.6906476020813, loss : 1.5028549432754517\n",
            "epoch: 13, classification_loss: 1.4956499338150024, Val Loss: 79.82189559936523, loss : 1.4956499338150024\n",
            "epoch: 14, classification_loss: 1.491386890411377, Val Loss: 79.62616229057312, loss : 1.491386890411377\n",
            "epoch: 15, classification_loss: 1.4874634742736816, Val Loss: 79.0710768699646, loss : 1.4874634742736816\n",
            "epoch: 16, classification_loss: 1.4838745594024658, Val Loss: 78.7444623708725, loss : 1.4838745594024658\n",
            "epoch: 17, classification_loss: 1.478204369544983, Val Loss: 78.72285461425781, loss : 1.478204369544983\n",
            "epoch: 18, classification_loss: 1.4766514301300049, Val Loss: 78.46001195907593, loss : 1.4766514301300049\n",
            "epoch: 19, classification_loss: 1.474415898323059, Val Loss: 77.99699795246124, loss : 1.474415898323059\n",
            "Batch: 3, Test Acc: 0.44250801282051283\n",
            "Batch: 4:\n",
            "epoch: 0, classification_loss: 1.8817399740219116, Val Loss: 74.74042773246765, loss : 1.8817399740219116\n",
            "epoch: 1, classification_loss: 1.7485657930374146, Val Loss: 75.29471182823181, loss : 1.7485657930374146\n",
            "epoch: 2, classification_loss: 1.67400324344635, Val Loss: 74.24922204017639, loss : 2.746342420578003\n",
            "epoch: 3, classification_loss: 1.6667118072509766, Val Loss: 75.97134804725647, loss : 1.6667118072509766\n",
            "epoch: 4, classification_loss: 1.659886121749878, Val Loss: 74.62991201877594, loss : 3.160226345062256\n",
            "epoch: 5, classification_loss: 1.6418893337249756, Val Loss: 73.3399258852005, loss : 2.700944423675537\n",
            "epoch: 6, classification_loss: 1.6748199462890625, Val Loss: 74.56822466850281, loss : 1.6748199462890625\n",
            "epoch: 7, classification_loss: 1.693814992904663, Val Loss: 73.84708189964294, loss : 3.2592220306396484\n",
            "epoch: 8, classification_loss: 1.656167984008789, Val Loss: 72.8609539270401, loss : 2.9272193908691406\n",
            "epoch: 9, classification_loss: 1.6393864154815674, Val Loss: 73.55910873413086, loss : 1.6393864154815674\n",
            "epoch: 10, classification_loss: 1.6256670951843262, Val Loss: 73.33683371543884, loss : 3.0649049282073975\n",
            "epoch: 11, classification_loss: 1.6026912927627563, Val Loss: 72.87224471569061, loss : 2.8667261600494385\n",
            "epoch: 12, classification_loss: 1.6101516485214233, Val Loss: 73.10988008975983, loss : 2.3202457427978516\n",
            "epoch: 13, classification_loss: 1.6595335006713867, Val Loss: 72.86406767368317, loss : 2.684227705001831\n",
            "epoch: 14, classification_loss: 1.6642770767211914, Val Loss: 72.42426717281342, loss : 2.7040605545043945\n",
            "epoch: 15, classification_loss: 1.621456503868103, Val Loss: 73.10309195518494, loss : 1.621456503868103\n",
            "epoch: 16, classification_loss: 1.599010944366455, Val Loss: 72.69338500499725, loss : 2.6024715900421143\n",
            "epoch: 17, classification_loss: 1.5931051969528198, Val Loss: 72.47266006469727, loss : 2.367325782775879\n",
            "epoch: 18, classification_loss: 1.626516342163086, Val Loss: 72.39081954956055, loss : 2.2305617332458496\n",
            "epoch: 19, classification_loss: 1.6393672227859497, Val Loss: 72.31105649471283, loss : 2.3968307971954346\n",
            "Batch: 4, Test Acc: 0.5475761217948718\n",
            "Batch: 5:\n",
            "epoch: 0, classification_loss: 1.8491894006729126, Val Loss: 72.18482530117035, loss : 1.8491894006729126\n",
            "epoch: 1, classification_loss: 1.7625809907913208, Val Loss: 72.54752993583679, loss : 1.7625809907913208\n",
            "epoch: 2, classification_loss: 1.6952574253082275, Val Loss: 72.44228136539459, loss : 2.5327308177948\n",
            "epoch: 3, classification_loss: 1.666438102722168, Val Loss: 72.17538118362427, loss : 2.307796001434326\n",
            "epoch: 4, classification_loss: 1.748046636581421, Val Loss: 72.01340734958649, loss : 2.457221746444702\n",
            "epoch: 5, classification_loss: 1.7768195867538452, Val Loss: 72.00527107715607, loss : 1.7768195867538452\n",
            "epoch: 6, classification_loss: 1.7539924383163452, Val Loss: 72.05457007884979, loss : 2.4477481842041016\n",
            "epoch: 7, classification_loss: 1.7242639064788818, Val Loss: 72.11983394622803, loss : 2.2233293056488037\n",
            "epoch: 8, classification_loss: 1.7562479972839355, Val Loss: 72.12970077991486, loss : 2.372004508972168\n",
            "epoch: 9, classification_loss: 1.730591058731079, Val Loss: 72.0192883014679, loss : 2.2974019050598145\n",
            "epoch: 10, classification_loss: 1.7111538648605347, Val Loss: 72.06864321231842, loss : 2.3601512908935547\n",
            "epoch: 11, classification_loss: 1.7153855562210083, Val Loss: 72.1456583738327, loss : 2.2936813831329346\n",
            "epoch: 12, classification_loss: 1.7772060632705688, Val Loss: 72.07864546775818, loss : 2.3144617080688477\n",
            "epoch: 13, classification_loss: 1.7545167207717896, Val Loss: 71.9507360458374, loss : 2.2911911010742188\n",
            "epoch: 14, classification_loss: 1.7274872064590454, Val Loss: 72.03136122226715, loss : 2.2769973278045654\n",
            "epoch: 15, classification_loss: 1.7155406475067139, Val Loss: 72.11257612705231, loss : 2.2606940269470215\n",
            "epoch: 16, classification_loss: 1.728737711906433, Val Loss: 71.91207933425903, loss : 2.2289302349090576\n",
            "epoch: 17, classification_loss: 1.7498301267623901, Val Loss: 72.23218381404877, loss : 1.7498301267623901\n",
            "epoch: 18, classification_loss: 1.693942666053772, Val Loss: 72.02016544342041, loss : 2.349168300628662\n",
            "epoch: 19, classification_loss: 1.664987564086914, Val Loss: 72.0614846944809, loss : 2.1695303916931152\n",
            "Batch: 5, Test Acc: 0.5495793269230769\n",
            "Batch: 6:\n",
            "epoch: 0, classification_loss: 1.8575358390808105, Val Loss: 72.2024564743042, loss : 1.8575358390808105\n",
            "epoch: 1, classification_loss: 1.7876383066177368, Val Loss: 72.01375794410706, loss : 2.5748672485351562\n",
            "epoch: 2, classification_loss: 1.7431535720825195, Val Loss: 72.22214388847351, loss : 2.497004985809326\n",
            "epoch: 3, classification_loss: 1.7565815448760986, Val Loss: 72.04299008846283, loss : 2.278179168701172\n",
            "epoch: 4, classification_loss: 1.8473246097564697, Val Loss: 71.89067149162292, loss : 2.4406707286834717\n",
            "epoch: 5, classification_loss: 1.843489170074463, Val Loss: 71.79230916500092, loss : 2.42393159866333\n",
            "epoch: 6, classification_loss: 1.8123911619186401, Val Loss: 72.21953046321869, loss : 1.8123911619186401\n",
            "epoch: 7, classification_loss: 1.7616206407546997, Val Loss: 71.8603652715683, loss : 2.4512522220611572\n",
            "epoch: 8, classification_loss: 1.7257890701293945, Val Loss: 71.71249341964722, loss : 2.3352584838867188\n",
            "epoch: 9, classification_loss: 1.761904001235962, Val Loss: 71.97180163860321, loss : 1.761904001235962\n",
            "epoch: 10, classification_loss: 1.7696053981781006, Val Loss: 71.92307245731354, loss : 2.4002633094787598\n",
            "epoch: 11, classification_loss: 1.7056645154953003, Val Loss: 71.73384666442871, loss : 2.233405590057373\n",
            "epoch: 12, classification_loss: 1.7154591083526611, Val Loss: 71.66767108440399, loss : 2.1932241916656494\n",
            "epoch: 13, classification_loss: 1.7528947591781616, Val Loss: 71.63973820209503, loss : 2.2596049308776855\n",
            "epoch: 14, classification_loss: 1.771620273590088, Val Loss: 71.82800602912903, loss : 1.771620273590088\n",
            "epoch: 15, classification_loss: 1.7660993337631226, Val Loss: 71.60751867294312, loss : 2.276674270629883\n",
            "epoch: 16, classification_loss: 1.7397347688674927, Val Loss: 71.61985659599304, loss : 2.1021862030029297\n",
            "epoch: 17, classification_loss: 1.745202660560608, Val Loss: 71.71787130832672, loss : 2.2529728412628174\n",
            "epoch: 18, classification_loss: 1.754040241241455, Val Loss: 71.71251130104065, loss : 2.2216591835021973\n",
            "epoch: 19, classification_loss: 1.748061180114746, Val Loss: 71.62822365760803, loss : 2.1666367053985596\n",
            "Batch: 6, Test Acc: 0.5532852564102564\n",
            "Batch: 7:\n",
            "epoch: 0, classification_loss: 1.839127540588379, Val Loss: 71.71099245548248, loss : 1.839127540588379\n",
            "epoch: 1, classification_loss: 1.7705777883529663, Val Loss: 71.9249279499054, loss : 2.3640689849853516\n",
            "epoch: 2, classification_loss: 1.7303154468536377, Val Loss: 71.83439826965332, loss : 2.247152328491211\n",
            "epoch: 3, classification_loss: 1.7527567148208618, Val Loss: 71.86318492889404, loss : 2.2472410202026367\n",
            "epoch: 4, classification_loss: 1.798406958580017, Val Loss: 71.68516480922699, loss : 2.27223539352417\n",
            "epoch: 5, classification_loss: 1.8030755519866943, Val Loss: 71.74524414539337, loss : 2.254610061645508\n",
            "epoch: 6, classification_loss: 1.7935361862182617, Val Loss: 71.62634837627411, loss : 2.2289645671844482\n",
            "epoch: 7, classification_loss: 1.7908955812454224, Val Loss: 71.62125563621521, loss : 2.235952615737915\n",
            "epoch: 8, classification_loss: 1.8021787405014038, Val Loss: 71.5923718214035, loss : 2.219667911529541\n",
            "epoch: 9, classification_loss: 1.772342562675476, Val Loss: 71.62276244163513, loss : 2.2071735858917236\n",
            "epoch: 10, classification_loss: 1.7921667098999023, Val Loss: 71.80639982223511, loss : 2.216865301132202\n",
            "epoch: 11, classification_loss: 1.7816119194030762, Val Loss: 71.60406613349915, loss : 2.202444314956665\n",
            "epoch: 12, classification_loss: 1.7838280200958252, Val Loss: 71.625967502594, loss : 2.2037479877471924\n",
            "epoch: 13, classification_loss: 1.7877354621887207, Val Loss: 71.61674654483795, loss : 2.2028002738952637\n",
            "epoch: 14, classification_loss: 1.7836600542068481, Val Loss: 71.5948576927185, loss : 2.1871650218963623\n",
            "epoch: 15, classification_loss: 1.7975177764892578, Val Loss: 71.60269963741302, loss : 2.190150260925293\n",
            "epoch: 16, classification_loss: 1.7948631048202515, Val Loss: 71.53544878959656, loss : 2.1887078285217285\n",
            "epoch: 17, classification_loss: 1.786123275756836, Val Loss: 71.81911492347717, loss : 1.786123275756836\n",
            "epoch: 18, classification_loss: 1.7234017848968506, Val Loss: 71.75912702083588, loss : 2.2440755367279053\n",
            "epoch: 19, classification_loss: 1.6995433568954468, Val Loss: 71.79353737831116, loss : 2.101175308227539\n",
            "Batch: 7, Test Acc: 0.551582532051282\n",
            "Batch: 8:\n",
            "epoch: 0, classification_loss: 1.887141466140747, Val Loss: 71.58706223964691, loss : 1.887141466140747\n",
            "epoch: 1, classification_loss: 1.8280770778656006, Val Loss: 71.56037867069244, loss : 2.427936553955078\n",
            "epoch: 2, classification_loss: 1.7900642156600952, Val Loss: 71.4695600271225, loss : 2.3552086353302\n",
            "epoch: 3, classification_loss: 1.7891708612442017, Val Loss: 71.5782595872879, loss : 1.7891708612442017\n",
            "epoch: 4, classification_loss: 1.765990138053894, Val Loss: 71.66244375705719, loss : 2.3462557792663574\n",
            "epoch: 5, classification_loss: 1.7199125289916992, Val Loss: 71.56663751602173, loss : 2.227688789367676\n",
            "epoch: 6, classification_loss: 1.7474310398101807, Val Loss: 71.57527017593384, loss : 2.142888069152832\n",
            "epoch: 7, classification_loss: 1.8037785291671753, Val Loss: 71.47468507289886, loss : 2.229025363922119\n",
            "epoch: 8, classification_loss: 1.7708864212036133, Val Loss: 71.49938499927521, loss : 2.137953519821167\n",
            "epoch: 9, classification_loss: 1.754704236984253, Val Loss: 71.51479828357697, loss : 2.1467859745025635\n",
            "epoch: 10, classification_loss: 1.7518010139465332, Val Loss: 71.50338304042816, loss : 2.1607773303985596\n",
            "epoch: 11, classification_loss: 1.7598669528961182, Val Loss: 71.58334624767303, loss : 2.114774227142334\n",
            "epoch: 12, classification_loss: 1.78209388256073, Val Loss: 71.50644946098328, loss : 2.1461732387542725\n",
            "epoch: 13, classification_loss: 1.7525068521499634, Val Loss: 71.40640079975128, loss : 2.109041690826416\n",
            "epoch: 14, classification_loss: 1.766654372215271, Val Loss: 71.47166240215302, loss : 1.766654372215271\n",
            "epoch: 15, classification_loss: 1.7458025217056274, Val Loss: 71.54626500606537, loss : 2.2096076011657715\n",
            "epoch: 16, classification_loss: 1.7084693908691406, Val Loss: 71.53833079338074, loss : 2.065340995788574\n",
            "epoch: 17, classification_loss: 1.747396469116211, Val Loss: 71.43129074573517, loss : 2.125516176223755\n",
            "epoch: 18, classification_loss: 1.7821249961853027, Val Loss: 71.47251808643341, loss : 2.136241912841797\n",
            "epoch: 19, classification_loss: 1.7561767101287842, Val Loss: 71.47544729709625, loss : 2.090395927429199\n",
            "Batch: 8, Test Acc: 0.5603966346153846\n",
            "Batch: 9:\n",
            "epoch: 0, classification_loss: 1.8598905801773071, Val Loss: 71.40587830543518, loss : 1.8598905801773071\n",
            "epoch: 1, classification_loss: 1.8227148056030273, Val Loss: 71.44898629188538, loss : 2.325623035430908\n",
            "epoch: 2, classification_loss: 1.7902997732162476, Val Loss: 71.55837559700012, loss : 2.2162024974823\n",
            "epoch: 3, classification_loss: 1.8110973834991455, Val Loss: 71.45295143127441, loss : 2.2182564735412598\n",
            "epoch: 4, classification_loss: 1.8365716934204102, Val Loss: 71.47199523448944, loss : 2.2127771377563477\n",
            "epoch: 5, classification_loss: 1.8339658975601196, Val Loss: 71.44183993339539, loss : 2.207465887069702\n",
            "epoch: 6, classification_loss: 1.828869104385376, Val Loss: 71.42647445201874, loss : 2.167729139328003\n",
            "epoch: 7, classification_loss: 1.8312678337097168, Val Loss: 71.45143103599548, loss : 2.2206647396087646\n",
            "epoch: 8, classification_loss: 1.8215397596359253, Val Loss: 71.48468840122223, loss : 2.1665589809417725\n",
            "epoch: 9, classification_loss: 1.8282172679901123, Val Loss: 71.48269248008728, loss : 2.205465316772461\n",
            "epoch: 10, classification_loss: 1.8254178762435913, Val Loss: 71.44564878940582, loss : 2.1776957511901855\n",
            "epoch: 11, classification_loss: 1.8274215459823608, Val Loss: 71.48962271213531, loss : 2.186232328414917\n",
            "epoch: 12, classification_loss: 1.83331298828125, Val Loss: 71.49291455745697, loss : 2.181596279144287\n",
            "epoch: 13, classification_loss: 1.8333170413970947, Val Loss: 71.47043323516846, loss : 2.189087390899658\n",
            "epoch: 14, classification_loss: 1.8292356729507446, Val Loss: 71.564159989357, loss : 2.1618995666503906\n",
            "epoch: 15, classification_loss: 1.8392871618270874, Val Loss: 71.49375748634338, loss : 2.1779286861419678\n",
            "epoch: 16, classification_loss: 1.8165011405944824, Val Loss: 71.41324210166931, loss : 2.15287709236145\n",
            "epoch: 17, classification_loss: 1.8223217725753784, Val Loss: 71.57297146320343, loss : 2.163484573364258\n",
            "epoch: 18, classification_loss: 1.8222558498382568, Val Loss: 71.47679829597473, loss : 2.1747241020202637\n",
            "epoch: 19, classification_loss: 1.8256239891052246, Val Loss: 71.55227220058441, loss : 2.162397623062134\n",
            "Batch: 9, Test Acc: 0.5586939102564102\n",
            "Batch: 10:\n",
            "epoch: 0, classification_loss: 1.8111249208450317, Val Loss: 71.57806050777435, loss : 1.8111249208450317\n",
            "epoch: 1, classification_loss: 1.750481367111206, Val Loss: 71.58747172355652, loss : 2.2185752391815186\n",
            "epoch: 2, classification_loss: 1.723690390586853, Val Loss: 71.42040479183197, loss : 2.106706142425537\n",
            "epoch: 3, classification_loss: 1.7479100227355957, Val Loss: 71.44576156139374, loss : 2.1547789573669434\n",
            "epoch: 4, classification_loss: 1.78184974193573, Val Loss: 71.46400773525238, loss : 2.134053945541382\n",
            "epoch: 5, classification_loss: 1.766042709350586, Val Loss: 71.35377526283264, loss : 2.125990390777588\n",
            "epoch: 6, classification_loss: 1.758949637413025, Val Loss: 71.46363747119904, loss : 1.758949637413025\n",
            "epoch: 7, classification_loss: 1.727987289428711, Val Loss: 71.46402370929718, loss : 2.1762943267822266\n",
            "epoch: 8, classification_loss: 1.6992175579071045, Val Loss: 71.40871047973633, loss : 2.0605270862579346\n",
            "epoch: 9, classification_loss: 1.7395401000976562, Val Loss: 71.43878185749054, loss : 2.0836808681488037\n",
            "epoch: 10, classification_loss: 1.7715308666229248, Val Loss: 71.35758972167969, loss : 2.0982248783111572\n",
            "epoch: 11, classification_loss: 1.736383080482483, Val Loss: 71.40523481369019, loss : 2.058551073074341\n",
            "epoch: 12, classification_loss: 1.7215261459350586, Val Loss: 71.51102757453918, loss : 2.0813393592834473\n",
            "epoch: 13, classification_loss: 1.7181658744812012, Val Loss: 71.44569766521454, loss : 2.071699380874634\n",
            "epoch: 14, classification_loss: 1.743208646774292, Val Loss: 71.35084557533264, loss : 2.0698013305664062\n",
            "epoch: 15, classification_loss: 1.7456609010696411, Val Loss: 71.33333456516266, loss : 2.0788326263427734\n",
            "epoch: 16, classification_loss: 1.7315419912338257, Val Loss: 71.37821102142334, loss : 2.0613856315612793\n",
            "epoch: 17, classification_loss: 1.7302939891815186, Val Loss: 71.43615221977234, loss : 2.056230068206787\n",
            "epoch: 18, classification_loss: 1.7355587482452393, Val Loss: 71.37509369850159, loss : 2.0608911514282227\n",
            "epoch: 19, classification_loss: 1.7341574430465698, Val Loss: 71.37870132923126, loss : 2.047700881958008\n",
            "Batch: 10, Test Acc: 0.5583934294871795\n",
            "Batch: 11:\n",
            "epoch: 0, classification_loss: 1.8261783123016357, Val Loss: 71.3503270149231, loss : 1.8261783123016357\n",
            "epoch: 1, classification_loss: 1.7826366424560547, Val Loss: 71.45328938961029, loss : 2.248140811920166\n",
            "epoch: 2, classification_loss: 1.7516196966171265, Val Loss: 71.51855707168579, loss : 2.13596773147583\n",
            "epoch: 3, classification_loss: 1.7760694026947021, Val Loss: 71.67425096035004, loss : 2.1735973358154297\n",
            "epoch: 4, classification_loss: 1.8180370330810547, Val Loss: 71.39643335342407, loss : 2.175720453262329\n",
            "epoch: 5, classification_loss: 1.8041213750839233, Val Loss: 71.33995735645294, loss : 2.1526527404785156\n",
            "epoch: 6, classification_loss: 1.7945492267608643, Val Loss: 71.38370192050934, loss : 2.1430180072784424\n",
            "epoch: 7, classification_loss: 1.805100440979004, Val Loss: 71.40093052387238, loss : 2.1587729454040527\n",
            "epoch: 8, classification_loss: 1.7898766994476318, Val Loss: 71.38986420631409, loss : 2.126495838165283\n",
            "epoch: 9, classification_loss: 1.7960338592529297, Val Loss: 71.34558475017548, loss : 2.1297645568847656\n",
            "epoch: 10, classification_loss: 1.8072699308395386, Val Loss: 71.47950518131256, loss : 2.1362221240997314\n",
            "epoch: 11, classification_loss: 1.8083503246307373, Val Loss: 71.39902710914612, loss : 2.129279613494873\n",
            "epoch: 12, classification_loss: 1.8093825578689575, Val Loss: 71.4680483341217, loss : 2.1329505443573\n",
            "epoch: 13, classification_loss: 1.7931386232376099, Val Loss: 71.48299300670624, loss : 2.125281810760498\n",
            "epoch: 14, classification_loss: 1.7938320636749268, Val Loss: 71.46116149425507, loss : 2.112567901611328\n",
            "epoch: 15, classification_loss: 1.8067128658294678, Val Loss: 71.39648425579071, loss : 2.11875057220459\n",
            "epoch: 16, classification_loss: 1.7982330322265625, Val Loss: 71.38747906684875, loss : 2.108170509338379\n",
            "epoch: 17, classification_loss: 1.7992303371429443, Val Loss: 71.41228926181793, loss : 2.111618757247925\n",
            "epoch: 18, classification_loss: 1.8096815347671509, Val Loss: 71.42943572998047, loss : 2.11820125579834\n",
            "epoch: 19, classification_loss: 1.8016542196273804, Val Loss: 71.41220986843109, loss : 2.112454652786255\n",
            "Batch: 11, Test Acc: 0.5578926282051282\n",
            "Batch: 12:\n",
            "epoch: 0, classification_loss: 1.8720088005065918, Val Loss: 71.39328014850616, loss : 1.8720088005065918\n",
            "epoch: 1, classification_loss: 1.8301165103912354, Val Loss: 71.37506115436554, loss : 2.252967119216919\n",
            "epoch: 2, classification_loss: 1.7934000492095947, Val Loss: 71.54972732067108, loss : 2.112091302871704\n",
            "epoch: 3, classification_loss: 1.831480622291565, Val Loss: 71.45019054412842, loss : 2.1943657398223877\n",
            "epoch: 4, classification_loss: 1.8520420789718628, Val Loss: 71.33378756046295, loss : 2.163106918334961\n",
            "epoch: 5, classification_loss: 1.8424463272094727, Val Loss: 71.35085439682007, loss : 2.1818172931671143\n",
            "epoch: 6, classification_loss: 1.8177003860473633, Val Loss: 71.4977593421936, loss : 2.156444549560547\n",
            "epoch: 7, classification_loss: 1.836993932723999, Val Loss: 71.44494915008545, loss : 2.1593329906463623\n",
            "epoch: 8, classification_loss: 1.8428410291671753, Val Loss: 71.39340949058533, loss : 2.1616153717041016\n",
            "epoch: 9, classification_loss: 1.835515022277832, Val Loss: 71.36097180843353, loss : 2.1602766513824463\n",
            "epoch: 10, classification_loss: 1.8263134956359863, Val Loss: 71.33792686462402, loss : 2.1520910263061523\n",
            "epoch: 11, classification_loss: 1.8370826244354248, Val Loss: 71.32643616199493, loss : 2.1516072750091553\n",
            "epoch: 12, classification_loss: 1.8357757329940796, Val Loss: 71.37924945354462, loss : 2.1443986892700195\n",
            "epoch: 13, classification_loss: 1.839972972869873, Val Loss: 71.44044542312622, loss : 2.149394989013672\n",
            "epoch: 14, classification_loss: 1.8335802555084229, Val Loss: 71.35432195663452, loss : 2.140458345413208\n",
            "epoch: 15, classification_loss: 1.839262843132019, Val Loss: 71.28379964828491, loss : 2.1520307064056396\n",
            "epoch: 16, classification_loss: 1.836954116821289, Val Loss: 71.40395927429199, loss : 1.836954116821289\n",
            "epoch: 17, classification_loss: 1.7780054807662964, Val Loss: 71.32359635829926, loss : 2.1968533992767334\n",
            "epoch: 18, classification_loss: 1.7607711553573608, Val Loss: 71.39508402347565, loss : 2.074131488800049\n",
            "epoch: 19, classification_loss: 1.8024485111236572, Val Loss: 71.4023666381836, loss : 2.155151128768921\n",
            "Batch: 12, Test Acc: 0.5618990384615384\n",
            "Batch: 13:\n",
            "epoch: 0, classification_loss: 1.8017618656158447, Val Loss: 71.25967812538147, loss : 1.8017618656158447\n",
            "epoch: 1, classification_loss: 1.7547168731689453, Val Loss: 71.28887343406677, loss : 2.135920286178589\n",
            "epoch: 2, classification_loss: 1.7272956371307373, Val Loss: 71.322385430336, loss : 2.0589606761932373\n",
            "epoch: 3, classification_loss: 1.7570765018463135, Val Loss: 71.32371032238007, loss : 2.086090564727783\n",
            "epoch: 4, classification_loss: 1.7821261882781982, Val Loss: 71.26857817173004, loss : 2.0653183460235596\n",
            "epoch: 5, classification_loss: 1.7659580707550049, Val Loss: 71.2458860874176, loss : 2.0817112922668457\n",
            "epoch: 6, classification_loss: 1.7607347965240479, Val Loss: 71.28927731513977, loss : 1.7607347965240479\n",
            "epoch: 7, classification_loss: 1.7179639339447021, Val Loss: 71.2536369562149, loss : 2.146268367767334\n",
            "epoch: 8, classification_loss: 1.6956804990768433, Val Loss: 71.29420840740204, loss : 2.0294742584228516\n",
            "epoch: 9, classification_loss: 1.7397191524505615, Val Loss: 71.3073239326477, loss : 2.0655298233032227\n",
            "epoch: 10, classification_loss: 1.7569961547851562, Val Loss: 71.24434423446655, loss : 2.064884662628174\n",
            "epoch: 11, classification_loss: 1.7468280792236328, Val Loss: 71.24521720409393, loss : 2.0563340187072754\n",
            "epoch: 12, classification_loss: 1.7148813009262085, Val Loss: 71.29693353176117, loss : 2.0588326454162598\n",
            "epoch: 13, classification_loss: 1.7229946851730347, Val Loss: 71.30451142787933, loss : 2.056870937347412\n",
            "epoch: 14, classification_loss: 1.7411199808120728, Val Loss: 71.27757322788239, loss : 2.040083408355713\n",
            "epoch: 15, classification_loss: 1.7324681282043457, Val Loss: 71.25896418094635, loss : 2.033215045928955\n",
            "epoch: 16, classification_loss: 1.7315046787261963, Val Loss: 71.28480315208435, loss : 2.0269577503204346\n",
            "epoch: 17, classification_loss: 1.720366358757019, Val Loss: 71.28214430809021, loss : 2.022002935409546\n",
            "epoch: 18, classification_loss: 1.7395490407943726, Val Loss: 71.30991780757904, loss : 2.029967784881592\n",
            "epoch: 19, classification_loss: 1.7391088008880615, Val Loss: 71.31498193740845, loss : 2.0297558307647705\n",
            "Batch: 13, Test Acc: 0.5602964743589743\n",
            "Batch: 14:\n",
            "epoch: 0, classification_loss: 1.8444490432739258, Val Loss: 71.46579313278198, loss : 1.8444490432739258\n",
            "epoch: 1, classification_loss: 1.783485770225525, Val Loss: 71.34643089771271, loss : 2.2100517749786377\n",
            "epoch: 2, classification_loss: 1.7460236549377441, Val Loss: 71.35805201530457, loss : 2.0901107788085938\n",
            "epoch: 3, classification_loss: 1.7818809747695923, Val Loss: 71.3650574684143, loss : 2.1714704036712646\n",
            "epoch: 4, classification_loss: 1.8023087978363037, Val Loss: 71.33965456485748, loss : 2.1438214778900146\n",
            "epoch: 5, classification_loss: 1.7860603332519531, Val Loss: 71.29639756679535, loss : 2.141122341156006\n",
            "epoch: 6, classification_loss: 1.8000458478927612, Val Loss: 71.2136607170105, loss : 2.146472692489624\n",
            "epoch: 7, classification_loss: 1.802627682685852, Val Loss: 71.19184899330139, loss : 2.1402413845062256\n",
            "epoch: 8, classification_loss: 1.7865707874298096, Val Loss: 71.39001595973969, loss : 1.7865707874298096\n",
            "epoch: 9, classification_loss: 1.7513922452926636, Val Loss: 71.25440323352814, loss : 2.1717920303344727\n",
            "epoch: 10, classification_loss: 1.7183386087417603, Val Loss: 71.21890354156494, loss : 2.057171106338501\n",
            "epoch: 11, classification_loss: 1.7406784296035767, Val Loss: 71.24747002124786, loss : 2.124995708465576\n",
            "epoch: 12, classification_loss: 1.7886914014816284, Val Loss: 71.28996312618256, loss : 2.1168367862701416\n",
            "epoch: 13, classification_loss: 1.7748790979385376, Val Loss: 71.18711185455322, loss : 2.1123385429382324\n",
            "epoch: 14, classification_loss: 1.7687180042266846, Val Loss: 71.24826955795288, loss : 2.098154306411743\n",
            "epoch: 15, classification_loss: 1.7626562118530273, Val Loss: 71.20361638069153, loss : 2.1109087467193604\n",
            "epoch: 16, classification_loss: 1.746904730796814, Val Loss: 71.20541226863861, loss : 2.0760793685913086\n",
            "epoch: 17, classification_loss: 1.7537482976913452, Val Loss: 71.25495564937592, loss : 2.1046857833862305\n",
            "epoch: 18, classification_loss: 1.7693842649459839, Val Loss: 71.23171830177307, loss : 2.084174633026123\n",
            "epoch: 19, classification_loss: 1.7768194675445557, Val Loss: 71.34619855880737, loss : 2.097032070159912\n",
            "Batch: 14, Test Acc: 0.5592948717948718\n",
            "Batch: 15:\n",
            "epoch: 0, classification_loss: 1.812538981437683, Val Loss: 71.49713373184204, loss : 1.812538981437683\n",
            "epoch: 1, classification_loss: 1.7512130737304688, Val Loss: 71.32042789459229, loss : 2.1812171936035156\n",
            "epoch: 2, classification_loss: 1.7127034664154053, Val Loss: 71.44626033306122, loss : 2.0945448875427246\n",
            "epoch: 3, classification_loss: 1.7342348098754883, Val Loss: 71.29963505268097, loss : 2.1135590076446533\n",
            "epoch: 4, classification_loss: 1.78083074092865, Val Loss: 71.30363023281097, loss : 2.115042209625244\n",
            "epoch: 5, classification_loss: 1.7773445844650269, Val Loss: 71.2469881772995, loss : 2.115553617477417\n",
            "epoch: 6, classification_loss: 1.765924096107483, Val Loss: 71.27325928211212, loss : 2.1080996990203857\n",
            "epoch: 7, classification_loss: 1.7626280784606934, Val Loss: 71.29953420162201, loss : 2.1064646244049072\n",
            "epoch: 8, classification_loss: 1.764203667640686, Val Loss: 71.24972629547119, loss : 2.0974111557006836\n",
            "epoch: 9, classification_loss: 1.7619516849517822, Val Loss: 71.28981292247772, loss : 2.097114086151123\n",
            "epoch: 10, classification_loss: 1.7433323860168457, Val Loss: 71.25737583637238, loss : 2.080070734024048\n",
            "epoch: 11, classification_loss: 1.7549781799316406, Val Loss: 71.27813374996185, loss : 2.086879253387451\n",
            "epoch: 12, classification_loss: 1.755981683731079, Val Loss: 71.22614598274231, loss : 2.072880983352661\n",
            "epoch: 13, classification_loss: 1.7673895359039307, Val Loss: 71.32042670249939, loss : 2.07938289642334\n",
            "epoch: 14, classification_loss: 1.7593127489089966, Val Loss: 71.32836961746216, loss : 2.066265106201172\n",
            "epoch: 15, classification_loss: 1.7589318752288818, Val Loss: 71.25800502300262, loss : 2.0728654861450195\n",
            "epoch: 16, classification_loss: 1.7680943012237549, Val Loss: 71.29338645935059, loss : 2.072039842605591\n",
            "epoch: 17, classification_loss: 1.76747465133667, Val Loss: 71.2434834241867, loss : 2.0839192867279053\n",
            "epoch: 18, classification_loss: 1.7547818422317505, Val Loss: 71.32380163669586, loss : 2.0690853595733643\n",
            "epoch: 19, classification_loss: 1.7671215534210205, Val Loss: 71.31328105926514, loss : 2.0864856243133545\n",
            "Batch: 15, Test Acc: 0.5628004807692307\n",
            "Batch: 16:\n",
            "epoch: 0, classification_loss: 1.8409420251846313, Val Loss: 71.1986836194992, loss : 1.8409420251846313\n",
            "epoch: 1, classification_loss: 1.7839077711105347, Val Loss: 71.2308702468872, loss : 2.197784662246704\n",
            "epoch: 2, classification_loss: 1.7443320751190186, Val Loss: 71.29401051998138, loss : 2.0830166339874268\n",
            "epoch: 3, classification_loss: 1.7871689796447754, Val Loss: 71.31181800365448, loss : 2.136766195297241\n",
            "epoch: 4, classification_loss: 1.8288366794586182, Val Loss: 71.13598477840424, loss : 2.1289467811584473\n",
            "epoch: 5, classification_loss: 1.8091100454330444, Val Loss: 71.22524344921112, loss : 1.8091100454330444\n",
            "epoch: 6, classification_loss: 1.7726789712905884, Val Loss: 71.17761814594269, loss : 2.232466459274292\n",
            "epoch: 7, classification_loss: 1.748002529144287, Val Loss: 71.24102187156677, loss : 2.1372263431549072\n",
            "epoch: 8, classification_loss: 1.7745091915130615, Val Loss: 71.3385329246521, loss : 2.0891313552856445\n",
            "epoch: 9, classification_loss: 1.80962336063385, Val Loss: 71.19889509677887, loss : 2.1605541706085205\n",
            "epoch: 10, classification_loss: 1.7884044647216797, Val Loss: 71.16834354400635, loss : 2.071465492248535\n",
            "epoch: 11, classification_loss: 1.7755926847457886, Val Loss: 71.2110161781311, loss : 2.149425983428955\n",
            "epoch: 12, classification_loss: 1.7612531185150146, Val Loss: 71.25664043426514, loss : 2.1033220291137695\n",
            "epoch: 13, classification_loss: 1.7893452644348145, Val Loss: 71.23537123203278, loss : 2.09598970413208\n",
            "epoch: 14, classification_loss: 1.7829127311706543, Val Loss: 71.21147310733795, loss : 2.0964248180389404\n",
            "epoch: 15, classification_loss: 1.786125659942627, Val Loss: 71.2443335056305, loss : 2.0909411907196045\n",
            "epoch: 16, classification_loss: 1.7723568677902222, Val Loss: 71.18421959877014, loss : 2.092841625213623\n",
            "epoch: 17, classification_loss: 1.776656150817871, Val Loss: 71.19109499454498, loss : 2.0820095539093018\n",
            "epoch: 18, classification_loss: 1.7897381782531738, Val Loss: 71.21459317207336, loss : 2.0813941955566406\n",
            "epoch: 19, classification_loss: 1.7801735401153564, Val Loss: 71.28333556652069, loss : 2.0662760734558105\n",
            "Batch: 16, Test Acc: 0.5619991987179487\n",
            "Batch: 17:\n",
            "epoch: 0, classification_loss: 1.8362038135528564, Val Loss: 71.25122129917145, loss : 1.8362038135528564\n",
            "epoch: 1, classification_loss: 1.7884325981140137, Val Loss: 71.23421347141266, loss : 2.2003581523895264\n",
            "epoch: 2, classification_loss: 1.7562177181243896, Val Loss: 71.19931077957153, loss : 2.1045899391174316\n",
            "epoch: 3, classification_loss: 1.7662410736083984, Val Loss: 71.2113037109375, loss : 2.1344776153564453\n",
            "epoch: 4, classification_loss: 1.7989306449890137, Val Loss: 71.18569493293762, loss : 2.143059492111206\n",
            "epoch: 5, classification_loss: 1.8038970232009888, Val Loss: 71.20893704891205, loss : 2.106407642364502\n",
            "epoch: 6, classification_loss: 1.811021089553833, Val Loss: 71.17862057685852, loss : 2.1377577781677246\n",
            "epoch: 7, classification_loss: 1.7969038486480713, Val Loss: 71.21844244003296, loss : 2.114112615585327\n",
            "epoch: 8, classification_loss: 1.8003087043762207, Val Loss: 71.22245860099792, loss : 2.109891653060913\n",
            "epoch: 9, classification_loss: 1.7989524602890015, Val Loss: 71.20534491539001, loss : 2.10624623298645\n",
            "epoch: 10, classification_loss: 1.8032461404800415, Val Loss: 71.19084405899048, loss : 2.095585346221924\n",
            "epoch: 11, classification_loss: 1.7914214134216309, Val Loss: 71.19103336334229, loss : 2.101968765258789\n",
            "epoch: 12, classification_loss: 1.7957992553710938, Val Loss: 71.23050308227539, loss : 2.087244987487793\n",
            "epoch: 13, classification_loss: 1.8038514852523804, Val Loss: 71.28148114681244, loss : 2.0950136184692383\n",
            "epoch: 14, classification_loss: 1.796949863433838, Val Loss: 71.26260232925415, loss : 2.081801176071167\n",
            "epoch: 15, classification_loss: 1.8077492713928223, Val Loss: 71.23365843296051, loss : 2.0905508995056152\n",
            "epoch: 16, classification_loss: 1.784287691116333, Val Loss: 71.22362184524536, loss : 2.072016477584839\n",
            "epoch: 17, classification_loss: 1.8048723936080933, Val Loss: 71.26658821105957, loss : 2.0843615531921387\n",
            "epoch: 18, classification_loss: 1.8075119256973267, Val Loss: 71.25548958778381, loss : 2.0897834300994873\n",
            "epoch: 19, classification_loss: 1.7968392372131348, Val Loss: 71.26284861564636, loss : 2.071890354156494\n",
            "Batch: 17, Test Acc: 0.5613982371794872\n",
            "Batch: 18:\n",
            "epoch: 0, classification_loss: 1.83844792842865, Val Loss: 71.1195160150528, loss : 1.83844792842865\n",
            "epoch: 1, classification_loss: 1.791046380996704, Val Loss: 71.12729394435883, loss : 2.1888251304626465\n",
            "epoch: 2, classification_loss: 1.7631404399871826, Val Loss: 71.18697786331177, loss : 2.090634822845459\n",
            "epoch: 3, classification_loss: 1.787941813468933, Val Loss: 71.19014942646027, loss : 2.1428287029266357\n",
            "epoch: 4, classification_loss: 1.8109240531921387, Val Loss: 71.11363673210144, loss : 2.125087261199951\n",
            "epoch: 5, classification_loss: 1.8114521503448486, Val Loss: 71.10635054111481, loss : 2.108350992202759\n",
            "epoch: 6, classification_loss: 1.7961236238479614, Val Loss: 71.1604871749878, loss : 1.7961236238479614\n",
            "epoch: 7, classification_loss: 1.7630959749221802, Val Loss: 71.14836061000824, loss : 2.162637948989868\n",
            "epoch: 8, classification_loss: 1.7439959049224854, Val Loss: 71.104278922081, loss : 2.066972494125366\n",
            "epoch: 9, classification_loss: 1.7676721811294556, Val Loss: 71.15813517570496, loss : 2.0767276287078857\n",
            "epoch: 10, classification_loss: 1.817386507987976, Val Loss: 71.17658269405365, loss : 2.1072068214416504\n",
            "epoch: 11, classification_loss: 1.7836506366729736, Val Loss: 71.15066576004028, loss : 2.0780131816864014\n",
            "epoch: 12, classification_loss: 1.7655110359191895, Val Loss: 71.1638834476471, loss : 2.0736966133117676\n",
            "epoch: 13, classification_loss: 1.766109585762024, Val Loss: 71.15252792835236, loss : 2.0656914710998535\n",
            "epoch: 14, classification_loss: 1.7828350067138672, Val Loss: 71.23861491680145, loss : 2.0556416511535645\n",
            "epoch: 15, classification_loss: 1.7785706520080566, Val Loss: 71.13626444339752, loss : 2.0670738220214844\n",
            "epoch: 16, classification_loss: 1.7699886560440063, Val Loss: 71.11718606948853, loss : 2.0588529109954834\n",
            "epoch: 17, classification_loss: 1.7677664756774902, Val Loss: 71.14293551445007, loss : 2.0526013374328613\n",
            "epoch: 18, classification_loss: 1.7697927951812744, Val Loss: 71.15650641918182, loss : 2.045825481414795\n",
            "epoch: 19, classification_loss: 1.777141809463501, Val Loss: 71.11798799037933, loss : 2.046800136566162\n",
            "Batch: 18, Test Acc: 0.5620993589743589\n",
            "Batch: 19:\n",
            "epoch: 0, classification_loss: 1.8684748411178589, Val Loss: 71.08017027378082, loss : 1.8684748411178589\n",
            "epoch: 1, classification_loss: 1.8058711290359497, Val Loss: 71.27226603031158, loss : 1.8058711290359497\n",
            "epoch: 2, classification_loss: 1.752701997756958, Val Loss: 71.24385464191437, loss : 2.195805788040161\n",
            "epoch: 3, classification_loss: 1.727461814880371, Val Loss: 71.09916174411774, loss : 2.130579710006714\n",
            "epoch: 4, classification_loss: 1.7314374446868896, Val Loss: 71.12055468559265, loss : 2.0942752361297607\n",
            "epoch: 5, classification_loss: 1.7978841066360474, Val Loss: 71.2107115983963, loss : 2.1184449195861816\n",
            "epoch: 6, classification_loss: 1.785305142402649, Val Loss: 71.16822850704193, loss : 2.092529296875\n",
            "epoch: 7, classification_loss: 1.771313190460205, Val Loss: 71.10295331478119, loss : 2.0883853435516357\n",
            "epoch: 8, classification_loss: 1.7602858543395996, Val Loss: 71.11374282836914, loss : 2.098836898803711\n",
            "epoch: 9, classification_loss: 1.7689156532287598, Val Loss: 71.19959592819214, loss : 2.0768611431121826\n",
            "epoch: 10, classification_loss: 1.7726404666900635, Val Loss: 71.1867926120758, loss : 2.089696168899536\n",
            "epoch: 11, classification_loss: 1.7720268964767456, Val Loss: 71.18381595611572, loss : 2.0776193141937256\n",
            "epoch: 12, classification_loss: 1.7756704092025757, Val Loss: 71.12340688705444, loss : 2.0797715187072754\n",
            "epoch: 13, classification_loss: 1.771755337715149, Val Loss: 71.12084579467773, loss : 2.064927339553833\n",
            "epoch: 14, classification_loss: 1.782486081123352, Val Loss: 71.1671611070633, loss : 2.074721574783325\n",
            "epoch: 15, classification_loss: 1.78266441822052, Val Loss: 71.19631385803223, loss : 2.0609514713287354\n",
            "epoch: 16, classification_loss: 1.7731999158859253, Val Loss: 71.11885201931, loss : 2.062230110168457\n",
            "epoch: 17, classification_loss: 1.769336462020874, Val Loss: 71.17616271972656, loss : 2.0487303733825684\n",
            "epoch: 18, classification_loss: 1.775048017501831, Val Loss: 71.20673632621765, loss : 2.0629501342773438\n",
            "epoch: 19, classification_loss: 1.7789562940597534, Val Loss: 71.10877323150635, loss : 2.0583364963531494\n",
            "Batch: 19, Test Acc: 0.5660056089743589\n",
            "Batch: 20:\n",
            "epoch: 0, classification_loss: 1.8004913330078125, Val Loss: 71.14906334877014, loss : 1.8004913330078125\n",
            "epoch: 1, classification_loss: 1.7505097389221191, Val Loss: 71.08860433101654, loss : 2.1467931270599365\n",
            "epoch: 2, classification_loss: 1.7265292406082153, Val Loss: 71.11884248256683, loss : 2.0477256774902344\n",
            "epoch: 3, classification_loss: 1.7525460720062256, Val Loss: 71.14383816719055, loss : 2.100790500640869\n",
            "epoch: 4, classification_loss: 1.7820451259613037, Val Loss: 71.09367775917053, loss : 2.089407444000244\n",
            "epoch: 5, classification_loss: 1.7706643342971802, Val Loss: 71.09364485740662, loss : 2.075291395187378\n",
            "epoch: 6, classification_loss: 1.7567986249923706, Val Loss: 71.15475463867188, loss : 2.0740323066711426\n",
            "epoch: 7, classification_loss: 1.7618045806884766, Val Loss: 71.10551476478577, loss : 2.0669190883636475\n",
            "epoch: 8, classification_loss: 1.7687509059906006, Val Loss: 71.06391215324402, loss : 2.0754218101501465\n",
            "epoch: 9, classification_loss: 1.749567985534668, Val Loss: 71.08873474597931, loss : 2.057232618331909\n",
            "epoch: 10, classification_loss: 1.7521578073501587, Val Loss: 71.08810436725616, loss : 2.0540523529052734\n",
            "epoch: 11, classification_loss: 1.7863658666610718, Val Loss: 71.12851202487946, loss : 2.079585313796997\n",
            "epoch: 12, classification_loss: 1.7799395322799683, Val Loss: 71.22503018379211, loss : 2.0703628063201904\n",
            "epoch: 13, classification_loss: 1.7754777669906616, Val Loss: 71.15574085712433, loss : 2.0615644454956055\n",
            "epoch: 14, classification_loss: 1.764336109161377, Val Loss: 71.10640776157379, loss : 2.0573582649230957\n",
            "epoch: 15, classification_loss: 1.7761350870132446, Val Loss: 71.08165717124939, loss : 2.0621824264526367\n",
            "epoch: 16, classification_loss: 1.7581939697265625, Val Loss: 71.13302683830261, loss : 2.053626537322998\n",
            "epoch: 17, classification_loss: 1.7644535303115845, Val Loss: 71.09946763515472, loss : 2.046912431716919\n",
            "epoch: 18, classification_loss: 1.7571914196014404, Val Loss: 71.14598035812378, loss : 2.0446038246154785\n",
            "epoch: 19, classification_loss: 1.7591735124588013, Val Loss: 71.11234474182129, loss : 2.0392518043518066\n",
            "Batch: 20, Test Acc: 0.5648036858974359\n",
            "Batch: 21:\n",
            "epoch: 0, classification_loss: 1.8270288705825806, Val Loss: 71.1203920841217, loss : 1.8270288705825806\n",
            "epoch: 1, classification_loss: 1.7867770195007324, Val Loss: 71.1090658903122, loss : 2.172280788421631\n",
            "epoch: 2, classification_loss: 1.760703444480896, Val Loss: 71.13638114929199, loss : 2.055187702178955\n",
            "epoch: 3, classification_loss: 1.7882124185562134, Val Loss: 71.1446875333786, loss : 2.0978074073791504\n",
            "epoch: 4, classification_loss: 1.8233815431594849, Val Loss: 71.09314024448395, loss : 2.105713129043579\n",
            "epoch: 5, classification_loss: 1.8005645275115967, Val Loss: 71.05806946754456, loss : 2.0780298709869385\n",
            "epoch: 6, classification_loss: 1.7953790426254272, Val Loss: 71.11306619644165, loss : 2.087672710418701\n",
            "epoch: 7, classification_loss: 1.8053237199783325, Val Loss: 71.16290092468262, loss : 2.0854310989379883\n",
            "epoch: 8, classification_loss: 1.8054574728012085, Val Loss: 71.15684521198273, loss : 2.0793561935424805\n",
            "epoch: 9, classification_loss: 1.7980360984802246, Val Loss: 71.11404550075531, loss : 2.0714943408966064\n",
            "epoch: 10, classification_loss: 1.802842378616333, Val Loss: 71.1045104265213, loss : 2.0723812580108643\n",
            "epoch: 11, classification_loss: 1.7942957878112793, Val Loss: 71.1636928319931, loss : 2.059070348739624\n",
            "epoch: 12, classification_loss: 1.8091944456100464, Val Loss: 71.13561737537384, loss : 2.0735127925872803\n",
            "epoch: 13, classification_loss: 1.7994577884674072, Val Loss: 71.12660002708435, loss : 2.0592169761657715\n",
            "epoch: 14, classification_loss: 1.7967524528503418, Val Loss: 71.17533898353577, loss : 2.060765266418457\n",
            "epoch: 15, classification_loss: 1.7980064153671265, Val Loss: 71.12483716011047, loss : 2.060818910598755\n",
            "epoch: 16, classification_loss: 1.7974356412887573, Val Loss: 71.13273072242737, loss : 2.054755687713623\n",
            "epoch: 17, classification_loss: 1.8088881969451904, Val Loss: 71.16309475898743, loss : 2.0628931522369385\n",
            "epoch: 18, classification_loss: 1.8039507865905762, Val Loss: 71.17603409290314, loss : 2.0622992515563965\n",
            "epoch: 19, classification_loss: 1.798757553100586, Val Loss: 71.12796700000763, loss : 2.0527586936950684\n",
            "Batch: 21, Test Acc: 0.5627003205128205\n",
            "Batch: 22:\n",
            "epoch: 0, classification_loss: 1.847536325454712, Val Loss: 71.09185469150543, loss : 1.847536325454712\n",
            "epoch: 1, classification_loss: 1.8000621795654297, Val Loss: 71.17005753517151, loss : 2.1867616176605225\n",
            "epoch: 2, classification_loss: 1.7759743928909302, Val Loss: 71.2392395734787, loss : 2.0880417823791504\n",
            "epoch: 3, classification_loss: 1.7844170331954956, Val Loss: 71.16091322898865, loss : 2.1309683322906494\n",
            "epoch: 4, classification_loss: 1.8061187267303467, Val Loss: 71.11282861232758, loss : 2.1080474853515625\n",
            "epoch: 5, classification_loss: 1.8056528568267822, Val Loss: 71.14475739002228, loss : 2.097952365875244\n",
            "epoch: 6, classification_loss: 1.8009231090545654, Val Loss: 71.17939078807831, loss : 2.1048855781555176\n",
            "epoch: 7, classification_loss: 1.8109184503555298, Val Loss: 71.13703119754791, loss : 2.1108503341674805\n",
            "epoch: 8, classification_loss: 1.794292688369751, Val Loss: 71.19697964191437, loss : 2.098008394241333\n",
            "epoch: 9, classification_loss: 1.7898865938186646, Val Loss: 71.1491447687149, loss : 2.0870773792266846\n",
            "epoch: 10, classification_loss: 1.8044145107269287, Val Loss: 71.14455151557922, loss : 2.098637342453003\n",
            "epoch: 11, classification_loss: 1.80299711227417, Val Loss: 71.18132936954498, loss : 2.088045597076416\n",
            "epoch: 12, classification_loss: 1.8043876886367798, Val Loss: 71.18030226230621, loss : 2.0890047550201416\n",
            "epoch: 13, classification_loss: 1.8091055154800415, Val Loss: 71.13674068450928, loss : 2.0770485401153564\n",
            "epoch: 14, classification_loss: 1.8180522918701172, Val Loss: 71.16918003559113, loss : 2.0889482498168945\n",
            "epoch: 15, classification_loss: 1.8032722473144531, Val Loss: 71.15228950977325, loss : 2.0788450241088867\n",
            "epoch: 16, classification_loss: 1.7960399389266968, Val Loss: 71.1502286195755, loss : 2.076160192489624\n",
            "epoch: 17, classification_loss: 1.7883638143539429, Val Loss: 71.18135976791382, loss : 2.0602316856384277\n",
            "epoch: 18, classification_loss: 1.7966465950012207, Val Loss: 71.18325698375702, loss : 2.0697169303894043\n",
            "epoch: 19, classification_loss: 1.801269292831421, Val Loss: 71.147620677948, loss : 2.073113441467285\n",
            "Batch: 22, Test Acc: 0.5640024038461539\n",
            "Batch: 23:\n",
            "epoch: 0, classification_loss: 1.8226286172866821, Val Loss: 71.35287594795227, loss : 1.8226286172866821\n",
            "epoch: 1, classification_loss: 1.785424828529358, Val Loss: 71.2547333240509, loss : 2.16646409034729\n",
            "epoch: 2, classification_loss: 1.7492706775665283, Val Loss: 71.10404205322266, loss : 2.0530500411987305\n",
            "epoch: 3, classification_loss: 1.7728393077850342, Val Loss: 71.14608836174011, loss : 2.0927937030792236\n",
            "epoch: 4, classification_loss: 1.8106800317764282, Val Loss: 71.1929942369461, loss : 2.092587471008301\n",
            "epoch: 5, classification_loss: 1.7926836013793945, Val Loss: 71.125039935112, loss : 2.077824831008911\n",
            "epoch: 6, classification_loss: 1.7970186471939087, Val Loss: 71.15307545661926, loss : 2.0865719318389893\n",
            "epoch: 7, classification_loss: 1.7967146635055542, Val Loss: 71.1782648563385, loss : 2.0759336948394775\n",
            "epoch: 8, classification_loss: 1.7942768335342407, Val Loss: 71.20901906490326, loss : 2.0723092555999756\n",
            "epoch: 9, classification_loss: 1.8078744411468506, Val Loss: 71.22251439094543, loss : 2.0929481983184814\n",
            "epoch: 10, classification_loss: 1.7907744646072388, Val Loss: 71.24913835525513, loss : 2.066983461380005\n",
            "epoch: 11, classification_loss: 1.7910627126693726, Val Loss: 71.20361006259918, loss : 2.068441390991211\n",
            "epoch: 12, classification_loss: 1.8032487630844116, Val Loss: 71.17864847183228, loss : 2.0660598278045654\n",
            "epoch: 13, classification_loss: 1.7934582233428955, Val Loss: 71.20724511146545, loss : 2.059170722961426\n",
            "epoch: 14, classification_loss: 1.7903087139129639, Val Loss: 71.21095740795135, loss : 2.0591788291931152\n",
            "epoch: 15, classification_loss: 1.8134359121322632, Val Loss: 71.21058714389801, loss : 2.0836985111236572\n",
            "epoch: 16, classification_loss: 1.8055331707000732, Val Loss: 71.25927126407623, loss : 2.077364683151245\n",
            "epoch: 17, classification_loss: 1.796612024307251, Val Loss: 71.25939095020294, loss : 2.0672152042388916\n",
            "epoch: 18, classification_loss: 1.7995203733444214, Val Loss: 71.19972884654999, loss : 2.065939426422119\n",
            "epoch: 19, classification_loss: 1.7989094257354736, Val Loss: 71.22837233543396, loss : 2.0596702098846436\n",
            "Batch: 23, Test Acc: 0.5644030448717948\n",
            "Batch: 24:\n",
            "epoch: 0, classification_loss: 1.8546345233917236, Val Loss: 71.25067460536957, loss : 1.8546345233917236\n",
            "epoch: 1, classification_loss: 1.8141000270843506, Val Loss: 71.22697710990906, loss : 2.19400954246521\n",
            "epoch: 2, classification_loss: 1.7873889207839966, Val Loss: 71.17555451393127, loss : 2.083893060684204\n",
            "epoch: 3, classification_loss: 1.808557152748108, Val Loss: 71.24220335483551, loss : 2.133840799331665\n",
            "epoch: 4, classification_loss: 1.8502216339111328, Val Loss: 71.15830504894257, loss : 2.136606454849243\n",
            "epoch: 5, classification_loss: 1.8405567407608032, Val Loss: 71.16401171684265, loss : 2.1285109519958496\n",
            "epoch: 6, classification_loss: 1.831455111503601, Val Loss: 71.23597240447998, loss : 2.1243767738342285\n",
            "epoch: 7, classification_loss: 1.8326983451843262, Val Loss: 71.28234052658081, loss : 2.122502326965332\n",
            "epoch: 8, classification_loss: 1.823159098625183, Val Loss: 71.15466713905334, loss : 2.1177470684051514\n",
            "epoch: 9, classification_loss: 1.822967290878296, Val Loss: 71.20697379112244, loss : 2.1087698936462402\n",
            "epoch: 10, classification_loss: 1.824362874031067, Val Loss: 71.30293953418732, loss : 2.104438304901123\n",
            "epoch: 11, classification_loss: 1.8151633739471436, Val Loss: 71.22816932201385, loss : 2.0964913368225098\n",
            "epoch: 12, classification_loss: 1.8266421556472778, Val Loss: 71.23583221435547, loss : 2.1035025119781494\n",
            "epoch: 13, classification_loss: 1.8340258598327637, Val Loss: 71.26563775539398, loss : 2.099748373031616\n",
            "epoch: 14, classification_loss: 1.823803186416626, Val Loss: 71.28363370895386, loss : 2.0945162773132324\n",
            "epoch: 15, classification_loss: 1.8385924100875854, Val Loss: 71.2484530210495, loss : 2.1080358028411865\n",
            "epoch: 16, classification_loss: 1.820282220840454, Val Loss: 71.28115129470825, loss : 2.0942604541778564\n",
            "epoch: 17, classification_loss: 1.8236901760101318, Val Loss: 71.29761242866516, loss : 2.0945587158203125\n",
            "epoch: 18, classification_loss: 1.8233726024627686, Val Loss: 71.30328965187073, loss : 2.0928943157196045\n",
            "epoch: 19, classification_loss: 1.8288019895553589, Val Loss: 71.31413209438324, loss : 2.0932459831237793\n",
            "Batch: 24, Test Acc: 0.5599959935897436\n",
            "Batch: 25:\n",
            "epoch: 0, classification_loss: 1.8124651908874512, Val Loss: 71.13253390789032, loss : 1.8124651908874512\n",
            "epoch: 1, classification_loss: 1.762397289276123, Val Loss: 71.16045641899109, loss : 2.140803575515747\n",
            "epoch: 2, classification_loss: 1.736929178237915, Val Loss: 71.19832193851471, loss : 2.030261993408203\n",
            "epoch: 3, classification_loss: 1.7693856954574585, Val Loss: 71.23545145988464, loss : 2.0855798721313477\n",
            "epoch: 4, classification_loss: 1.7871966361999512, Val Loss: 71.17154216766357, loss : 2.0677266120910645\n",
            "epoch: 5, classification_loss: 1.7895619869232178, Val Loss: 71.09895169734955, loss : 2.068373203277588\n",
            "epoch: 6, classification_loss: 1.7724711894989014, Val Loss: 71.13891422748566, loss : 2.064880847930908\n",
            "epoch: 7, classification_loss: 1.7835181951522827, Val Loss: 71.13663601875305, loss : 2.0577118396759033\n",
            "epoch: 8, classification_loss: 1.7711079120635986, Val Loss: 71.17072105407715, loss : 2.059148073196411\n",
            "epoch: 9, classification_loss: 1.7710392475128174, Val Loss: 71.14490628242493, loss : 2.050591230392456\n",
            "epoch: 10, classification_loss: 1.7821719646453857, Val Loss: 71.14468359947205, loss : 2.0569210052490234\n",
            "epoch: 11, classification_loss: 1.7729829549789429, Val Loss: 71.13233304023743, loss : 2.04312801361084\n",
            "epoch: 12, classification_loss: 1.7797468900680542, Val Loss: 71.18768572807312, loss : 2.050189971923828\n",
            "epoch: 13, classification_loss: 1.774364948272705, Val Loss: 71.16440665721893, loss : 2.0411932468414307\n",
            "epoch: 14, classification_loss: 1.7777975797653198, Val Loss: 71.1752108335495, loss : 2.0351061820983887\n",
            "epoch: 15, classification_loss: 1.7647593021392822, Val Loss: 71.1237884759903, loss : 2.030196189880371\n",
            "epoch: 16, classification_loss: 1.7595313787460327, Val Loss: 71.14182674884796, loss : 2.021620988845825\n",
            "epoch: 17, classification_loss: 1.771654486656189, Val Loss: 71.15207505226135, loss : 2.035534381866455\n",
            "epoch: 18, classification_loss: 1.7819318771362305, Val Loss: 71.15691602230072, loss : 2.037355661392212\n",
            "epoch: 19, classification_loss: 1.7787880897521973, Val Loss: 71.1703075170517, loss : 2.033529281616211\n",
            "Batch: 25, Test Acc: 0.5628004807692307\n",
            "Batch: 26:\n",
            "epoch: 0, classification_loss: 1.8212474584579468, Val Loss: 71.11548268795013, loss : 1.8212474584579468\n",
            "epoch: 1, classification_loss: 1.7901740074157715, Val Loss: 71.1073260307312, loss : 2.1495301723480225\n",
            "epoch: 2, classification_loss: 1.77005934715271, Val Loss: 71.11460411548615, loss : 2.047344446182251\n",
            "epoch: 3, classification_loss: 1.78745698928833, Val Loss: 71.1470011472702, loss : 2.093045234680176\n",
            "epoch: 4, classification_loss: 1.8213295936584473, Val Loss: 71.12279284000397, loss : 2.08316969871521\n",
            "epoch: 5, classification_loss: 1.8021601438522339, Val Loss: 71.13226735591888, loss : 2.0875630378723145\n",
            "epoch: 6, classification_loss: 1.7989954948425293, Val Loss: 71.11084318161011, loss : 2.0879929065704346\n",
            "epoch: 7, classification_loss: 1.7980753183364868, Val Loss: 71.18910002708435, loss : 2.08201265335083\n",
            "epoch: 8, classification_loss: 1.7958687543869019, Val Loss: 71.17741239070892, loss : 2.075669050216675\n",
            "epoch: 9, classification_loss: 1.796585202217102, Val Loss: 71.24696731567383, loss : 2.0722765922546387\n",
            "epoch: 10, classification_loss: 1.7871633768081665, Val Loss: 71.11664581298828, loss : 2.0642189979553223\n",
            "epoch: 11, classification_loss: 1.7996822595596313, Val Loss: 71.17162346839905, loss : 2.0645453929901123\n",
            "epoch: 12, classification_loss: 1.7957319021224976, Val Loss: 71.23198795318604, loss : 2.0636754035949707\n",
            "epoch: 13, classification_loss: 1.7950692176818848, Val Loss: 71.26059377193451, loss : 2.0632336139678955\n",
            "epoch: 14, classification_loss: 1.7974454164505005, Val Loss: 71.18205606937408, loss : 2.0598649978637695\n",
            "epoch: 15, classification_loss: 1.8089091777801514, Val Loss: 71.1778427362442, loss : 2.0679357051849365\n",
            "epoch: 16, classification_loss: 1.800041913986206, Val Loss: 71.16662096977234, loss : 2.0584676265716553\n",
            "epoch: 17, classification_loss: 1.8043124675750732, Val Loss: 71.1534241437912, loss : 2.067319393157959\n",
            "epoch: 18, classification_loss: 1.7925444841384888, Val Loss: 71.27905285358429, loss : 2.054718494415283\n",
            "epoch: 19, classification_loss: 1.8103135824203491, Val Loss: 71.21913862228394, loss : 2.070297956466675\n",
            "Batch: 26, Test Acc: 0.5657051282051282\n",
            "Batch: 27:\n",
            "epoch: 0, classification_loss: 1.845881700515747, Val Loss: 71.20004856586456, loss : 1.845881700515747\n",
            "epoch: 1, classification_loss: 1.7950959205627441, Val Loss: 71.17945861816406, loss : 2.1728909015655518\n",
            "epoch: 2, classification_loss: 1.7623573541641235, Val Loss: 71.12324130535126, loss : 2.0805747509002686\n",
            "epoch: 3, classification_loss: 1.7769408226013184, Val Loss: 71.10588836669922, loss : 2.1064693927764893\n",
            "epoch: 4, classification_loss: 1.8179939985275269, Val Loss: 71.1400636434555, loss : 2.1187357902526855\n",
            "epoch: 5, classification_loss: 1.804868459701538, Val Loss: 71.13877975940704, loss : 2.089305877685547\n",
            "epoch: 6, classification_loss: 1.800779938697815, Val Loss: 71.13232231140137, loss : 2.095156669616699\n",
            "epoch: 7, classification_loss: 1.8007723093032837, Val Loss: 71.13943576812744, loss : 2.0940206050872803\n",
            "epoch: 8, classification_loss: 1.795973539352417, Val Loss: 71.17792463302612, loss : 2.0883588790893555\n",
            "epoch: 9, classification_loss: 1.7927005290985107, Val Loss: 71.12262463569641, loss : 2.087038993835449\n",
            "epoch: 10, classification_loss: 1.798060417175293, Val Loss: 71.11863768100739, loss : 2.0855119228363037\n",
            "epoch: 11, classification_loss: 1.7990919351577759, Val Loss: 71.07426261901855, loss : 2.0840744972229004\n",
            "epoch: 12, classification_loss: 1.7903995513916016, Val Loss: 71.16000723838806, loss : 2.0677664279937744\n",
            "epoch: 13, classification_loss: 1.7901513576507568, Val Loss: 71.14210605621338, loss : 2.0731048583984375\n",
            "epoch: 14, classification_loss: 1.800995945930481, Val Loss: 71.13462948799133, loss : 2.0781326293945312\n",
            "epoch: 15, classification_loss: 1.7979040145874023, Val Loss: 71.10727381706238, loss : 2.0789294242858887\n",
            "epoch: 16, classification_loss: 1.7994593381881714, Val Loss: 71.09292674064636, loss : 2.0790293216705322\n",
            "epoch: 17, classification_loss: 1.787716269493103, Val Loss: 71.10604393482208, loss : 2.065577983856201\n",
            "epoch: 18, classification_loss: 1.8005163669586182, Val Loss: 71.11274373531342, loss : 2.0764355659484863\n",
            "epoch: 19, classification_loss: 1.791387677192688, Val Loss: 71.1188679933548, loss : 2.07619047164917\n",
            "Batch: 27, Test Acc: 0.5653044871794872\n",
            "Batch: 28:\n",
            "epoch: 0, classification_loss: 1.796834111213684, Val Loss: 71.06392443180084, loss : 1.796834111213684\n",
            "epoch: 1, classification_loss: 1.7658954858779907, Val Loss: 71.12327921390533, loss : 2.131129741668701\n",
            "epoch: 2, classification_loss: 1.7290058135986328, Val Loss: 71.19224977493286, loss : 2.029055595397949\n",
            "epoch: 3, classification_loss: 1.7434308528900146, Val Loss: 71.16148698329926, loss : 2.0582025051116943\n",
            "epoch: 4, classification_loss: 1.7881063222885132, Val Loss: 71.06545424461365, loss : 2.060227394104004\n",
            "epoch: 5, classification_loss: 1.7663036584854126, Val Loss: 71.0599867105484, loss : 2.0461783409118652\n",
            "epoch: 6, classification_loss: 1.759650707244873, Val Loss: 71.1569105386734, loss : 1.759650707244873\n",
            "epoch: 7, classification_loss: 1.7292399406433105, Val Loss: 71.11741030216217, loss : 2.092730760574341\n",
            "epoch: 8, classification_loss: 1.7134748697280884, Val Loss: 71.08792269229889, loss : 2.0180211067199707\n",
            "epoch: 9, classification_loss: 1.7440073490142822, Val Loss: 71.11987328529358, loss : 2.0132317543029785\n",
            "epoch: 10, classification_loss: 1.7634007930755615, Val Loss: 71.10057866573334, loss : 2.0180470943450928\n",
            "epoch: 11, classification_loss: 1.7573130130767822, Val Loss: 71.08374118804932, loss : 2.0139083862304688\n",
            "epoch: 12, classification_loss: 1.7303800582885742, Val Loss: 71.10237562656403, loss : 2.0146381855010986\n",
            "epoch: 13, classification_loss: 1.7433222532272339, Val Loss: 71.14030659198761, loss : 2.0183303356170654\n",
            "epoch: 14, classification_loss: 1.7370023727416992, Val Loss: 71.12538838386536, loss : 2.0201306343078613\n",
            "epoch: 15, classification_loss: 1.7518775463104248, Val Loss: 71.08949387073517, loss : 2.00705885887146\n",
            "epoch: 16, classification_loss: 1.7535480260849, Val Loss: 71.09549605846405, loss : 2.0299975872039795\n",
            "epoch: 17, classification_loss: 1.7386175394058228, Val Loss: 71.09734737873077, loss : 2.005711078643799\n",
            "epoch: 18, classification_loss: 1.7430554628372192, Val Loss: 71.11349701881409, loss : 2.0154740810394287\n",
            "epoch: 19, classification_loss: 1.7378902435302734, Val Loss: 71.11673498153687, loss : 1.9989019632339478\n",
            "Batch: 28, Test Acc: 0.5661057692307693\n",
            "Batch: 29:\n",
            "epoch: 0, classification_loss: 1.854611873626709, Val Loss: 71.11188554763794, loss : 1.854611873626709\n",
            "epoch: 1, classification_loss: 1.7955390214920044, Val Loss: 71.13320004940033, loss : 2.177661418914795\n",
            "epoch: 2, classification_loss: 1.766731858253479, Val Loss: 71.06464302539825, loss : 2.0899524688720703\n",
            "epoch: 3, classification_loss: 1.7895509004592896, Val Loss: 71.10947299003601, loss : 2.112321376800537\n",
            "epoch: 4, classification_loss: 1.8240634202957153, Val Loss: 71.15807592868805, loss : 2.1268835067749023\n",
            "epoch: 5, classification_loss: 1.8220282793045044, Val Loss: 71.07960987091064, loss : 2.0982534885406494\n",
            "epoch: 6, classification_loss: 1.8001965284347534, Val Loss: 71.08125519752502, loss : 2.1079154014587402\n",
            "epoch: 7, classification_loss: 1.8003629446029663, Val Loss: 71.13058686256409, loss : 2.084719657897949\n",
            "epoch: 8, classification_loss: 1.8161585330963135, Val Loss: 71.08124649524689, loss : 2.1032395362854004\n",
            "epoch: 9, classification_loss: 1.8094422817230225, Val Loss: 71.11132335662842, loss : 2.090191125869751\n",
            "epoch: 10, classification_loss: 1.81300687789917, Val Loss: 71.09203708171844, loss : 2.1041858196258545\n",
            "epoch: 11, classification_loss: 1.813433051109314, Val Loss: 71.0799617767334, loss : 2.098422050476074\n",
            "epoch: 12, classification_loss: 1.7997803688049316, Val Loss: 71.0765506029129, loss : 2.0807695388793945\n",
            "epoch: 13, classification_loss: 1.818691611289978, Val Loss: 71.12448239326477, loss : 2.0975160598754883\n",
            "epoch: 14, classification_loss: 1.8233686685562134, Val Loss: 71.10258793830872, loss : 2.0883545875549316\n",
            "epoch: 15, classification_loss: 1.8128324747085571, Val Loss: 71.05791699886322, loss : 2.091299533843994\n",
            "epoch: 16, classification_loss: 1.8170136213302612, Val Loss: 71.13019704818726, loss : 2.0845303535461426\n",
            "epoch: 17, classification_loss: 1.8208342790603638, Val Loss: 71.09487652778625, loss : 2.0941011905670166\n",
            "epoch: 18, classification_loss: 1.811873197555542, Val Loss: 71.04053199291229, loss : 2.0773470401763916\n",
            "epoch: 19, classification_loss: 1.819365382194519, Val Loss: 71.08092749118805, loss : 1.819365382194519\n",
            "Batch: 29, Test Acc: 0.5640024038461539\n",
            "Batch: 30:\n",
            "epoch: 0, classification_loss: 1.8516331911087036, Val Loss: 71.056356549263, loss : 1.8516331911087036\n",
            "epoch: 1, classification_loss: 1.8186326026916504, Val Loss: 71.04357016086578, loss : 2.1884381771087646\n",
            "epoch: 2, classification_loss: 1.7911144495010376, Val Loss: 71.07120871543884, loss : 2.100813627243042\n",
            "epoch: 3, classification_loss: 1.8080050945281982, Val Loss: 71.14556407928467, loss : 2.1335902214050293\n",
            "epoch: 4, classification_loss: 1.8403406143188477, Val Loss: 71.079594373703, loss : 2.134131908416748\n",
            "epoch: 5, classification_loss: 1.8321813344955444, Val Loss: 71.0253074169159, loss : 2.1098477840423584\n",
            "epoch: 6, classification_loss: 1.827314019203186, Val Loss: 71.08903336524963, loss : 2.1093850135803223\n",
            "epoch: 7, classification_loss: 1.818853735923767, Val Loss: 71.14765965938568, loss : 2.1039910316467285\n",
            "epoch: 8, classification_loss: 1.8229789733886719, Val Loss: 71.14678800106049, loss : 2.1087546348571777\n",
            "epoch: 9, classification_loss: 1.8173606395721436, Val Loss: 71.07882046699524, loss : 2.1082420349121094\n",
            "epoch: 10, classification_loss: 1.81606924533844, Val Loss: 71.09062433242798, loss : 2.1034653186798096\n",
            "epoch: 11, classification_loss: 1.8247737884521484, Val Loss: 71.10959351062775, loss : 2.1071999073028564\n",
            "epoch: 12, classification_loss: 1.8301513195037842, Val Loss: 71.09021127223969, loss : 2.101827621459961\n",
            "epoch: 13, classification_loss: 1.8285832405090332, Val Loss: 71.11427462100983, loss : 2.103196144104004\n",
            "epoch: 14, classification_loss: 1.8257912397384644, Val Loss: 71.19634699821472, loss : 2.0912764072418213\n",
            "epoch: 15, classification_loss: 1.8268470764160156, Val Loss: 71.19468307495117, loss : 2.0983264446258545\n",
            "epoch: 16, classification_loss: 1.8162877559661865, Val Loss: 71.11184215545654, loss : 2.079484462738037\n",
            "epoch: 17, classification_loss: 1.814821720123291, Val Loss: 71.09523463249207, loss : 2.080655574798584\n",
            "epoch: 18, classification_loss: 1.8322619199752808, Val Loss: 71.14384329319, loss : 2.0931739807128906\n",
            "epoch: 19, classification_loss: 1.817726969718933, Val Loss: 71.10567331314087, loss : 2.0792417526245117\n",
            "Batch: 30, Test Acc: 0.5638020833333334\n",
            "Batch: 31:\n",
            "epoch: 0, classification_loss: 1.8517000675201416, Val Loss: 71.0495593547821, loss : 1.8517000675201416\n",
            "epoch: 1, classification_loss: 1.8158456087112427, Val Loss: 71.08524227142334, loss : 2.169982671737671\n",
            "epoch: 2, classification_loss: 1.7946012020111084, Val Loss: 71.12056124210358, loss : 2.0568065643310547\n",
            "epoch: 3, classification_loss: 1.8244277238845825, Val Loss: 71.10169684886932, loss : 2.1132586002349854\n",
            "epoch: 4, classification_loss: 1.8319742679595947, Val Loss: 71.06150698661804, loss : 2.0795953273773193\n",
            "epoch: 5, classification_loss: 1.8368779420852661, Val Loss: 71.0948988199234, loss : 2.1109604835510254\n",
            "epoch: 6, classification_loss: 1.8177745342254639, Val Loss: 71.14531874656677, loss : 2.1071367263793945\n",
            "epoch: 7, classification_loss: 1.8225632905960083, Val Loss: 71.11441254615784, loss : 2.089059352874756\n",
            "epoch: 8, classification_loss: 1.838156819343567, Val Loss: 71.08662295341492, loss : 2.1082377433776855\n",
            "epoch: 9, classification_loss: 1.8249684572219849, Val Loss: 71.0989420413971, loss : 2.0899710655212402\n",
            "epoch: 10, classification_loss: 1.8242663145065308, Val Loss: 71.1121369600296, loss : 2.0819361209869385\n",
            "epoch: 11, classification_loss: 1.824994683265686, Val Loss: 71.10288393497467, loss : 2.085124969482422\n",
            "epoch: 12, classification_loss: 1.8272627592086792, Val Loss: 71.12043726444244, loss : 2.079357862472534\n",
            "epoch: 13, classification_loss: 1.8324918746948242, Val Loss: 71.13484489917755, loss : 2.0920324325561523\n",
            "epoch: 14, classification_loss: 1.8190865516662598, Val Loss: 71.13284742832184, loss : 2.0743391513824463\n",
            "epoch: 15, classification_loss: 1.8256914615631104, Val Loss: 71.11502623558044, loss : 2.0819666385650635\n",
            "epoch: 16, classification_loss: 1.8327666521072388, Val Loss: 71.10231709480286, loss : 2.082122325897217\n",
            "epoch: 17, classification_loss: 1.8387246131896973, Val Loss: 71.12328433990479, loss : 2.0816304683685303\n",
            "epoch: 18, classification_loss: 1.841481328010559, Val Loss: 71.14831876754761, loss : 2.0809717178344727\n",
            "epoch: 19, classification_loss: 1.826980710029602, Val Loss: 71.12270843982697, loss : 2.070436716079712\n",
            "Batch: 31, Test Acc: 0.5642027243589743\n",
            "Batch: 32:\n",
            "epoch: 0, classification_loss: 1.8374344110488892, Val Loss: 71.11813700199127, loss : 1.8374344110488892\n",
            "epoch: 1, classification_loss: 1.7924779653549194, Val Loss: 71.12999784946442, loss : 2.1522819995880127\n",
            "epoch: 2, classification_loss: 1.7562652826309204, Val Loss: 71.13596904277802, loss : 2.045640707015991\n",
            "epoch: 3, classification_loss: 1.7808043956756592, Val Loss: 71.06480753421783, loss : 2.0722787380218506\n",
            "epoch: 4, classification_loss: 1.8205885887145996, Val Loss: 71.08076167106628, loss : 2.0819694995880127\n",
            "epoch: 5, classification_loss: 1.7967338562011719, Val Loss: 71.04712843894958, loss : 2.0696427822113037\n",
            "epoch: 6, classification_loss: 1.7935664653778076, Val Loss: 71.08523547649384, loss : 2.069082021713257\n",
            "epoch: 7, classification_loss: 1.7997965812683105, Val Loss: 71.11889731884003, loss : 2.074608564376831\n",
            "epoch: 8, classification_loss: 1.7946444749832153, Val Loss: 71.10691106319427, loss : 2.066697120666504\n",
            "epoch: 9, classification_loss: 1.794533371925354, Val Loss: 71.03073728084564, loss : 2.0730068683624268\n",
            "epoch: 10, classification_loss: 1.7920467853546143, Val Loss: 71.06412589550018, loss : 2.0506556034088135\n",
            "epoch: 11, classification_loss: 1.8095899820327759, Val Loss: 71.1214748620987, loss : 2.072566509246826\n",
            "epoch: 12, classification_loss: 1.8059173822402954, Val Loss: 71.05563461780548, loss : 2.064976215362549\n",
            "epoch: 13, classification_loss: 1.798877239227295, Val Loss: 71.03375256061554, loss : 2.0612072944641113\n",
            "epoch: 14, classification_loss: 1.803704023361206, Val Loss: 71.08076810836792, loss : 2.0588886737823486\n",
            "epoch: 15, classification_loss: 1.8064217567443848, Val Loss: 71.04388332366943, loss : 2.0702879428863525\n",
            "epoch: 16, classification_loss: 1.802964448928833, Val Loss: 71.0635883808136, loss : 2.0631558895111084\n",
            "epoch: 17, classification_loss: 1.7937777042388916, Val Loss: 71.10954201221466, loss : 2.063138008117676\n",
            "epoch: 18, classification_loss: 1.8005902767181396, Val Loss: 71.0834230184555, loss : 2.049901008605957\n",
            "epoch: 19, classification_loss: 1.7891696691513062, Val Loss: 71.08521342277527, loss : 2.0494918823242188\n",
            "Batch: 32, Test Acc: 0.5658052884615384\n",
            "Batch: 33:\n",
            "epoch: 0, classification_loss: 1.8627372980117798, Val Loss: 71.09130299091339, loss : 1.8627372980117798\n",
            "epoch: 1, classification_loss: 1.8022886514663696, Val Loss: 71.17463898658752, loss : 2.155473470687866\n",
            "epoch: 2, classification_loss: 1.7785178422927856, Val Loss: 71.17704451084137, loss : 2.071899652481079\n",
            "epoch: 3, classification_loss: 1.8005640506744385, Val Loss: 71.15104734897614, loss : 2.1137871742248535\n",
            "epoch: 4, classification_loss: 1.815562129020691, Val Loss: 71.10768604278564, loss : 2.0840625762939453\n",
            "epoch: 5, classification_loss: 1.8263773918151855, Val Loss: 71.0629894733429, loss : 2.11137056350708\n",
            "epoch: 6, classification_loss: 1.8253451585769653, Val Loss: 71.12013828754425, loss : 2.1011905670166016\n",
            "epoch: 7, classification_loss: 1.8174134492874146, Val Loss: 71.161829829216, loss : 2.0934853553771973\n",
            "epoch: 8, classification_loss: 1.81778085231781, Val Loss: 71.1706895828247, loss : 2.0864291191101074\n",
            "epoch: 9, classification_loss: 1.817413091659546, Val Loss: 71.12143862247467, loss : 2.0965869426727295\n",
            "epoch: 10, classification_loss: 1.7986499071121216, Val Loss: 71.12152516841888, loss : 2.077462911605835\n",
            "epoch: 11, classification_loss: 1.8222862482070923, Val Loss: 71.19591093063354, loss : 2.0934629440307617\n",
            "epoch: 12, classification_loss: 1.808113694190979, Val Loss: 71.1317652463913, loss : 2.0777409076690674\n",
            "epoch: 13, classification_loss: 1.8229296207427979, Val Loss: 71.15587186813354, loss : 2.086249589920044\n",
            "epoch: 14, classification_loss: 1.8173136711120605, Val Loss: 71.19664001464844, loss : 2.0796148777008057\n",
            "epoch: 15, classification_loss: 1.8248684406280518, Val Loss: 71.15919899940491, loss : 2.0849056243896484\n",
            "epoch: 16, classification_loss: 1.8249590396881104, Val Loss: 71.17331838607788, loss : 2.0839767456054688\n",
            "epoch: 17, classification_loss: 1.8101106882095337, Val Loss: 71.19279599189758, loss : 2.072657823562622\n",
            "epoch: 18, classification_loss: 1.8040109872817993, Val Loss: 71.15276288986206, loss : 2.063666343688965\n",
            "epoch: 19, classification_loss: 1.8117189407348633, Val Loss: 71.14949548244476, loss : 2.069415807723999\n",
            "Batch: 33, Test Acc: 0.5653044871794872\n",
            "Batch: 34:\n",
            "epoch: 0, classification_loss: 1.8567441701889038, Val Loss: 71.03022277355194, loss : 1.8567441701889038\n",
            "epoch: 1, classification_loss: 1.7996238470077515, Val Loss: 71.03783357143402, loss : 2.1586356163024902\n",
            "epoch: 2, classification_loss: 1.7777000665664673, Val Loss: 71.14904248714447, loss : 2.0573108196258545\n",
            "epoch: 3, classification_loss: 1.7878036499023438, Val Loss: 71.10096335411072, loss : 2.0971457958221436\n",
            "epoch: 4, classification_loss: 1.8286153078079224, Val Loss: 71.04967832565308, loss : 2.09006667137146\n",
            "epoch: 5, classification_loss: 1.8210327625274658, Val Loss: 71.05364990234375, loss : 2.0983195304870605\n",
            "epoch: 6, classification_loss: 1.7939494848251343, Val Loss: 71.07968878746033, loss : 2.085374116897583\n",
            "epoch: 7, classification_loss: 1.8152626752853394, Val Loss: 71.10404777526855, loss : 2.087707281112671\n",
            "epoch: 8, classification_loss: 1.8145753145217896, Val Loss: 71.07124269008636, loss : 2.088408946990967\n",
            "epoch: 9, classification_loss: 1.8121953010559082, Val Loss: 71.05741786956787, loss : 2.079296112060547\n",
            "epoch: 10, classification_loss: 1.810092806816101, Val Loss: 71.12190079689026, loss : 2.0773508548736572\n",
            "epoch: 11, classification_loss: 1.8190909624099731, Val Loss: 71.10537648200989, loss : 2.077598810195923\n",
            "epoch: 12, classification_loss: 1.8088446855545044, Val Loss: 71.09507691860199, loss : 2.063211441040039\n",
            "epoch: 13, classification_loss: 1.8276267051696777, Val Loss: 71.07907915115356, loss : 2.081321954727173\n",
            "epoch: 14, classification_loss: 1.8273558616638184, Val Loss: 71.07339656352997, loss : 2.07426381111145\n",
            "epoch: 15, classification_loss: 1.8322142362594604, Val Loss: 71.08736276626587, loss : 2.084251642227173\n",
            "epoch: 16, classification_loss: 1.823595404624939, Val Loss: 71.11250972747803, loss : 2.074860095977783\n",
            "epoch: 17, classification_loss: 1.8212422132492065, Val Loss: 71.10782849788666, loss : 2.0758442878723145\n",
            "epoch: 18, classification_loss: 1.822228193283081, Val Loss: 71.09713470935822, loss : 2.073054552078247\n",
            "epoch: 19, classification_loss: 1.818293809890747, Val Loss: 71.14916515350342, loss : 2.0706090927124023\n",
            "Batch: 34, Test Acc: 0.5660056089743589\n",
            "Batch: 35:\n",
            "epoch: 0, classification_loss: 1.8815635442733765, Val Loss: 71.2017252445221, loss : 1.8815635442733765\n",
            "epoch: 1, classification_loss: 1.833767056465149, Val Loss: 71.13348543643951, loss : 2.183990716934204\n",
            "epoch: 2, classification_loss: 1.8140103816986084, Val Loss: 71.10180246829987, loss : 2.0844626426696777\n",
            "epoch: 3, classification_loss: 1.838564157485962, Val Loss: 71.14166367053986, loss : 2.1435515880584717\n",
            "epoch: 4, classification_loss: 1.8436051607131958, Val Loss: 71.0621737241745, loss : 2.1194610595703125\n",
            "epoch: 5, classification_loss: 1.8457461595535278, Val Loss: 71.09323787689209, loss : 2.1185200214385986\n",
            "epoch: 6, classification_loss: 1.8586523532867432, Val Loss: 71.08663856983185, loss : 2.1277871131896973\n",
            "epoch: 7, classification_loss: 1.8460776805877686, Val Loss: 71.0835953950882, loss : 2.1183269023895264\n",
            "epoch: 8, classification_loss: 1.8452988862991333, Val Loss: 71.08999514579773, loss : 2.109830856323242\n",
            "epoch: 9, classification_loss: 1.8336548805236816, Val Loss: 71.12162613868713, loss : 2.1026275157928467\n",
            "epoch: 10, classification_loss: 1.840023159980774, Val Loss: 71.07001543045044, loss : 2.1118626594543457\n",
            "epoch: 11, classification_loss: 1.8420580625534058, Val Loss: 71.15310192108154, loss : 2.101656675338745\n",
            "epoch: 12, classification_loss: 1.8499455451965332, Val Loss: 71.10414588451385, loss : 2.1064605712890625\n",
            "epoch: 13, classification_loss: 1.8463044166564941, Val Loss: 71.07244575023651, loss : 2.099106788635254\n",
            "epoch: 14, classification_loss: 1.8452035188674927, Val Loss: 71.06195175647736, loss : 2.10225772857666\n",
            "epoch: 15, classification_loss: 1.8467319011688232, Val Loss: 71.07439124584198, loss : 2.1001853942871094\n",
            "epoch: 16, classification_loss: 1.8433603048324585, Val Loss: 71.10200536251068, loss : 2.102445602416992\n",
            "epoch: 17, classification_loss: 1.8409411907196045, Val Loss: 71.11446368694305, loss : 2.090988874435425\n",
            "epoch: 18, classification_loss: 1.8447904586791992, Val Loss: 71.11190724372864, loss : 2.094550371170044\n",
            "epoch: 19, classification_loss: 1.8588647842407227, Val Loss: 71.07380855083466, loss : 2.1111629009246826\n",
            "Batch: 35, Test Acc: 0.5660056089743589\n",
            "Batch: 36:\n",
            "epoch: 0, classification_loss: 1.7849957942962646, Val Loss: 71.3419463634491, loss : 1.7849957942962646\n",
            "epoch: 1, classification_loss: 1.7399184703826904, Val Loss: 71.21906840801239, loss : 2.1015758514404297\n",
            "epoch: 2, classification_loss: 1.698611855506897, Val Loss: 71.14465379714966, loss : 1.9947247505187988\n",
            "epoch: 3, classification_loss: 1.7321460247039795, Val Loss: 71.1353188753128, loss : 2.041349411010742\n",
            "epoch: 4, classification_loss: 1.7620408535003662, Val Loss: 71.15716779232025, loss : 2.039027214050293\n",
            "epoch: 5, classification_loss: 1.7533729076385498, Val Loss: 71.10794389247894, loss : 2.021775245666504\n",
            "epoch: 6, classification_loss: 1.7435529232025146, Val Loss: 71.10183691978455, loss : 2.029557704925537\n",
            "epoch: 7, classification_loss: 1.7519540786743164, Val Loss: 71.17686212062836, loss : 2.0302274227142334\n",
            "epoch: 8, classification_loss: 1.7456202507019043, Val Loss: 71.14421212673187, loss : 2.031240224838257\n",
            "epoch: 9, classification_loss: 1.7467142343521118, Val Loss: 71.10485541820526, loss : 2.015651226043701\n",
            "epoch: 10, classification_loss: 1.7309632301330566, Val Loss: 71.12674963474274, loss : 2.003927707672119\n",
            "epoch: 11, classification_loss: 1.7460204362869263, Val Loss: 71.16855931282043, loss : 2.0080528259277344\n",
            "epoch: 12, classification_loss: 1.7369346618652344, Val Loss: 71.13531947135925, loss : 2.0008175373077393\n",
            "epoch: 13, classification_loss: 1.7563961744308472, Val Loss: 71.11888301372528, loss : 2.0112624168395996\n",
            "epoch: 14, classification_loss: 1.7631964683532715, Val Loss: 71.14610290527344, loss : 2.0155961513519287\n",
            "epoch: 15, classification_loss: 1.7445704936981201, Val Loss: 71.1606057882309, loss : 2.0039567947387695\n",
            "epoch: 16, classification_loss: 1.751213788986206, Val Loss: 71.18532156944275, loss : 2.0083751678466797\n",
            "epoch: 17, classification_loss: 1.7512327432632446, Val Loss: 71.16015493869781, loss : 2.0144271850585938\n",
            "epoch: 18, classification_loss: 1.7432643175125122, Val Loss: 71.17350447177887, loss : 1.9964044094085693\n",
            "epoch: 19, classification_loss: 1.758775234222412, Val Loss: 71.16676807403564, loss : 2.0184385776519775\n",
            "Batch: 36, Test Acc: 0.5630008012820513\n",
            "Batch: 37:\n",
            "epoch: 0, classification_loss: 1.8378652334213257, Val Loss: 71.09730744361877, loss : 1.8378652334213257\n",
            "epoch: 1, classification_loss: 1.7915886640548706, Val Loss: 71.08027422428131, loss : 2.1370737552642822\n",
            "epoch: 2, classification_loss: 1.7812896966934204, Val Loss: 71.09022963047028, loss : 2.042288303375244\n",
            "epoch: 3, classification_loss: 1.8015556335449219, Val Loss: 71.08955073356628, loss : 2.0966858863830566\n",
            "epoch: 4, classification_loss: 1.811795711517334, Val Loss: 71.08928275108337, loss : 2.0579123497009277\n",
            "epoch: 5, classification_loss: 1.8141509294509888, Val Loss: 71.06278920173645, loss : 2.0927741527557373\n",
            "epoch: 6, classification_loss: 1.7995885610580444, Val Loss: 71.06802046298981, loss : 2.077291965484619\n",
            "epoch: 7, classification_loss: 1.805391550064087, Val Loss: 71.0647257566452, loss : 2.076270818710327\n",
            "epoch: 8, classification_loss: 1.8097542524337769, Val Loss: 71.11756992340088, loss : 2.077362298965454\n",
            "epoch: 9, classification_loss: 1.806395173072815, Val Loss: 71.08926272392273, loss : 2.068160057067871\n",
            "epoch: 10, classification_loss: 1.8014954328536987, Val Loss: 71.07851111888885, loss : 2.0520412921905518\n",
            "epoch: 11, classification_loss: 1.8118042945861816, Val Loss: 71.07753014564514, loss : 2.0579257011413574\n",
            "epoch: 12, classification_loss: 1.8020554780960083, Val Loss: 71.1188725233078, loss : 2.0532286167144775\n",
            "epoch: 13, classification_loss: 1.7956819534301758, Val Loss: 71.06690382957458, loss : 2.047800064086914\n",
            "epoch: 14, classification_loss: 1.804642915725708, Val Loss: 71.11393582820892, loss : 2.051665782928467\n",
            "epoch: 15, classification_loss: 1.8034162521362305, Val Loss: 71.11357009410858, loss : 2.046048164367676\n",
            "epoch: 16, classification_loss: 1.8073304891586304, Val Loss: 71.10670125484467, loss : 2.0507965087890625\n",
            "epoch: 17, classification_loss: 1.7960498332977295, Val Loss: 71.1030912399292, loss : 2.0376195907592773\n",
            "epoch: 18, classification_loss: 1.8122236728668213, Val Loss: 71.10692405700684, loss : 2.046457290649414\n",
            "epoch: 19, classification_loss: 1.8074843883514404, Val Loss: 71.07817113399506, loss : 2.04423189163208\n",
            "Batch: 37, Test Acc: 0.56640625\n",
            "Batch: 38:\n",
            "epoch: 0, classification_loss: 1.8520700931549072, Val Loss: 71.08443093299866, loss : 1.8520700931549072\n",
            "epoch: 1, classification_loss: 1.805205225944519, Val Loss: 71.03718197345734, loss : 2.1587328910827637\n",
            "epoch: 2, classification_loss: 1.7778240442276, Val Loss: 71.06463015079498, loss : 2.046898365020752\n",
            "epoch: 3, classification_loss: 1.8172463178634644, Val Loss: 71.07519209384918, loss : 2.1133742332458496\n",
            "epoch: 4, classification_loss: 1.8321961164474487, Val Loss: 71.0590215921402, loss : 2.088310718536377\n",
            "epoch: 5, classification_loss: 1.819494366645813, Val Loss: 71.0527195930481, loss : 2.0921154022216797\n",
            "epoch: 6, classification_loss: 1.813854455947876, Val Loss: 71.05836701393127, loss : 2.0903873443603516\n",
            "epoch: 7, classification_loss: 1.8207879066467285, Val Loss: 71.06296348571777, loss : 2.087144374847412\n",
            "epoch: 8, classification_loss: 1.8117817640304565, Val Loss: 71.05248737335205, loss : 2.0825679302215576\n",
            "epoch: 9, classification_loss: 1.8099254369735718, Val Loss: 71.05857646465302, loss : 2.0730388164520264\n",
            "epoch: 10, classification_loss: 1.826475739479065, Val Loss: 71.04538869857788, loss : 2.083813190460205\n",
            "epoch: 11, classification_loss: 1.8088098764419556, Val Loss: 71.07899761199951, loss : 2.0607333183288574\n",
            "epoch: 12, classification_loss: 1.8092200756072998, Val Loss: 71.06804013252258, loss : 2.06363582611084\n",
            "epoch: 13, classification_loss: 1.8165193796157837, Val Loss: 71.04834365844727, loss : 2.0715689659118652\n",
            "epoch: 14, classification_loss: 1.8198728561401367, Val Loss: 71.02953684329987, loss : 2.077392101287842\n",
            "epoch: 15, classification_loss: 1.8190945386886597, Val Loss: 71.06461775302887, loss : 2.0721468925476074\n",
            "epoch: 16, classification_loss: 1.8141486644744873, Val Loss: 71.08380508422852, loss : 2.0720813274383545\n",
            "epoch: 17, classification_loss: 1.8244414329528809, Val Loss: 71.06376373767853, loss : 2.0744872093200684\n",
            "epoch: 18, classification_loss: 1.8215082883834839, Val Loss: 71.08951318264008, loss : 2.0655832290649414\n",
            "epoch: 19, classification_loss: 1.823470115661621, Val Loss: 71.05167233943939, loss : 2.078563928604126\n",
            "Batch: 38, Test Acc: 0.5650040064102564\n",
            "Batch: 39:\n",
            "epoch: 0, classification_loss: 1.8156442642211914, Val Loss: 71.07043814659119, loss : 1.8156442642211914\n",
            "epoch: 1, classification_loss: 1.7783056497573853, Val Loss: 71.11571645736694, loss : 2.126495599746704\n",
            "epoch: 2, classification_loss: 1.760610580444336, Val Loss: 71.0650202035904, loss : 2.044414758682251\n",
            "epoch: 3, classification_loss: 1.764522910118103, Val Loss: 71.08911418914795, loss : 2.0678422451019287\n",
            "epoch: 4, classification_loss: 1.7836874723434448, Val Loss: 71.10407102108002, loss : 2.0524871349334717\n",
            "epoch: 5, classification_loss: 1.791727900505066, Val Loss: 71.06407928466797, loss : 2.0572128295898438\n",
            "epoch: 6, classification_loss: 1.7834352254867554, Val Loss: 71.15116047859192, loss : 2.0556583404541016\n",
            "epoch: 7, classification_loss: 1.7905642986297607, Val Loss: 71.12713515758514, loss : 2.0595319271087646\n",
            "epoch: 8, classification_loss: 1.7896480560302734, Val Loss: 71.11944627761841, loss : 2.056610584259033\n",
            "epoch: 9, classification_loss: 1.7884235382080078, Val Loss: 71.10193228721619, loss : 2.0513720512390137\n",
            "epoch: 10, classification_loss: 1.792742371559143, Val Loss: 71.12856030464172, loss : 2.056603193283081\n",
            "epoch: 11, classification_loss: 1.7951085567474365, Val Loss: 71.13792371749878, loss : 2.054086685180664\n",
            "epoch: 12, classification_loss: 1.776845932006836, Val Loss: 71.15041422843933, loss : 2.037618637084961\n",
            "epoch: 13, classification_loss: 1.8002774715423584, Val Loss: 71.198761343956, loss : 2.060617446899414\n",
            "epoch: 14, classification_loss: 1.7961097955703735, Val Loss: 71.14650392532349, loss : 2.04830002784729\n",
            "epoch: 15, classification_loss: 1.797006368637085, Val Loss: 71.15546834468842, loss : 2.0458667278289795\n",
            "epoch: 16, classification_loss: 1.7847694158554077, Val Loss: 71.1646249294281, loss : 2.0353591442108154\n",
            "epoch: 17, classification_loss: 1.7971343994140625, Val Loss: 71.17428207397461, loss : 2.0451035499572754\n",
            "epoch: 18, classification_loss: 1.8019371032714844, Val Loss: 71.16573536396027, loss : 2.051199197769165\n",
            "epoch: 19, classification_loss: 1.7932804822921753, Val Loss: 71.18815660476685, loss : 2.040071964263916\n",
            "Batch: 39, Test Acc: 0.5645032051282052\n",
            "Batch: 40:\n",
            "epoch: 0, classification_loss: 1.8481982946395874, Val Loss: 71.1099420785904, loss : 1.8481982946395874\n",
            "epoch: 1, classification_loss: 1.789633870124817, Val Loss: 71.11898899078369, loss : 2.1592462062835693\n",
            "epoch: 2, classification_loss: 1.7503167390823364, Val Loss: 71.13389074802399, loss : 2.0624823570251465\n",
            "epoch: 3, classification_loss: 1.7618129253387451, Val Loss: 71.15835320949554, loss : 2.089620590209961\n",
            "epoch: 4, classification_loss: 1.8094570636749268, Val Loss: 71.10727345943451, loss : 2.1024422645568848\n",
            "epoch: 5, classification_loss: 1.8045392036437988, Val Loss: 71.07783806324005, loss : 2.0951271057128906\n",
            "epoch: 6, classification_loss: 1.797776699066162, Val Loss: 71.05967402458191, loss : 2.0990476608276367\n",
            "epoch: 7, classification_loss: 1.805145025253296, Val Loss: 71.09828913211823, loss : 2.0983006954193115\n",
            "epoch: 8, classification_loss: 1.7914912700653076, Val Loss: 71.10217952728271, loss : 2.073850154876709\n",
            "epoch: 9, classification_loss: 1.8078110218048096, Val Loss: 71.14358532428741, loss : 2.093538284301758\n",
            "epoch: 10, classification_loss: 1.7913193702697754, Val Loss: 71.08250617980957, loss : 2.0781707763671875\n",
            "epoch: 11, classification_loss: 1.7913658618927002, Val Loss: 71.12825953960419, loss : 2.075639009475708\n",
            "epoch: 12, classification_loss: 1.7884726524353027, Val Loss: 71.24531018733978, loss : 2.0717804431915283\n",
            "epoch: 13, classification_loss: 1.798184871673584, Val Loss: 71.10002756118774, loss : 2.0796024799346924\n",
            "epoch: 14, classification_loss: 1.8006772994995117, Val Loss: 71.13962399959564, loss : 2.0640289783477783\n",
            "epoch: 15, classification_loss: 1.8135106563568115, Val Loss: 71.11319279670715, loss : 2.0840466022491455\n",
            "epoch: 16, classification_loss: 1.7999809980392456, Val Loss: 71.11666131019592, loss : 2.0647428035736084\n",
            "epoch: 17, classification_loss: 1.7807999849319458, Val Loss: 71.1338495016098, loss : 2.0514707565307617\n",
            "epoch: 18, classification_loss: 1.7987079620361328, Val Loss: 71.16705429553986, loss : 2.069709062576294\n",
            "epoch: 19, classification_loss: 1.795469880104065, Val Loss: 71.13472473621368, loss : 2.0711514949798584\n",
            "Batch: 40, Test Acc: 0.5634014423076923\n",
            "Batch: 41:\n",
            "epoch: 0, classification_loss: 1.8726141452789307, Val Loss: 71.13137173652649, loss : 1.8726141452789307\n",
            "epoch: 1, classification_loss: 1.8386136293411255, Val Loss: 71.13244795799255, loss : 2.185746669769287\n",
            "epoch: 2, classification_loss: 1.8006939888000488, Val Loss: 71.09089314937592, loss : 2.080096483230591\n",
            "epoch: 3, classification_loss: 1.8226982355117798, Val Loss: 71.14507699012756, loss : 2.1152498722076416\n",
            "epoch: 4, classification_loss: 1.8663311004638672, Val Loss: 71.08666467666626, loss : 2.120464563369751\n",
            "epoch: 5, classification_loss: 1.8531972169876099, Val Loss: 71.04372107982635, loss : 2.1165242195129395\n",
            "epoch: 6, classification_loss: 1.8283978700637817, Val Loss: 71.11865031719208, loss : 2.098612070083618\n",
            "epoch: 7, classification_loss: 1.8265774250030518, Val Loss: 71.13108241558075, loss : 2.102123975753784\n",
            "epoch: 8, classification_loss: 1.822503924369812, Val Loss: 71.1097264289856, loss : 2.094430923461914\n",
            "epoch: 9, classification_loss: 1.8412532806396484, Val Loss: 71.11817800998688, loss : 2.1103928089141846\n",
            "epoch: 10, classification_loss: 1.8312363624572754, Val Loss: 71.16974747180939, loss : 2.0899240970611572\n",
            "epoch: 11, classification_loss: 1.8411328792572021, Val Loss: 71.12369585037231, loss : 2.0911989212036133\n",
            "epoch: 12, classification_loss: 1.8447520732879639, Val Loss: 71.13634073734283, loss : 2.100271701812744\n",
            "epoch: 13, classification_loss: 1.8264702558517456, Val Loss: 71.0993002653122, loss : 2.0913543701171875\n",
            "epoch: 14, classification_loss: 1.8251657485961914, Val Loss: 71.15854871273041, loss : 2.0836215019226074\n",
            "epoch: 15, classification_loss: 1.8473446369171143, Val Loss: 71.1284157037735, loss : 2.0962297916412354\n",
            "epoch: 16, classification_loss: 1.8342105150222778, Val Loss: 71.11481249332428, loss : 2.089034080505371\n",
            "epoch: 17, classification_loss: 1.8333241939544678, Val Loss: 71.16922104358673, loss : 2.0826406478881836\n",
            "epoch: 18, classification_loss: 1.845751166343689, Val Loss: 71.19009983539581, loss : 2.091472625732422\n",
            "epoch: 19, classification_loss: 1.8306328058242798, Val Loss: 71.13278305530548, loss : 2.0825629234313965\n",
            "Batch: 41, Test Acc: 0.5653044871794872\n",
            "Batch: 42:\n",
            "epoch: 0, classification_loss: 1.8571547269821167, Val Loss: 71.0303680896759, loss : 1.8571547269821167\n",
            "epoch: 1, classification_loss: 1.802395224571228, Val Loss: 71.01596295833588, loss : 2.1480867862701416\n",
            "epoch: 2, classification_loss: 1.768900990486145, Val Loss: 71.07458221912384, loss : 1.768900990486145\n",
            "epoch: 3, classification_loss: 1.7211097478866577, Val Loss: 71.07441997528076, loss : 2.0822088718414307\n",
            "epoch: 4, classification_loss: 1.7099486589431763, Val Loss: 71.05473053455353, loss : 1.965912938117981\n",
            "epoch: 5, classification_loss: 1.7685656547546387, Val Loss: 71.05742609500885, loss : 2.0565106868743896\n",
            "epoch: 6, classification_loss: 1.7823915481567383, Val Loss: 71.06997311115265, loss : 2.0043745040893555\n",
            "epoch: 7, classification_loss: 1.7498023509979248, Val Loss: 71.09596335887909, loss : 2.061978578567505\n",
            "epoch: 8, classification_loss: 1.7352827787399292, Val Loss: 71.07441890239716, loss : 2.049492597579956\n",
            "epoch: 9, classification_loss: 1.7483879327774048, Val Loss: 71.0752363204956, loss : 2.0304455757141113\n",
            "epoch: 10, classification_loss: 1.7689942121505737, Val Loss: 71.07495892047882, loss : 2.0443179607391357\n",
            "epoch: 11, classification_loss: 1.7628141641616821, Val Loss: 71.0897126197815, loss : 2.024815320968628\n",
            "epoch: 12, classification_loss: 1.762097716331482, Val Loss: 71.07772886753082, loss : 2.0360615253448486\n",
            "epoch: 13, classification_loss: 1.7669599056243896, Val Loss: 71.08444333076477, loss : 2.023268461227417\n",
            "epoch: 14, classification_loss: 1.7626166343688965, Val Loss: 71.08779442310333, loss : 2.0242416858673096\n",
            "epoch: 15, classification_loss: 1.7551214694976807, Val Loss: 71.07786643505096, loss : 2.0074641704559326\n",
            "epoch: 16, classification_loss: 1.7577710151672363, Val Loss: 71.13388228416443, loss : 2.0150105953216553\n",
            "epoch: 17, classification_loss: 1.7621530294418335, Val Loss: 71.12104332447052, loss : 2.0051839351654053\n",
            "epoch: 18, classification_loss: 1.7622593641281128, Val Loss: 71.07070231437683, loss : 2.0065009593963623\n",
            "epoch: 19, classification_loss: 1.7652251720428467, Val Loss: 71.18454933166504, loss : 2.005601644515991\n",
            "Batch: 42, Test Acc: 0.5641025641025641\n",
            "Batch: 43:\n",
            "epoch: 0, classification_loss: 1.8602265119552612, Val Loss: 71.00212502479553, loss : 1.8602265119552612\n",
            "epoch: 1, classification_loss: 1.826385498046875, Val Loss: 71.15816235542297, loss : 1.826385498046875\n",
            "epoch: 2, classification_loss: 1.7594841718673706, Val Loss: 71.07902193069458, loss : 2.1527085304260254\n",
            "epoch: 3, classification_loss: 1.757104754447937, Val Loss: 71.02851974964142, loss : 2.0986011028289795\n",
            "epoch: 4, classification_loss: 1.7855186462402344, Val Loss: 71.02204608917236, loss : 2.0991241931915283\n",
            "epoch: 5, classification_loss: 1.8353935480117798, Val Loss: 71.06709444522858, loss : 2.114896774291992\n",
            "epoch: 6, classification_loss: 1.8202568292617798, Val Loss: 71.0899304151535, loss : 2.088116407394409\n",
            "epoch: 7, classification_loss: 1.77981698513031, Val Loss: 71.01454877853394, loss : 2.092442750930786\n",
            "epoch: 8, classification_loss: 1.781919002532959, Val Loss: 71.06275165081024, loss : 2.088256359100342\n",
            "epoch: 9, classification_loss: 1.7888081073760986, Val Loss: 71.14139938354492, loss : 2.0869061946868896\n",
            "epoch: 10, classification_loss: 1.7966432571411133, Val Loss: 71.05634701251984, loss : 2.072023391723633\n",
            "epoch: 11, classification_loss: 1.809637427330017, Val Loss: 71.04935252666473, loss : 2.0837597846984863\n",
            "epoch: 12, classification_loss: 1.7911971807479858, Val Loss: 71.03932201862335, loss : 2.0617048740386963\n",
            "epoch: 13, classification_loss: 1.7994111776351929, Val Loss: 71.0556960105896, loss : 2.058621644973755\n",
            "epoch: 14, classification_loss: 1.7947362661361694, Val Loss: 71.09640657901764, loss : 2.0614640712738037\n",
            "epoch: 15, classification_loss: 1.8030095100402832, Val Loss: 71.08748471736908, loss : 2.062709331512451\n",
            "epoch: 16, classification_loss: 1.7955024242401123, Val Loss: 71.05781769752502, loss : 2.0598716735839844\n",
            "epoch: 17, classification_loss: 1.7969176769256592, Val Loss: 71.03508567810059, loss : 2.05192232131958\n",
            "epoch: 18, classification_loss: 1.7953497171401978, Val Loss: 71.0761206150055, loss : 2.046506643295288\n",
            "epoch: 19, classification_loss: 1.7983256578445435, Val Loss: 71.08753633499146, loss : 2.049139976501465\n",
            "Batch: 43, Test Acc: 0.5662059294871795\n",
            "Batch: 44:\n",
            "epoch: 0, classification_loss: 1.8164457082748413, Val Loss: 71.07934725284576, loss : 1.8164457082748413\n",
            "epoch: 1, classification_loss: 1.7704737186431885, Val Loss: 71.10692131519318, loss : 2.127653121948242\n",
            "epoch: 2, classification_loss: 1.7561094760894775, Val Loss: 71.02951407432556, loss : 2.0473015308380127\n",
            "epoch: 3, classification_loss: 1.7780143022537231, Val Loss: 71.03111600875854, loss : 2.0824739933013916\n",
            "epoch: 4, classification_loss: 1.8046824932098389, Val Loss: 71.03889083862305, loss : 2.0783791542053223\n",
            "epoch: 5, classification_loss: 1.8024157285690308, Val Loss: 71.0481036901474, loss : 2.0707578659057617\n",
            "epoch: 6, classification_loss: 1.7899147272109985, Val Loss: 71.05339825153351, loss : 2.077298402786255\n",
            "epoch: 7, classification_loss: 1.7854775190353394, Val Loss: 71.0732147693634, loss : 2.063981533050537\n",
            "epoch: 8, classification_loss: 1.8034735918045044, Val Loss: 71.0319060087204, loss : 2.07757830619812\n",
            "epoch: 9, classification_loss: 1.7825536727905273, Val Loss: 70.99054765701294, loss : 2.0550599098205566\n",
            "epoch: 10, classification_loss: 1.788492202758789, Val Loss: 71.00325107574463, loss : 1.788492202758789\n",
            "epoch: 11, classification_loss: 1.7510066032409668, Val Loss: 71.07228326797485, loss : 2.1031248569488525\n",
            "epoch: 12, classification_loss: 1.7280477285385132, Val Loss: 71.03633141517639, loss : 2.005214214324951\n",
            "epoch: 13, classification_loss: 1.7690107822418213, Val Loss: 71.0675641298294, loss : 2.0559232234954834\n",
            "epoch: 14, classification_loss: 1.789878010749817, Val Loss: 71.00847709178925, loss : 2.054901361465454\n",
            "epoch: 15, classification_loss: 1.7751028537750244, Val Loss: 71.05328440666199, loss : 2.0374653339385986\n",
            "epoch: 16, classification_loss: 1.7554728984832764, Val Loss: 71.02867519855499, loss : 2.045475721359253\n",
            "epoch: 17, classification_loss: 1.7587552070617676, Val Loss: 71.10000133514404, loss : 2.029216766357422\n",
            "epoch: 18, classification_loss: 1.7700645923614502, Val Loss: 71.0176465511322, loss : 2.025702476501465\n",
            "epoch: 19, classification_loss: 1.75819730758667, Val Loss: 70.96827185153961, loss : 2.031456708908081\n",
            "Batch: 44, Test Acc: 0.5652043269230769\n",
            "Batch: 45:\n",
            "epoch: 0, classification_loss: 1.839235544204712, Val Loss: 70.97907543182373, loss : 1.839235544204712\n",
            "epoch: 1, classification_loss: 1.7832951545715332, Val Loss: 70.97042608261108, loss : 2.140387535095215\n",
            "epoch: 2, classification_loss: 1.7368658781051636, Val Loss: 71.02794206142426, loss : 2.0474295616149902\n",
            "epoch: 3, classification_loss: 1.7692828178405762, Val Loss: 71.05406999588013, loss : 2.0747921466827393\n",
            "epoch: 4, classification_loss: 1.8125959634780884, Val Loss: 70.98074209690094, loss : 2.0929176807403564\n",
            "epoch: 5, classification_loss: 1.799912929534912, Val Loss: 70.94994974136353, loss : 2.0624988079071045\n",
            "epoch: 6, classification_loss: 1.783828616142273, Val Loss: 71.03592562675476, loss : 1.783828616142273\n",
            "epoch: 7, classification_loss: 1.752450704574585, Val Loss: 71.01287198066711, loss : 2.1186399459838867\n",
            "epoch: 8, classification_loss: 1.726571798324585, Val Loss: 70.98565447330475, loss : 2.0450282096862793\n",
            "epoch: 9, classification_loss: 1.7661635875701904, Val Loss: 71.00419616699219, loss : 2.038280487060547\n",
            "epoch: 10, classification_loss: 1.8029026985168457, Val Loss: 70.96781575679779, loss : 2.0740346908569336\n",
            "epoch: 11, classification_loss: 1.778250813484192, Val Loss: 70.9534981250763, loss : 2.0237221717834473\n",
            "epoch: 12, classification_loss: 1.7670725584030151, Val Loss: 70.99403786659241, loss : 2.0621228218078613\n",
            "epoch: 13, classification_loss: 1.7628095149993896, Val Loss: 70.97532248497009, loss : 2.044218063354492\n",
            "epoch: 14, classification_loss: 1.7766934633255005, Val Loss: 70.96779215335846, loss : 2.054084539413452\n",
            "epoch: 15, classification_loss: 1.7680013179779053, Val Loss: 70.9231333732605, loss : 2.0379185676574707\n",
            "epoch: 16, classification_loss: 1.7679892778396606, Val Loss: 70.94684982299805, loss : 1.7679892778396606\n",
            "epoch: 17, classification_loss: 1.7447983026504517, Val Loss: 70.97212314605713, loss : 2.1004960536956787\n",
            "epoch: 18, classification_loss: 1.727370262145996, Val Loss: 70.99364840984344, loss : 2.0062971115112305\n",
            "epoch: 19, classification_loss: 1.7448394298553467, Val Loss: 70.96505856513977, loss : 2.026193380355835\n",
            "Batch: 45, Test Acc: 0.5675080128205128\n",
            "Batch: 46:\n",
            "epoch: 0, classification_loss: 1.8532270193099976, Val Loss: 71.01645147800446, loss : 1.8532270193099976\n",
            "epoch: 1, classification_loss: 1.8072007894515991, Val Loss: 70.95773899555206, loss : 2.1150548458099365\n",
            "epoch: 2, classification_loss: 1.7873038053512573, Val Loss: 70.99054610729218, loss : 2.0443365573883057\n",
            "epoch: 3, classification_loss: 1.8263572454452515, Val Loss: 70.96916925907135, loss : 2.0892674922943115\n",
            "epoch: 4, classification_loss: 1.8425242900848389, Val Loss: 70.93351018428802, loss : 2.0812501907348633\n",
            "epoch: 5, classification_loss: 1.82932448387146, Val Loss: 70.91862678527832, loss : 2.0889852046966553\n",
            "epoch: 6, classification_loss: 1.8240772485733032, Val Loss: 70.91958355903625, loss : 2.1040353775024414\n",
            "epoch: 7, classification_loss: 1.8266969919204712, Val Loss: 70.97499096393585, loss : 2.0859572887420654\n",
            "epoch: 8, classification_loss: 1.8142417669296265, Val Loss: 71.07626378536224, loss : 2.0991716384887695\n",
            "epoch: 9, classification_loss: 1.8213393688201904, Val Loss: 70.93754971027374, loss : 2.0972115993499756\n",
            "epoch: 10, classification_loss: 1.8269613981246948, Val Loss: 70.91667950153351, loss : 2.099773406982422\n",
            "epoch: 11, classification_loss: 1.8260334730148315, Val Loss: 70.9059374332428, loss : 2.1051063537597656\n",
            "epoch: 12, classification_loss: 1.8144028186798096, Val Loss: 71.10654020309448, loss : 1.8144028186798096\n",
            "epoch: 13, classification_loss: 1.7728233337402344, Val Loss: 70.98599755764008, loss : 2.134803056716919\n",
            "epoch: 14, classification_loss: 1.755382776260376, Val Loss: 70.92197072505951, loss : 2.042656183242798\n",
            "epoch: 15, classification_loss: 1.771863579750061, Val Loss: 70.93940389156342, loss : 2.0843169689178467\n",
            "epoch: 16, classification_loss: 1.809591293334961, Val Loss: 70.99169671535492, loss : 2.0827419757843018\n",
            "epoch: 17, classification_loss: 1.7991976737976074, Val Loss: 70.91859292984009, loss : 2.07908034324646\n",
            "epoch: 18, classification_loss: 1.7786321640014648, Val Loss: 70.97220289707184, loss : 2.0814146995544434\n",
            "epoch: 19, classification_loss: 1.7750639915466309, Val Loss: 70.92417323589325, loss : 2.0679574012756348\n",
            "Batch: 46, Test Acc: 0.5667067307692307\n",
            "Batch: 47:\n",
            "epoch: 0, classification_loss: 1.8192524909973145, Val Loss: 70.99500346183777, loss : 1.8192524909973145\n",
            "epoch: 1, classification_loss: 1.7915304899215698, Val Loss: 70.96150326728821, loss : 2.148090362548828\n",
            "epoch: 2, classification_loss: 1.7669639587402344, Val Loss: 70.99584138393402, loss : 2.0471084117889404\n",
            "epoch: 3, classification_loss: 1.793399453163147, Val Loss: 71.00710320472717, loss : 2.07987642288208\n",
            "epoch: 4, classification_loss: 1.8116713762283325, Val Loss: 70.92529356479645, loss : 2.075127601623535\n",
            "epoch: 5, classification_loss: 1.804717779159546, Val Loss: 70.90579891204834, loss : 2.057879686355591\n",
            "epoch: 6, classification_loss: 1.7839298248291016, Val Loss: 70.91997861862183, loss : 2.0733938217163086\n",
            "epoch: 7, classification_loss: 1.799202799797058, Val Loss: 70.95902478694916, loss : 2.068380117416382\n",
            "epoch: 8, classification_loss: 1.813288927078247, Val Loss: 70.976731300354, loss : 2.068251848220825\n",
            "epoch: 9, classification_loss: 1.801754117012024, Val Loss: 70.94502282142639, loss : 2.0533995628356934\n",
            "epoch: 10, classification_loss: 1.8026981353759766, Val Loss: 70.94972157478333, loss : 2.05446195602417\n",
            "epoch: 11, classification_loss: 1.801023006439209, Val Loss: 70.95641493797302, loss : 2.058328628540039\n",
            "epoch: 12, classification_loss: 1.8003290891647339, Val Loss: 70.9659343957901, loss : 2.0408756732940674\n",
            "epoch: 13, classification_loss: 1.7934914827346802, Val Loss: 70.97844970226288, loss : 2.0334601402282715\n",
            "epoch: 14, classification_loss: 1.803236722946167, Val Loss: 70.94445478916168, loss : 2.0399622917175293\n",
            "epoch: 15, classification_loss: 1.8008724451065063, Val Loss: 70.92614698410034, loss : 2.041592836380005\n",
            "epoch: 16, classification_loss: 1.7962968349456787, Val Loss: 70.97769916057587, loss : 2.034613609313965\n",
            "epoch: 17, classification_loss: 1.7974387407302856, Val Loss: 70.95934045314789, loss : 2.029052734375\n",
            "epoch: 18, classification_loss: 1.8012359142303467, Val Loss: 70.95467460155487, loss : 2.037551164627075\n",
            "epoch: 19, classification_loss: 1.794983983039856, Val Loss: 70.97238540649414, loss : 2.031168222427368\n",
            "Batch: 47, Test Acc: 0.567207532051282\n",
            "Batch: 48:\n",
            "epoch: 0, classification_loss: 1.8438514471054077, Val Loss: 70.9254344701767, loss : 1.8438514471054077\n",
            "epoch: 1, classification_loss: 1.8092243671417236, Val Loss: 70.9249439239502, loss : 2.1556949615478516\n",
            "epoch: 2, classification_loss: 1.7850894927978516, Val Loss: 70.96241796016693, loss : 2.0379977226257324\n",
            "epoch: 3, classification_loss: 1.799738883972168, Val Loss: 70.94612514972687, loss : 2.0839827060699463\n",
            "epoch: 4, classification_loss: 1.8443074226379395, Val Loss: 70.88316011428833, loss : 2.095919609069824\n",
            "epoch: 5, classification_loss: 1.8143712282180786, Val Loss: 71.06399083137512, loss : 1.8143712282180786\n",
            "epoch: 6, classification_loss: 1.7992634773254395, Val Loss: 70.97037160396576, loss : 2.2052955627441406\n",
            "epoch: 7, classification_loss: 1.7782152891159058, Val Loss: 70.89530086517334, loss : 2.1121249198913574\n",
            "epoch: 8, classification_loss: 1.791218876838684, Val Loss: 70.9470180273056, loss : 2.0684595108032227\n",
            "epoch: 9, classification_loss: 1.8280681371688843, Val Loss: 70.91194069385529, loss : 2.144484519958496\n",
            "epoch: 10, classification_loss: 1.8143813610076904, Val Loss: 70.89176797866821, loss : 2.0551323890686035\n",
            "epoch: 11, classification_loss: 1.7867379188537598, Val Loss: 70.91145443916321, loss : 2.111380100250244\n",
            "epoch: 12, classification_loss: 1.791763186454773, Val Loss: 70.94610822200775, loss : 2.1103782653808594\n",
            "epoch: 13, classification_loss: 1.7941226959228516, Val Loss: 70.95327293872833, loss : 2.0757761001586914\n",
            "epoch: 14, classification_loss: 1.809370994567871, Val Loss: 70.92059135437012, loss : 2.0806665420532227\n",
            "epoch: 15, classification_loss: 1.7991433143615723, Val Loss: 70.89507830142975, loss : 2.0674378871917725\n",
            "epoch: 16, classification_loss: 1.7940319776535034, Val Loss: 70.92899119853973, loss : 2.0680007934570312\n",
            "epoch: 17, classification_loss: 1.787400722503662, Val Loss: 70.94901847839355, loss : 2.062351703643799\n",
            "epoch: 18, classification_loss: 1.7967734336853027, Val Loss: 70.92693138122559, loss : 2.052156686782837\n",
            "epoch: 19, classification_loss: 1.8041999340057373, Val Loss: 70.93184781074524, loss : 2.0547447204589844\n",
            "Batch: 48, Test Acc: 0.5682091346153846\n",
            "Batch: 49:\n",
            "epoch: 0, classification_loss: 1.7940080165863037, Val Loss: 70.92417764663696, loss : 1.7940080165863037\n",
            "epoch: 1, classification_loss: 1.7634927034378052, Val Loss: 70.9364584684372, loss : 2.1050734519958496\n",
            "epoch: 2, classification_loss: 1.7401341199874878, Val Loss: 70.94655251502991, loss : 2.015348434448242\n",
            "epoch: 3, classification_loss: 1.7501224279403687, Val Loss: 70.92316126823425, loss : 2.0416221618652344\n",
            "epoch: 4, classification_loss: 1.7900536060333252, Val Loss: 70.86488378047943, loss : 2.053385019302368\n",
            "epoch: 5, classification_loss: 1.795754075050354, Val Loss: 70.88323819637299, loss : 1.795754075050354\n",
            "epoch: 6, classification_loss: 1.7621943950653076, Val Loss: 70.86635529994965, loss : 2.1449575424194336\n",
            "epoch: 7, classification_loss: 1.7419956922531128, Val Loss: 70.94558453559875, loss : 2.0623536109924316\n",
            "epoch: 8, classification_loss: 1.7540773153305054, Val Loss: 70.92937803268433, loss : 2.0138909816741943\n",
            "epoch: 9, classification_loss: 1.7810111045837402, Val Loss: 70.88675725460052, loss : 2.0757923126220703\n",
            "epoch: 10, classification_loss: 1.780825138092041, Val Loss: 70.89839541912079, loss : 2.0184450149536133\n",
            "epoch: 11, classification_loss: 1.7493504285812378, Val Loss: 70.90148973464966, loss : 2.0596768856048584\n",
            "epoch: 12, classification_loss: 1.7674529552459717, Val Loss: 70.89869439601898, loss : 2.0534303188323975\n",
            "epoch: 13, classification_loss: 1.7704551219940186, Val Loss: 70.90602815151215, loss : 2.0291213989257812\n",
            "epoch: 14, classification_loss: 1.755618929862976, Val Loss: 70.98022258281708, loss : 2.0312864780426025\n",
            "epoch: 15, classification_loss: 1.7652320861816406, Val Loss: 70.88982498645782, loss : 2.0230207443237305\n",
            "epoch: 16, classification_loss: 1.7581355571746826, Val Loss: 70.87250936031342, loss : 2.0279464721679688\n",
            "epoch: 17, classification_loss: 1.7677271366119385, Val Loss: 70.90966606140137, loss : 2.0247724056243896\n",
            "epoch: 18, classification_loss: 1.765051007270813, Val Loss: 70.90752458572388, loss : 2.0230181217193604\n",
            "epoch: 19, classification_loss: 1.7644178867340088, Val Loss: 70.88822817802429, loss : 2.022566795349121\n",
            "Batch: 49, Test Acc: 0.5714142628205128\n",
            "Batch: 50:\n",
            "epoch: 0, classification_loss: 1.7678650617599487, Val Loss: 70.92366337776184, loss : 1.7678650617599487\n",
            "epoch: 1, classification_loss: 1.7272882461547852, Val Loss: 70.95640587806702, loss : 2.0660030841827393\n",
            "epoch: 2, classification_loss: 1.7017768621444702, Val Loss: 70.92028784751892, loss : 1.9879056215286255\n",
            "epoch: 3, classification_loss: 1.719173789024353, Val Loss: 70.96141386032104, loss : 2.000791549682617\n",
            "epoch: 4, classification_loss: 1.7547762393951416, Val Loss: 70.91806495189667, loss : 2.0019376277923584\n",
            "epoch: 5, classification_loss: 1.754564642906189, Val Loss: 70.91090452671051, loss : 2.0129213333129883\n",
            "epoch: 6, classification_loss: 1.732471227645874, Val Loss: 70.90729188919067, loss : 2.0005078315734863\n",
            "epoch: 7, classification_loss: 1.726434588432312, Val Loss: 70.95070254802704, loss : 1.9866998195648193\n",
            "epoch: 8, classification_loss: 1.737046480178833, Val Loss: 70.98196578025818, loss : 1.9950960874557495\n",
            "epoch: 9, classification_loss: 1.7514288425445557, Val Loss: 70.97063446044922, loss : 1.9987084865570068\n",
            "epoch: 10, classification_loss: 1.7339487075805664, Val Loss: 70.94573426246643, loss : 1.9977670907974243\n",
            "epoch: 11, classification_loss: 1.7317966222763062, Val Loss: 70.94942438602448, loss : 1.9896180629730225\n",
            "epoch: 12, classification_loss: 1.7346915006637573, Val Loss: 70.94801998138428, loss : 1.9931902885437012\n",
            "epoch: 13, classification_loss: 1.7473033666610718, Val Loss: 70.99554407596588, loss : 1.9936507940292358\n",
            "epoch: 14, classification_loss: 1.7298521995544434, Val Loss: 70.99419164657593, loss : 1.9868874549865723\n",
            "epoch: 15, classification_loss: 1.7386242151260376, Val Loss: 70.99793136119843, loss : 1.9824810028076172\n",
            "epoch: 16, classification_loss: 1.737807273864746, Val Loss: 70.95325124263763, loss : 1.9861383438110352\n",
            "epoch: 17, classification_loss: 1.7522355318069458, Val Loss: 70.97048282623291, loss : 1.9836691617965698\n",
            "epoch: 18, classification_loss: 1.7346473932266235, Val Loss: 71.00522005558014, loss : 1.9822089672088623\n",
            "epoch: 19, classification_loss: 1.7380813360214233, Val Loss: 71.00819599628448, loss : 1.9787282943725586\n",
            "Batch: 50, Test Acc: 0.5687099358974359\n",
            "Batch: 51:\n",
            "epoch: 0, classification_loss: 1.8162927627563477, Val Loss: 70.88234746456146, loss : 1.8162927627563477\n",
            "epoch: 1, classification_loss: 1.7835044860839844, Val Loss: 70.90286672115326, loss : 2.1375179290771484\n",
            "epoch: 2, classification_loss: 1.764646291732788, Val Loss: 70.90748727321625, loss : 2.044008731842041\n",
            "epoch: 3, classification_loss: 1.7910956144332886, Val Loss: 70.89223456382751, loss : 2.069174289703369\n",
            "epoch: 4, classification_loss: 1.8203167915344238, Val Loss: 70.87516462802887, loss : 2.070286273956299\n",
            "epoch: 5, classification_loss: 1.796875, Val Loss: 70.89583647251129, loss : 2.057187795639038\n",
            "epoch: 6, classification_loss: 1.7912477254867554, Val Loss: 70.91006827354431, loss : 2.0607948303222656\n",
            "epoch: 7, classification_loss: 1.7841951847076416, Val Loss: 70.92086863517761, loss : 2.0440502166748047\n",
            "epoch: 8, classification_loss: 1.7901341915130615, Val Loss: 70.89631283283234, loss : 2.044640302658081\n",
            "epoch: 9, classification_loss: 1.7896479368209839, Val Loss: 70.96042597293854, loss : 2.037763833999634\n",
            "epoch: 10, classification_loss: 1.7955244779586792, Val Loss: 70.96048307418823, loss : 2.0409092903137207\n",
            "epoch: 11, classification_loss: 1.807592749595642, Val Loss: 70.93198108673096, loss : 2.044640302658081\n",
            "epoch: 12, classification_loss: 1.8002561330795288, Val Loss: 70.9054183959961, loss : 2.04782772064209\n",
            "epoch: 13, classification_loss: 1.8007512092590332, Val Loss: 70.96980857849121, loss : 2.0483391284942627\n",
            "epoch: 14, classification_loss: 1.7949340343475342, Val Loss: 71.00727128982544, loss : 2.0302021503448486\n",
            "epoch: 15, classification_loss: 1.8077006340026855, Val Loss: 70.95409262180328, loss : 2.0463521480560303\n",
            "epoch: 16, classification_loss: 1.7964756488800049, Val Loss: 70.94945466518402, loss : 2.0337352752685547\n",
            "epoch: 17, classification_loss: 1.793278694152832, Val Loss: 70.98251450061798, loss : 2.031796455383301\n",
            "epoch: 18, classification_loss: 1.7996301651000977, Val Loss: 70.94371044635773, loss : 2.0307953357696533\n",
            "epoch: 19, classification_loss: 1.8017578125, Val Loss: 70.9611599445343, loss : 2.03838849067688\n",
            "Batch: 51, Test Acc: 0.5670072115384616\n",
            "Batch: 52:\n",
            "epoch: 0, classification_loss: 1.7752947807312012, Val Loss: 70.89987397193909, loss : 1.7752947807312012\n",
            "epoch: 1, classification_loss: 1.7507706880569458, Val Loss: 70.90982842445374, loss : 2.0962367057800293\n",
            "epoch: 2, classification_loss: 1.7220544815063477, Val Loss: 70.91992437839508, loss : 1.9984806776046753\n",
            "epoch: 3, classification_loss: 1.7420095205307007, Val Loss: 70.9673398733139, loss : 2.0199503898620605\n",
            "epoch: 4, classification_loss: 1.763737678527832, Val Loss: 70.89852368831635, loss : 2.012061595916748\n",
            "epoch: 5, classification_loss: 1.7622897624969482, Val Loss: 70.90549623966217, loss : 2.0165464878082275\n",
            "epoch: 6, classification_loss: 1.7554782629013062, Val Loss: 70.89969992637634, loss : 2.022059202194214\n",
            "epoch: 7, classification_loss: 1.7497965097427368, Val Loss: 70.92529881000519, loss : 2.0182361602783203\n",
            "epoch: 8, classification_loss: 1.757828950881958, Val Loss: 70.8917738199234, loss : 2.0155129432678223\n",
            "epoch: 9, classification_loss: 1.7650249004364014, Val Loss: 70.92172336578369, loss : 2.0189857482910156\n",
            "epoch: 10, classification_loss: 1.7667185068130493, Val Loss: 70.97756361961365, loss : 2.015719413757324\n",
            "epoch: 11, classification_loss: 1.7654985189437866, Val Loss: 70.91486871242523, loss : 2.0188326835632324\n",
            "epoch: 12, classification_loss: 1.756820559501648, Val Loss: 70.89934873580933, loss : 2.006208658218384\n",
            "epoch: 13, classification_loss: 1.7577499151229858, Val Loss: 70.91125953197479, loss : 2.0054593086242676\n",
            "epoch: 14, classification_loss: 1.762622594833374, Val Loss: 70.94762325286865, loss : 2.003153085708618\n",
            "epoch: 15, classification_loss: 1.764005422592163, Val Loss: 70.95738863945007, loss : 2.007906436920166\n",
            "epoch: 16, classification_loss: 1.760559320449829, Val Loss: 70.92027878761292, loss : 2.0096607208251953\n",
            "epoch: 17, classification_loss: 1.7661311626434326, Val Loss: 70.95758938789368, loss : 2.0153043270111084\n",
            "epoch: 18, classification_loss: 1.7557872533798218, Val Loss: 70.93689668178558, loss : 1.9946945905685425\n",
            "epoch: 19, classification_loss: 1.7621166706085205, Val Loss: 70.93889558315277, loss : 2.005411386489868\n",
            "Batch: 52, Test Acc: 0.5665064102564102\n",
            "Batch: 53:\n",
            "epoch: 0, classification_loss: 1.8161418437957764, Val Loss: 70.9149957895279, loss : 1.8161418437957764\n",
            "epoch: 1, classification_loss: 1.7748832702636719, Val Loss: 70.9071307182312, loss : 2.1247262954711914\n",
            "epoch: 2, classification_loss: 1.7538938522338867, Val Loss: 70.88779354095459, loss : 2.034811019897461\n",
            "epoch: 3, classification_loss: 1.7718784809112549, Val Loss: 70.92813456058502, loss : 2.0621283054351807\n",
            "epoch: 4, classification_loss: 1.8058650493621826, Val Loss: 70.86202824115753, loss : 2.0550577640533447\n",
            "epoch: 5, classification_loss: 1.7985385656356812, Val Loss: 70.903249502182, loss : 2.070888042449951\n",
            "epoch: 6, classification_loss: 1.7840287685394287, Val Loss: 70.95681548118591, loss : 2.061666250228882\n",
            "epoch: 7, classification_loss: 1.78976571559906, Val Loss: 71.00224149227142, loss : 2.0609374046325684\n",
            "epoch: 8, classification_loss: 1.779218077659607, Val Loss: 70.92602276802063, loss : 2.0496695041656494\n",
            "epoch: 9, classification_loss: 1.7840338945388794, Val Loss: 70.95141041278839, loss : 2.0530967712402344\n",
            "epoch: 10, classification_loss: 1.7913990020751953, Val Loss: 70.89667797088623, loss : 2.0495803356170654\n",
            "epoch: 11, classification_loss: 1.788316249847412, Val Loss: 70.94292521476746, loss : 2.058152198791504\n",
            "epoch: 12, classification_loss: 1.7773854732513428, Val Loss: 70.93183612823486, loss : 2.0335991382598877\n",
            "epoch: 13, classification_loss: 1.7714991569519043, Val Loss: 70.88244020938873, loss : 2.0346033573150635\n",
            "epoch: 14, classification_loss: 1.790001392364502, Val Loss: 70.94160556793213, loss : 2.044170379638672\n",
            "epoch: 15, classification_loss: 1.7929362058639526, Val Loss: 70.8894590139389, loss : 2.050081968307495\n",
            "epoch: 16, classification_loss: 1.7860665321350098, Val Loss: 70.92861998081207, loss : 2.0418860912323\n",
            "epoch: 17, classification_loss: 1.7881910800933838, Val Loss: 70.97049880027771, loss : 2.0433950424194336\n",
            "epoch: 18, classification_loss: 1.7873358726501465, Val Loss: 71.00630986690521, loss : 2.034125566482544\n",
            "epoch: 19, classification_loss: 1.7923634052276611, Val Loss: 70.91391110420227, loss : 2.0374488830566406\n",
            "Batch: 53, Test Acc: 0.5694110576923077\n",
            "Batch: 54:\n",
            "epoch: 0, classification_loss: 1.8670021295547485, Val Loss: 71.02378785610199, loss : 1.8670021295547485\n",
            "epoch: 1, classification_loss: 1.8188729286193848, Val Loss: 71.02965128421783, loss : 2.1747488975524902\n",
            "epoch: 2, classification_loss: 1.7983852624893188, Val Loss: 70.98050725460052, loss : 2.093021869659424\n",
            "epoch: 3, classification_loss: 1.8254165649414062, Val Loss: 70.91634929180145, loss : 2.1217753887176514\n",
            "epoch: 4, classification_loss: 1.8546110391616821, Val Loss: 70.95994412899017, loss : 2.122823715209961\n",
            "epoch: 5, classification_loss: 1.847883701324463, Val Loss: 70.921799659729, loss : 2.112858295440674\n",
            "epoch: 6, classification_loss: 1.8409708738327026, Val Loss: 70.90412044525146, loss : 2.115719795227051\n",
            "epoch: 7, classification_loss: 1.8310621976852417, Val Loss: 70.96356749534607, loss : 2.105402708053589\n",
            "epoch: 8, classification_loss: 1.827035903930664, Val Loss: 70.99686598777771, loss : 2.0868782997131348\n",
            "epoch: 9, classification_loss: 1.8277945518493652, Val Loss: 70.99955976009369, loss : 2.0954761505126953\n",
            "epoch: 10, classification_loss: 1.8479342460632324, Val Loss: 70.90433716773987, loss : 2.100883722305298\n",
            "epoch: 11, classification_loss: 1.840006709098816, Val Loss: 70.94655108451843, loss : 2.0954527854919434\n",
            "epoch: 12, classification_loss: 1.8396273851394653, Val Loss: 70.90378999710083, loss : 2.084346294403076\n",
            "epoch: 13, classification_loss: 1.832397699356079, Val Loss: 70.9555047750473, loss : 2.0869815349578857\n",
            "epoch: 14, classification_loss: 1.842141032218933, Val Loss: 71.01858389377594, loss : 2.0838546752929688\n",
            "epoch: 15, classification_loss: 1.8348548412322998, Val Loss: 70.93665552139282, loss : 2.0869064331054688\n",
            "epoch: 16, classification_loss: 1.8202487230300903, Val Loss: 70.91816639900208, loss : 2.0685386657714844\n",
            "epoch: 17, classification_loss: 1.833634614944458, Val Loss: 70.9046082496643, loss : 2.0820157527923584\n",
            "epoch: 18, classification_loss: 1.8465709686279297, Val Loss: 70.94039916992188, loss : 2.0829427242279053\n",
            "epoch: 19, classification_loss: 1.8355971574783325, Val Loss: 70.93123495578766, loss : 2.0805835723876953\n",
            "Batch: 54, Test Acc: 0.5700120192307693\n",
            "Batch: 55:\n",
            "epoch: 0, classification_loss: 1.829166293144226, Val Loss: 70.97096180915833, loss : 1.829166293144226\n",
            "epoch: 1, classification_loss: 1.7918224334716797, Val Loss: 71.1110327243805, loss : 2.128716230392456\n",
            "epoch: 2, classification_loss: 1.7634456157684326, Val Loss: 70.9061508178711, loss : 2.0381836891174316\n",
            "epoch: 3, classification_loss: 1.7937613725662231, Val Loss: 70.87411403656006, loss : 2.076467275619507\n",
            "epoch: 4, classification_loss: 1.8154394626617432, Val Loss: 70.88384711742401, loss : 2.064336061477661\n",
            "epoch: 5, classification_loss: 1.8111896514892578, Val Loss: 70.93081557750702, loss : 2.0691733360290527\n",
            "epoch: 6, classification_loss: 1.8052483797073364, Val Loss: 70.93598628044128, loss : 2.069014072418213\n",
            "epoch: 7, classification_loss: 1.8029347658157349, Val Loss: 70.93513715267181, loss : 2.0709986686706543\n",
            "epoch: 8, classification_loss: 1.8020637035369873, Val Loss: 70.92837655544281, loss : 2.0624382495880127\n",
            "epoch: 9, classification_loss: 1.8088501691818237, Val Loss: 70.88090431690216, loss : 2.0708630084991455\n",
            "epoch: 10, classification_loss: 1.8011090755462646, Val Loss: 70.91296207904816, loss : 2.0515074729919434\n",
            "epoch: 11, classification_loss: 1.8102566003799438, Val Loss: 70.94157254695892, loss : 2.0562219619750977\n",
            "epoch: 12, classification_loss: 1.8092049360275269, Val Loss: 70.93296682834625, loss : 2.0469672679901123\n",
            "epoch: 13, classification_loss: 1.7975621223449707, Val Loss: 70.92613518238068, loss : 2.050175666809082\n",
            "epoch: 14, classification_loss: 1.7951611280441284, Val Loss: 70.91722190380096, loss : 2.0462117195129395\n",
            "epoch: 15, classification_loss: 1.812182903289795, Val Loss: 70.89370501041412, loss : 2.056431770324707\n",
            "epoch: 16, classification_loss: 1.8116824626922607, Val Loss: 70.9310222864151, loss : 2.0541505813598633\n",
            "epoch: 17, classification_loss: 1.811293363571167, Val Loss: 70.95973443984985, loss : 2.0508289337158203\n",
            "epoch: 18, classification_loss: 1.8109902143478394, Val Loss: 70.93871510028839, loss : 2.044912815093994\n",
            "epoch: 19, classification_loss: 1.7903308868408203, Val Loss: 70.91387808322906, loss : 2.0310122966766357\n",
            "Batch: 55, Test Acc: 0.5691105769230769\n",
            "Batch: 56:\n",
            "epoch: 0, classification_loss: 1.8382455110549927, Val Loss: 71.01611495018005, loss : 1.8382455110549927\n",
            "epoch: 1, classification_loss: 1.7944989204406738, Val Loss: 70.93673300743103, loss : 2.1420681476593018\n",
            "epoch: 2, classification_loss: 1.7688980102539062, Val Loss: 70.94534516334534, loss : 2.0284006595611572\n",
            "epoch: 3, classification_loss: 1.7969266176223755, Val Loss: 70.90531122684479, loss : 2.085650682449341\n",
            "epoch: 4, classification_loss: 1.8204541206359863, Val Loss: 70.93731093406677, loss : 2.0714528560638428\n",
            "epoch: 5, classification_loss: 1.8092234134674072, Val Loss: 70.8930493593216, loss : 2.0817856788635254\n",
            "epoch: 6, classification_loss: 1.7963230609893799, Val Loss: 70.92847037315369, loss : 2.0770647525787354\n",
            "epoch: 7, classification_loss: 1.8160676956176758, Val Loss: 70.90409624576569, loss : 2.0713837146759033\n",
            "epoch: 8, classification_loss: 1.8095836639404297, Val Loss: 70.89026498794556, loss : 2.07324481010437\n",
            "epoch: 9, classification_loss: 1.8130061626434326, Val Loss: 70.90016603469849, loss : 2.0674991607666016\n",
            "epoch: 10, classification_loss: 1.8065539598464966, Val Loss: 70.89155685901642, loss : 2.0590977668762207\n",
            "epoch: 11, classification_loss: 1.813432216644287, Val Loss: 70.88881587982178, loss : 2.067988872528076\n",
            "epoch: 12, classification_loss: 1.7947473526000977, Val Loss: 70.92094397544861, loss : 2.0417582988739014\n",
            "epoch: 13, classification_loss: 1.802236557006836, Val Loss: 70.94701683521271, loss : 2.0486397743225098\n",
            "epoch: 14, classification_loss: 1.7966773509979248, Val Loss: 70.93876993656158, loss : 2.044543981552124\n",
            "epoch: 15, classification_loss: 1.805854320526123, Val Loss: 70.89998149871826, loss : 2.0562257766723633\n",
            "epoch: 16, classification_loss: 1.8087753057479858, Val Loss: 70.90142011642456, loss : 2.0605762004852295\n",
            "epoch: 17, classification_loss: 1.8091610670089722, Val Loss: 70.91485571861267, loss : 2.0460360050201416\n",
            "epoch: 18, classification_loss: 1.8083752393722534, Val Loss: 70.9467681646347, loss : 2.0518112182617188\n",
            "epoch: 19, classification_loss: 1.802233338356018, Val Loss: 70.928391456604, loss : 2.040914535522461\n",
            "Batch: 56, Test Acc: 0.567207532051282\n",
            "Batch: 57:\n",
            "epoch: 0, classification_loss: 1.8201920986175537, Val Loss: 70.88185966014862, loss : 1.8201920986175537\n",
            "epoch: 1, classification_loss: 1.7772306203842163, Val Loss: 70.95255744457245, loss : 2.1364145278930664\n",
            "epoch: 2, classification_loss: 1.7473483085632324, Val Loss: 70.93390035629272, loss : 2.051243782043457\n",
            "epoch: 3, classification_loss: 1.7550128698349, Val Loss: 70.90109717845917, loss : 2.062642812728882\n",
            "epoch: 4, classification_loss: 1.7920432090759277, Val Loss: 70.84996807575226, loss : 2.074270248413086\n",
            "epoch: 5, classification_loss: 1.8009945154190063, Val Loss: 70.95736944675446, loss : 1.8009945154190063\n",
            "epoch: 6, classification_loss: 1.768009066581726, Val Loss: 70.99322509765625, loss : 2.1447267532348633\n",
            "epoch: 7, classification_loss: 1.733624815940857, Val Loss: 70.8966943025589, loss : 2.0479562282562256\n",
            "epoch: 8, classification_loss: 1.7597936391830444, Val Loss: 70.88686120510101, loss : 2.046412706375122\n",
            "epoch: 9, classification_loss: 1.7822448015213013, Val Loss: 70.91526663303375, loss : 2.076749324798584\n",
            "epoch: 10, classification_loss: 1.7661787271499634, Val Loss: 70.85956299304962, loss : 2.0184226036071777\n",
            "epoch: 11, classification_loss: 1.7574524879455566, Val Loss: 70.86944961547852, loss : 2.07712459564209\n",
            "epoch: 12, classification_loss: 1.7601372003555298, Val Loss: 70.93067538738251, loss : 2.049708843231201\n",
            "epoch: 13, classification_loss: 1.76321280002594, Val Loss: 70.91235983371735, loss : 2.0419468879699707\n",
            "epoch: 14, classification_loss: 1.7644915580749512, Val Loss: 70.8421061038971, loss : 2.0386083126068115\n",
            "epoch: 15, classification_loss: 1.7793558835983276, Val Loss: 70.85225522518158, loss : 2.0525448322296143\n",
            "epoch: 16, classification_loss: 1.762428879737854, Val Loss: 70.96822690963745, loss : 2.04042387008667\n",
            "epoch: 17, classification_loss: 1.747825026512146, Val Loss: 70.90960264205933, loss : 2.017345428466797\n",
            "epoch: 18, classification_loss: 1.7443411350250244, Val Loss: 70.90025997161865, loss : 2.0074164867401123\n",
            "epoch: 19, classification_loss: 1.7663401365280151, Val Loss: 70.88072001934052, loss : 2.026075839996338\n",
            "Batch: 57, Test Acc: 0.5686097756410257\n",
            "Batch: 58:\n",
            "epoch: 0, classification_loss: 1.8078747987747192, Val Loss: 70.95738542079926, loss : 1.8078747987747192\n",
            "epoch: 1, classification_loss: 1.7644871473312378, Val Loss: 70.96052193641663, loss : 2.1224284172058105\n",
            "epoch: 2, classification_loss: 1.7329246997833252, Val Loss: 70.90698373317719, loss : 2.030238628387451\n",
            "epoch: 3, classification_loss: 1.7692681550979614, Val Loss: 70.92393016815186, loss : 2.072760581970215\n",
            "epoch: 4, classification_loss: 1.8029447793960571, Val Loss: 70.86907207965851, loss : 2.0782155990600586\n",
            "epoch: 5, classification_loss: 1.7863619327545166, Val Loss: 70.86612868309021, loss : 2.0427842140197754\n",
            "epoch: 6, classification_loss: 1.7770135402679443, Val Loss: 70.83013021945953, loss : 2.0586774349212646\n",
            "epoch: 7, classification_loss: 1.77167809009552, Val Loss: 70.98958992958069, loss : 1.77167809009552\n",
            "epoch: 8, classification_loss: 1.7445546388626099, Val Loss: 70.99910736083984, loss : 2.0729293823242188\n",
            "epoch: 9, classification_loss: 1.7029184103012085, Val Loss: 70.92425405979156, loss : 1.97257661819458\n",
            "epoch: 10, classification_loss: 1.7364253997802734, Val Loss: 70.8385717868805, loss : 2.0337376594543457\n",
            "epoch: 11, classification_loss: 1.7714711427688599, Val Loss: 70.88273179531097, loss : 2.0072665214538574\n",
            "epoch: 12, classification_loss: 1.7639087438583374, Val Loss: 70.91796576976776, loss : 2.0314648151397705\n",
            "epoch: 13, classification_loss: 1.7333500385284424, Val Loss: 70.89065980911255, loss : 2.0297088623046875\n",
            "epoch: 14, classification_loss: 1.730517029762268, Val Loss: 70.86158585548401, loss : 2.008268356323242\n",
            "epoch: 15, classification_loss: 1.7647576332092285, Val Loss: 70.84378635883331, loss : 2.0243782997131348\n",
            "epoch: 16, classification_loss: 1.7528460025787354, Val Loss: 70.86681163311005, loss : 2.004995822906494\n",
            "epoch: 17, classification_loss: 1.7559878826141357, Val Loss: 70.86624872684479, loss : 2.0178470611572266\n",
            "epoch: 18, classification_loss: 1.755102276802063, Val Loss: 70.85079610347748, loss : 2.0083508491516113\n",
            "epoch: 19, classification_loss: 1.7417763471603394, Val Loss: 70.81962764263153, loss : 1.999754548072815\n",
            "Batch: 58, Test Acc: 0.5698116987179487\n",
            "Batch: 59:\n",
            "epoch: 0, classification_loss: 1.827657699584961, Val Loss: 70.93345034122467, loss : 1.827657699584961\n",
            "epoch: 1, classification_loss: 1.792542576789856, Val Loss: 70.82323622703552, loss : 2.1270084381103516\n",
            "epoch: 2, classification_loss: 1.7745187282562256, Val Loss: 70.84250903129578, loss : 2.0586764812469482\n",
            "epoch: 3, classification_loss: 1.7913886308670044, Val Loss: 70.85506880283356, loss : 2.080679178237915\n",
            "epoch: 4, classification_loss: 1.8196940422058105, Val Loss: 70.87209737300873, loss : 2.0780835151672363\n",
            "epoch: 5, classification_loss: 1.8301187753677368, Val Loss: 70.79786276817322, loss : 2.0880942344665527\n",
            "epoch: 6, classification_loss: 1.8102890253067017, Val Loss: 70.86467611789703, loss : 1.8102890253067017\n",
            "epoch: 7, classification_loss: 1.7740447521209717, Val Loss: 70.89897692203522, loss : 2.124399185180664\n",
            "epoch: 8, classification_loss: 1.7484275102615356, Val Loss: 70.86618220806122, loss : 2.0444869995117188\n",
            "epoch: 9, classification_loss: 1.7702637910842896, Val Loss: 70.85247313976288, loss : 2.0301966667175293\n",
            "epoch: 10, classification_loss: 1.8106900453567505, Val Loss: 70.8426114320755, loss : 2.0668985843658447\n",
            "epoch: 11, classification_loss: 1.7988256216049194, Val Loss: 70.82955348491669, loss : 2.0566112995147705\n",
            "epoch: 12, classification_loss: 1.775624394416809, Val Loss: 70.82357048988342, loss : 2.055070638656616\n",
            "epoch: 13, classification_loss: 1.7702819108963013, Val Loss: 70.81758868694305, loss : 2.0496206283569336\n",
            "epoch: 14, classification_loss: 1.7899796962738037, Val Loss: 70.83918058872223, loss : 2.0614798069000244\n",
            "epoch: 15, classification_loss: 1.7881908416748047, Val Loss: 70.85948133468628, loss : 2.036360263824463\n",
            "epoch: 16, classification_loss: 1.796319603919983, Val Loss: 70.85310351848602, loss : 2.050874948501587\n",
            "epoch: 17, classification_loss: 1.786660075187683, Val Loss: 70.83787071704865, loss : 2.0435078144073486\n",
            "epoch: 18, classification_loss: 1.7876489162445068, Val Loss: 70.83763885498047, loss : 2.0435049533843994\n",
            "epoch: 19, classification_loss: 1.7906526327133179, Val Loss: 70.85792422294617, loss : 2.0415573120117188\n",
            "Batch: 59, Test Acc: 0.5710136217948718\n",
            "Batch: 60:\n",
            "epoch: 0, classification_loss: 1.822035551071167, Val Loss: 70.87829864025116, loss : 1.822035551071167\n",
            "epoch: 1, classification_loss: 1.772326111793518, Val Loss: 70.87766420841217, loss : 2.1239750385284424\n",
            "epoch: 2, classification_loss: 1.7543468475341797, Val Loss: 70.84151828289032, loss : 2.029000997543335\n",
            "epoch: 3, classification_loss: 1.7713353633880615, Val Loss: 70.81008207798004, loss : 2.0603768825531006\n",
            "epoch: 4, classification_loss: 1.7995107173919678, Val Loss: 70.79955470561981, loss : 2.062507152557373\n",
            "epoch: 5, classification_loss: 1.7978346347808838, Val Loss: 70.79936683177948, loss : 2.056323766708374\n",
            "epoch: 6, classification_loss: 1.7768049240112305, Val Loss: 70.81064200401306, loss : 2.052112579345703\n",
            "epoch: 7, classification_loss: 1.7825936079025269, Val Loss: 70.86310279369354, loss : 2.057084798812866\n",
            "epoch: 8, classification_loss: 1.770712971687317, Val Loss: 70.8115154504776, loss : 2.0316739082336426\n",
            "epoch: 9, classification_loss: 1.787898063659668, Val Loss: 70.82037353515625, loss : 2.045642614364624\n",
            "epoch: 10, classification_loss: 1.7735533714294434, Val Loss: 70.77502965927124, loss : 2.019091844558716\n",
            "epoch: 11, classification_loss: 1.7891957759857178, Val Loss: 70.81048345565796, loss : 1.7891957759857178\n",
            "epoch: 12, classification_loss: 1.7612497806549072, Val Loss: 70.8731005191803, loss : 2.1050257682800293\n",
            "epoch: 13, classification_loss: 1.7317523956298828, Val Loss: 70.76600778102875, loss : 2.0086569786071777\n",
            "epoch: 14, classification_loss: 1.7546544075012207, Val Loss: 70.81780505180359, loss : 1.7546544075012207\n",
            "epoch: 15, classification_loss: 1.752710223197937, Val Loss: 70.8657238483429, loss : 2.108975887298584\n",
            "epoch: 16, classification_loss: 1.7193130254745483, Val Loss: 70.92697155475616, loss : 2.003322124481201\n",
            "epoch: 17, classification_loss: 1.715391755104065, Val Loss: 70.90056526660919, loss : 2.0195868015289307\n",
            "epoch: 18, classification_loss: 1.7440286874771118, Val Loss: 70.78562641143799, loss : 2.0465850830078125\n",
            "epoch: 19, classification_loss: 1.7408276796340942, Val Loss: 70.78043782711029, loss : 1.9942176342010498\n",
            "Batch: 60, Test Acc: 0.5702123397435898\n",
            "Batch: 61:\n",
            "epoch: 0, classification_loss: 1.8453251123428345, Val Loss: 70.83971416950226, loss : 1.8453251123428345\n",
            "epoch: 1, classification_loss: 1.7955955266952515, Val Loss: 70.85484671592712, loss : 2.1653919219970703\n",
            "epoch: 2, classification_loss: 1.7691329717636108, Val Loss: 70.82719254493713, loss : 2.096963882446289\n",
            "epoch: 3, classification_loss: 1.7974181175231934, Val Loss: 70.91536664962769, loss : 2.1090683937072754\n",
            "epoch: 4, classification_loss: 1.8140619993209839, Val Loss: 70.84702491760254, loss : 2.112083911895752\n",
            "epoch: 5, classification_loss: 1.827282428741455, Val Loss: 70.73463571071625, loss : 2.111666202545166\n",
            "epoch: 6, classification_loss: 1.805487036705017, Val Loss: 70.77390933036804, loss : 1.805487036705017\n",
            "epoch: 7, classification_loss: 1.7714183330535889, Val Loss: 70.8603265285492, loss : 2.149013042449951\n",
            "epoch: 8, classification_loss: 1.740574598312378, Val Loss: 70.87311315536499, loss : 2.0736324787139893\n",
            "epoch: 9, classification_loss: 1.7777854204177856, Val Loss: 70.81841468811035, loss : 2.075993776321411\n",
            "epoch: 10, classification_loss: 1.8054099082946777, Val Loss: 70.77642714977264, loss : 2.0777738094329834\n",
            "epoch: 11, classification_loss: 1.794028878211975, Val Loss: 70.80048298835754, loss : 2.0685596466064453\n",
            "epoch: 12, classification_loss: 1.7704585790634155, Val Loss: 70.79167568683624, loss : 2.0508522987365723\n",
            "epoch: 13, classification_loss: 1.764377474784851, Val Loss: 70.74784445762634, loss : 2.0644824504852295\n",
            "epoch: 14, classification_loss: 1.7946497201919556, Val Loss: 70.77149510383606, loss : 2.054931402206421\n",
            "epoch: 15, classification_loss: 1.782554268836975, Val Loss: 70.86207818984985, loss : 2.059197187423706\n",
            "epoch: 16, classification_loss: 1.7697505950927734, Val Loss: 70.83902978897095, loss : 2.047433853149414\n",
            "epoch: 17, classification_loss: 1.7807962894439697, Val Loss: 70.73133885860443, loss : 2.0573315620422363\n",
            "epoch: 18, classification_loss: 1.7785557508468628, Val Loss: 70.73155152797699, loss : 2.048030138015747\n",
            "epoch: 19, classification_loss: 1.7843776941299438, Val Loss: 70.81549370288849, loss : 2.0480575561523438\n",
            "Batch: 61, Test Acc: 0.5715144230769231\n",
            "Batch: 62:\n",
            "epoch: 0, classification_loss: 1.844218134880066, Val Loss: 70.87128341197968, loss : 1.844218134880066\n",
            "epoch: 1, classification_loss: 1.799189567565918, Val Loss: 70.829021692276, loss : 2.15967059135437\n",
            "epoch: 2, classification_loss: 1.766859531402588, Val Loss: 70.76325285434723, loss : 2.074936628341675\n",
            "epoch: 3, classification_loss: 1.780299425125122, Val Loss: 70.85012137889862, loss : 2.0901472568511963\n",
            "epoch: 4, classification_loss: 1.8244460821151733, Val Loss: 70.85978162288666, loss : 2.100480556488037\n",
            "epoch: 5, classification_loss: 1.8132195472717285, Val Loss: 70.78029572963715, loss : 2.091771364212036\n",
            "epoch: 6, classification_loss: 1.8150578737258911, Val Loss: 70.77491092681885, loss : 2.0862720012664795\n",
            "epoch: 7, classification_loss: 1.8061977624893188, Val Loss: 70.87673270702362, loss : 2.0895092487335205\n",
            "epoch: 8, classification_loss: 1.8068820238113403, Val Loss: 70.83016610145569, loss : 2.086676836013794\n",
            "epoch: 9, classification_loss: 1.7939327955245972, Val Loss: 70.8043383359909, loss : 2.0779056549072266\n",
            "epoch: 10, classification_loss: 1.7923296689987183, Val Loss: 70.81436240673065, loss : 2.068042039871216\n",
            "epoch: 11, classification_loss: 1.8139194250106812, Val Loss: 70.83515930175781, loss : 2.0777976512908936\n",
            "epoch: 12, classification_loss: 1.804395079612732, Val Loss: 70.7753678560257, loss : 2.0619773864746094\n",
            "epoch: 13, classification_loss: 1.8014705181121826, Val Loss: 70.79311347007751, loss : 2.0675132274627686\n",
            "epoch: 14, classification_loss: 1.8032883405685425, Val Loss: 70.85053706169128, loss : 2.0611636638641357\n",
            "epoch: 15, classification_loss: 1.8046088218688965, Val Loss: 70.87858176231384, loss : 2.073709726333618\n",
            "epoch: 16, classification_loss: 1.790496587753296, Val Loss: 70.85511636734009, loss : 2.0515296459198\n",
            "epoch: 17, classification_loss: 1.8021106719970703, Val Loss: 70.82286441326141, loss : 2.074789524078369\n",
            "epoch: 18, classification_loss: 1.8040058612823486, Val Loss: 70.83098769187927, loss : 2.0583553314208984\n",
            "epoch: 19, classification_loss: 1.8045951128005981, Val Loss: 70.79000473022461, loss : 2.06111478805542\n",
            "Batch: 62, Test Acc: 0.5705128205128205\n",
            "Batch: 63:\n",
            "epoch: 0, classification_loss: 1.8049521446228027, Val Loss: 70.72372829914093, loss : 1.8049521446228027\n",
            "epoch: 1, classification_loss: 1.7631809711456299, Val Loss: 70.80174648761749, loss : 1.7631809711456299\n",
            "epoch: 2, classification_loss: 1.7086906433105469, Val Loss: 70.79781496524811, loss : 2.087594509124756\n",
            "epoch: 3, classification_loss: 1.7001020908355713, Val Loss: 70.78032624721527, loss : 2.043389320373535\n",
            "epoch: 4, classification_loss: 1.7222034931182861, Val Loss: 70.78652942180634, loss : 2.0325703620910645\n",
            "epoch: 5, classification_loss: 1.776323914527893, Val Loss: 70.74176442623138, loss : 2.044070243835449\n",
            "epoch: 6, classification_loss: 1.762364149093628, Val Loss: 70.74257385730743, loss : 2.038146495819092\n",
            "epoch: 7, classification_loss: 1.7487318515777588, Val Loss: 70.7523044347763, loss : 2.041050910949707\n",
            "epoch: 8, classification_loss: 1.7527108192443848, Val Loss: 70.80080330371857, loss : 2.0444114208221436\n",
            "epoch: 9, classification_loss: 1.7466436624526978, Val Loss: 70.73387789726257, loss : 2.0286641120910645\n",
            "epoch: 10, classification_loss: 1.7519227266311646, Val Loss: 70.73684859275818, loss : 2.022287368774414\n",
            "epoch: 11, classification_loss: 1.7473506927490234, Val Loss: 70.71523010730743, loss : 2.0249276161193848\n",
            "epoch: 12, classification_loss: 1.7509537935256958, Val Loss: 70.76577877998352, loss : 1.7509537935256958\n",
            "epoch: 13, classification_loss: 1.723580002784729, Val Loss: 70.74158108234406, loss : 2.0556344985961914\n",
            "epoch: 14, classification_loss: 1.7044060230255127, Val Loss: 70.74891257286072, loss : 1.9590617418289185\n",
            "epoch: 15, classification_loss: 1.733379602432251, Val Loss: 70.73120427131653, loss : 1.9995009899139404\n",
            "epoch: 16, classification_loss: 1.7406913042068481, Val Loss: 70.71230089664459, loss : 1.9890131950378418\n",
            "epoch: 17, classification_loss: 1.734094262123108, Val Loss: 70.73504292964935, loss : 1.9735944271087646\n",
            "epoch: 18, classification_loss: 1.7193747758865356, Val Loss: 70.75647151470184, loss : 1.9998619556427002\n",
            "epoch: 19, classification_loss: 1.71183180809021, Val Loss: 70.8004195690155, loss : 1.9979026317596436\n",
            "Batch: 63, Test Acc: 0.5731169871794872\n",
            "Batch: 64:\n",
            "epoch: 0, classification_loss: 1.8320682048797607, Val Loss: 70.95604228973389, loss : 1.8320682048797607\n",
            "epoch: 1, classification_loss: 1.7871685028076172, Val Loss: 70.94710397720337, loss : 2.1483542919158936\n",
            "epoch: 2, classification_loss: 1.7624197006225586, Val Loss: 70.73787426948547, loss : 2.0690176486968994\n",
            "epoch: 3, classification_loss: 1.786344051361084, Val Loss: 70.7776198387146, loss : 2.0601720809936523\n",
            "epoch: 4, classification_loss: 1.8199490308761597, Val Loss: 70.79611563682556, loss : 2.086118459701538\n",
            "epoch: 5, classification_loss: 1.806855320930481, Val Loss: 70.82163214683533, loss : 2.0594093799591064\n",
            "epoch: 6, classification_loss: 1.792041301727295, Val Loss: 70.73443853855133, loss : 2.063401222229004\n",
            "epoch: 7, classification_loss: 1.7824453115463257, Val Loss: 70.78457069396973, loss : 2.0713138580322266\n",
            "epoch: 8, classification_loss: 1.7978147268295288, Val Loss: 70.83731460571289, loss : 2.047433376312256\n",
            "epoch: 9, classification_loss: 1.8059465885162354, Val Loss: 70.80334484577179, loss : 2.0766801834106445\n",
            "epoch: 10, classification_loss: 1.7990258932113647, Val Loss: 70.74194002151489, loss : 2.048849582672119\n",
            "epoch: 11, classification_loss: 1.8017017841339111, Val Loss: 70.80851662158966, loss : 2.0522491931915283\n",
            "epoch: 12, classification_loss: 1.7906527519226074, Val Loss: 70.8016768693924, loss : 2.039301872253418\n",
            "epoch: 13, classification_loss: 1.8013731241226196, Val Loss: 70.78778302669525, loss : 2.0570297241210938\n",
            "epoch: 14, classification_loss: 1.8064396381378174, Val Loss: 70.76399874687195, loss : 2.0532922744750977\n",
            "epoch: 15, classification_loss: 1.803452491760254, Val Loss: 70.7797657251358, loss : 2.047150135040283\n",
            "epoch: 16, classification_loss: 1.7867481708526611, Val Loss: 70.8104350566864, loss : 2.0290966033935547\n",
            "epoch: 17, classification_loss: 1.8057348728179932, Val Loss: 70.79974710941315, loss : 2.0503506660461426\n",
            "epoch: 18, classification_loss: 1.7999131679534912, Val Loss: 70.78550434112549, loss : 2.029768943786621\n",
            "epoch: 19, classification_loss: 1.8096306324005127, Val Loss: 70.78973984718323, loss : 2.0472285747528076\n",
            "Batch: 64, Test Acc: 0.5702123397435898\n",
            "Batch: 65:\n",
            "epoch: 0, classification_loss: 1.7906994819641113, Val Loss: 70.78588831424713, loss : 1.7906994819641113\n",
            "epoch: 1, classification_loss: 1.7525302171707153, Val Loss: 70.84439659118652, loss : 2.095078468322754\n",
            "epoch: 2, classification_loss: 1.7237924337387085, Val Loss: 70.79538714885712, loss : 1.9885716438293457\n",
            "epoch: 3, classification_loss: 1.7564647197723389, Val Loss: 70.76710534095764, loss : 2.034881114959717\n",
            "epoch: 4, classification_loss: 1.7888668775558472, Val Loss: 70.74900364875793, loss : 2.03265380859375\n",
            "epoch: 5, classification_loss: 1.7635648250579834, Val Loss: 70.73374247550964, loss : 2.0303690433502197\n",
            "epoch: 6, classification_loss: 1.7525086402893066, Val Loss: 70.7596971988678, loss : 2.024148464202881\n",
            "epoch: 7, classification_loss: 1.7649670839309692, Val Loss: 70.77537775039673, loss : 2.0310912132263184\n",
            "epoch: 8, classification_loss: 1.766119360923767, Val Loss: 70.74752998352051, loss : 2.013765573501587\n",
            "epoch: 9, classification_loss: 1.7711524963378906, Val Loss: 70.74423205852509, loss : 2.0287680625915527\n",
            "epoch: 10, classification_loss: 1.7572005987167358, Val Loss: 70.73500406742096, loss : 2.0068838596343994\n",
            "epoch: 11, classification_loss: 1.7734431028366089, Val Loss: 70.74198734760284, loss : 2.018672227859497\n",
            "epoch: 12, classification_loss: 1.7623108625411987, Val Loss: 70.79835057258606, loss : 1.9991599321365356\n",
            "epoch: 13, classification_loss: 1.760960578918457, Val Loss: 70.74626886844635, loss : 2.0113301277160645\n",
            "epoch: 14, classification_loss: 1.7675471305847168, Val Loss: 70.732346534729, loss : 2.0050036907196045\n",
            "epoch: 15, classification_loss: 1.7577639818191528, Val Loss: 70.77829992771149, loss : 2.0040650367736816\n",
            "epoch: 16, classification_loss: 1.7708784341812134, Val Loss: 70.76869106292725, loss : 2.0018036365509033\n",
            "epoch: 17, classification_loss: 1.7597200870513916, Val Loss: 70.75995814800262, loss : 2.0060231685638428\n",
            "epoch: 18, classification_loss: 1.7652792930603027, Val Loss: 70.77816379070282, loss : 1.9953252077102661\n",
            "epoch: 19, classification_loss: 1.7774052619934082, Val Loss: 70.75563788414001, loss : 2.0147483348846436\n",
            "Batch: 65, Test Acc: 0.5717147435897436\n",
            "Batch: 66:\n",
            "epoch: 0, classification_loss: 1.822277307510376, Val Loss: 70.79406368732452, loss : 1.822277307510376\n",
            "epoch: 1, classification_loss: 1.775730013847351, Val Loss: 70.79792618751526, loss : 2.11661434173584\n",
            "epoch: 2, classification_loss: 1.7528153657913208, Val Loss: 70.78525352478027, loss : 2.0256099700927734\n",
            "epoch: 3, classification_loss: 1.7590657472610474, Val Loss: 70.77829599380493, loss : 2.0538859367370605\n",
            "epoch: 4, classification_loss: 1.7888531684875488, Val Loss: 70.73270034790039, loss : 2.043508529663086\n",
            "epoch: 5, classification_loss: 1.7928669452667236, Val Loss: 70.76008296012878, loss : 2.0662407875061035\n",
            "epoch: 6, classification_loss: 1.7881630659103394, Val Loss: 70.76460337638855, loss : 2.051070213317871\n",
            "epoch: 7, classification_loss: 1.7977229356765747, Val Loss: 70.76466798782349, loss : 2.068164110183716\n",
            "epoch: 8, classification_loss: 1.7937182188034058, Val Loss: 70.77924489974976, loss : 2.0537612438201904\n",
            "epoch: 9, classification_loss: 1.7874404191970825, Val Loss: 70.80880153179169, loss : 2.056429862976074\n",
            "epoch: 10, classification_loss: 1.7919920682907104, Val Loss: 70.75704860687256, loss : 2.054441213607788\n",
            "epoch: 11, classification_loss: 1.7903732061386108, Val Loss: 70.73538625240326, loss : 2.043236255645752\n",
            "epoch: 12, classification_loss: 1.7858062982559204, Val Loss: 70.76531612873077, loss : 2.043231248855591\n",
            "epoch: 13, classification_loss: 1.784851312637329, Val Loss: 70.77563846111298, loss : 2.0402379035949707\n",
            "epoch: 14, classification_loss: 1.7823562622070312, Val Loss: 70.7682101726532, loss : 2.032754421234131\n",
            "epoch: 15, classification_loss: 1.7948060035705566, Val Loss: 70.76256120204926, loss : 2.0492188930511475\n",
            "epoch: 16, classification_loss: 1.795723795890808, Val Loss: 70.8077164888382, loss : 2.047868251800537\n",
            "epoch: 17, classification_loss: 1.7990691661834717, Val Loss: 70.78637826442719, loss : 2.049017906188965\n",
            "epoch: 18, classification_loss: 1.7961070537567139, Val Loss: 70.8183718919754, loss : 2.043255090713501\n",
            "epoch: 19, classification_loss: 1.8000541925430298, Val Loss: 70.72758960723877, loss : 2.046534776687622\n",
            "Batch: 66, Test Acc: 0.5694110576923077\n",
            "Batch: 67:\n",
            "epoch: 0, classification_loss: 1.851892113685608, Val Loss: 70.81182193756104, loss : 1.851892113685608\n",
            "epoch: 1, classification_loss: 1.792291283607483, Val Loss: 70.81372702121735, loss : 2.1401100158691406\n",
            "epoch: 2, classification_loss: 1.784259557723999, Val Loss: 70.70671916007996, loss : 2.067277669906616\n",
            "epoch: 3, classification_loss: 1.7929248809814453, Val Loss: 70.7847090959549, loss : 1.7929248809814453\n",
            "epoch: 4, classification_loss: 1.7902320623397827, Val Loss: 70.78637194633484, loss : 2.1431167125701904\n",
            "epoch: 5, classification_loss: 1.7648396492004395, Val Loss: 70.79295480251312, loss : 2.050710439682007\n",
            "epoch: 6, classification_loss: 1.7538293600082397, Val Loss: 70.73330557346344, loss : 2.07942533493042\n",
            "epoch: 7, classification_loss: 1.7771121263504028, Val Loss: 70.74732160568237, loss : 2.091700553894043\n",
            "epoch: 8, classification_loss: 1.7996044158935547, Val Loss: 70.74407732486725, loss : 2.042294502258301\n",
            "epoch: 9, classification_loss: 1.785662293434143, Val Loss: 70.71423828601837, loss : 2.0748889446258545\n",
            "epoch: 10, classification_loss: 1.7709383964538574, Val Loss: 70.72451865673065, loss : 2.055655002593994\n",
            "epoch: 11, classification_loss: 1.7666051387786865, Val Loss: 70.82763206958771, loss : 2.0417652130126953\n",
            "epoch: 12, classification_loss: 1.7799137830734253, Val Loss: 70.76523327827454, loss : 2.0574090480804443\n",
            "epoch: 13, classification_loss: 1.769934892654419, Val Loss: 70.73169696331024, loss : 2.0248892307281494\n",
            "epoch: 14, classification_loss: 1.779829502105713, Val Loss: 70.73783493041992, loss : 2.0410947799682617\n",
            "epoch: 15, classification_loss: 1.7685739994049072, Val Loss: 70.74666607379913, loss : 2.0211734771728516\n",
            "epoch: 16, classification_loss: 1.775683879852295, Val Loss: 70.7304300069809, loss : 2.0294477939605713\n",
            "epoch: 17, classification_loss: 1.7756394147872925, Val Loss: 70.78682219982147, loss : 2.0308620929718018\n",
            "epoch: 18, classification_loss: 1.7716253995895386, Val Loss: 70.7533746957779, loss : 2.021517276763916\n",
            "epoch: 19, classification_loss: 1.777417540550232, Val Loss: 70.7052150964737, loss : 2.0261831283569336\n",
            "Batch: 67, Test Acc: 0.5718149038461539\n",
            "Batch: 68:\n",
            "epoch: 0, classification_loss: 1.8600327968597412, Val Loss: 70.78679466247559, loss : 1.8600327968597412\n",
            "epoch: 1, classification_loss: 1.815674066543579, Val Loss: 70.83438849449158, loss : 2.1549360752105713\n",
            "epoch: 2, classification_loss: 1.7971445322036743, Val Loss: 70.78719794750214, loss : 2.0672974586486816\n",
            "epoch: 3, classification_loss: 1.8155567646026611, Val Loss: 70.75936472415924, loss : 2.0968503952026367\n",
            "epoch: 4, classification_loss: 1.8463733196258545, Val Loss: 70.76951992511749, loss : 2.0980114936828613\n",
            "epoch: 5, classification_loss: 1.8385542631149292, Val Loss: 70.77056109905243, loss : 2.0896077156066895\n",
            "epoch: 6, classification_loss: 1.8272210359573364, Val Loss: 70.76399302482605, loss : 2.0966920852661133\n",
            "epoch: 7, classification_loss: 1.8341076374053955, Val Loss: 70.7848424911499, loss : 2.0863335132598877\n",
            "epoch: 8, classification_loss: 1.8322410583496094, Val Loss: 70.80256760120392, loss : 2.096064329147339\n",
            "epoch: 9, classification_loss: 1.8231652975082397, Val Loss: 70.78247308731079, loss : 2.0768508911132812\n",
            "epoch: 10, classification_loss: 1.8336323499679565, Val Loss: 70.79054999351501, loss : 2.087794780731201\n",
            "epoch: 11, classification_loss: 1.8498749732971191, Val Loss: 70.80771255493164, loss : 2.0906031131744385\n",
            "epoch: 12, classification_loss: 1.8416060209274292, Val Loss: 70.7558331489563, loss : 2.0869107246398926\n",
            "epoch: 13, classification_loss: 1.825408935546875, Val Loss: 70.76464939117432, loss : 2.071822166442871\n",
            "epoch: 14, classification_loss: 1.8287683725357056, Val Loss: 70.77661156654358, loss : 2.081415891647339\n",
            "epoch: 15, classification_loss: 1.8308687210083008, Val Loss: 70.77401542663574, loss : 2.0828263759613037\n",
            "epoch: 16, classification_loss: 1.8359322547912598, Val Loss: 70.8028107881546, loss : 2.080423593521118\n",
            "epoch: 17, classification_loss: 1.8400113582611084, Val Loss: 70.84525644779205, loss : 2.081596851348877\n",
            "epoch: 18, classification_loss: 1.839080572128296, Val Loss: 70.80693626403809, loss : 2.074296712875366\n",
            "epoch: 19, classification_loss: 1.8297196626663208, Val Loss: 70.78159260749817, loss : 2.06734037399292\n",
            "Batch: 68, Test Acc: 0.5700120192307693\n",
            "Batch: 69:\n",
            "epoch: 0, classification_loss: 1.8222576379776, Val Loss: 70.6947546005249, loss : 1.8222576379776\n",
            "epoch: 1, classification_loss: 1.7704179286956787, Val Loss: 70.79711544513702, loss : 1.7704179286956787\n",
            "epoch: 2, classification_loss: 1.730932593345642, Val Loss: 70.78860461711884, loss : 2.096332311630249\n",
            "epoch: 3, classification_loss: 1.704444408416748, Val Loss: 70.68899011611938, loss : 2.023334503173828\n",
            "epoch: 4, classification_loss: 1.7433489561080933, Val Loss: 70.71863305568695, loss : 2.025172233581543\n",
            "epoch: 5, classification_loss: 1.779736042022705, Val Loss: 70.75102913379669, loss : 2.0237033367156982\n",
            "epoch: 6, classification_loss: 1.760623812675476, Val Loss: 70.69419252872467, loss : 2.0217766761779785\n",
            "epoch: 7, classification_loss: 1.7500616312026978, Val Loss: 70.70463705062866, loss : 2.0255112648010254\n",
            "epoch: 8, classification_loss: 1.731594443321228, Val Loss: 70.7158854007721, loss : 2.0158112049102783\n",
            "epoch: 9, classification_loss: 1.740946888923645, Val Loss: 70.76095628738403, loss : 2.0103886127471924\n",
            "epoch: 10, classification_loss: 1.7506364583969116, Val Loss: 70.69746899604797, loss : 2.0114338397979736\n",
            "epoch: 11, classification_loss: 1.7594587802886963, Val Loss: 70.68337082862854, loss : 2.0146167278289795\n",
            "epoch: 12, classification_loss: 1.7671337127685547, Val Loss: 70.70797348022461, loss : 1.7671337127685547\n",
            "epoch: 13, classification_loss: 1.739618182182312, Val Loss: 70.73087465763092, loss : 2.0716309547424316\n",
            "epoch: 14, classification_loss: 1.72366201877594, Val Loss: 70.7511134147644, loss : 1.9768517017364502\n",
            "epoch: 15, classification_loss: 1.7332576513290405, Val Loss: 70.71865975856781, loss : 1.9965063333511353\n",
            "epoch: 16, classification_loss: 1.7641388177871704, Val Loss: 70.74996733665466, loss : 2.0074257850646973\n",
            "epoch: 17, classification_loss: 1.7586019039154053, Val Loss: 70.72085356712341, loss : 1.9996254444122314\n",
            "epoch: 18, classification_loss: 1.732243537902832, Val Loss: 70.70580530166626, loss : 2.0138401985168457\n",
            "epoch: 19, classification_loss: 1.7319122552871704, Val Loss: 70.74953365325928, loss : 2.010631799697876\n",
            "Batch: 69, Test Acc: 0.5726161858974359\n",
            "Batch: 70:\n",
            "epoch: 0, classification_loss: 1.8400906324386597, Val Loss: 70.85889661312103, loss : 1.8400906324386597\n",
            "epoch: 1, classification_loss: 1.8007131814956665, Val Loss: 70.8368946313858, loss : 2.1548712253570557\n",
            "epoch: 2, classification_loss: 1.7804653644561768, Val Loss: 70.71367585659027, loss : 2.070408821105957\n",
            "epoch: 3, classification_loss: 1.7968287467956543, Val Loss: 70.76325142383575, loss : 2.0938127040863037\n",
            "epoch: 4, classification_loss: 1.8283485174179077, Val Loss: 70.75479578971863, loss : 2.092766046524048\n",
            "epoch: 5, classification_loss: 1.8172459602355957, Val Loss: 70.73433363437653, loss : 2.091073751449585\n",
            "epoch: 6, classification_loss: 1.8100948333740234, Val Loss: 70.78883695602417, loss : 2.0853185653686523\n",
            "epoch: 7, classification_loss: 1.8050038814544678, Val Loss: 70.8157548904419, loss : 2.0817153453826904\n",
            "epoch: 8, classification_loss: 1.816831350326538, Val Loss: 70.76502799987793, loss : 2.066350221633911\n",
            "epoch: 9, classification_loss: 1.8179237842559814, Val Loss: 70.7906768321991, loss : 2.0944156646728516\n",
            "epoch: 10, classification_loss: 1.8070523738861084, Val Loss: 70.80878758430481, loss : 2.067966938018799\n",
            "epoch: 11, classification_loss: 1.8073490858078003, Val Loss: 70.80880165100098, loss : 2.083040237426758\n",
            "epoch: 12, classification_loss: 1.8175958395004272, Val Loss: 70.83478558063507, loss : 2.0722639560699463\n",
            "epoch: 13, classification_loss: 1.8177759647369385, Val Loss: 70.82963132858276, loss : 2.0760021209716797\n",
            "epoch: 14, classification_loss: 1.8142640590667725, Val Loss: 70.82940435409546, loss : 2.063486337661743\n",
            "epoch: 15, classification_loss: 1.811070203781128, Val Loss: 70.80513036251068, loss : 2.058532953262329\n",
            "epoch: 16, classification_loss: 1.805467128753662, Val Loss: 70.84503984451294, loss : 2.046738386154175\n",
            "epoch: 17, classification_loss: 1.8218313455581665, Val Loss: 70.87446367740631, loss : 2.0737762451171875\n",
            "epoch: 18, classification_loss: 1.8071521520614624, Val Loss: 70.87303650379181, loss : 2.0433349609375\n",
            "epoch: 19, classification_loss: 1.8271803855895996, Val Loss: 70.85924708843231, loss : 2.0622994899749756\n",
            "Batch: 70, Test Acc: 0.5683092948717948\n",
            "Batch: 71:\n",
            "epoch: 0, classification_loss: 1.8216465711593628, Val Loss: 70.80696439743042, loss : 1.8216465711593628\n",
            "epoch: 1, classification_loss: 1.7802072763442993, Val Loss: 70.70437848567963, loss : 2.1087894439697266\n",
            "epoch: 2, classification_loss: 1.7601745128631592, Val Loss: 70.76749289035797, loss : 2.016049385070801\n",
            "epoch: 3, classification_loss: 1.7800543308258057, Val Loss: 70.75537943840027, loss : 2.0724942684173584\n",
            "epoch: 4, classification_loss: 1.8048410415649414, Val Loss: 70.75621449947357, loss : 2.0507054328918457\n",
            "epoch: 5, classification_loss: 1.801899790763855, Val Loss: 70.69359147548676, loss : 2.0710623264312744\n",
            "epoch: 6, classification_loss: 1.7794898748397827, Val Loss: 70.7214103937149, loss : 2.0595345497131348\n",
            "epoch: 7, classification_loss: 1.7880147695541382, Val Loss: 70.70933890342712, loss : 2.0592691898345947\n",
            "epoch: 8, classification_loss: 1.7864725589752197, Val Loss: 70.7093448638916, loss : 2.0498054027557373\n",
            "epoch: 9, classification_loss: 1.7957675457000732, Val Loss: 70.70312583446503, loss : 2.0506198406219482\n",
            "epoch: 10, classification_loss: 1.7949804067611694, Val Loss: 70.72630524635315, loss : 2.0458970069885254\n",
            "epoch: 11, classification_loss: 1.796861171722412, Val Loss: 70.70475673675537, loss : 2.041142225265503\n",
            "epoch: 12, classification_loss: 1.7955809831619263, Val Loss: 70.72147953510284, loss : 2.0532920360565186\n",
            "epoch: 13, classification_loss: 1.7933233976364136, Val Loss: 70.7256851196289, loss : 2.052278757095337\n",
            "epoch: 14, classification_loss: 1.7866101264953613, Val Loss: 70.73063778877258, loss : 2.0415356159210205\n",
            "epoch: 15, classification_loss: 1.7988530397415161, Val Loss: 70.72626531124115, loss : 2.045255661010742\n",
            "epoch: 16, classification_loss: 1.7922033071517944, Val Loss: 70.71614503860474, loss : 2.0429418087005615\n",
            "epoch: 17, classification_loss: 1.793528437614441, Val Loss: 70.72394561767578, loss : 2.0393013954162598\n",
            "epoch: 18, classification_loss: 1.8011844158172607, Val Loss: 70.76810026168823, loss : 2.0471911430358887\n",
            "epoch: 19, classification_loss: 1.8018348217010498, Val Loss: 70.7115547657013, loss : 2.0461957454681396\n",
            "Batch: 71, Test Acc: 0.5710136217948718\n",
            "Batch: 72:\n",
            "epoch: 0, classification_loss: 1.8557521104812622, Val Loss: 70.86345374584198, loss : 1.8557521104812622\n",
            "epoch: 1, classification_loss: 1.8143532276153564, Val Loss: 70.7624808549881, loss : 2.1594412326812744\n",
            "epoch: 2, classification_loss: 1.7986218929290771, Val Loss: 70.728879570961, loss : 2.0562283992767334\n",
            "epoch: 3, classification_loss: 1.818878173828125, Val Loss: 70.70731735229492, loss : 2.1000723838806152\n",
            "epoch: 4, classification_loss: 1.8399096727371216, Val Loss: 70.7500137090683, loss : 2.086740255355835\n",
            "epoch: 5, classification_loss: 1.8253072500228882, Val Loss: 70.73021924495697, loss : 2.090636968612671\n",
            "epoch: 6, classification_loss: 1.8123974800109863, Val Loss: 70.71997666358948, loss : 2.0919055938720703\n",
            "epoch: 7, classification_loss: 1.8252007961273193, Val Loss: 70.75591695308685, loss : 2.0823590755462646\n",
            "epoch: 8, classification_loss: 1.8261786699295044, Val Loss: 70.79487252235413, loss : 2.080320119857788\n",
            "epoch: 9, classification_loss: 1.8240435123443604, Val Loss: 70.69431984424591, loss : 2.072978973388672\n",
            "epoch: 10, classification_loss: 1.8352185487747192, Val Loss: 70.72539639472961, loss : 2.079153299331665\n",
            "epoch: 11, classification_loss: 1.8377059698104858, Val Loss: 70.8060714006424, loss : 2.077925443649292\n",
            "epoch: 12, classification_loss: 1.823804259300232, Val Loss: 70.7364182472229, loss : 2.0668253898620605\n",
            "epoch: 13, classification_loss: 1.8239845037460327, Val Loss: 70.70952904224396, loss : 2.070675849914551\n",
            "epoch: 14, classification_loss: 1.8329542875289917, Val Loss: 70.76276516914368, loss : 2.0781164169311523\n",
            "epoch: 15, classification_loss: 1.8307632207870483, Val Loss: 70.79493188858032, loss : 2.066511869430542\n",
            "epoch: 16, classification_loss: 1.8371481895446777, Val Loss: 70.7401602268219, loss : 2.0729641914367676\n",
            "epoch: 17, classification_loss: 1.8281677961349487, Val Loss: 70.71092224121094, loss : 2.064812421798706\n",
            "epoch: 18, classification_loss: 1.8298460245132446, Val Loss: 70.74687850475311, loss : 2.0696117877960205\n",
            "epoch: 19, classification_loss: 1.822356939315796, Val Loss: 70.75843179225922, loss : 2.0576205253601074\n",
            "Batch: 72, Test Acc: 0.5744190705128205\n",
            "Batch: 73:\n",
            "epoch: 0, classification_loss: 1.8517162799835205, Val Loss: 70.73362112045288, loss : 1.8517162799835205\n",
            "epoch: 1, classification_loss: 1.8184890747070312, Val Loss: 70.77721703052521, loss : 2.171328067779541\n",
            "epoch: 2, classification_loss: 1.7859426736831665, Val Loss: 70.7086170911789, loss : 2.0667364597320557\n",
            "epoch: 3, classification_loss: 1.8175491094589233, Val Loss: 70.70583045482635, loss : 2.1101813316345215\n",
            "epoch: 4, classification_loss: 1.8448988199234009, Val Loss: 70.690505027771, loss : 2.1083693504333496\n",
            "epoch: 5, classification_loss: 1.8461683988571167, Val Loss: 70.70900428295135, loss : 2.107468366622925\n",
            "epoch: 6, classification_loss: 1.8353482484817505, Val Loss: 70.73380124568939, loss : 2.10502552986145\n",
            "epoch: 7, classification_loss: 1.8333182334899902, Val Loss: 70.71513676643372, loss : 2.10353946685791\n",
            "epoch: 8, classification_loss: 1.8238359689712524, Val Loss: 70.71272373199463, loss : 2.08123779296875\n",
            "epoch: 9, classification_loss: 1.8184504508972168, Val Loss: 70.72329103946686, loss : 2.0860068798065186\n",
            "epoch: 10, classification_loss: 1.8151488304138184, Val Loss: 70.72891736030579, loss : 2.0796945095062256\n",
            "epoch: 11, classification_loss: 1.8318837881088257, Val Loss: 70.73423612117767, loss : 2.0884249210357666\n",
            "epoch: 12, classification_loss: 1.8342933654785156, Val Loss: 70.7202183008194, loss : 2.0850183963775635\n",
            "epoch: 13, classification_loss: 1.8310106992721558, Val Loss: 70.72857904434204, loss : 2.084319591522217\n",
            "epoch: 14, classification_loss: 1.8308302164077759, Val Loss: 70.74141466617584, loss : 2.0786337852478027\n",
            "epoch: 15, classification_loss: 1.8232249021530151, Val Loss: 70.75729990005493, loss : 2.0745432376861572\n",
            "epoch: 16, classification_loss: 1.830908179283142, Val Loss: 70.7405788898468, loss : 2.0852959156036377\n",
            "epoch: 17, classification_loss: 1.817136526107788, Val Loss: 70.71949923038483, loss : 2.063096523284912\n",
            "epoch: 18, classification_loss: 1.8343089818954468, Val Loss: 70.73576068878174, loss : 2.076160192489624\n",
            "epoch: 19, classification_loss: 1.8217380046844482, Val Loss: 70.73902380466461, loss : 2.064098596572876\n",
            "Batch: 73, Test Acc: 0.5733173076923077\n",
            "Batch: 74:\n",
            "epoch: 0, classification_loss: 1.8528366088867188, Val Loss: 70.73635017871857, loss : 1.8528366088867188\n",
            "epoch: 1, classification_loss: 1.8155102729797363, Val Loss: 70.79743611812592, loss : 2.1599655151367188\n",
            "epoch: 2, classification_loss: 1.7954038381576538, Val Loss: 70.75733721256256, loss : 2.071906328201294\n",
            "epoch: 3, classification_loss: 1.816096305847168, Val Loss: 70.72412824630737, loss : 2.097507953643799\n",
            "epoch: 4, classification_loss: 1.838145136833191, Val Loss: 70.70816349983215, loss : 2.089418888092041\n",
            "epoch: 5, classification_loss: 1.8394304513931274, Val Loss: 70.73302268981934, loss : 2.0957040786743164\n",
            "epoch: 6, classification_loss: 1.8317656517028809, Val Loss: 70.72578608989716, loss : 2.092942714691162\n",
            "epoch: 7, classification_loss: 1.8047603368759155, Val Loss: 70.73580825328827, loss : 2.072784185409546\n",
            "epoch: 8, classification_loss: 1.8314199447631836, Val Loss: 70.72552001476288, loss : 2.0894222259521484\n",
            "epoch: 9, classification_loss: 1.8264288902282715, Val Loss: 70.72333812713623, loss : 2.088292121887207\n",
            "epoch: 10, classification_loss: 1.8210110664367676, Val Loss: 70.71273124217987, loss : 2.0710463523864746\n",
            "epoch: 11, classification_loss: 1.8334572315216064, Val Loss: 70.7315024137497, loss : 2.072148323059082\n",
            "epoch: 12, classification_loss: 1.83387291431427, Val Loss: 70.7425799369812, loss : 2.0742056369781494\n",
            "epoch: 13, classification_loss: 1.827518105506897, Val Loss: 70.72242212295532, loss : 2.069207191467285\n",
            "epoch: 14, classification_loss: 1.8367546796798706, Val Loss: 70.72181248664856, loss : 2.0798568725585938\n",
            "epoch: 15, classification_loss: 1.8156447410583496, Val Loss: 70.7165459394455, loss : 2.058931589126587\n",
            "epoch: 16, classification_loss: 1.8339217901229858, Val Loss: 70.75771117210388, loss : 2.068319797515869\n",
            "epoch: 17, classification_loss: 1.8312962055206299, Val Loss: 70.76192283630371, loss : 2.069350004196167\n",
            "epoch: 18, classification_loss: 1.8370709419250488, Val Loss: 70.72964608669281, loss : 2.071659803390503\n",
            "epoch: 19, classification_loss: 1.8240102529525757, Val Loss: 70.73843455314636, loss : 2.064965009689331\n",
            "Batch: 74, Test Acc: 0.5726161858974359\n",
            "Batch: 75:\n",
            "epoch: 0, classification_loss: 1.835753083229065, Val Loss: 70.73061573505402, loss : 1.835753083229065\n",
            "epoch: 1, classification_loss: 1.800368070602417, Val Loss: 70.71991801261902, loss : 2.1300551891326904\n",
            "epoch: 2, classification_loss: 1.7785592079162598, Val Loss: 70.75425338745117, loss : 2.0439815521240234\n",
            "epoch: 3, classification_loss: 1.8060108423233032, Val Loss: 70.73324012756348, loss : 2.081115245819092\n",
            "epoch: 4, classification_loss: 1.8260440826416016, Val Loss: 70.7285133600235, loss : 2.071669578552246\n",
            "epoch: 5, classification_loss: 1.8244816064834595, Val Loss: 70.7416501045227, loss : 2.0799200534820557\n",
            "epoch: 6, classification_loss: 1.8133330345153809, Val Loss: 70.73633694648743, loss : 2.0710628032684326\n",
            "epoch: 7, classification_loss: 1.811129093170166, Val Loss: 70.75392365455627, loss : 2.071404457092285\n",
            "epoch: 8, classification_loss: 1.8169182538986206, Val Loss: 70.70165574550629, loss : 2.0708627700805664\n",
            "epoch: 9, classification_loss: 1.8067272901535034, Val Loss: 70.71735072135925, loss : 2.062340021133423\n",
            "epoch: 10, classification_loss: 1.810046911239624, Val Loss: 70.80458068847656, loss : 2.0641918182373047\n",
            "epoch: 11, classification_loss: 1.8229410648345947, Val Loss: 70.78454864025116, loss : 2.0680253505706787\n",
            "epoch: 12, classification_loss: 1.8055346012115479, Val Loss: 70.7476634979248, loss : 2.0472946166992188\n",
            "epoch: 13, classification_loss: 1.8044618368148804, Val Loss: 70.74452304840088, loss : 2.0498745441436768\n",
            "epoch: 14, classification_loss: 1.7980263233184814, Val Loss: 70.76976382732391, loss : 2.0394580364227295\n",
            "epoch: 15, classification_loss: 1.8245794773101807, Val Loss: 70.77791798114777, loss : 2.0606698989868164\n",
            "epoch: 16, classification_loss: 1.8146741390228271, Val Loss: 70.76271617412567, loss : 2.051508903503418\n",
            "epoch: 17, classification_loss: 1.8177330493927002, Val Loss: 70.77000534534454, loss : 2.057323694229126\n",
            "epoch: 18, classification_loss: 1.8133020401000977, Val Loss: 70.7822083234787, loss : 2.044966459274292\n",
            "epoch: 19, classification_loss: 1.8072795867919922, Val Loss: 70.80475962162018, loss : 2.0410683155059814\n",
            "Batch: 75, Test Acc: 0.5679086538461539\n",
            "Batch: 76:\n",
            "epoch: 0, classification_loss: 1.8278121948242188, Val Loss: 70.69884848594666, loss : 1.8278121948242188\n",
            "epoch: 1, classification_loss: 1.7962931394577026, Val Loss: 70.71863508224487, loss : 2.1326980590820312\n",
            "epoch: 2, classification_loss: 1.7683959007263184, Val Loss: 70.71133422851562, loss : 2.042137384414673\n",
            "epoch: 3, classification_loss: 1.7915009260177612, Val Loss: 70.73669147491455, loss : 2.072763442993164\n",
            "epoch: 4, classification_loss: 1.8168118000030518, Val Loss: 70.68152832984924, loss : 2.0701184272766113\n",
            "epoch: 5, classification_loss: 1.820483922958374, Val Loss: 70.69626986980438, loss : 2.0763144493103027\n",
            "epoch: 6, classification_loss: 1.802258014678955, Val Loss: 70.73285210132599, loss : 2.071051597595215\n",
            "epoch: 7, classification_loss: 1.7924396991729736, Val Loss: 70.7285908460617, loss : 2.065415859222412\n",
            "epoch: 8, classification_loss: 1.7867021560668945, Val Loss: 70.7118501663208, loss : 2.0647590160369873\n",
            "epoch: 9, classification_loss: 1.7947295904159546, Val Loss: 70.71233093738556, loss : 2.056312322616577\n",
            "epoch: 10, classification_loss: 1.7957344055175781, Val Loss: 70.72543239593506, loss : 2.0624887943267822\n",
            "epoch: 11, classification_loss: 1.7952600717544556, Val Loss: 70.757293343544, loss : 2.0464699268341064\n",
            "epoch: 12, classification_loss: 1.8096383810043335, Val Loss: 70.7779051065445, loss : 2.0624237060546875\n",
            "epoch: 13, classification_loss: 1.8007235527038574, Val Loss: 70.72938668727875, loss : 2.0561981201171875\n",
            "epoch: 14, classification_loss: 1.806261658668518, Val Loss: 70.74590015411377, loss : 2.0556201934814453\n",
            "epoch: 15, classification_loss: 1.8010847568511963, Val Loss: 70.75480723381042, loss : 2.052757740020752\n",
            "epoch: 16, classification_loss: 1.794480323791504, Val Loss: 70.73957991600037, loss : 2.035234212875366\n",
            "epoch: 17, classification_loss: 1.8069366216659546, Val Loss: 70.78625869750977, loss : 2.0616884231567383\n",
            "epoch: 18, classification_loss: 1.8114867210388184, Val Loss: 70.78439521789551, loss : 2.0550942420959473\n",
            "epoch: 19, classification_loss: 1.7981295585632324, Val Loss: 70.74862062931061, loss : 2.0539515018463135\n",
            "Batch: 76, Test Acc: 0.5739182692307693\n",
            "Batch: 77:\n",
            "epoch: 0, classification_loss: 1.836745023727417, Val Loss: 70.78444957733154, loss : 1.836745023727417\n",
            "epoch: 1, classification_loss: 1.7953330278396606, Val Loss: 70.82521259784698, loss : 2.127171516418457\n",
            "epoch: 2, classification_loss: 1.7719484567642212, Val Loss: 70.79579532146454, loss : 2.048691511154175\n",
            "epoch: 3, classification_loss: 1.7842737436294556, Val Loss: 70.74069619178772, loss : 2.069722890853882\n",
            "epoch: 4, classification_loss: 1.8197110891342163, Val Loss: 70.75250518321991, loss : 2.0718111991882324\n",
            "epoch: 5, classification_loss: 1.8131669759750366, Val Loss: 70.70675480365753, loss : 2.067197561264038\n",
            "epoch: 6, classification_loss: 1.797773003578186, Val Loss: 70.70571684837341, loss : 2.0551979541778564\n",
            "epoch: 7, classification_loss: 1.8143682479858398, Val Loss: 70.78502333164215, loss : 2.0787315368652344\n",
            "epoch: 8, classification_loss: 1.796797513961792, Val Loss: 70.78607833385468, loss : 2.0546324253082275\n",
            "epoch: 9, classification_loss: 1.7985066175460815, Val Loss: 70.75246286392212, loss : 2.0610013008117676\n",
            "epoch: 10, classification_loss: 1.8134998083114624, Val Loss: 70.73617136478424, loss : 2.0661814212799072\n",
            "epoch: 11, classification_loss: 1.8045681715011597, Val Loss: 70.77245891094208, loss : 2.0491323471069336\n",
            "epoch: 12, classification_loss: 1.8112834692001343, Val Loss: 70.74552297592163, loss : 2.051863431930542\n",
            "epoch: 13, classification_loss: 1.7998311519622803, Val Loss: 70.73729503154755, loss : 2.046480178833008\n",
            "epoch: 14, classification_loss: 1.8076037168502808, Val Loss: 70.76680123806, loss : 2.0476858615875244\n",
            "epoch: 15, classification_loss: 1.7994812726974487, Val Loss: 70.79400789737701, loss : 2.034935712814331\n",
            "epoch: 16, classification_loss: 1.814442753791809, Val Loss: 70.76262187957764, loss : 2.0529658794403076\n",
            "epoch: 17, classification_loss: 1.8062424659729004, Val Loss: 70.7577154636383, loss : 2.0446901321411133\n",
            "epoch: 18, classification_loss: 1.8044146299362183, Val Loss: 70.77335715293884, loss : 2.0374996662139893\n",
            "epoch: 19, classification_loss: 1.8187859058380127, Val Loss: 70.77968049049377, loss : 2.0502827167510986\n",
            "Batch: 77, Test Acc: 0.5701121794871795\n",
            "Batch: 78:\n",
            "epoch: 0, classification_loss: 1.819762110710144, Val Loss: 70.76036143302917, loss : 1.819762110710144\n",
            "epoch: 1, classification_loss: 1.7669719457626343, Val Loss: 70.78380489349365, loss : 2.110933780670166\n",
            "epoch: 2, classification_loss: 1.753842830657959, Val Loss: 70.7400096654892, loss : 2.024705648422241\n",
            "epoch: 3, classification_loss: 1.7843338251113892, Val Loss: 70.69127357006073, loss : 2.064176082611084\n",
            "epoch: 4, classification_loss: 1.8065125942230225, Val Loss: 70.73268449306488, loss : 2.0532238483428955\n",
            "epoch: 5, classification_loss: 1.799291968345642, Val Loss: 70.71009469032288, loss : 2.049082040786743\n",
            "epoch: 6, classification_loss: 1.7847520112991333, Val Loss: 70.71678793430328, loss : 2.0498127937316895\n",
            "epoch: 7, classification_loss: 1.7803020477294922, Val Loss: 70.76499164104462, loss : 2.0463247299194336\n",
            "epoch: 8, classification_loss: 1.7772061824798584, Val Loss: 70.72479915618896, loss : 2.0344605445861816\n",
            "epoch: 9, classification_loss: 1.789481520652771, Val Loss: 70.7134085893631, loss : 2.041269302368164\n",
            "epoch: 10, classification_loss: 1.783729076385498, Val Loss: 70.74696385860443, loss : 2.0333995819091797\n",
            "epoch: 11, classification_loss: 1.7892577648162842, Val Loss: 70.73982512950897, loss : 2.0320591926574707\n",
            "epoch: 12, classification_loss: 1.7794263362884521, Val Loss: 70.73002910614014, loss : 2.0252933502197266\n",
            "epoch: 13, classification_loss: 1.783097267150879, Val Loss: 70.71484625339508, loss : 2.0316343307495117\n",
            "epoch: 14, classification_loss: 1.778680682182312, Val Loss: 70.82519257068634, loss : 2.0170984268188477\n",
            "epoch: 15, classification_loss: 1.793806552886963, Val Loss: 70.77983951568604, loss : 2.036013603210449\n",
            "epoch: 16, classification_loss: 1.7900104522705078, Val Loss: 70.71512758731842, loss : 2.026336193084717\n",
            "epoch: 17, classification_loss: 1.7877318859100342, Val Loss: 70.75356936454773, loss : 2.0334155559539795\n",
            "epoch: 18, classification_loss: 1.792668104171753, Val Loss: 70.80651664733887, loss : 2.0253798961639404\n",
            "epoch: 19, classification_loss: 1.7887192964553833, Val Loss: 70.70987868309021, loss : 2.0318048000335693\n",
            "Batch: 78, Test Acc: 0.5728165064102564\n",
            "Batch: 79:\n",
            "epoch: 0, classification_loss: 1.8199365139007568, Val Loss: 70.74098813533783, loss : 1.8199365139007568\n",
            "epoch: 1, classification_loss: 1.770998239517212, Val Loss: 70.74387431144714, loss : 2.0978755950927734\n",
            "epoch: 2, classification_loss: 1.7418551445007324, Val Loss: 70.75961720943451, loss : 1.9971215724945068\n",
            "epoch: 3, classification_loss: 1.7729074954986572, Val Loss: 70.74702560901642, loss : 2.046442985534668\n",
            "epoch: 4, classification_loss: 1.8040553331375122, Val Loss: 70.7033486366272, loss : 2.0358543395996094\n",
            "epoch: 5, classification_loss: 1.79043710231781, Val Loss: 70.7468832731247, loss : 2.0536506175994873\n",
            "epoch: 6, classification_loss: 1.7705152034759521, Val Loss: 70.74393689632416, loss : 2.0438392162323\n",
            "epoch: 7, classification_loss: 1.782091498374939, Val Loss: 70.76098155975342, loss : 2.03472900390625\n",
            "epoch: 8, classification_loss: 1.7896167039871216, Val Loss: 70.7175681591034, loss : 2.0355064868927\n",
            "epoch: 9, classification_loss: 1.7900571823120117, Val Loss: 70.69053089618683, loss : 2.037691116333008\n",
            "epoch: 10, classification_loss: 1.7871593236923218, Val Loss: 70.75401771068573, loss : 2.0393927097320557\n",
            "epoch: 11, classification_loss: 1.780001163482666, Val Loss: 70.7772388458252, loss : 2.034343957901001\n",
            "epoch: 12, classification_loss: 1.791773796081543, Val Loss: 70.70491194725037, loss : 2.0291190147399902\n",
            "epoch: 13, classification_loss: 1.7921279668807983, Val Loss: 70.74127662181854, loss : 2.037742853164673\n",
            "epoch: 14, classification_loss: 1.7854186296463013, Val Loss: 70.76514983177185, loss : 2.0196595191955566\n",
            "epoch: 15, classification_loss: 1.7742129564285278, Val Loss: 70.77112364768982, loss : 2.0193610191345215\n",
            "epoch: 16, classification_loss: 1.7816181182861328, Val Loss: 70.75393605232239, loss : 2.016252040863037\n",
            "epoch: 17, classification_loss: 1.783267855644226, Val Loss: 70.72588169574738, loss : 2.0255184173583984\n",
            "epoch: 18, classification_loss: 1.7907344102859497, Val Loss: 70.75287246704102, loss : 2.0196332931518555\n",
            "epoch: 19, classification_loss: 1.8004512786865234, Val Loss: 70.75054132938385, loss : 2.0342931747436523\n",
            "Batch: 79, Test Acc: 0.5720152243589743\n",
            "Batch: 80:\n",
            "epoch: 0, classification_loss: 1.8574895858764648, Val Loss: 70.67101812362671, loss : 1.8574895858764648\n",
            "epoch: 1, classification_loss: 1.8104138374328613, Val Loss: 70.88107419013977, loss : 1.8104138374328613\n",
            "epoch: 2, classification_loss: 1.74688720703125, Val Loss: 70.96431064605713, loss : 2.1260554790496826\n",
            "epoch: 3, classification_loss: 1.7271517515182495, Val Loss: 70.79388928413391, loss : 2.0750179290771484\n",
            "epoch: 4, classification_loss: 1.7494412660598755, Val Loss: 70.72321379184723, loss : 2.0636746883392334\n",
            "epoch: 5, classification_loss: 1.7946233749389648, Val Loss: 70.69711208343506, loss : 2.064504623413086\n",
            "epoch: 6, classification_loss: 1.7900198698043823, Val Loss: 70.71152710914612, loss : 2.0554423332214355\n",
            "epoch: 7, classification_loss: 1.7749607563018799, Val Loss: 70.71179533004761, loss : 2.062847852706909\n",
            "epoch: 8, classification_loss: 1.7590389251708984, Val Loss: 70.70725464820862, loss : 2.053689956665039\n",
            "epoch: 9, classification_loss: 1.7642552852630615, Val Loss: 70.69188439846039, loss : 2.0445711612701416\n",
            "epoch: 10, classification_loss: 1.7833583354949951, Val Loss: 70.67055571079254, loss : 2.0507736206054688\n",
            "epoch: 11, classification_loss: 1.777311086654663, Val Loss: 70.66765832901001, loss : 2.0455687046051025\n",
            "epoch: 12, classification_loss: 1.778489589691162, Val Loss: 70.70402598381042, loss : 2.0377962589263916\n",
            "epoch: 13, classification_loss: 1.7730830907821655, Val Loss: 70.69544649124146, loss : 2.031799554824829\n",
            "epoch: 14, classification_loss: 1.7742208242416382, Val Loss: 70.68657207489014, loss : 2.024746894836426\n",
            "epoch: 15, classification_loss: 1.7870241403579712, Val Loss: 70.67311418056488, loss : 2.033963918685913\n",
            "epoch: 16, classification_loss: 1.7805217504501343, Val Loss: 70.67450439929962, loss : 2.0316944122314453\n",
            "epoch: 17, classification_loss: 1.7843421697616577, Val Loss: 70.68932640552521, loss : 2.0276260375976562\n",
            "epoch: 18, classification_loss: 1.779572606086731, Val Loss: 70.7544481754303, loss : 2.0294923782348633\n",
            "epoch: 19, classification_loss: 1.766191840171814, Val Loss: 70.72997236251831, loss : 2.00126314163208\n",
            "Batch: 80, Test Acc: 0.5700120192307693\n",
            "Batch: 81:\n",
            "epoch: 0, classification_loss: 1.80337655544281, Val Loss: 70.92009818553925, loss : 1.80337655544281\n",
            "epoch: 1, classification_loss: 1.7769237756729126, Val Loss: 70.84862852096558, loss : 2.127329111099243\n",
            "epoch: 2, classification_loss: 1.73739492893219, Val Loss: 70.66594243049622, loss : 2.0220632553100586\n",
            "epoch: 3, classification_loss: 1.7544127702713013, Val Loss: 70.70407700538635, loss : 2.0525717735290527\n",
            "epoch: 4, classification_loss: 1.7919036149978638, Val Loss: 70.74915027618408, loss : 2.0612080097198486\n",
            "epoch: 5, classification_loss: 1.791040062904358, Val Loss: 70.70871257781982, loss : 2.049363613128662\n",
            "epoch: 6, classification_loss: 1.76712965965271, Val Loss: 70.69031417369843, loss : 2.0491645336151123\n",
            "epoch: 7, classification_loss: 1.7727808952331543, Val Loss: 70.72005987167358, loss : 2.0549631118774414\n",
            "epoch: 8, classification_loss: 1.7581031322479248, Val Loss: 70.79571807384491, loss : 2.0323708057403564\n",
            "epoch: 9, classification_loss: 1.777106523513794, Val Loss: 70.67456984519958, loss : 2.047935962677002\n",
            "epoch: 10, classification_loss: 1.768171787261963, Val Loss: 70.65385723114014, loss : 2.039947986602783\n",
            "epoch: 11, classification_loss: 1.77446448802948, Val Loss: 70.76113903522491, loss : 1.77446448802948\n",
            "epoch: 12, classification_loss: 1.7465555667877197, Val Loss: 70.75056195259094, loss : 2.085880756378174\n",
            "epoch: 13, classification_loss: 1.7176587581634521, Val Loss: 70.67302417755127, loss : 1.9733448028564453\n",
            "epoch: 14, classification_loss: 1.755312442779541, Val Loss: 70.66524171829224, loss : 2.047515630722046\n",
            "epoch: 15, classification_loss: 1.7606958150863647, Val Loss: 70.73204183578491, loss : 2.0188119411468506\n",
            "epoch: 16, classification_loss: 1.7582265138626099, Val Loss: 70.7188812494278, loss : 2.0271289348602295\n",
            "epoch: 17, classification_loss: 1.752156376838684, Val Loss: 70.676833152771, loss : 2.0361227989196777\n",
            "epoch: 18, classification_loss: 1.7504688501358032, Val Loss: 70.70771479606628, loss : 2.0079214572906494\n",
            "epoch: 19, classification_loss: 1.766284465789795, Val Loss: 70.70175325870514, loss : 2.0233592987060547\n",
            "Batch: 81, Test Acc: 0.5721153846153846\n",
            "Batch: 82:\n",
            "epoch: 0, classification_loss: 1.8353500366210938, Val Loss: 70.73387467861176, loss : 1.8353500366210938\n",
            "epoch: 1, classification_loss: 1.8014484643936157, Val Loss: 70.74505984783173, loss : 2.1466214656829834\n",
            "epoch: 2, classification_loss: 1.7728089094161987, Val Loss: 70.83575284481049, loss : 2.064394235610962\n",
            "epoch: 3, classification_loss: 1.7810043096542358, Val Loss: 70.70849251747131, loss : 2.0707266330718994\n",
            "epoch: 4, classification_loss: 1.8309117555618286, Val Loss: 70.70029628276825, loss : 2.0914673805236816\n",
            "epoch: 5, classification_loss: 1.8303383588790894, Val Loss: 70.71736443042755, loss : 2.090117931365967\n",
            "epoch: 6, classification_loss: 1.7948609590530396, Val Loss: 70.70555436611176, loss : 2.0863802433013916\n",
            "epoch: 7, classification_loss: 1.8039629459381104, Val Loss: 70.70553815364838, loss : 2.0791449546813965\n",
            "epoch: 8, classification_loss: 1.8081004619598389, Val Loss: 70.71285855770111, loss : 2.0790162086486816\n",
            "epoch: 9, classification_loss: 1.7978947162628174, Val Loss: 70.70670354366302, loss : 2.0546302795410156\n",
            "epoch: 10, classification_loss: 1.8165580034255981, Val Loss: 70.67655551433563, loss : 2.08648419380188\n",
            "epoch: 11, classification_loss: 1.8129678964614868, Val Loss: 70.81133556365967, loss : 2.058952808380127\n",
            "epoch: 12, classification_loss: 1.8084827661514282, Val Loss: 70.72907316684723, loss : 2.07377290725708\n",
            "epoch: 13, classification_loss: 1.8060157299041748, Val Loss: 70.6847505569458, loss : 2.0577573776245117\n",
            "epoch: 14, classification_loss: 1.7995328903198242, Val Loss: 70.7215029001236, loss : 2.054455280303955\n",
            "epoch: 15, classification_loss: 1.8160645961761475, Val Loss: 70.70880794525146, loss : 2.067476749420166\n",
            "epoch: 16, classification_loss: 1.807364583015442, Val Loss: 70.71729111671448, loss : 2.058269739151001\n",
            "epoch: 17, classification_loss: 1.8137024641036987, Val Loss: 70.74728059768677, loss : 2.0558252334594727\n",
            "epoch: 18, classification_loss: 1.8121957778930664, Val Loss: 70.71303963661194, loss : 2.0581417083740234\n",
            "epoch: 19, classification_loss: 1.8239209651947021, Val Loss: 70.68093061447144, loss : 2.064624071121216\n",
            "Batch: 82, Test Acc: 0.5732171474358975\n",
            "Batch: 83:\n",
            "epoch: 0, classification_loss: 1.7950177192687988, Val Loss: 70.69801020622253, loss : 1.7950177192687988\n",
            "epoch: 1, classification_loss: 1.7636148929595947, Val Loss: 70.7708340883255, loss : 2.1010782718658447\n",
            "epoch: 2, classification_loss: 1.7365920543670654, Val Loss: 70.72172248363495, loss : 2.019031524658203\n",
            "epoch: 3, classification_loss: 1.7470035552978516, Val Loss: 70.71074736118317, loss : 2.0269501209259033\n",
            "epoch: 4, classification_loss: 1.7773476839065552, Val Loss: 70.6815276145935, loss : 2.02506422996521\n",
            "epoch: 5, classification_loss: 1.7829281091690063, Val Loss: 70.72666144371033, loss : 2.0323264598846436\n",
            "epoch: 6, classification_loss: 1.7638566493988037, Val Loss: 70.70902812480927, loss : 2.0418243408203125\n",
            "epoch: 7, classification_loss: 1.7636892795562744, Val Loss: 70.70908033847809, loss : 2.026109457015991\n",
            "epoch: 8, classification_loss: 1.771713137626648, Val Loss: 70.70427513122559, loss : 2.0389108657836914\n",
            "epoch: 9, classification_loss: 1.7651997804641724, Val Loss: 70.71238267421722, loss : 2.0191361904144287\n",
            "epoch: 10, classification_loss: 1.7704012393951416, Val Loss: 70.77674901485443, loss : 2.0303239822387695\n",
            "epoch: 11, classification_loss: 1.764728307723999, Val Loss: 70.69896948337555, loss : 2.0230789184570312\n",
            "epoch: 12, classification_loss: 1.7713477611541748, Val Loss: 70.70949470996857, loss : 2.0135157108306885\n",
            "epoch: 13, classification_loss: 1.7803714275360107, Val Loss: 70.71524488925934, loss : 2.0292229652404785\n",
            "epoch: 14, classification_loss: 1.7768287658691406, Val Loss: 70.74060666561127, loss : 2.01836895942688\n",
            "epoch: 15, classification_loss: 1.7586363554000854, Val Loss: 70.75842607021332, loss : 2.0129973888397217\n",
            "epoch: 16, classification_loss: 1.7626250982284546, Val Loss: 70.70552122592926, loss : 2.0121445655822754\n",
            "epoch: 17, classification_loss: 1.7725789546966553, Val Loss: 70.77592194080353, loss : 2.019305944442749\n",
            "epoch: 18, classification_loss: 1.7714662551879883, Val Loss: 70.72676289081573, loss : 2.023967742919922\n",
            "epoch: 19, classification_loss: 1.7752788066864014, Val Loss: 70.72450494766235, loss : 2.0193090438842773\n",
            "Batch: 83, Test Acc: 0.5724158653846154\n",
            "Batch: 84:\n",
            "epoch: 0, classification_loss: 1.8540621995925903, Val Loss: 70.69130527973175, loss : 1.8540621995925903\n",
            "epoch: 1, classification_loss: 1.8184237480163574, Val Loss: 70.64617621898651, loss : 2.1621928215026855\n",
            "epoch: 2, classification_loss: 1.7966066598892212, Val Loss: 70.6705310344696, loss : 1.7966066598892212\n",
            "epoch: 3, classification_loss: 1.7537367343902588, Val Loss: 70.6757961511612, loss : 2.0537526607513428\n",
            "epoch: 4, classification_loss: 1.745527744293213, Val Loss: 70.62306320667267, loss : 1.9628690481185913\n",
            "epoch: 5, classification_loss: 1.7998063564300537, Val Loss: 70.67692697048187, loss : 1.7998063564300537\n",
            "epoch: 6, classification_loss: 1.8133561611175537, Val Loss: 70.65537369251251, loss : 2.0982580184936523\n",
            "epoch: 7, classification_loss: 1.7676223516464233, Val Loss: 70.66400969028473, loss : 1.9710222482681274\n",
            "epoch: 8, classification_loss: 1.7472604513168335, Val Loss: 70.6719731092453, loss : 2.0464696884155273\n",
            "epoch: 9, classification_loss: 1.7695820331573486, Val Loss: 70.66771972179413, loss : 2.0154848098754883\n",
            "epoch: 10, classification_loss: 1.7944904565811157, Val Loss: 70.66192078590393, loss : 2.0535149574279785\n",
            "epoch: 11, classification_loss: 1.789273738861084, Val Loss: 70.70287954807281, loss : 2.0308873653411865\n",
            "epoch: 12, classification_loss: 1.7733112573623657, Val Loss: 70.63849794864655, loss : 2.0537376403808594\n",
            "epoch: 13, classification_loss: 1.7622647285461426, Val Loss: 70.69178378582001, loss : 2.0363779067993164\n",
            "epoch: 14, classification_loss: 1.7674630880355835, Val Loss: 70.64403748512268, loss : 2.0220541954040527\n",
            "epoch: 15, classification_loss: 1.781626582145691, Val Loss: 70.67888927459717, loss : 2.019685745239258\n",
            "epoch: 16, classification_loss: 1.7864961624145508, Val Loss: 70.64701914787292, loss : 2.0247929096221924\n",
            "epoch: 17, classification_loss: 1.763592004776001, Val Loss: 70.66283643245697, loss : 2.016284227371216\n",
            "epoch: 18, classification_loss: 1.7691540718078613, Val Loss: 70.64915990829468, loss : 2.021975517272949\n",
            "epoch: 19, classification_loss: 1.7738432884216309, Val Loss: 70.68494629859924, loss : 2.016075611114502\n",
            "Batch: 84, Test Acc: 0.5753205128205128\n",
            "Batch: 85:\n",
            "epoch: 0, classification_loss: 1.8028950691223145, Val Loss: 70.71346819400787, loss : 1.8028950691223145\n",
            "epoch: 1, classification_loss: 1.7680991888046265, Val Loss: 70.70370674133301, loss : 2.1135945320129395\n",
            "epoch: 2, classification_loss: 1.7505619525909424, Val Loss: 70.69354021549225, loss : 2.022249460220337\n",
            "epoch: 3, classification_loss: 1.7765207290649414, Val Loss: 70.68922638893127, loss : 2.0686678886413574\n",
            "epoch: 4, classification_loss: 1.8092899322509766, Val Loss: 70.6729131937027, loss : 2.063748359680176\n",
            "epoch: 5, classification_loss: 1.787567377090454, Val Loss: 70.63223385810852, loss : 2.039982318878174\n",
            "epoch: 6, classification_loss: 1.7891560792922974, Val Loss: 70.66255700588226, loss : 2.0696256160736084\n",
            "epoch: 7, classification_loss: 1.7774646282196045, Val Loss: 70.7530323266983, loss : 2.042160987854004\n",
            "epoch: 8, classification_loss: 1.7779786586761475, Val Loss: 70.68615424633026, loss : 2.050187110900879\n",
            "epoch: 9, classification_loss: 1.776386022567749, Val Loss: 70.6192067861557, loss : 2.0365490913391113\n",
            "epoch: 10, classification_loss: 1.789420247077942, Val Loss: 70.69880664348602, loss : 2.0452446937561035\n",
            "epoch: 11, classification_loss: 1.796258807182312, Val Loss: 70.69850993156433, loss : 2.041940927505493\n",
            "epoch: 12, classification_loss: 1.7852487564086914, Val Loss: 70.66533672809601, loss : 2.0311200618743896\n",
            "epoch: 13, classification_loss: 1.7872439622879028, Val Loss: 70.64154267311096, loss : 2.0421833992004395\n",
            "epoch: 14, classification_loss: 1.78484046459198, Val Loss: 70.68724918365479, loss : 2.0309085845947266\n",
            "epoch: 15, classification_loss: 1.7866100072860718, Val Loss: 70.68735933303833, loss : 2.0259666442871094\n",
            "epoch: 16, classification_loss: 1.790582537651062, Val Loss: 70.70396625995636, loss : 2.0242626667022705\n",
            "epoch: 17, classification_loss: 1.7821555137634277, Val Loss: 70.6717221736908, loss : 2.0182645320892334\n",
            "epoch: 18, classification_loss: 1.790295124053955, Val Loss: 70.70288550853729, loss : 2.01550555229187\n",
            "epoch: 19, classification_loss: 1.7885140180587769, Val Loss: 70.70730769634247, loss : 2.023247718811035\n",
            "Batch: 85, Test Acc: 0.5720152243589743\n",
            "Batch: 86:\n",
            "epoch: 0, classification_loss: 1.843435287475586, Val Loss: 70.67860472202301, loss : 1.843435287475586\n",
            "epoch: 1, classification_loss: 1.807519793510437, Val Loss: 70.66648662090302, loss : 2.146583080291748\n",
            "epoch: 2, classification_loss: 1.7754645347595215, Val Loss: 70.65366208553314, loss : 2.0568628311157227\n",
            "epoch: 3, classification_loss: 1.796956181526184, Val Loss: 70.68710017204285, loss : 2.075713872909546\n",
            "epoch: 4, classification_loss: 1.8307790756225586, Val Loss: 70.64409232139587, loss : 2.079378604888916\n",
            "epoch: 5, classification_loss: 1.8337624073028564, Val Loss: 70.6474962234497, loss : 2.091433525085449\n",
            "epoch: 6, classification_loss: 1.809308648109436, Val Loss: 70.67753732204437, loss : 2.079254627227783\n",
            "epoch: 7, classification_loss: 1.8161849975585938, Val Loss: 70.6707410812378, loss : 2.0814783573150635\n",
            "epoch: 8, classification_loss: 1.8058388233184814, Val Loss: 70.63586437702179, loss : 2.0624492168426514\n",
            "epoch: 9, classification_loss: 1.8132625818252563, Val Loss: 70.63388001918793, loss : 2.0755696296691895\n",
            "epoch: 10, classification_loss: 1.819076418876648, Val Loss: 70.70530200004578, loss : 2.077028512954712\n",
            "epoch: 11, classification_loss: 1.828789472579956, Val Loss: 70.70094525814056, loss : 2.087390184402466\n",
            "epoch: 12, classification_loss: 1.8105647563934326, Val Loss: 70.63153278827667, loss : 2.0668327808380127\n",
            "epoch: 13, classification_loss: 1.8072874546051025, Val Loss: 70.68545269966125, loss : 2.059812068939209\n",
            "epoch: 14, classification_loss: 1.8231126070022583, Val Loss: 70.74508678913116, loss : 2.0748789310455322\n",
            "epoch: 15, classification_loss: 1.8087706565856934, Val Loss: 70.65016949176788, loss : 2.058208465576172\n",
            "epoch: 16, classification_loss: 1.819207787513733, Val Loss: 70.66613531112671, loss : 2.0690970420837402\n",
            "epoch: 17, classification_loss: 1.8117060661315918, Val Loss: 70.69180846214294, loss : 2.0606722831726074\n",
            "epoch: 18, classification_loss: 1.8205283880233765, Val Loss: 70.71265733242035, loss : 2.061955690383911\n",
            "epoch: 19, classification_loss: 1.8215233087539673, Val Loss: 70.6956912279129, loss : 2.0714633464813232\n",
            "Batch: 86, Test Acc: 0.5694110576923077\n",
            "Batch: 87:\n",
            "epoch: 0, classification_loss: 1.7822998762130737, Val Loss: 70.68495261669159, loss : 1.7822998762130737\n",
            "epoch: 1, classification_loss: 1.7465306520462036, Val Loss: 70.71427154541016, loss : 2.0750503540039062\n",
            "epoch: 2, classification_loss: 1.7244586944580078, Val Loss: 70.71356081962585, loss : 1.9825770854949951\n",
            "epoch: 3, classification_loss: 1.745930552482605, Val Loss: 70.66173589229584, loss : 2.029755115509033\n",
            "epoch: 4, classification_loss: 1.7676550149917603, Val Loss: 70.66808938980103, loss : 2.0134685039520264\n",
            "epoch: 5, classification_loss: 1.7701927423477173, Val Loss: 70.6869866847992, loss : 2.0286948680877686\n",
            "epoch: 6, classification_loss: 1.762558102607727, Val Loss: 70.70860040187836, loss : 2.027132034301758\n",
            "epoch: 7, classification_loss: 1.7588809728622437, Val Loss: 70.69692778587341, loss : 2.0147383213043213\n",
            "epoch: 8, classification_loss: 1.757781744003296, Val Loss: 70.7002385854721, loss : 2.017329454421997\n",
            "epoch: 9, classification_loss: 1.758859634399414, Val Loss: 70.73668110370636, loss : 2.012845754623413\n",
            "epoch: 10, classification_loss: 1.7680740356445312, Val Loss: 70.73875558376312, loss : 2.0163094997406006\n",
            "epoch: 11, classification_loss: 1.7548969984054565, Val Loss: 70.67741322517395, loss : 1.9994380474090576\n",
            "epoch: 12, classification_loss: 1.7592272758483887, Val Loss: 70.73407483100891, loss : 1.9988110065460205\n",
            "epoch: 13, classification_loss: 1.7716702222824097, Val Loss: 70.70889604091644, loss : 2.0073182582855225\n",
            "epoch: 14, classification_loss: 1.750606656074524, Val Loss: 70.72750353813171, loss : 1.9914956092834473\n",
            "epoch: 15, classification_loss: 1.7589876651763916, Val Loss: 70.70882415771484, loss : 2.00123929977417\n",
            "epoch: 16, classification_loss: 1.7596635818481445, Val Loss: 70.74402356147766, loss : 2.00276780128479\n",
            "epoch: 17, classification_loss: 1.7538814544677734, Val Loss: 70.72596418857574, loss : 1.9948773384094238\n",
            "epoch: 18, classification_loss: 1.7659006118774414, Val Loss: 70.70605027675629, loss : 2.0012214183807373\n",
            "epoch: 19, classification_loss: 1.7659602165222168, Val Loss: 70.6879494190216, loss : 1.9983288049697876\n",
            "Batch: 87, Test Acc: 0.5737179487179487\n",
            "Batch: 88:\n",
            "epoch: 0, classification_loss: 1.8562912940979004, Val Loss: 70.64832496643066, loss : 1.8562912940979004\n",
            "epoch: 1, classification_loss: 1.8186101913452148, Val Loss: 70.63860952854156, loss : 2.161190986633301\n",
            "epoch: 2, classification_loss: 1.8012410402297974, Val Loss: 70.72425270080566, loss : 2.0760011672973633\n",
            "epoch: 3, classification_loss: 1.8264681100845337, Val Loss: 70.66341364383698, loss : 2.1098623275756836\n",
            "epoch: 4, classification_loss: 1.8414136171340942, Val Loss: 70.66848886013031, loss : 2.0940897464752197\n",
            "epoch: 5, classification_loss: 1.845184564590454, Val Loss: 70.63563930988312, loss : 2.1033172607421875\n",
            "epoch: 6, classification_loss: 1.8311569690704346, Val Loss: 70.6511459350586, loss : 2.099369764328003\n",
            "epoch: 7, classification_loss: 1.8283495903015137, Val Loss: 70.67394196987152, loss : 2.0900304317474365\n",
            "epoch: 8, classification_loss: 1.840056300163269, Val Loss: 70.64983594417572, loss : 2.1045544147491455\n",
            "epoch: 9, classification_loss: 1.843311071395874, Val Loss: 70.66844999790192, loss : 2.1049835681915283\n",
            "epoch: 10, classification_loss: 1.838634729385376, Val Loss: 70.69371199607849, loss : 2.0956130027770996\n",
            "epoch: 11, classification_loss: 1.8332798480987549, Val Loss: 70.64283514022827, loss : 2.084247589111328\n",
            "epoch: 12, classification_loss: 1.8484230041503906, Val Loss: 70.69290661811829, loss : 2.092207193374634\n",
            "epoch: 13, classification_loss: 1.8331103324890137, Val Loss: 70.68196082115173, loss : 2.0776376724243164\n",
            "epoch: 14, classification_loss: 1.8351765871047974, Val Loss: 70.64127445220947, loss : 2.0842092037200928\n",
            "epoch: 15, classification_loss: 1.8370922803878784, Val Loss: 70.65527975559235, loss : 2.083181619644165\n",
            "epoch: 16, classification_loss: 1.8346781730651855, Val Loss: 70.67198348045349, loss : 2.0755388736724854\n",
            "epoch: 17, classification_loss: 1.8242875337600708, Val Loss: 70.69943165779114, loss : 2.0641376972198486\n",
            "epoch: 18, classification_loss: 1.8415511846542358, Val Loss: 70.6612104177475, loss : 2.073883533477783\n",
            "epoch: 19, classification_loss: 1.8352265357971191, Val Loss: 70.68451607227325, loss : 2.083251714706421\n",
            "Batch: 88, Test Acc: 0.57421875\n",
            "Batch: 89:\n",
            "epoch: 0, classification_loss: 1.7830239534378052, Val Loss: 70.64922535419464, loss : 1.7830239534378052\n",
            "epoch: 1, classification_loss: 1.7478938102722168, Val Loss: 70.62532722949982, loss : 2.0726332664489746\n",
            "epoch: 2, classification_loss: 1.7216004133224487, Val Loss: 70.7097840309143, loss : 1.9822194576263428\n",
            "epoch: 3, classification_loss: 1.738364338874817, Val Loss: 70.66354656219482, loss : 2.0111300945281982\n",
            "epoch: 4, classification_loss: 1.7574114799499512, Val Loss: 70.63202059268951, loss : 1.9941340684890747\n",
            "epoch: 5, classification_loss: 1.7555809020996094, Val Loss: 70.63757359981537, loss : 2.016070604324341\n",
            "epoch: 6, classification_loss: 1.7446129322052002, Val Loss: 70.67747449874878, loss : 2.0094053745269775\n",
            "epoch: 7, classification_loss: 1.7567322254180908, Val Loss: 70.67461168766022, loss : 2.0102105140686035\n",
            "epoch: 8, classification_loss: 1.756134033203125, Val Loss: 70.65946805477142, loss : 2.0069985389709473\n",
            "epoch: 9, classification_loss: 1.763527750968933, Val Loss: 70.65205657482147, loss : 2.0040395259857178\n",
            "epoch: 10, classification_loss: 1.7457315921783447, Val Loss: 70.6673332452774, loss : 1.9868800640106201\n",
            "epoch: 11, classification_loss: 1.7546985149383545, Val Loss: 70.67362999916077, loss : 1.9920967817306519\n",
            "epoch: 12, classification_loss: 1.76570725440979, Val Loss: 70.67662215232849, loss : 2.0015780925750732\n",
            "epoch: 13, classification_loss: 1.7522002458572388, Val Loss: 70.67124271392822, loss : 1.994397521018982\n",
            "epoch: 14, classification_loss: 1.7606946229934692, Val Loss: 70.67396759986877, loss : 1.9982349872589111\n",
            "epoch: 15, classification_loss: 1.7479147911071777, Val Loss: 70.6810587644577, loss : 1.9877196550369263\n",
            "epoch: 16, classification_loss: 1.7495627403259277, Val Loss: 70.6911346912384, loss : 1.983474850654602\n",
            "epoch: 17, classification_loss: 1.7618110179901123, Val Loss: 70.66595089435577, loss : 1.9923698902130127\n",
            "epoch: 18, classification_loss: 1.7665603160858154, Val Loss: 70.69229030609131, loss : 1.9919154644012451\n",
            "epoch: 19, classification_loss: 1.7682358026504517, Val Loss: 70.68391382694244, loss : 2.003244638442993\n",
            "Batch: 89, Test Acc: 0.5732171474358975\n",
            "Batch: 90:\n",
            "epoch: 0, classification_loss: 1.8440568447113037, Val Loss: 70.68103611469269, loss : 1.8440568447113037\n",
            "epoch: 1, classification_loss: 1.8015891313552856, Val Loss: 70.70638573169708, loss : 2.132044792175293\n",
            "epoch: 2, classification_loss: 1.7840183973312378, Val Loss: 70.66358089447021, loss : 2.048614501953125\n",
            "epoch: 3, classification_loss: 1.8160347938537598, Val Loss: 70.67007732391357, loss : 2.088977098464966\n",
            "epoch: 4, classification_loss: 1.8442710638046265, Val Loss: 70.70091235637665, loss : 2.087493896484375\n",
            "epoch: 5, classification_loss: 1.8243916034698486, Val Loss: 70.61295926570892, loss : 2.0884978771209717\n",
            "epoch: 6, classification_loss: 1.8044151067733765, Val Loss: 70.74774146080017, loss : 1.8044151067733765\n",
            "epoch: 7, classification_loss: 1.7886743545532227, Val Loss: 70.68097913265228, loss : 2.102277994155884\n",
            "epoch: 8, classification_loss: 1.7654367685317993, Val Loss: 70.68622636795044, loss : 2.013467788696289\n",
            "epoch: 9, classification_loss: 1.8048378229141235, Val Loss: 70.62851095199585, loss : 2.0425503253936768\n",
            "epoch: 10, classification_loss: 1.819828987121582, Val Loss: 70.64953589439392, loss : 2.047029733657837\n",
            "epoch: 11, classification_loss: 1.7981395721435547, Val Loss: 70.63830041885376, loss : 2.0272464752197266\n",
            "epoch: 12, classification_loss: 1.790092945098877, Val Loss: 70.63554179668427, loss : 2.043539047241211\n",
            "epoch: 13, classification_loss: 1.7947213649749756, Val Loss: 70.66136288642883, loss : 2.031419038772583\n",
            "epoch: 14, classification_loss: 1.8020315170288086, Val Loss: 70.73978507518768, loss : 2.0579991340637207\n",
            "epoch: 15, classification_loss: 1.7967965602874756, Val Loss: 70.6399587392807, loss : 2.0326287746429443\n",
            "epoch: 16, classification_loss: 1.7968568801879883, Val Loss: 70.61409044265747, loss : 2.05012583732605\n",
            "epoch: 17, classification_loss: 1.8019673824310303, Val Loss: 70.66375458240509, loss : 2.0323779582977295\n",
            "epoch: 18, classification_loss: 1.8074681758880615, Val Loss: 70.65346109867096, loss : 2.059647798538208\n",
            "epoch: 19, classification_loss: 1.7959760427474976, Val Loss: 70.63101553916931, loss : 2.023700714111328\n",
            "Batch: 90, Test Acc: 0.5756209935897436\n",
            "Batch: 91:\n",
            "epoch: 0, classification_loss: 1.7897021770477295, Val Loss: 70.80509042739868, loss : 1.7897021770477295\n",
            "epoch: 1, classification_loss: 1.7476236820220947, Val Loss: 70.75110077857971, loss : 2.115410566329956\n",
            "epoch: 2, classification_loss: 1.7253659963607788, Val Loss: 70.77087330818176, loss : 2.032336473464966\n",
            "epoch: 3, classification_loss: 1.7596378326416016, Val Loss: 70.73612582683563, loss : 2.0568161010742188\n",
            "epoch: 4, classification_loss: 1.7821292877197266, Val Loss: 70.66681063175201, loss : 2.067999839782715\n",
            "epoch: 5, classification_loss: 1.7744560241699219, Val Loss: 70.6344622373581, loss : 2.0200183391571045\n",
            "epoch: 6, classification_loss: 1.7635875940322876, Val Loss: 70.71306359767914, loss : 2.0512380599975586\n",
            "epoch: 7, classification_loss: 1.7571396827697754, Val Loss: 70.71836018562317, loss : 2.0358357429504395\n",
            "epoch: 8, classification_loss: 1.743880033493042, Val Loss: 70.70645296573639, loss : 2.0244150161743164\n",
            "epoch: 9, classification_loss: 1.7438251972198486, Val Loss: 70.65711712837219, loss : 2.0170364379882812\n",
            "epoch: 10, classification_loss: 1.7754658460617065, Val Loss: 70.66727256774902, loss : 2.040614366531372\n",
            "epoch: 11, classification_loss: 1.759224534034729, Val Loss: 70.70722687244415, loss : 2.015336751937866\n",
            "epoch: 12, classification_loss: 1.7591241598129272, Val Loss: 70.67625451087952, loss : 2.0176210403442383\n",
            "epoch: 13, classification_loss: 1.7596734762191772, Val Loss: 70.6936674118042, loss : 2.005685329437256\n",
            "epoch: 14, classification_loss: 1.7725993394851685, Val Loss: 70.639817237854, loss : 2.0154120922088623\n",
            "epoch: 15, classification_loss: 1.7510981559753418, Val Loss: 70.69355428218842, loss : 2.0019373893737793\n",
            "epoch: 16, classification_loss: 1.7689136266708374, Val Loss: 70.70942103862762, loss : 2.0105535984039307\n",
            "epoch: 17, classification_loss: 1.7694225311279297, Val Loss: 70.69516551494598, loss : 2.0116465091705322\n",
            "epoch: 18, classification_loss: 1.7635040283203125, Val Loss: 70.65239667892456, loss : 1.9972822666168213\n",
            "epoch: 19, classification_loss: 1.7696785926818848, Val Loss: 70.66008412837982, loss : 2.0100643634796143\n",
            "Batch: 91, Test Acc: 0.5728165064102564\n",
            "Batch: 92:\n",
            "epoch: 0, classification_loss: 1.775929570198059, Val Loss: 70.65940833091736, loss : 1.775929570198059\n",
            "epoch: 1, classification_loss: 1.7474243640899658, Val Loss: 70.65824294090271, loss : 2.0751352310180664\n",
            "epoch: 2, classification_loss: 1.7255443334579468, Val Loss: 70.63158285617828, loss : 1.9838885068893433\n",
            "epoch: 3, classification_loss: 1.7490588426589966, Val Loss: 70.64184308052063, loss : 2.0277748107910156\n",
            "epoch: 4, classification_loss: 1.7789218425750732, Val Loss: 70.65986716747284, loss : 2.0181992053985596\n",
            "epoch: 5, classification_loss: 1.7617183923721313, Val Loss: 70.65192222595215, loss : 2.0169451236724854\n",
            "epoch: 6, classification_loss: 1.7508504390716553, Val Loss: 70.66310214996338, loss : 2.0112271308898926\n",
            "epoch: 7, classification_loss: 1.7589402198791504, Val Loss: 70.68165683746338, loss : 2.0034005641937256\n",
            "epoch: 8, classification_loss: 1.7560831308364868, Val Loss: 70.68405616283417, loss : 2.0122621059417725\n",
            "epoch: 9, classification_loss: 1.7719221115112305, Val Loss: 70.6628030538559, loss : 2.017210006713867\n",
            "epoch: 10, classification_loss: 1.757230520248413, Val Loss: 70.6815550327301, loss : 2.0041143894195557\n",
            "epoch: 11, classification_loss: 1.7532315254211426, Val Loss: 70.65372323989868, loss : 1.9915732145309448\n",
            "epoch: 12, classification_loss: 1.7576022148132324, Val Loss: 70.69119906425476, loss : 1.9915823936462402\n",
            "epoch: 13, classification_loss: 1.7571886777877808, Val Loss: 70.71865165233612, loss : 1.9968290328979492\n",
            "epoch: 14, classification_loss: 1.7531726360321045, Val Loss: 70.71181225776672, loss : 1.988966941833496\n",
            "epoch: 15, classification_loss: 1.7582900524139404, Val Loss: 70.65340423583984, loss : 1.9919651746749878\n",
            "epoch: 16, classification_loss: 1.7571380138397217, Val Loss: 70.67799007892609, loss : 1.981526494026184\n",
            "epoch: 17, classification_loss: 1.7697440385818481, Val Loss: 70.69725000858307, loss : 1.9941049814224243\n",
            "epoch: 18, classification_loss: 1.7627074718475342, Val Loss: 70.69635283946991, loss : 1.9801888465881348\n",
            "epoch: 19, classification_loss: 1.7511082887649536, Val Loss: 70.70964431762695, loss : 1.98222017288208\n",
            "Batch: 92, Test Acc: 0.5729166666666666\n",
            "Batch: 93:\n",
            "epoch: 0, classification_loss: 1.8017494678497314, Val Loss: 70.66818952560425, loss : 1.8017494678497314\n",
            "epoch: 1, classification_loss: 1.752207636833191, Val Loss: 70.65383791923523, loss : 2.079653263092041\n",
            "epoch: 2, classification_loss: 1.7441686391830444, Val Loss: 70.66636002063751, loss : 1.9918513298034668\n",
            "epoch: 3, classification_loss: 1.7665201425552368, Val Loss: 70.66359174251556, loss : 2.032824993133545\n",
            "epoch: 4, classification_loss: 1.7844856977462769, Val Loss: 70.66070425510406, loss : 2.011568069458008\n",
            "epoch: 5, classification_loss: 1.7769356966018677, Val Loss: 70.651318192482, loss : 2.042940139770508\n",
            "epoch: 6, classification_loss: 1.7635976076126099, Val Loss: 70.66827499866486, loss : 2.0265300273895264\n",
            "epoch: 7, classification_loss: 1.769397258758545, Val Loss: 70.71829187870026, loss : 2.030252456665039\n",
            "epoch: 8, classification_loss: 1.7801940441131592, Val Loss: 70.75771129131317, loss : 2.0341694355010986\n",
            "epoch: 9, classification_loss: 1.776391863822937, Val Loss: 70.68405485153198, loss : 2.029665470123291\n",
            "epoch: 10, classification_loss: 1.7675296068191528, Val Loss: 70.65487599372864, loss : 2.0191867351531982\n",
            "epoch: 11, classification_loss: 1.7692797183990479, Val Loss: 70.73304176330566, loss : 2.013425350189209\n",
            "epoch: 12, classification_loss: 1.7812925577163696, Val Loss: 70.74179232120514, loss : 2.0245344638824463\n",
            "epoch: 13, classification_loss: 1.7753183841705322, Val Loss: 70.7405709028244, loss : 2.0216472148895264\n",
            "epoch: 14, classification_loss: 1.766268253326416, Val Loss: 70.69330167770386, loss : 2.004878520965576\n",
            "epoch: 15, classification_loss: 1.7717558145523071, Val Loss: 70.73517835140228, loss : 2.0110762119293213\n",
            "epoch: 16, classification_loss: 1.7735053300857544, Val Loss: 70.72353839874268, loss : 2.0120491981506348\n",
            "epoch: 17, classification_loss: 1.7760590314865112, Val Loss: 70.785471200943, loss : 2.016998291015625\n",
            "epoch: 18, classification_loss: 1.7744574546813965, Val Loss: 70.74719572067261, loss : 2.0009405612945557\n",
            "epoch: 19, classification_loss: 1.7858785390853882, Val Loss: 70.69874811172485, loss : 2.017901659011841\n",
            "Batch: 93, Test Acc: 0.5716145833333334\n",
            "Batch: 94:\n",
            "epoch: 0, classification_loss: 1.8142499923706055, Val Loss: 70.67444717884064, loss : 1.8142499923706055\n",
            "epoch: 1, classification_loss: 1.7762445211410522, Val Loss: 70.68733477592468, loss : 2.1085329055786133\n",
            "epoch: 2, classification_loss: 1.7477073669433594, Val Loss: 70.68341732025146, loss : 2.0091054439544678\n",
            "epoch: 3, classification_loss: 1.797262191772461, Val Loss: 70.72416114807129, loss : 2.067047119140625\n",
            "epoch: 4, classification_loss: 1.8049830198287964, Val Loss: 70.65677976608276, loss : 2.0402982234954834\n",
            "epoch: 5, classification_loss: 1.797934889793396, Val Loss: 70.69120132923126, loss : 2.0567219257354736\n",
            "epoch: 6, classification_loss: 1.792492389678955, Val Loss: 70.7167797088623, loss : 2.057462453842163\n",
            "epoch: 7, classification_loss: 1.7900234460830688, Val Loss: 70.73971724510193, loss : 2.0549449920654297\n",
            "epoch: 8, classification_loss: 1.7905350923538208, Val Loss: 70.71595799922943, loss : 2.05069899559021\n",
            "epoch: 9, classification_loss: 1.7904256582260132, Val Loss: 70.74016582965851, loss : 2.043344497680664\n",
            "epoch: 10, classification_loss: 1.787642002105713, Val Loss: 70.747305393219, loss : 2.0347354412078857\n",
            "epoch: 11, classification_loss: 1.7887135744094849, Val Loss: 70.70090365409851, loss : 2.0330865383148193\n",
            "epoch: 12, classification_loss: 1.7999323606491089, Val Loss: 70.71765172481537, loss : 2.0379672050476074\n",
            "epoch: 13, classification_loss: 1.7865406274795532, Val Loss: 70.72257339954376, loss : 2.025045156478882\n",
            "epoch: 14, classification_loss: 1.7946827411651611, Val Loss: 70.72466492652893, loss : 2.0319149494171143\n",
            "epoch: 15, classification_loss: 1.7796742916107178, Val Loss: 70.7649929523468, loss : 2.022348165512085\n",
            "epoch: 16, classification_loss: 1.784411072731018, Val Loss: 70.77383923530579, loss : 2.0186212062835693\n",
            "epoch: 17, classification_loss: 1.801743507385254, Val Loss: 70.72636139392853, loss : 2.0355443954467773\n",
            "epoch: 18, classification_loss: 1.8003385066986084, Val Loss: 70.7422194480896, loss : 2.0321593284606934\n",
            "epoch: 19, classification_loss: 1.7822637557983398, Val Loss: 70.74829244613647, loss : 2.023279905319214\n",
            "Batch: 94, Test Acc: 0.57421875\n",
            "Batch: 95:\n",
            "epoch: 0, classification_loss: 1.8501195907592773, Val Loss: 70.74732947349548, loss : 1.8501195907592773\n",
            "epoch: 1, classification_loss: 1.8108057975769043, Val Loss: 70.70272815227509, loss : 2.148324966430664\n",
            "epoch: 2, classification_loss: 1.7810611724853516, Val Loss: 70.65455079078674, loss : 2.070310115814209\n",
            "epoch: 3, classification_loss: 1.8237192630767822, Val Loss: 70.66582500934601, loss : 2.105653762817383\n",
            "epoch: 4, classification_loss: 1.854348063468933, Val Loss: 70.77933990955353, loss : 2.103029251098633\n",
            "epoch: 5, classification_loss: 1.842011570930481, Val Loss: 70.6770510673523, loss : 2.0926873683929443\n",
            "epoch: 6, classification_loss: 1.818166971206665, Val Loss: 70.64392602443695, loss : 2.0831682682037354\n",
            "epoch: 7, classification_loss: 1.8182541131973267, Val Loss: 70.66502451896667, loss : 2.0884947776794434\n",
            "epoch: 8, classification_loss: 1.8125022649765015, Val Loss: 70.73782110214233, loss : 2.0784084796905518\n",
            "epoch: 9, classification_loss: 1.8259130716323853, Val Loss: 70.75285530090332, loss : 2.083817720413208\n",
            "epoch: 10, classification_loss: 1.8325775861740112, Val Loss: 70.71421146392822, loss : 2.0821962356567383\n",
            "epoch: 11, classification_loss: 1.8235441446304321, Val Loss: 70.71628296375275, loss : 2.0707268714904785\n",
            "epoch: 12, classification_loss: 1.8221642971038818, Val Loss: 70.672034740448, loss : 2.0626180171966553\n",
            "epoch: 13, classification_loss: 1.8135091066360474, Val Loss: 70.73276793956757, loss : 2.0628347396850586\n",
            "epoch: 14, classification_loss: 1.821028709411621, Val Loss: 70.80078542232513, loss : 2.0629405975341797\n",
            "epoch: 15, classification_loss: 1.8232614994049072, Val Loss: 70.72855246067047, loss : 2.0683751106262207\n",
            "epoch: 16, classification_loss: 1.8279876708984375, Val Loss: 70.72791028022766, loss : 2.065070867538452\n",
            "epoch: 17, classification_loss: 1.8323291540145874, Val Loss: 70.72322058677673, loss : 2.0688552856445312\n",
            "epoch: 18, classification_loss: 1.8232152462005615, Val Loss: 70.68848323822021, loss : 2.0608742237091064\n",
            "epoch: 19, classification_loss: 1.838851809501648, Val Loss: 70.72126483917236, loss : 2.075572967529297\n",
            "Batch: 95, Test Acc: 0.5706129807692307\n",
            "Batch: 96:\n",
            "epoch: 0, classification_loss: 1.847186803817749, Val Loss: 70.64225733280182, loss : 1.847186803817749\n",
            "epoch: 1, classification_loss: 1.810901403427124, Val Loss: 70.63987350463867, loss : 2.1449601650238037\n",
            "epoch: 2, classification_loss: 1.7911092042922974, Val Loss: 70.62996006011963, loss : 2.0589213371276855\n",
            "epoch: 3, classification_loss: 1.822679877281189, Val Loss: 70.62977755069733, loss : 2.100529909133911\n",
            "epoch: 4, classification_loss: 1.8283828496932983, Val Loss: 70.6017335653305, loss : 2.077676296234131\n",
            "epoch: 5, classification_loss: 1.8201912641525269, Val Loss: 70.75236511230469, loss : 1.8201912641525269\n",
            "epoch: 6, classification_loss: 1.7774155139923096, Val Loss: 70.78069710731506, loss : 2.164015769958496\n",
            "epoch: 7, classification_loss: 1.7530070543289185, Val Loss: 70.63990247249603, loss : 2.0872395038604736\n",
            "epoch: 8, classification_loss: 1.7838598489761353, Val Loss: 70.64095687866211, loss : 2.056370735168457\n",
            "epoch: 9, classification_loss: 1.83272123336792, Val Loss: 70.68805158138275, loss : 2.131218910217285\n",
            "epoch: 10, classification_loss: 1.8033233880996704, Val Loss: 70.62094259262085, loss : 2.0508594512939453\n",
            "epoch: 11, classification_loss: 1.7861140966415405, Val Loss: 70.63557243347168, loss : 2.098903179168701\n",
            "epoch: 12, classification_loss: 1.7821019887924194, Val Loss: 70.66364073753357, loss : 2.0829110145568848\n",
            "epoch: 13, classification_loss: 1.799328327178955, Val Loss: 70.71922373771667, loss : 2.0695745944976807\n",
            "epoch: 14, classification_loss: 1.7996761798858643, Val Loss: 70.63491225242615, loss : 2.0791244506835938\n",
            "epoch: 15, classification_loss: 1.7940573692321777, Val Loss: 70.70411443710327, loss : 2.0655953884124756\n",
            "epoch: 16, classification_loss: 1.7873224020004272, Val Loss: 70.65037286281586, loss : 2.05452561378479\n",
            "epoch: 17, classification_loss: 1.8015525341033936, Val Loss: 70.63815176486969, loss : 2.064370632171631\n",
            "epoch: 18, classification_loss: 1.7937617301940918, Val Loss: 70.61334919929504, loss : 2.050724506378174\n",
            "epoch: 19, classification_loss: 1.787877082824707, Val Loss: 70.66231119632721, loss : 2.042818784713745\n",
            "Batch: 96, Test Acc: 0.5739182692307693\n",
            "Batch: 97:\n",
            "epoch: 0, classification_loss: 1.8689508438110352, Val Loss: 70.7084481716156, loss : 1.8689508438110352\n",
            "epoch: 1, classification_loss: 1.8273499011993408, Val Loss: 70.69739162921906, loss : 2.167832851409912\n",
            "epoch: 2, classification_loss: 1.8115657567977905, Val Loss: 70.61534667015076, loss : 2.0959534645080566\n",
            "epoch: 3, classification_loss: 1.8217912912368774, Val Loss: 70.61730515956879, loss : 2.121274471282959\n",
            "epoch: 4, classification_loss: 1.845091462135315, Val Loss: 70.63486349582672, loss : 2.1104609966278076\n",
            "epoch: 5, classification_loss: 1.8560047149658203, Val Loss: 70.61837077140808, loss : 2.114298105239868\n",
            "epoch: 6, classification_loss: 1.8310015201568604, Val Loss: 70.55706524848938, loss : 2.1022818088531494\n",
            "epoch: 7, classification_loss: 1.8381859064102173, Val Loss: 70.58646380901337, loss : 1.8381859064102173\n",
            "epoch: 8, classification_loss: 1.7921754121780396, Val Loss: 70.61428606510162, loss : 2.1254820823669434\n",
            "epoch: 9, classification_loss: 1.7784984111785889, Val Loss: 70.66843008995056, loss : 2.0556602478027344\n",
            "epoch: 10, classification_loss: 1.783517599105835, Val Loss: 70.59199261665344, loss : 2.0839943885803223\n",
            "epoch: 11, classification_loss: 1.825657844543457, Val Loss: 70.56682741641998, loss : 2.0752997398376465\n",
            "epoch: 12, classification_loss: 1.8233386278152466, Val Loss: 70.58293509483337, loss : 2.085214138031006\n",
            "epoch: 13, classification_loss: 1.786760687828064, Val Loss: 70.60773837566376, loss : 2.0835063457489014\n",
            "epoch: 14, classification_loss: 1.8018313646316528, Val Loss: 70.56869268417358, loss : 2.066298246383667\n",
            "epoch: 15, classification_loss: 1.8185231685638428, Val Loss: 70.59148597717285, loss : 2.095472574234009\n",
            "epoch: 16, classification_loss: 1.808205008506775, Val Loss: 70.62075912952423, loss : 2.061904191970825\n",
            "epoch: 17, classification_loss: 1.8136990070343018, Val Loss: 70.60783040523529, loss : 2.0810840129852295\n",
            "epoch: 18, classification_loss: 1.8087658882141113, Val Loss: 70.56919121742249, loss : 2.069059371948242\n",
            "epoch: 19, classification_loss: 1.8163533210754395, Val Loss: 70.60408186912537, loss : 2.0684337615966797\n",
            "Batch: 97, Test Acc: 0.5737179487179487\n",
            "Batch: 98:\n",
            "epoch: 0, classification_loss: 1.8488081693649292, Val Loss: 70.63795137405396, loss : 1.8488081693649292\n",
            "epoch: 1, classification_loss: 1.8090856075286865, Val Loss: 70.71959698200226, loss : 2.148696184158325\n",
            "epoch: 2, classification_loss: 1.7895150184631348, Val Loss: 70.6017872095108, loss : 2.065112590789795\n",
            "epoch: 3, classification_loss: 1.827193260192871, Val Loss: 70.59323418140411, loss : 2.1251883506774902\n",
            "epoch: 4, classification_loss: 1.835860252380371, Val Loss: 70.65579402446747, loss : 2.1034088134765625\n",
            "epoch: 5, classification_loss: 1.828050971031189, Val Loss: 70.60357236862183, loss : 2.086778163909912\n",
            "epoch: 6, classification_loss: 1.8257051706314087, Val Loss: 70.56687498092651, loss : 2.096036434173584\n",
            "epoch: 7, classification_loss: 1.8192453384399414, Val Loss: 70.60919988155365, loss : 2.080726385116577\n",
            "epoch: 8, classification_loss: 1.8343970775604248, Val Loss: 70.63660776615143, loss : 2.0882766246795654\n",
            "epoch: 9, classification_loss: 1.8244209289550781, Val Loss: 70.64969766139984, loss : 2.0892281532287598\n",
            "epoch: 10, classification_loss: 1.8195840120315552, Val Loss: 70.64037382602692, loss : 2.0788209438323975\n",
            "epoch: 11, classification_loss: 1.8308231830596924, Val Loss: 70.63937282562256, loss : 2.083641290664673\n",
            "epoch: 12, classification_loss: 1.8316468000411987, Val Loss: 70.64500629901886, loss : 2.0773208141326904\n",
            "epoch: 13, classification_loss: 1.8154795169830322, Val Loss: 70.62712371349335, loss : 2.064192056655884\n",
            "epoch: 14, classification_loss: 1.8298066854476929, Val Loss: 70.67238450050354, loss : 2.0772554874420166\n",
            "epoch: 15, classification_loss: 1.8273414373397827, Val Loss: 70.67104542255402, loss : 2.0699105262756348\n",
            "epoch: 16, classification_loss: 1.8239604234695435, Val Loss: 70.6448392868042, loss : 2.066664457321167\n",
            "epoch: 17, classification_loss: 1.832004427909851, Val Loss: 70.6708459854126, loss : 2.0758121013641357\n",
            "epoch: 18, classification_loss: 1.821284294128418, Val Loss: 70.62055349349976, loss : 2.0710315704345703\n",
            "epoch: 19, classification_loss: 1.8250387907028198, Val Loss: 70.62914168834686, loss : 2.0643272399902344\n",
            "Batch: 98, Test Acc: 0.573417467948718\n",
            "Batch: 99:\n",
            "epoch: 0, classification_loss: 1.8233634233474731, Val Loss: 70.77805161476135, loss : 1.8233634233474731\n",
            "epoch: 1, classification_loss: 1.769545555114746, Val Loss: 70.78148448467255, loss : 2.1204676628112793\n",
            "epoch: 2, classification_loss: 1.7484912872314453, Val Loss: 70.59296607971191, loss : 2.0352132320404053\n",
            "epoch: 3, classification_loss: 1.7808059453964233, Val Loss: 70.61120176315308, loss : 2.066779613494873\n",
            "epoch: 4, classification_loss: 1.8045061826705933, Val Loss: 70.61805331707001, loss : 2.0758352279663086\n",
            "epoch: 5, classification_loss: 1.7920347452163696, Val Loss: 70.62092304229736, loss : 2.0515501499176025\n",
            "epoch: 6, classification_loss: 1.784981608390808, Val Loss: 70.65940427780151, loss : 2.0602564811706543\n",
            "epoch: 7, classification_loss: 1.7894492149353027, Val Loss: 70.74792206287384, loss : 2.067267417907715\n",
            "epoch: 8, classification_loss: 1.7773780822753906, Val Loss: 70.5998260974884, loss : 2.0488247871398926\n",
            "epoch: 9, classification_loss: 1.7913330793380737, Val Loss: 70.61195516586304, loss : 2.057941198348999\n",
            "epoch: 10, classification_loss: 1.7919142246246338, Val Loss: 70.67855131626129, loss : 2.042438507080078\n",
            "epoch: 11, classification_loss: 1.783783197402954, Val Loss: 70.70542466640472, loss : 2.043257236480713\n",
            "epoch: 12, classification_loss: 1.7850337028503418, Val Loss: 70.65224611759186, loss : 2.0318729877471924\n",
            "epoch: 13, classification_loss: 1.7932301759719849, Val Loss: 70.67854833602905, loss : 2.049039363861084\n",
            "epoch: 14, classification_loss: 1.7812933921813965, Val Loss: 70.67994213104248, loss : 2.034116744995117\n",
            "epoch: 15, classification_loss: 1.7793872356414795, Val Loss: 70.68613743782043, loss : 2.035796642303467\n",
            "epoch: 16, classification_loss: 1.7912830114364624, Val Loss: 70.71018266677856, loss : 2.040255308151245\n",
            "epoch: 17, classification_loss: 1.7839221954345703, Val Loss: 70.66772544384003, loss : 2.0359699726104736\n",
            "epoch: 18, classification_loss: 1.791994571685791, Val Loss: 70.69211614131927, loss : 2.0344436168670654\n",
            "epoch: 19, classification_loss: 1.7931064367294312, Val Loss: 70.70727229118347, loss : 2.0377018451690674\n",
            "Batch: 99, Test Acc: 0.5772235576923077\n",
            "Batch: 100:\n",
            "epoch: 0, classification_loss: 1.859790563583374, Val Loss: 70.61529588699341, loss : 1.859790563583374\n",
            "epoch: 1, classification_loss: 1.827174186706543, Val Loss: 70.60217678546906, loss : 2.1525075435638428\n",
            "epoch: 2, classification_loss: 1.8092756271362305, Val Loss: 70.56657564640045, loss : 2.0596654415130615\n",
            "epoch: 3, classification_loss: 1.8411654233932495, Val Loss: 70.64491605758667, loss : 2.111424446105957\n",
            "epoch: 4, classification_loss: 1.8494620323181152, Val Loss: 70.5654628276825, loss : 2.078669309616089\n",
            "epoch: 5, classification_loss: 1.8341134786605835, Val Loss: 70.5736072063446, loss : 2.1066970825195312\n",
            "epoch: 6, classification_loss: 1.8123338222503662, Val Loss: 70.559943318367, loss : 2.09537672996521\n",
            "epoch: 7, classification_loss: 1.8334695100784302, Val Loss: 70.56364607810974, loss : 2.0903637409210205\n",
            "epoch: 8, classification_loss: 1.8291634321212769, Val Loss: 70.57263648509979, loss : 2.081406593322754\n",
            "epoch: 9, classification_loss: 1.8392384052276611, Val Loss: 70.58628141880035, loss : 2.085772752761841\n",
            "epoch: 10, classification_loss: 1.841712474822998, Val Loss: 70.64173066616058, loss : 2.0852036476135254\n",
            "epoch: 11, classification_loss: 1.8380459547042847, Val Loss: 70.56229412555695, loss : 2.083752393722534\n",
            "epoch: 12, classification_loss: 1.8439979553222656, Val Loss: 70.6195501089096, loss : 2.077657699584961\n",
            "epoch: 13, classification_loss: 1.8372269868850708, Val Loss: 70.64088189601898, loss : 2.081111192703247\n",
            "epoch: 14, classification_loss: 1.8514258861541748, Val Loss: 70.65151381492615, loss : 2.0830068588256836\n",
            "epoch: 15, classification_loss: 1.847359299659729, Val Loss: 70.59224033355713, loss : 2.0792365074157715\n",
            "epoch: 16, classification_loss: 1.8440927267074585, Val Loss: 70.59075391292572, loss : 2.073824405670166\n",
            "epoch: 17, classification_loss: 1.844321608543396, Val Loss: 70.59717977046967, loss : 2.0808842182159424\n",
            "epoch: 18, classification_loss: 1.827095627784729, Val Loss: 70.58831882476807, loss : 2.0473217964172363\n",
            "epoch: 19, classification_loss: 1.8402608633041382, Val Loss: 70.5872768163681, loss : 2.0761988162994385\n",
            "Batch: 100, Test Acc: 0.5752203525641025\n",
            "Batch: 101:\n",
            "epoch: 0, classification_loss: 1.808331847190857, Val Loss: 70.62945711612701, loss : 1.808331847190857\n",
            "epoch: 1, classification_loss: 1.7706749439239502, Val Loss: 70.59283864498138, loss : 2.093897819519043\n",
            "epoch: 2, classification_loss: 1.7524651288986206, Val Loss: 70.59546995162964, loss : 2.0109496116638184\n",
            "epoch: 3, classification_loss: 1.7763762474060059, Val Loss: 70.58881771564484, loss : 2.0425307750701904\n",
            "epoch: 4, classification_loss: 1.8026834726333618, Val Loss: 70.56309401988983, loss : 2.0317370891571045\n",
            "epoch: 5, classification_loss: 1.792296051979065, Val Loss: 70.56318593025208, loss : 2.04641056060791\n",
            "epoch: 6, classification_loss: 1.7811533212661743, Val Loss: 70.62413239479065, loss : 2.046400547027588\n",
            "epoch: 7, classification_loss: 1.7759884595870972, Val Loss: 70.63406002521515, loss : 2.0340826511383057\n",
            "epoch: 8, classification_loss: 1.7913092374801636, Val Loss: 70.57163345813751, loss : 2.045560598373413\n",
            "epoch: 9, classification_loss: 1.7788792848587036, Val Loss: 70.57257354259491, loss : 2.033940076828003\n",
            "epoch: 10, classification_loss: 1.7907087802886963, Val Loss: 70.62982165813446, loss : 2.029303550720215\n",
            "epoch: 11, classification_loss: 1.793216586112976, Val Loss: 70.57658958435059, loss : 2.039451837539673\n",
            "epoch: 12, classification_loss: 1.8037149906158447, Val Loss: 70.57391393184662, loss : 2.0345661640167236\n",
            "epoch: 13, classification_loss: 1.797437071800232, Val Loss: 70.56480515003204, loss : 2.0469908714294434\n",
            "epoch: 14, classification_loss: 1.7740741968154907, Val Loss: 70.59319794178009, loss : 2.017796516418457\n",
            "epoch: 15, classification_loss: 1.7876449823379517, Val Loss: 70.55877876281738, loss : 2.0295321941375732\n",
            "epoch: 16, classification_loss: 1.7848820686340332, Val Loss: 70.57616925239563, loss : 2.0266900062561035\n",
            "epoch: 17, classification_loss: 1.783345103263855, Val Loss: 70.61524248123169, loss : 2.030571222305298\n",
            "epoch: 18, classification_loss: 1.7975035905838013, Val Loss: 70.62552750110626, loss : 2.027557611465454\n",
            "epoch: 19, classification_loss: 1.7887309789657593, Val Loss: 70.60179030895233, loss : 2.0238702297210693\n",
            "Batch: 101, Test Acc: 0.5740184294871795\n",
            "Batch: 102:\n",
            "epoch: 0, classification_loss: 1.8396084308624268, Val Loss: 70.7366977930069, loss : 1.8396084308624268\n",
            "epoch: 1, classification_loss: 1.8026381731033325, Val Loss: 70.67634904384613, loss : 2.1471009254455566\n",
            "epoch: 2, classification_loss: 1.7829093933105469, Val Loss: 70.62406182289124, loss : 2.063915491104126\n",
            "epoch: 3, classification_loss: 1.8057752847671509, Val Loss: 70.61700415611267, loss : 2.094813108444214\n",
            "epoch: 4, classification_loss: 1.8384207487106323, Val Loss: 70.66264605522156, loss : 2.099623680114746\n",
            "epoch: 5, classification_loss: 1.8304996490478516, Val Loss: 70.58136534690857, loss : 2.0933356285095215\n",
            "epoch: 6, classification_loss: 1.8090972900390625, Val Loss: 70.59538340568542, loss : 2.0763585567474365\n",
            "epoch: 7, classification_loss: 1.8196091651916504, Val Loss: 70.65624189376831, loss : 2.085716724395752\n",
            "epoch: 8, classification_loss: 1.818123698234558, Val Loss: 70.69350922107697, loss : 2.0743725299835205\n",
            "epoch: 9, classification_loss: 1.8126901388168335, Val Loss: 70.62535905838013, loss : 2.0757222175598145\n",
            "epoch: 10, classification_loss: 1.8149511814117432, Val Loss: 70.62645959854126, loss : 2.072018623352051\n",
            "epoch: 11, classification_loss: 1.817530870437622, Val Loss: 70.69402527809143, loss : 2.067575454711914\n",
            "epoch: 12, classification_loss: 1.8199496269226074, Val Loss: 70.65697777271271, loss : 2.071831703186035\n",
            "epoch: 13, classification_loss: 1.8174588680267334, Val Loss: 70.69540095329285, loss : 2.064293622970581\n",
            "epoch: 14, classification_loss: 1.811518907546997, Val Loss: 70.63131272792816, loss : 2.058204412460327\n",
            "epoch: 15, classification_loss: 1.8165353536605835, Val Loss: 70.62032520771027, loss : 2.058156728744507\n",
            "epoch: 16, classification_loss: 1.8086639642715454, Val Loss: 70.62021791934967, loss : 2.049668073654175\n",
            "epoch: 17, classification_loss: 1.820482850074768, Val Loss: 70.65202987194061, loss : 2.068760395050049\n",
            "epoch: 18, classification_loss: 1.8110625743865967, Val Loss: 70.69515776634216, loss : 2.0470614433288574\n",
            "epoch: 19, classification_loss: 1.813349962234497, Val Loss: 70.68116188049316, loss : 2.0524954795837402\n",
            "Batch: 102, Test Acc: 0.5716145833333334\n",
            "Batch: 103:\n",
            "epoch: 0, classification_loss: 1.8428831100463867, Val Loss: 70.5784330368042, loss : 1.8428831100463867\n",
            "epoch: 1, classification_loss: 1.8003242015838623, Val Loss: 70.58225178718567, loss : 2.1365785598754883\n",
            "epoch: 2, classification_loss: 1.7719725370407104, Val Loss: 70.59208631515503, loss : 2.041144847869873\n",
            "epoch: 3, classification_loss: 1.801931381225586, Val Loss: 70.59358823299408, loss : 2.0722835063934326\n",
            "epoch: 4, classification_loss: 1.8338369131088257, Val Loss: 70.60322892665863, loss : 2.0661044120788574\n",
            "epoch: 5, classification_loss: 1.7948424816131592, Val Loss: 70.60203325748444, loss : 2.0600624084472656\n",
            "epoch: 6, classification_loss: 1.805033564567566, Val Loss: 70.6010353565216, loss : 2.073739767074585\n",
            "epoch: 7, classification_loss: 1.7975447177886963, Val Loss: 70.62549185752869, loss : 2.050421714782715\n",
            "epoch: 8, classification_loss: 1.815068244934082, Val Loss: 70.60071003437042, loss : 2.0634961128234863\n",
            "epoch: 9, classification_loss: 1.808152437210083, Val Loss: 70.61022770404816, loss : 2.05963397026062\n",
            "epoch: 10, classification_loss: 1.806925892829895, Val Loss: 70.65874636173248, loss : 2.053262948989868\n",
            "epoch: 11, classification_loss: 1.8111740350723267, Val Loss: 70.63031673431396, loss : 2.0539255142211914\n",
            "epoch: 12, classification_loss: 1.8062963485717773, Val Loss: 70.64917957782745, loss : 2.048224687576294\n",
            "epoch: 13, classification_loss: 1.801011323928833, Val Loss: 70.69967126846313, loss : 2.043395519256592\n",
            "epoch: 14, classification_loss: 1.8186153173446655, Val Loss: 70.62913179397583, loss : 2.052323579788208\n",
            "epoch: 15, classification_loss: 1.817118525505066, Val Loss: 70.64579856395721, loss : 2.052400827407837\n",
            "epoch: 16, classification_loss: 1.8016430139541626, Val Loss: 70.65254175662994, loss : 2.0418541431427\n",
            "epoch: 17, classification_loss: 1.8196144104003906, Val Loss: 70.72349607944489, loss : 2.0580902099609375\n",
            "epoch: 18, classification_loss: 1.808797836303711, Val Loss: 70.72444808483124, loss : 2.037177085876465\n",
            "epoch: 19, classification_loss: 1.8076883554458618, Val Loss: 70.65471923351288, loss : 2.043069362640381\n",
            "Batch: 103, Test Acc: 0.5735176282051282\n",
            "Batch: 104:\n",
            "epoch: 0, classification_loss: 1.7837249040603638, Val Loss: 70.6786538362503, loss : 1.7837249040603638\n",
            "epoch: 1, classification_loss: 1.7593543529510498, Val Loss: 70.58853721618652, loss : 2.0853028297424316\n",
            "epoch: 2, classification_loss: 1.7334953546524048, Val Loss: 70.60499441623688, loss : 1.985408067703247\n",
            "epoch: 3, classification_loss: 1.7559938430786133, Val Loss: 70.64145123958588, loss : 2.0204029083251953\n",
            "epoch: 4, classification_loss: 1.7700198888778687, Val Loss: 70.5946090221405, loss : 1.9978517293930054\n",
            "epoch: 5, classification_loss: 1.7635040283203125, Val Loss: 70.63891386985779, loss : 2.020198345184326\n",
            "epoch: 6, classification_loss: 1.7574063539505005, Val Loss: 70.63010287284851, loss : 2.0245189666748047\n",
            "epoch: 7, classification_loss: 1.7405266761779785, Val Loss: 70.6718932390213, loss : 2.0016088485717773\n",
            "epoch: 8, classification_loss: 1.7609156370162964, Val Loss: 70.62985050678253, loss : 2.0143768787384033\n",
            "epoch: 9, classification_loss: 1.7527655363082886, Val Loss: 70.64083683490753, loss : 1.9969784021377563\n",
            "epoch: 10, classification_loss: 1.765128493309021, Val Loss: 70.67019438743591, loss : 2.006915807723999\n",
            "epoch: 11, classification_loss: 1.7575331926345825, Val Loss: 70.67449021339417, loss : 2.0007121562957764\n",
            "epoch: 12, classification_loss: 1.7646842002868652, Val Loss: 70.72409343719482, loss : 2.0001211166381836\n",
            "epoch: 13, classification_loss: 1.7582042217254639, Val Loss: 70.69771575927734, loss : 2.007937431335449\n",
            "epoch: 14, classification_loss: 1.7540700435638428, Val Loss: 70.71395063400269, loss : 1.9899991750717163\n",
            "epoch: 15, classification_loss: 1.7549422979354858, Val Loss: 70.65706896781921, loss : 1.997728943824768\n",
            "epoch: 16, classification_loss: 1.7529798746109009, Val Loss: 70.68946611881256, loss : 1.9875332117080688\n",
            "epoch: 17, classification_loss: 1.7688939571380615, Val Loss: 70.74379599094391, loss : 2.003108501434326\n",
            "epoch: 18, classification_loss: 1.7713539600372314, Val Loss: 70.71690809726715, loss : 1.9973734617233276\n",
            "epoch: 19, classification_loss: 1.7767093181610107, Val Loss: 70.7156013250351, loss : 2.005558729171753\n",
            "Batch: 104, Test Acc: 0.571113782051282\n",
            "Batch: 105:\n",
            "epoch: 0, classification_loss: 1.8561100959777832, Val Loss: 70.5788801908493, loss : 1.8561100959777832\n",
            "epoch: 1, classification_loss: 1.8092010021209717, Val Loss: 70.61187207698822, loss : 2.1380374431610107\n",
            "epoch: 2, classification_loss: 1.801814079284668, Val Loss: 70.65033495426178, loss : 2.078192949295044\n",
            "epoch: 3, classification_loss: 1.8127020597457886, Val Loss: 70.59649014472961, loss : 2.097076654434204\n",
            "epoch: 4, classification_loss: 1.8434025049209595, Val Loss: 70.56469941139221, loss : 2.091970920562744\n",
            "epoch: 5, classification_loss: 1.8421037197113037, Val Loss: 70.58649635314941, loss : 2.0986874103546143\n",
            "epoch: 6, classification_loss: 1.8367971181869507, Val Loss: 70.61222159862518, loss : 2.09623122215271\n",
            "epoch: 7, classification_loss: 1.8273358345031738, Val Loss: 70.64037728309631, loss : 2.0944724082946777\n",
            "epoch: 8, classification_loss: 1.836212158203125, Val Loss: 70.62768709659576, loss : 2.0961532592773438\n",
            "epoch: 9, classification_loss: 1.8422672748565674, Val Loss: 70.61240029335022, loss : 2.1042778491973877\n",
            "epoch: 10, classification_loss: 1.836378574371338, Val Loss: 70.61183798313141, loss : 2.087418794631958\n",
            "epoch: 11, classification_loss: 1.8499432802200317, Val Loss: 70.61032998561859, loss : 2.098801374435425\n",
            "epoch: 12, classification_loss: 1.8417850732803345, Val Loss: 70.59939849376678, loss : 2.0909361839294434\n",
            "epoch: 13, classification_loss: 1.8409945964813232, Val Loss: 70.61322057247162, loss : 2.092040538787842\n",
            "epoch: 14, classification_loss: 1.8325374126434326, Val Loss: 70.61160492897034, loss : 2.069521903991699\n",
            "epoch: 15, classification_loss: 1.8334336280822754, Val Loss: 70.60637092590332, loss : 2.084103584289551\n",
            "epoch: 16, classification_loss: 1.8352893590927124, Val Loss: 70.63782680034637, loss : 2.0799379348754883\n",
            "epoch: 17, classification_loss: 1.8366953134536743, Val Loss: 70.60821616649628, loss : 2.0860755443573\n",
            "epoch: 18, classification_loss: 1.8413081169128418, Val Loss: 70.6304063796997, loss : 2.075205087661743\n",
            "epoch: 19, classification_loss: 1.8454704284667969, Val Loss: 70.64450991153717, loss : 2.092658042907715\n",
            "Batch: 105, Test Acc: 0.5727163461538461\n",
            "Batch: 106:\n",
            "epoch: 0, classification_loss: 1.8287421464920044, Val Loss: 70.63765251636505, loss : 1.8287421464920044\n",
            "epoch: 1, classification_loss: 1.7924110889434814, Val Loss: 70.57020568847656, loss : 2.117053508758545\n",
            "epoch: 2, classification_loss: 1.759147047996521, Val Loss: 70.61172568798065, loss : 2.0080597400665283\n",
            "epoch: 3, classification_loss: 1.7878508567810059, Val Loss: 70.64306664466858, loss : 2.0618362426757812\n",
            "epoch: 4, classification_loss: 1.805496096611023, Val Loss: 70.60748386383057, loss : 2.037064790725708\n",
            "epoch: 5, classification_loss: 1.8117713928222656, Val Loss: 70.59587752819061, loss : 2.0637879371643066\n",
            "epoch: 6, classification_loss: 1.8035898208618164, Val Loss: 70.60628819465637, loss : 2.0635414123535156\n",
            "epoch: 7, classification_loss: 1.7878135442733765, Val Loss: 70.63168585300446, loss : 2.0439538955688477\n",
            "epoch: 8, classification_loss: 1.788973331451416, Val Loss: 70.61642134189606, loss : 2.0416479110717773\n",
            "epoch: 9, classification_loss: 1.7919824123382568, Val Loss: 70.6045755147934, loss : 2.038078546524048\n",
            "epoch: 10, classification_loss: 1.8017772436141968, Val Loss: 70.64820325374603, loss : 2.0432450771331787\n",
            "epoch: 11, classification_loss: 1.8001502752304077, Val Loss: 70.65164887905121, loss : 2.0329623222351074\n",
            "epoch: 12, classification_loss: 1.7968884706497192, Val Loss: 70.61921811103821, loss : 2.0335497856140137\n",
            "epoch: 13, classification_loss: 1.7866708040237427, Val Loss: 70.65832197666168, loss : 2.0235471725463867\n",
            "epoch: 14, classification_loss: 1.7969247102737427, Val Loss: 70.68787252902985, loss : 2.030215263366699\n",
            "epoch: 15, classification_loss: 1.7990416288375854, Val Loss: 70.67920589447021, loss : 2.0309898853302\n",
            "epoch: 16, classification_loss: 1.8072065114974976, Val Loss: 70.65161573886871, loss : 2.0392584800720215\n",
            "epoch: 17, classification_loss: 1.8061069250106812, Val Loss: 70.66178131103516, loss : 2.033360719680786\n",
            "epoch: 18, classification_loss: 1.8083089590072632, Val Loss: 70.66376507282257, loss : 2.036425828933716\n",
            "epoch: 19, classification_loss: 1.7988789081573486, Val Loss: 70.67143678665161, loss : 2.030015230178833\n",
            "Batch: 106, Test Acc: 0.5732171474358975\n",
            "Batch: 107:\n",
            "epoch: 0, classification_loss: 1.8242460489273071, Val Loss: 70.6279616355896, loss : 1.8242460489273071\n",
            "epoch: 1, classification_loss: 1.7964969873428345, Val Loss: 70.62214422225952, loss : 2.13924241065979\n",
            "epoch: 2, classification_loss: 1.7661367654800415, Val Loss: 70.63092160224915, loss : 2.0529720783233643\n",
            "epoch: 3, classification_loss: 1.7927435636520386, Val Loss: 70.58523082733154, loss : 2.071662187576294\n",
            "epoch: 4, classification_loss: 1.8175852298736572, Val Loss: 70.5555808544159, loss : 2.0727782249450684\n",
            "epoch: 5, classification_loss: 1.8211010694503784, Val Loss: 70.60843122005463, loss : 2.0560567378997803\n",
            "epoch: 6, classification_loss: 1.801361083984375, Val Loss: 70.61721706390381, loss : 2.0695366859436035\n",
            "epoch: 7, classification_loss: 1.7805815935134888, Val Loss: 70.63382935523987, loss : 2.05777907371521\n",
            "epoch: 8, classification_loss: 1.7926920652389526, Val Loss: 70.59470319747925, loss : 2.053463935852051\n",
            "epoch: 9, classification_loss: 1.8046789169311523, Val Loss: 70.56888937950134, loss : 2.0599589347839355\n",
            "epoch: 10, classification_loss: 1.8098341226577759, Val Loss: 70.59539270401001, loss : 2.057976722717285\n",
            "epoch: 11, classification_loss: 1.8123501539230347, Val Loss: 70.60374343395233, loss : 2.0575385093688965\n",
            "epoch: 12, classification_loss: 1.8083378076553345, Val Loss: 70.59962248802185, loss : 2.065837860107422\n",
            "epoch: 13, classification_loss: 1.8068556785583496, Val Loss: 70.60718369483948, loss : 2.0561273097991943\n",
            "epoch: 14, classification_loss: 1.8109321594238281, Val Loss: 70.60926187038422, loss : 2.0577893257141113\n",
            "epoch: 15, classification_loss: 1.81673002243042, Val Loss: 70.58940827846527, loss : 2.0622589588165283\n",
            "epoch: 16, classification_loss: 1.8093311786651611, Val Loss: 70.63921046257019, loss : 2.046846628189087\n",
            "epoch: 17, classification_loss: 1.801344871520996, Val Loss: 70.6179666519165, loss : 2.050422430038452\n",
            "epoch: 18, classification_loss: 1.8018230199813843, Val Loss: 70.65427577495575, loss : 2.042067527770996\n",
            "epoch: 19, classification_loss: 1.8039920330047607, Val Loss: 70.59789514541626, loss : 2.0498077869415283\n",
            "Batch: 107, Test Acc: 0.5729166666666666\n",
            "Batch: 108:\n",
            "epoch: 0, classification_loss: 1.823527216911316, Val Loss: 70.62278056144714, loss : 1.823527216911316\n",
            "epoch: 1, classification_loss: 1.7751649618148804, Val Loss: 70.5935366153717, loss : 2.1093544960021973\n",
            "epoch: 2, classification_loss: 1.763444185256958, Val Loss: 70.60556840896606, loss : 2.039389133453369\n",
            "epoch: 3, classification_loss: 1.7755718231201172, Val Loss: 70.58926248550415, loss : 2.0537590980529785\n",
            "epoch: 4, classification_loss: 1.8016602993011475, Val Loss: 70.58998262882233, loss : 2.046473264694214\n",
            "epoch: 5, classification_loss: 1.8046455383300781, Val Loss: 70.6106287240982, loss : 2.063434362411499\n",
            "epoch: 6, classification_loss: 1.7890958786010742, Val Loss: 70.64707231521606, loss : 2.052493095397949\n",
            "epoch: 7, classification_loss: 1.7841116189956665, Val Loss: 70.61870849132538, loss : 2.0525460243225098\n",
            "epoch: 8, classification_loss: 1.7828700542449951, Val Loss: 70.6399177312851, loss : 2.0475566387176514\n",
            "epoch: 9, classification_loss: 1.7868366241455078, Val Loss: 70.60329020023346, loss : 2.0431385040283203\n",
            "epoch: 10, classification_loss: 1.7891075611114502, Val Loss: 70.60472762584686, loss : 2.041358232498169\n",
            "epoch: 11, classification_loss: 1.7930066585540771, Val Loss: 70.633948802948, loss : 2.040004014968872\n",
            "epoch: 12, classification_loss: 1.7923253774642944, Val Loss: 70.64632940292358, loss : 2.038541078567505\n",
            "epoch: 13, classification_loss: 1.7918808460235596, Val Loss: 70.61273777484894, loss : 2.040339708328247\n",
            "epoch: 14, classification_loss: 1.7911173105239868, Val Loss: 70.60948073863983, loss : 2.025080680847168\n",
            "epoch: 15, classification_loss: 1.7976906299591064, Val Loss: 70.63228642940521, loss : 2.045083522796631\n",
            "epoch: 16, classification_loss: 1.7890920639038086, Val Loss: 70.63987827301025, loss : 2.034123420715332\n",
            "epoch: 17, classification_loss: 1.7866594791412354, Val Loss: 70.63924419879913, loss : 2.033149003982544\n",
            "epoch: 18, classification_loss: 1.8001147508621216, Val Loss: 70.59742259979248, loss : 2.036630868911743\n",
            "epoch: 19, classification_loss: 1.7876514196395874, Val Loss: 70.6171783208847, loss : 2.0361697673797607\n",
            "Batch: 108, Test Acc: 0.5735176282051282\n",
            "Batch: 109:\n",
            "epoch: 0, classification_loss: 1.816046953201294, Val Loss: 70.6438798904419, loss : 1.816046953201294\n",
            "epoch: 1, classification_loss: 1.7659273147583008, Val Loss: 70.65481233596802, loss : 2.089468002319336\n",
            "epoch: 2, classification_loss: 1.7480019330978394, Val Loss: 70.58906960487366, loss : 2.007032871246338\n",
            "epoch: 3, classification_loss: 1.779329538345337, Val Loss: 70.58897614479065, loss : 2.0568251609802246\n",
            "epoch: 4, classification_loss: 1.7935367822647095, Val Loss: 70.57691144943237, loss : 2.0396878719329834\n",
            "epoch: 5, classification_loss: 1.7847646474838257, Val Loss: 70.61125242710114, loss : 2.0421464443206787\n",
            "epoch: 6, classification_loss: 1.7858901023864746, Val Loss: 70.59927260875702, loss : 2.052072048187256\n",
            "epoch: 7, classification_loss: 1.7763280868530273, Val Loss: 70.60271847248077, loss : 2.038844347000122\n",
            "epoch: 8, classification_loss: 1.7832297086715698, Val Loss: 70.60491967201233, loss : 2.039008140563965\n",
            "epoch: 9, classification_loss: 1.774187445640564, Val Loss: 70.59266984462738, loss : 2.033017635345459\n",
            "epoch: 10, classification_loss: 1.7794694900512695, Val Loss: 70.58633363246918, loss : 2.0304691791534424\n",
            "epoch: 11, classification_loss: 1.7814230918884277, Val Loss: 70.61046588420868, loss : 2.026801347732544\n",
            "epoch: 12, classification_loss: 1.7839038372039795, Val Loss: 70.59591948986053, loss : 2.0242209434509277\n",
            "epoch: 13, classification_loss: 1.786426067352295, Val Loss: 70.60002636909485, loss : 2.0404038429260254\n",
            "epoch: 14, classification_loss: 1.775741696357727, Val Loss: 70.5755604505539, loss : 2.017679452896118\n",
            "epoch: 15, classification_loss: 1.773169994354248, Val Loss: 70.58500683307648, loss : 2.0183656215667725\n",
            "epoch: 16, classification_loss: 1.7769776582717896, Val Loss: 70.63506090641022, loss : 2.016099691390991\n",
            "epoch: 17, classification_loss: 1.7913599014282227, Val Loss: 70.67590260505676, loss : 2.0379889011383057\n",
            "epoch: 18, classification_loss: 1.7774461507797241, Val Loss: 70.62561762332916, loss : 2.024242639541626\n",
            "epoch: 19, classification_loss: 1.7876157760620117, Val Loss: 70.60436832904816, loss : 2.034170389175415\n",
            "Batch: 109, Test Acc: 0.5754206730769231\n",
            "Batch: 110:\n",
            "epoch: 0, classification_loss: 1.825081467628479, Val Loss: 70.58179688453674, loss : 1.825081467628479\n",
            "epoch: 1, classification_loss: 1.7871041297912598, Val Loss: 70.60657894611359, loss : 2.1139683723449707\n",
            "epoch: 2, classification_loss: 1.778177261352539, Val Loss: 70.63183033466339, loss : 2.033900737762451\n",
            "epoch: 3, classification_loss: 1.7934376001358032, Val Loss: 70.66097354888916, loss : 2.0630195140838623\n",
            "epoch: 4, classification_loss: 1.8105709552764893, Val Loss: 70.60410237312317, loss : 2.0466928482055664\n",
            "epoch: 5, classification_loss: 1.8048839569091797, Val Loss: 70.61137962341309, loss : 2.0479702949523926\n",
            "epoch: 6, classification_loss: 1.787895679473877, Val Loss: 70.59820997714996, loss : 2.042281150817871\n",
            "epoch: 7, classification_loss: 1.809207558631897, Val Loss: 70.6408816576004, loss : 2.05564546585083\n",
            "epoch: 8, classification_loss: 1.7991838455200195, Val Loss: 70.63415205478668, loss : 2.0368402004241943\n",
            "epoch: 9, classification_loss: 1.8140721321105957, Val Loss: 70.66028547286987, loss : 2.049999237060547\n",
            "epoch: 10, classification_loss: 1.803398609161377, Val Loss: 70.64451503753662, loss : 2.029010057449341\n",
            "epoch: 11, classification_loss: 1.8026490211486816, Val Loss: 70.64262068271637, loss : 2.0344789028167725\n",
            "epoch: 12, classification_loss: 1.797659993171692, Val Loss: 70.64841651916504, loss : 2.0279557704925537\n",
            "epoch: 13, classification_loss: 1.7979899644851685, Val Loss: 70.65500438213348, loss : 2.029491662979126\n",
            "epoch: 14, classification_loss: 1.8073363304138184, Val Loss: 70.64841330051422, loss : 2.03118896484375\n",
            "epoch: 15, classification_loss: 1.8058291673660278, Val Loss: 70.64507484436035, loss : 2.0287973880767822\n",
            "epoch: 16, classification_loss: 1.7986423969268799, Val Loss: 70.650062084198, loss : 2.019557476043701\n",
            "epoch: 17, classification_loss: 1.8055037260055542, Val Loss: 70.64842629432678, loss : 2.03072190284729\n",
            "epoch: 18, classification_loss: 1.8113038539886475, Val Loss: 70.65372931957245, loss : 2.0258476734161377\n",
            "epoch: 19, classification_loss: 1.809018850326538, Val Loss: 70.65112161636353, loss : 2.03395938873291\n",
            "Batch: 110, Test Acc: 0.5735176282051282\n",
            "Batch: 111:\n",
            "epoch: 0, classification_loss: 1.7880969047546387, Val Loss: 70.59265065193176, loss : 1.7880969047546387\n",
            "epoch: 1, classification_loss: 1.74318528175354, Val Loss: 70.65740549564362, loss : 2.0712742805480957\n",
            "epoch: 2, classification_loss: 1.726982593536377, Val Loss: 70.60681188106537, loss : 1.9908559322357178\n",
            "epoch: 3, classification_loss: 1.7466522455215454, Val Loss: 70.65887749195099, loss : 2.005404233932495\n",
            "epoch: 4, classification_loss: 1.7758592367172241, Val Loss: 70.63159835338593, loss : 2.0124964714050293\n",
            "epoch: 5, classification_loss: 1.7619800567626953, Val Loss: 70.69647216796875, loss : 2.009396553039551\n",
            "epoch: 6, classification_loss: 1.7483019828796387, Val Loss: 70.6749039888382, loss : 2.00354266166687\n",
            "epoch: 7, classification_loss: 1.7472320795059204, Val Loss: 70.6712189912796, loss : 2.0039966106414795\n",
            "epoch: 8, classification_loss: 1.7543929815292358, Val Loss: 70.64446270465851, loss : 2.0024986267089844\n",
            "epoch: 9, classification_loss: 1.7701829671859741, Val Loss: 70.71323573589325, loss : 2.016598701477051\n",
            "epoch: 10, classification_loss: 1.7755566835403442, Val Loss: 70.7022316455841, loss : 2.010270357131958\n",
            "epoch: 11, classification_loss: 1.7807010412216187, Val Loss: 70.66055417060852, loss : 2.017411947250366\n",
            "epoch: 12, classification_loss: 1.769403338432312, Val Loss: 70.70497846603394, loss : 2.0060031414031982\n",
            "epoch: 13, classification_loss: 1.7561943531036377, Val Loss: 70.74488508701324, loss : 1.9997594356536865\n",
            "epoch: 14, classification_loss: 1.7501335144042969, Val Loss: 70.67143094539642, loss : 1.9885293245315552\n",
            "epoch: 15, classification_loss: 1.7615206241607666, Val Loss: 70.68527793884277, loss : 1.9963955879211426\n",
            "epoch: 16, classification_loss: 1.7638391256332397, Val Loss: 70.72352957725525, loss : 1.9998300075531006\n",
            "epoch: 17, classification_loss: 1.7712072134017944, Val Loss: 70.76250112056732, loss : 2.009629011154175\n",
            "epoch: 18, classification_loss: 1.7656042575836182, Val Loss: 70.7169748544693, loss : 1.99688720703125\n",
            "epoch: 19, classification_loss: 1.769421100616455, Val Loss: 70.7109180688858, loss : 2.0061702728271484\n",
            "Batch: 111, Test Acc: 0.5733173076923077\n",
            "Batch: 112:\n",
            "epoch: 0, classification_loss: 1.799290657043457, Val Loss: 70.62046790122986, loss : 1.799290657043457\n",
            "epoch: 1, classification_loss: 1.7570219039916992, Val Loss: 70.66351079940796, loss : 2.0965049266815186\n",
            "epoch: 2, classification_loss: 1.7263513803482056, Val Loss: 70.61429345607758, loss : 1.999629020690918\n",
            "epoch: 3, classification_loss: 1.7631369829177856, Val Loss: 70.60925436019897, loss : 2.0400612354278564\n",
            "epoch: 4, classification_loss: 1.7851319313049316, Val Loss: 70.64749801158905, loss : 2.0243239402770996\n",
            "epoch: 5, classification_loss: 1.7708346843719482, Val Loss: 70.6207435131073, loss : 2.0364718437194824\n",
            "epoch: 6, classification_loss: 1.7692413330078125, Val Loss: 70.62323009967804, loss : 2.0377511978149414\n",
            "epoch: 7, classification_loss: 1.7715022563934326, Val Loss: 70.63076829910278, loss : 2.034372329711914\n",
            "epoch: 8, classification_loss: 1.7691937685012817, Val Loss: 70.70179104804993, loss : 2.0323283672332764\n",
            "epoch: 9, classification_loss: 1.7683954238891602, Val Loss: 70.65781366825104, loss : 2.0239815711975098\n",
            "epoch: 10, classification_loss: 1.772139549255371, Val Loss: 70.58676528930664, loss : 2.027207612991333\n",
            "epoch: 11, classification_loss: 1.7630414962768555, Val Loss: 70.63372731208801, loss : 2.012788772583008\n",
            "epoch: 12, classification_loss: 1.764415979385376, Val Loss: 70.6922836303711, loss : 2.0158493518829346\n",
            "epoch: 13, classification_loss: 1.7669310569763184, Val Loss: 70.65473926067352, loss : 2.0176596641540527\n",
            "epoch: 14, classification_loss: 1.7697867155075073, Val Loss: 70.61937260627747, loss : 2.0155014991760254\n",
            "epoch: 15, classification_loss: 1.7565709352493286, Val Loss: 70.60707020759583, loss : 2.0058059692382812\n",
            "epoch: 16, classification_loss: 1.7651021480560303, Val Loss: 70.66072702407837, loss : 2.005255937576294\n",
            "epoch: 17, classification_loss: 1.7786993980407715, Val Loss: 70.69081103801727, loss : 2.0204927921295166\n",
            "epoch: 18, classification_loss: 1.779029369354248, Val Loss: 70.67567467689514, loss : 2.0159473419189453\n",
            "epoch: 19, classification_loss: 1.769370198249817, Val Loss: 70.6254997253418, loss : 2.013493061065674\n",
            "Batch: 112, Test Acc: 0.5753205128205128\n",
            "Batch: 113:\n",
            "epoch: 0, classification_loss: 1.831040382385254, Val Loss: 70.56909763813019, loss : 1.831040382385254\n",
            "epoch: 1, classification_loss: 1.7867711782455444, Val Loss: 70.61593699455261, loss : 2.11297869682312\n",
            "epoch: 2, classification_loss: 1.7712595462799072, Val Loss: 70.59281265735626, loss : 2.0400521755218506\n",
            "epoch: 3, classification_loss: 1.789219617843628, Val Loss: 70.58670616149902, loss : 2.0785181522369385\n",
            "epoch: 4, classification_loss: 1.8145719766616821, Val Loss: 70.60755360126495, loss : 2.065537929534912\n",
            "epoch: 5, classification_loss: 1.8175209760665894, Val Loss: 70.54789578914642, loss : 2.081637382507324\n",
            "epoch: 6, classification_loss: 1.8193079233169556, Val Loss: 70.59588241577148, loss : 1.8193079233169556\n",
            "epoch: 7, classification_loss: 1.7624592781066895, Val Loss: 70.61712503433228, loss : 2.0966718196868896\n",
            "epoch: 8, classification_loss: 1.733077883720398, Val Loss: 70.57006394863129, loss : 2.009249687194824\n",
            "epoch: 9, classification_loss: 1.784567952156067, Val Loss: 70.58214724063873, loss : 2.033207893371582\n",
            "epoch: 10, classification_loss: 1.801849126815796, Val Loss: 70.64942240715027, loss : 2.0405900478363037\n",
            "epoch: 11, classification_loss: 1.775693416595459, Val Loss: 70.56491672992706, loss : 2.020033597946167\n",
            "epoch: 12, classification_loss: 1.75913405418396, Val Loss: 70.55435085296631, loss : 2.0178380012512207\n",
            "epoch: 13, classification_loss: 1.7825164794921875, Val Loss: 70.60589671134949, loss : 2.027122974395752\n",
            "epoch: 14, classification_loss: 1.7767935991287231, Val Loss: 70.6241763830185, loss : 2.0276355743408203\n",
            "epoch: 15, classification_loss: 1.7776455879211426, Val Loss: 70.60020124912262, loss : 2.021068572998047\n",
            "epoch: 16, classification_loss: 1.782651424407959, Val Loss: 70.57562327384949, loss : 2.031979560852051\n",
            "epoch: 17, classification_loss: 1.78267502784729, Val Loss: 70.55807876586914, loss : 2.023207902908325\n",
            "epoch: 18, classification_loss: 1.780809760093689, Val Loss: 70.60703647136688, loss : 2.0225415229797363\n",
            "epoch: 19, classification_loss: 1.7802839279174805, Val Loss: 70.58806359767914, loss : 2.009127378463745\n",
            "Batch: 113, Test Acc: 0.5728165064102564\n",
            "Batch: 114:\n",
            "epoch: 0, classification_loss: 1.8198949098587036, Val Loss: 70.61200416088104, loss : 1.8198949098587036\n",
            "epoch: 1, classification_loss: 1.7714126110076904, Val Loss: 70.61512279510498, loss : 2.1120312213897705\n",
            "epoch: 2, classification_loss: 1.7627131938934326, Val Loss: 70.64592516422272, loss : 2.032986640930176\n",
            "epoch: 3, classification_loss: 1.7869632244110107, Val Loss: 70.65836298465729, loss : 2.0719454288482666\n",
            "epoch: 4, classification_loss: 1.7994439601898193, Val Loss: 70.63256764411926, loss : 2.055947780609131\n",
            "epoch: 5, classification_loss: 1.7955869436264038, Val Loss: 70.64424586296082, loss : 2.0422685146331787\n",
            "epoch: 6, classification_loss: 1.7798466682434082, Val Loss: 70.65000796318054, loss : 2.0412650108337402\n",
            "epoch: 7, classification_loss: 1.78483247756958, Val Loss: 70.64301097393036, loss : 2.0365421772003174\n",
            "epoch: 8, classification_loss: 1.7804386615753174, Val Loss: 70.65566444396973, loss : 2.0385866165161133\n",
            "epoch: 9, classification_loss: 1.7830064296722412, Val Loss: 70.69166111946106, loss : 2.020542621612549\n",
            "epoch: 10, classification_loss: 1.7885545492172241, Val Loss: 70.62536561489105, loss : 2.035909414291382\n",
            "epoch: 11, classification_loss: 1.7875617742538452, Val Loss: 70.7126247882843, loss : 2.0233137607574463\n",
            "epoch: 12, classification_loss: 1.7790650129318237, Val Loss: 70.74834966659546, loss : 2.028291940689087\n",
            "epoch: 13, classification_loss: 1.7912006378173828, Val Loss: 70.62465941905975, loss : 2.032240867614746\n",
            "epoch: 14, classification_loss: 1.791402816772461, Val Loss: 70.67700910568237, loss : 2.0312163829803467\n",
            "epoch: 15, classification_loss: 1.7984296083450317, Val Loss: 70.77390921115875, loss : 2.033064365386963\n",
            "epoch: 16, classification_loss: 1.7855087518692017, Val Loss: 70.72898530960083, loss : 2.0208637714385986\n",
            "epoch: 17, classification_loss: 1.7761183977127075, Val Loss: 70.68171048164368, loss : 2.0069499015808105\n",
            "epoch: 18, classification_loss: 1.7899093627929688, Val Loss: 70.75807511806488, loss : 2.0199902057647705\n",
            "epoch: 19, classification_loss: 1.7875337600708008, Val Loss: 70.7322826385498, loss : 2.0163466930389404\n",
            "Batch: 114, Test Acc: 0.5710136217948718\n",
            "Batch: 115:\n",
            "epoch: 0, classification_loss: 1.8191663026809692, Val Loss: 70.64198708534241, loss : 1.8191663026809692\n",
            "epoch: 1, classification_loss: 1.7923495769500732, Val Loss: 70.72995376586914, loss : 2.117720365524292\n",
            "epoch: 2, classification_loss: 1.779318928718567, Val Loss: 70.58603429794312, loss : 2.0530805587768555\n",
            "epoch: 3, classification_loss: 1.7844680547714233, Val Loss: 70.56018400192261, loss : 2.0642480850219727\n",
            "epoch: 4, classification_loss: 1.8167513608932495, Val Loss: 70.62338745594025, loss : 2.0610620975494385\n",
            "epoch: 5, classification_loss: 1.8175803422927856, Val Loss: 70.66147887706757, loss : 2.076242446899414\n",
            "epoch: 6, classification_loss: 1.7946232557296753, Val Loss: 70.59949886798859, loss : 2.0757129192352295\n",
            "epoch: 7, classification_loss: 1.793528437614441, Val Loss: 70.57908380031586, loss : 2.047694444656372\n",
            "epoch: 8, classification_loss: 1.804559588432312, Val Loss: 70.5948166847229, loss : 2.0691006183624268\n",
            "epoch: 9, classification_loss: 1.804796576499939, Val Loss: 70.6174293756485, loss : 2.052823781967163\n",
            "epoch: 10, classification_loss: 1.7986688613891602, Val Loss: 70.60485899448395, loss : 2.0547733306884766\n",
            "epoch: 11, classification_loss: 1.8037697076797485, Val Loss: 70.58401572704315, loss : 2.0504674911499023\n",
            "epoch: 12, classification_loss: 1.8141942024230957, Val Loss: 70.61199688911438, loss : 2.0593700408935547\n",
            "epoch: 13, classification_loss: 1.8023372888565063, Val Loss: 70.62212598323822, loss : 2.050644874572754\n",
            "epoch: 14, classification_loss: 1.8079909086227417, Val Loss: 70.63179850578308, loss : 2.0498619079589844\n",
            "epoch: 15, classification_loss: 1.8066887855529785, Val Loss: 70.62592649459839, loss : 2.0498311519622803\n",
            "epoch: 16, classification_loss: 1.796873688697815, Val Loss: 70.58994328975677, loss : 2.0375003814697266\n",
            "epoch: 17, classification_loss: 1.7973406314849854, Val Loss: 70.63452732563019, loss : 2.0350148677825928\n",
            "epoch: 18, classification_loss: 1.7993505001068115, Val Loss: 70.64230823516846, loss : 2.0350492000579834\n",
            "epoch: 19, classification_loss: 1.816710114479065, Val Loss: 70.61535322666168, loss : 2.0512003898620605\n",
            "Batch: 115, Test Acc: 0.5763221153846154\n",
            "Batch: 116:\n",
            "epoch: 0, classification_loss: 1.7909141778945923, Val Loss: 70.58210265636444, loss : 1.7909141778945923\n",
            "epoch: 1, classification_loss: 1.753615140914917, Val Loss: 70.59628939628601, loss : 2.0796315670013428\n",
            "epoch: 2, classification_loss: 1.7421903610229492, Val Loss: 70.60767662525177, loss : 1.997503399848938\n",
            "epoch: 3, classification_loss: 1.769659161567688, Val Loss: 70.59369099140167, loss : 2.0470690727233887\n",
            "epoch: 4, classification_loss: 1.7694400548934937, Val Loss: 70.55318748950958, loss : 2.010399341583252\n",
            "epoch: 5, classification_loss: 1.7672024965286255, Val Loss: 70.58369243144989, loss : 2.020340919494629\n",
            "epoch: 6, classification_loss: 1.7603083848953247, Val Loss: 70.61616778373718, loss : 2.020893096923828\n",
            "epoch: 7, classification_loss: 1.7516669034957886, Val Loss: 70.6608190536499, loss : 2.0031044483184814\n",
            "epoch: 8, classification_loss: 1.768582820892334, Val Loss: 70.65534794330597, loss : 2.018705129623413\n",
            "epoch: 9, classification_loss: 1.7598284482955933, Val Loss: 70.67143213748932, loss : 2.000272750854492\n",
            "epoch: 10, classification_loss: 1.7693922519683838, Val Loss: 70.68821239471436, loss : 2.0096821784973145\n",
            "epoch: 11, classification_loss: 1.7721630334854126, Val Loss: 70.66882622241974, loss : 2.0081920623779297\n",
            "epoch: 12, classification_loss: 1.7716751098632812, Val Loss: 70.60683190822601, loss : 2.008331775665283\n",
            "epoch: 13, classification_loss: 1.7705836296081543, Val Loss: 70.6965879201889, loss : 2.0079426765441895\n",
            "epoch: 14, classification_loss: 1.7644009590148926, Val Loss: 70.69665110111237, loss : 1.99994957447052\n",
            "epoch: 15, classification_loss: 1.7662352323532104, Val Loss: 70.62441766262054, loss : 2.0037901401519775\n",
            "epoch: 16, classification_loss: 1.7727922201156616, Val Loss: 70.64486455917358, loss : 1.9986765384674072\n",
            "epoch: 17, classification_loss: 1.7691924571990967, Val Loss: 70.69062554836273, loss : 2.0065197944641113\n",
            "epoch: 18, classification_loss: 1.766247034072876, Val Loss: 70.67315590381622, loss : 1.98785400390625\n",
            "epoch: 19, classification_loss: 1.7734966278076172, Val Loss: 70.69114995002747, loss : 2.0086352825164795\n",
            "Batch: 116, Test Acc: 0.5729166666666666\n",
            "Batch: 117:\n",
            "epoch: 0, classification_loss: 1.8719781637191772, Val Loss: 70.7196934223175, loss : 1.8719781637191772\n",
            "epoch: 1, classification_loss: 1.8126108646392822, Val Loss: 70.69049382209778, loss : 2.146131992340088\n",
            "epoch: 2, classification_loss: 1.7857335805892944, Val Loss: 70.62325584888458, loss : 2.0646860599517822\n",
            "epoch: 3, classification_loss: 1.8024436235427856, Val Loss: 70.55974626541138, loss : 2.0858511924743652\n",
            "epoch: 4, classification_loss: 1.8351836204528809, Val Loss: 70.50222194194794, loss : 2.0897347927093506\n",
            "epoch: 5, classification_loss: 1.8325878381729126, Val Loss: 70.54712438583374, loss : 1.8325878381729126\n",
            "epoch: 6, classification_loss: 1.8052641153335571, Val Loss: 70.609041929245, loss : 2.183662176132202\n",
            "epoch: 7, classification_loss: 1.7812581062316895, Val Loss: 70.5685818195343, loss : 2.0987486839294434\n",
            "epoch: 8, classification_loss: 1.7954806089401245, Val Loss: 70.53612518310547, loss : 2.0792369842529297\n",
            "epoch: 9, classification_loss: 1.8354696035385132, Val Loss: 70.59182453155518, loss : 2.1288373470306396\n",
            "epoch: 10, classification_loss: 1.810964822769165, Val Loss: 70.54185140132904, loss : 2.0585873126983643\n",
            "epoch: 11, classification_loss: 1.8072781562805176, Val Loss: 70.53287160396576, loss : 2.123188018798828\n",
            "epoch: 12, classification_loss: 1.7824270725250244, Val Loss: 70.57225430011749, loss : 2.0845448970794678\n",
            "epoch: 13, classification_loss: 1.8112813234329224, Val Loss: 70.56723141670227, loss : 2.0959441661834717\n",
            "epoch: 14, classification_loss: 1.823464035987854, Val Loss: 70.52746653556824, loss : 2.100625514984131\n",
            "epoch: 15, classification_loss: 1.809511661529541, Val Loss: 70.50374710559845, loss : 2.088813066482544\n",
            "epoch: 16, classification_loss: 1.8077645301818848, Val Loss: 70.51760911941528, loss : 2.0763516426086426\n",
            "epoch: 17, classification_loss: 1.8024756908416748, Val Loss: 70.55149745941162, loss : 2.067578077316284\n",
            "epoch: 18, classification_loss: 1.813489317893982, Val Loss: 70.55861914157867, loss : 2.070553779602051\n",
            "epoch: 19, classification_loss: 1.818453073501587, Val Loss: 70.54773283004761, loss : 2.072402238845825\n",
            "Batch: 117, Test Acc: 0.5727163461538461\n",
            "Batch: 118:\n",
            "epoch: 0, classification_loss: 1.8286727666854858, Val Loss: 70.56546771526337, loss : 1.8286727666854858\n",
            "epoch: 1, classification_loss: 1.7776286602020264, Val Loss: 70.57968378067017, loss : 2.1190614700317383\n",
            "epoch: 2, classification_loss: 1.7557419538497925, Val Loss: 70.57253658771515, loss : 2.0397722721099854\n",
            "epoch: 3, classification_loss: 1.784677505493164, Val Loss: 70.59994387626648, loss : 2.0801126956939697\n",
            "epoch: 4, classification_loss: 1.8067824840545654, Val Loss: 70.62031507492065, loss : 2.0713284015655518\n",
            "epoch: 5, classification_loss: 1.8098245859146118, Val Loss: 70.55885970592499, loss : 2.0667524337768555\n",
            "epoch: 6, classification_loss: 1.8004462718963623, Val Loss: 70.60909700393677, loss : 2.0670809745788574\n",
            "epoch: 7, classification_loss: 1.803479790687561, Val Loss: 70.56251215934753, loss : 2.068909168243408\n",
            "epoch: 8, classification_loss: 1.7922295331954956, Val Loss: 70.63900554180145, loss : 2.048463821411133\n",
            "epoch: 9, classification_loss: 1.7934112548828125, Val Loss: 70.57123935222626, loss : 2.0615592002868652\n",
            "epoch: 10, classification_loss: 1.7938392162322998, Val Loss: 70.63551759719849, loss : 2.050997257232666\n",
            "epoch: 11, classification_loss: 1.8001182079315186, Val Loss: 70.63340651988983, loss : 2.059751272201538\n",
            "epoch: 12, classification_loss: 1.796578049659729, Val Loss: 70.57653021812439, loss : 2.046004295349121\n",
            "epoch: 13, classification_loss: 1.7943018674850464, Val Loss: 70.57838916778564, loss : 2.0496015548706055\n",
            "epoch: 14, classification_loss: 1.8014822006225586, Val Loss: 70.61535704135895, loss : 2.0501880645751953\n",
            "epoch: 15, classification_loss: 1.7941255569458008, Val Loss: 70.63167178630829, loss : 2.040290355682373\n",
            "epoch: 16, classification_loss: 1.8027883768081665, Val Loss: 70.6321792602539, loss : 2.0498268604278564\n",
            "epoch: 17, classification_loss: 1.7909159660339355, Val Loss: 70.63903474807739, loss : 2.0348286628723145\n",
            "epoch: 18, classification_loss: 1.7929925918579102, Val Loss: 70.62961101531982, loss : 2.0307157039642334\n",
            "epoch: 19, classification_loss: 1.7994301319122314, Val Loss: 70.63227784633636, loss : 2.040696144104004\n",
            "Batch: 118, Test Acc: 0.5743189102564102\n",
            "Batch: 119:\n",
            "epoch: 0, classification_loss: 1.8101022243499756, Val Loss: 70.56722068786621, loss : 1.8101022243499756\n",
            "epoch: 1, classification_loss: 1.7688628435134888, Val Loss: 70.61241626739502, loss : 2.094420909881592\n",
            "epoch: 2, classification_loss: 1.7467235326766968, Val Loss: 70.52535831928253, loss : 1.9991912841796875\n",
            "epoch: 3, classification_loss: 1.7694411277770996, Val Loss: 70.54006624221802, loss : 2.0518391132354736\n",
            "epoch: 4, classification_loss: 1.7975003719329834, Val Loss: 70.6286792755127, loss : 2.0476624965667725\n",
            "epoch: 5, classification_loss: 1.78684401512146, Val Loss: 70.63693261146545, loss : 2.0467159748077393\n",
            "epoch: 6, classification_loss: 1.778350830078125, Val Loss: 70.5646653175354, loss : 2.054288387298584\n",
            "epoch: 7, classification_loss: 1.7823307514190674, Val Loss: 70.59620547294617, loss : 2.0397698879241943\n",
            "epoch: 8, classification_loss: 1.7829647064208984, Val Loss: 70.57526051998138, loss : 2.0370328426361084\n",
            "epoch: 9, classification_loss: 1.7916959524154663, Val Loss: 70.60240244865417, loss : 2.045865774154663\n",
            "epoch: 10, classification_loss: 1.7798621654510498, Val Loss: 70.62003707885742, loss : 2.0198352336883545\n",
            "epoch: 11, classification_loss: 1.786923885345459, Val Loss: 70.6402997970581, loss : 2.0362625122070312\n",
            "epoch: 12, classification_loss: 1.7718380689620972, Val Loss: 70.58631694316864, loss : 2.0096607208251953\n",
            "epoch: 13, classification_loss: 1.788655400276184, Val Loss: 70.60828828811646, loss : 2.0364160537719727\n",
            "epoch: 14, classification_loss: 1.7829738855361938, Val Loss: 70.65088677406311, loss : 2.013538360595703\n",
            "epoch: 15, classification_loss: 1.778538465499878, Val Loss: 70.61313915252686, loss : 2.024630308151245\n",
            "epoch: 16, classification_loss: 1.7882808446884155, Val Loss: 70.62525379657745, loss : 2.021615505218506\n",
            "epoch: 17, classification_loss: 1.789650321006775, Val Loss: 70.6552357673645, loss : 2.0253360271453857\n",
            "epoch: 18, classification_loss: 1.7929996252059937, Val Loss: 70.63799679279327, loss : 2.0213112831115723\n",
            "epoch: 19, classification_loss: 1.7827662229537964, Val Loss: 70.61636137962341, loss : 2.015117883682251\n",
            "Batch: 119, Test Acc: 0.5759214743589743\n",
            "Batch: 120:\n",
            "epoch: 0, classification_loss: 1.7958345413208008, Val Loss: 70.50894629955292, loss : 1.7958345413208008\n",
            "epoch: 1, classification_loss: 1.754443645477295, Val Loss: 70.49963319301605, loss : 2.07281231880188\n",
            "epoch: 2, classification_loss: 1.736672282218933, Val Loss: 70.55687010288239, loss : 1.9988093376159668\n",
            "epoch: 3, classification_loss: 1.7581812143325806, Val Loss: 70.57770097255707, loss : 2.0325231552124023\n",
            "epoch: 4, classification_loss: 1.7788227796554565, Val Loss: 70.55898487567902, loss : 2.0130412578582764\n",
            "epoch: 5, classification_loss: 1.7746539115905762, Val Loss: 70.53807520866394, loss : 2.0258054733276367\n",
            "epoch: 6, classification_loss: 1.762668251991272, Val Loss: 70.5662089586258, loss : 2.0262644290924072\n",
            "epoch: 7, classification_loss: 1.7660690546035767, Val Loss: 70.57979881763458, loss : 2.0171258449554443\n",
            "epoch: 8, classification_loss: 1.7734591960906982, Val Loss: 70.58623218536377, loss : 2.0273499488830566\n",
            "epoch: 9, classification_loss: 1.7670918703079224, Val Loss: 70.54910159111023, loss : 2.014962911605835\n",
            "epoch: 10, classification_loss: 1.7663335800170898, Val Loss: 70.57933974266052, loss : 2.0117063522338867\n",
            "epoch: 11, classification_loss: 1.771536946296692, Val Loss: 70.61609363555908, loss : 2.0211708545684814\n",
            "epoch: 12, classification_loss: 1.7784247398376465, Val Loss: 70.66336607933044, loss : 2.020221471786499\n",
            "epoch: 13, classification_loss: 1.7798106670379639, Val Loss: 70.62698340415955, loss : 2.0232083797454834\n",
            "epoch: 14, classification_loss: 1.7834086418151855, Val Loss: 70.61028361320496, loss : 2.0181167125701904\n",
            "epoch: 15, classification_loss: 1.779703140258789, Val Loss: 70.62303245067596, loss : 2.014347553253174\n",
            "epoch: 16, classification_loss: 1.7736830711364746, Val Loss: 70.63289022445679, loss : 2.0033233165740967\n",
            "epoch: 17, classification_loss: 1.7792469263076782, Val Loss: 70.6046394109726, loss : 2.012295722961426\n",
            "epoch: 18, classification_loss: 1.773104190826416, Val Loss: 70.64089822769165, loss : 2.0055501461029053\n",
            "epoch: 19, classification_loss: 1.7768645286560059, Val Loss: 70.65930080413818, loss : 2.0096659660339355\n",
            "Batch: 120, Test Acc: 0.5726161858974359\n",
            "Batch: 121:\n",
            "epoch: 0, classification_loss: 1.8124996423721313, Val Loss: 70.60402619838715, loss : 1.8124996423721313\n",
            "epoch: 1, classification_loss: 1.77134370803833, Val Loss: 70.58016240596771, loss : 2.0959484577178955\n",
            "epoch: 2, classification_loss: 1.7434165477752686, Val Loss: 70.57230150699615, loss : 2.0092265605926514\n",
            "epoch: 3, classification_loss: 1.778631329536438, Val Loss: 70.57064294815063, loss : 2.0453152656555176\n",
            "epoch: 4, classification_loss: 1.792894721031189, Val Loss: 70.53734803199768, loss : 2.0281288623809814\n",
            "epoch: 5, classification_loss: 1.7826720476150513, Val Loss: 70.5776777267456, loss : 2.0385634899139404\n",
            "epoch: 6, classification_loss: 1.7748557329177856, Val Loss: 70.58712220191956, loss : 2.033806085586548\n",
            "epoch: 7, classification_loss: 1.771671175956726, Val Loss: 70.58481800556183, loss : 2.02665376663208\n",
            "epoch: 8, classification_loss: 1.7880905866622925, Val Loss: 70.57879912853241, loss : 2.035482883453369\n",
            "epoch: 9, classification_loss: 1.7847427129745483, Val Loss: 70.56690621376038, loss : 2.0325841903686523\n",
            "epoch: 10, classification_loss: 1.7789356708526611, Val Loss: 70.59573495388031, loss : 2.024141311645508\n",
            "epoch: 11, classification_loss: 1.7954767942428589, Val Loss: 70.64750957489014, loss : 2.0293755531311035\n",
            "epoch: 12, classification_loss: 1.7978086471557617, Val Loss: 70.59372854232788, loss : 2.0321261882781982\n",
            "epoch: 13, classification_loss: 1.7802379131317139, Val Loss: 70.59076607227325, loss : 2.01764178276062\n",
            "epoch: 14, classification_loss: 1.7808945178985596, Val Loss: 70.62648916244507, loss : 2.023543357849121\n",
            "epoch: 15, classification_loss: 1.7831333875656128, Val Loss: 70.67961299419403, loss : 2.0230157375335693\n",
            "epoch: 16, classification_loss: 1.7791413068771362, Val Loss: 70.5905830860138, loss : 2.02115797996521\n",
            "epoch: 17, classification_loss: 1.7908114194869995, Val Loss: 70.59916710853577, loss : 2.033129930496216\n",
            "epoch: 18, classification_loss: 1.790908694267273, Val Loss: 70.62127196788788, loss : 2.0218276977539062\n",
            "epoch: 19, classification_loss: 1.7899211645126343, Val Loss: 70.62552678585052, loss : 2.028010368347168\n",
            "Batch: 121, Test Acc: 0.5757211538461539\n",
            "Batch: 122:\n",
            "epoch: 0, classification_loss: 1.8556281328201294, Val Loss: 70.52212941646576, loss : 1.8556281328201294\n",
            "epoch: 1, classification_loss: 1.816756010055542, Val Loss: 70.54912519454956, loss : 2.143278121948242\n",
            "epoch: 2, classification_loss: 1.7975391149520874, Val Loss: 70.54110157489777, loss : 2.0582895278930664\n",
            "epoch: 3, classification_loss: 1.8331444263458252, Val Loss: 70.5460330247879, loss : 2.101442575454712\n",
            "epoch: 4, classification_loss: 1.8447959423065186, Val Loss: 70.53776121139526, loss : 2.086071491241455\n",
            "epoch: 5, classification_loss: 1.8354454040527344, Val Loss: 70.57555294036865, loss : 2.085631847381592\n",
            "epoch: 6, classification_loss: 1.8231760263442993, Val Loss: 70.6149171590805, loss : 2.085103750228882\n",
            "epoch: 7, classification_loss: 1.8109091520309448, Val Loss: 70.57399702072144, loss : 2.0677595138549805\n",
            "epoch: 8, classification_loss: 1.829174518585205, Val Loss: 70.5426697731018, loss : 2.081902027130127\n",
            "epoch: 9, classification_loss: 1.8346346616744995, Val Loss: 70.59856295585632, loss : 2.082062005996704\n",
            "epoch: 10, classification_loss: 1.8337301015853882, Val Loss: 70.63539695739746, loss : 2.0786824226379395\n",
            "epoch: 11, classification_loss: 1.8402128219604492, Val Loss: 70.6076807975769, loss : 2.0834524631500244\n",
            "epoch: 12, classification_loss: 1.8262227773666382, Val Loss: 70.5900799036026, loss : 2.0674967765808105\n",
            "epoch: 13, classification_loss: 1.8305933475494385, Val Loss: 70.62540900707245, loss : 2.076751232147217\n",
            "epoch: 14, classification_loss: 1.8290367126464844, Val Loss: 70.60915517807007, loss : 2.0667295455932617\n",
            "epoch: 15, classification_loss: 1.8408725261688232, Val Loss: 70.6380113363266, loss : 2.0780327320098877\n",
            "epoch: 16, classification_loss: 1.8314400911331177, Val Loss: 70.6952930688858, loss : 2.0619280338287354\n",
            "epoch: 17, classification_loss: 1.835991382598877, Val Loss: 70.63758981227875, loss : 2.0706095695495605\n",
            "epoch: 18, classification_loss: 1.8365446329116821, Val Loss: 70.61533582210541, loss : 2.0603647232055664\n",
            "epoch: 19, classification_loss: 1.8341951370239258, Val Loss: 70.64994704723358, loss : 2.0641865730285645\n",
            "Batch: 122, Test Acc: 0.5708133012820513\n",
            "Batch: 123:\n",
            "epoch: 0, classification_loss: 1.7907246351242065, Val Loss: 70.55236899852753, loss : 1.7907246351242065\n",
            "epoch: 1, classification_loss: 1.7621711492538452, Val Loss: 70.54460608959198, loss : 2.0912041664123535\n",
            "epoch: 2, classification_loss: 1.7344082593917847, Val Loss: 70.60413920879364, loss : 1.9988608360290527\n",
            "epoch: 3, classification_loss: 1.7657569646835327, Val Loss: 70.55579161643982, loss : 2.039555311203003\n",
            "epoch: 4, classification_loss: 1.786211371421814, Val Loss: 70.56554305553436, loss : 2.024355888366699\n",
            "epoch: 5, classification_loss: 1.7815803289413452, Val Loss: 70.53639650344849, loss : 2.041581630706787\n",
            "epoch: 6, classification_loss: 1.7490203380584717, Val Loss: 70.59335231781006, loss : 2.024833917617798\n",
            "epoch: 7, classification_loss: 1.759528636932373, Val Loss: 70.59416496753693, loss : 2.022027015686035\n",
            "epoch: 8, classification_loss: 1.7716494798660278, Val Loss: 70.54058909416199, loss : 2.0299072265625\n",
            "epoch: 9, classification_loss: 1.7720403671264648, Val Loss: 70.55852317810059, loss : 2.027374505996704\n",
            "epoch: 10, classification_loss: 1.7778639793395996, Val Loss: 70.6104017496109, loss : 2.0218567848205566\n",
            "epoch: 11, classification_loss: 1.778036117553711, Val Loss: 70.60099577903748, loss : 2.022315263748169\n",
            "epoch: 12, classification_loss: 1.7719112634658813, Val Loss: 70.5550446510315, loss : 2.016860008239746\n",
            "epoch: 13, classification_loss: 1.770575761795044, Val Loss: 70.56619787216187, loss : 2.0264973640441895\n",
            "epoch: 14, classification_loss: 1.769640564918518, Val Loss: 70.56574892997742, loss : 2.021597385406494\n",
            "epoch: 15, classification_loss: 1.7734750509262085, Val Loss: 70.59808135032654, loss : 2.0186405181884766\n",
            "epoch: 16, classification_loss: 1.7747831344604492, Val Loss: 70.59825789928436, loss : 2.0132458209991455\n",
            "epoch: 17, classification_loss: 1.7811079025268555, Val Loss: 70.60541558265686, loss : 2.022122621536255\n",
            "epoch: 18, classification_loss: 1.7675577402114868, Val Loss: 70.57171583175659, loss : 2.0075206756591797\n",
            "epoch: 19, classification_loss: 1.7713981866836548, Val Loss: 70.60023152828217, loss : 2.01165509223938\n",
            "Batch: 123, Test Acc: 0.5754206730769231\n",
            "Batch: 124:\n",
            "epoch: 0, classification_loss: 1.840497612953186, Val Loss: 70.54038524627686, loss : 1.840497612953186\n",
            "epoch: 1, classification_loss: 1.7951894998550415, Val Loss: 70.56102383136749, loss : 2.122769832611084\n",
            "epoch: 2, classification_loss: 1.7819650173187256, Val Loss: 70.55566656589508, loss : 2.034148931503296\n",
            "epoch: 3, classification_loss: 1.7998234033584595, Val Loss: 70.56467461585999, loss : 2.07615327835083\n",
            "epoch: 4, classification_loss: 1.8246935606002808, Val Loss: 70.54062354564667, loss : 2.0591108798980713\n",
            "epoch: 5, classification_loss: 1.8175058364868164, Val Loss: 70.59139668941498, loss : 2.0794546604156494\n",
            "epoch: 6, classification_loss: 1.8008893728256226, Val Loss: 70.60270142555237, loss : 2.071469306945801\n",
            "epoch: 7, classification_loss: 1.800743818283081, Val Loss: 70.63315677642822, loss : 2.061460256576538\n",
            "epoch: 8, classification_loss: 1.8038153648376465, Val Loss: 70.5884622335434, loss : 2.059110641479492\n",
            "epoch: 9, classification_loss: 1.820000410079956, Val Loss: 70.55990433692932, loss : 2.0650761127471924\n",
            "epoch: 10, classification_loss: 1.8090695142745972, Val Loss: 70.60241305828094, loss : 2.0570709705352783\n",
            "epoch: 11, classification_loss: 1.8043849468231201, Val Loss: 70.6104725599289, loss : 2.0457170009613037\n",
            "epoch: 12, classification_loss: 1.807311773300171, Val Loss: 70.59210658073425, loss : 2.0481245517730713\n",
            "epoch: 13, classification_loss: 1.8128639459609985, Val Loss: 70.59286201000214, loss : 2.054326295852661\n",
            "epoch: 14, classification_loss: 1.8087587356567383, Val Loss: 70.60257732868195, loss : 2.04463791847229\n",
            "epoch: 15, classification_loss: 1.8109081983566284, Val Loss: 70.60471308231354, loss : 2.0545706748962402\n",
            "epoch: 16, classification_loss: 1.818467617034912, Val Loss: 70.61694371700287, loss : 2.055332660675049\n",
            "epoch: 17, classification_loss: 1.8082813024520874, Val Loss: 70.63851726055145, loss : 2.043382406234741\n",
            "epoch: 18, classification_loss: 1.8114365339279175, Val Loss: 70.61258029937744, loss : 2.041783332824707\n",
            "epoch: 19, classification_loss: 1.8132872581481934, Val Loss: 70.62996220588684, loss : 2.0448827743530273\n",
            "Batch: 124, Test Acc: 0.5753205128205128\n",
            "Batch: 125:\n",
            "epoch: 0, classification_loss: 1.8278276920318604, Val Loss: 70.62060153484344, loss : 1.8278276920318604\n",
            "epoch: 1, classification_loss: 1.79500150680542, Val Loss: 70.62040269374847, loss : 2.1246044635772705\n",
            "epoch: 2, classification_loss: 1.7760740518569946, Val Loss: 70.57696056365967, loss : 2.0213937759399414\n",
            "epoch: 3, classification_loss: 1.801379680633545, Val Loss: 70.594881772995, loss : 2.066683292388916\n",
            "epoch: 4, classification_loss: 1.8058100938796997, Val Loss: 70.64940524101257, loss : 2.0368175506591797\n",
            "epoch: 5, classification_loss: 1.8004225492477417, Val Loss: 70.64342105388641, loss : 2.0564703941345215\n",
            "epoch: 6, classification_loss: 1.8009567260742188, Val Loss: 70.63409662246704, loss : 2.070612907409668\n",
            "epoch: 7, classification_loss: 1.8005176782608032, Val Loss: 70.63999688625336, loss : 2.049231767654419\n",
            "epoch: 8, classification_loss: 1.8184223175048828, Val Loss: 70.63117158412933, loss : 2.057842969894409\n",
            "epoch: 9, classification_loss: 1.81003999710083, Val Loss: 70.76051259040833, loss : 2.043445348739624\n",
            "epoch: 10, classification_loss: 1.8103060722351074, Val Loss: 70.67677187919617, loss : 2.0432841777801514\n",
            "epoch: 11, classification_loss: 1.8006590604782104, Val Loss: 70.67278921604156, loss : 2.032404899597168\n",
            "epoch: 12, classification_loss: 1.8070725202560425, Val Loss: 70.74827456474304, loss : 2.03365159034729\n",
            "epoch: 13, classification_loss: 1.8030335903167725, Val Loss: 70.74606919288635, loss : 2.028394937515259\n",
            "epoch: 14, classification_loss: 1.810652256011963, Val Loss: 70.67683529853821, loss : 2.0313754081726074\n",
            "epoch: 15, classification_loss: 1.810515284538269, Val Loss: 70.68198132514954, loss : 2.0379586219787598\n",
            "epoch: 16, classification_loss: 1.8065185546875, Val Loss: 70.73611116409302, loss : 2.025620460510254\n",
            "epoch: 17, classification_loss: 1.81423020362854, Val Loss: 70.76783680915833, loss : 2.03430438041687\n",
            "epoch: 18, classification_loss: 1.8109979629516602, Val Loss: 70.72489702701569, loss : 2.023451805114746\n",
            "epoch: 19, classification_loss: 1.8010059595108032, Val Loss: 70.7430659532547, loss : 2.030501365661621\n",
            "Batch: 125, Test Acc: 0.5713141025641025\n",
            "Batch: 126:\n",
            "epoch: 0, classification_loss: 1.774978518486023, Val Loss: 70.5251145362854, loss : 1.774978518486023\n",
            "epoch: 1, classification_loss: 1.7400963306427002, Val Loss: 70.52433776855469, loss : 2.0570709705352783\n",
            "epoch: 2, classification_loss: 1.7244733572006226, Val Loss: 70.59467399120331, loss : 1.9530895948410034\n",
            "epoch: 3, classification_loss: 1.7639778852462769, Val Loss: 70.61916506290436, loss : 2.0162739753723145\n",
            "epoch: 4, classification_loss: 1.779586672782898, Val Loss: 70.58195638656616, loss : 1.988279104232788\n",
            "epoch: 5, classification_loss: 1.7601416110992432, Val Loss: 70.5833809375763, loss : 2.0284957885742188\n",
            "epoch: 6, classification_loss: 1.740065574645996, Val Loss: 70.60958743095398, loss : 2.0049781799316406\n",
            "epoch: 7, classification_loss: 1.7467268705368042, Val Loss: 70.69401371479034, loss : 1.9883091449737549\n",
            "epoch: 8, classification_loss: 1.7536804676055908, Val Loss: 70.66494965553284, loss : 2.0065062046051025\n",
            "epoch: 9, classification_loss: 1.7592498064041138, Val Loss: 70.68293166160583, loss : 1.9881571531295776\n",
            "epoch: 10, classification_loss: 1.7645546197891235, Val Loss: 70.70008993148804, loss : 2.0045628547668457\n",
            "epoch: 11, classification_loss: 1.749167799949646, Val Loss: 70.6842588186264, loss : 1.990169882774353\n",
            "epoch: 12, classification_loss: 1.7469373941421509, Val Loss: 70.70737600326538, loss : 1.9772872924804688\n",
            "epoch: 13, classification_loss: 1.748522162437439, Val Loss: 70.76822888851166, loss : 1.9885625839233398\n",
            "epoch: 14, classification_loss: 1.7547415494918823, Val Loss: 70.80970358848572, loss : 1.979686975479126\n",
            "epoch: 15, classification_loss: 1.7453426122665405, Val Loss: 70.73240411281586, loss : 1.981223225593567\n",
            "epoch: 16, classification_loss: 1.7587804794311523, Val Loss: 70.75996387004852, loss : 1.9808125495910645\n",
            "epoch: 17, classification_loss: 1.7612577676773071, Val Loss: 70.7828813791275, loss : 1.9929449558258057\n",
            "epoch: 18, classification_loss: 1.7434751987457275, Val Loss: 70.77760076522827, loss : 1.9654488563537598\n",
            "epoch: 19, classification_loss: 1.7513705492019653, Val Loss: 70.79444670677185, loss : 1.9855824708938599\n",
            "Batch: 126, Test Acc: 0.5719150641025641\n",
            "Batch: 127:\n",
            "epoch: 0, classification_loss: 1.9008246660232544, Val Loss: 70.58525276184082, loss : 1.9008246660232544\n",
            "epoch: 1, classification_loss: 1.8460637331008911, Val Loss: 70.65141773223877, loss : 2.183185577392578\n",
            "epoch: 2, classification_loss: 1.81954824924469, Val Loss: 70.54255390167236, loss : 2.0843377113342285\n",
            "epoch: 3, classification_loss: 1.8585283756256104, Val Loss: 70.55522084236145, loss : 2.1328866481781006\n",
            "epoch: 4, classification_loss: 1.879102110862732, Val Loss: 70.5680136680603, loss : 2.1201388835906982\n",
            "epoch: 5, classification_loss: 1.8650325536727905, Val Loss: 70.54572975635529, loss : 2.1280579566955566\n",
            "epoch: 6, classification_loss: 1.8503239154815674, Val Loss: 70.58924984931946, loss : 2.1099443435668945\n",
            "epoch: 7, classification_loss: 1.8620965480804443, Val Loss: 70.60833585262299, loss : 2.132436990737915\n",
            "epoch: 8, classification_loss: 1.854246973991394, Val Loss: 70.65189814567566, loss : 2.113119125366211\n",
            "epoch: 9, classification_loss: 1.848025918006897, Val Loss: 70.56746065616608, loss : 2.1211466789245605\n",
            "epoch: 10, classification_loss: 1.8695025444030762, Val Loss: 70.52840101718903, loss : 2.121967077255249\n",
            "epoch: 11, classification_loss: 1.857423186302185, Val Loss: 70.56978344917297, loss : 2.108989953994751\n",
            "epoch: 12, classification_loss: 1.8696391582489014, Val Loss: 70.6380580663681, loss : 2.1145055294036865\n",
            "epoch: 13, classification_loss: 1.8589664697647095, Val Loss: 70.63720071315765, loss : 2.1062817573547363\n",
            "epoch: 14, classification_loss: 1.8765320777893066, Val Loss: 70.58891916275024, loss : 2.118936061859131\n",
            "epoch: 15, classification_loss: 1.8723305463790894, Val Loss: 70.60054039955139, loss : 2.1191534996032715\n",
            "epoch: 16, classification_loss: 1.8543351888656616, Val Loss: 70.59438502788544, loss : 2.1016573905944824\n",
            "epoch: 17, classification_loss: 1.8624848127365112, Val Loss: 70.61640477180481, loss : 2.109978675842285\n",
            "epoch: 18, classification_loss: 1.8608540296554565, Val Loss: 70.64872026443481, loss : 2.0931472778320312\n",
            "epoch: 19, classification_loss: 1.8684308528900146, Val Loss: 70.59938168525696, loss : 2.1095590591430664\n",
            "Batch: 127, Test Acc: 0.5755208333333334\n",
            "Batch: 128:\n",
            "epoch: 0, classification_loss: 1.8289214372634888, Val Loss: 70.48193311691284, loss : 1.8289214372634888\n",
            "epoch: 1, classification_loss: 1.8013615608215332, Val Loss: 70.5598578453064, loss : 1.8013615608215332\n",
            "epoch: 2, classification_loss: 1.7458086013793945, Val Loss: 70.58517491817474, loss : 2.1053786277770996\n",
            "epoch: 3, classification_loss: 1.7278773784637451, Val Loss: 70.55313420295715, loss : 2.0505380630493164\n",
            "epoch: 4, classification_loss: 1.7668851613998413, Val Loss: 70.55245780944824, loss : 2.0462698936462402\n",
            "epoch: 5, classification_loss: 1.801371455192566, Val Loss: 70.54377102851868, loss : 2.0508246421813965\n",
            "epoch: 6, classification_loss: 1.781415581703186, Val Loss: 70.52292609214783, loss : 2.043226957321167\n",
            "epoch: 7, classification_loss: 1.7658860683441162, Val Loss: 70.55684638023376, loss : 2.0439958572387695\n",
            "epoch: 8, classification_loss: 1.767339825630188, Val Loss: 70.5236645936966, loss : 2.0360701084136963\n",
            "epoch: 9, classification_loss: 1.7650446891784668, Val Loss: 70.55464279651642, loss : 2.026973009109497\n",
            "epoch: 10, classification_loss: 1.7808279991149902, Val Loss: 70.54700887203217, loss : 2.036959171295166\n",
            "epoch: 11, classification_loss: 1.7718325853347778, Val Loss: 70.5642181634903, loss : 2.024198055267334\n",
            "epoch: 12, classification_loss: 1.7680540084838867, Val Loss: 70.54528164863586, loss : 2.017066240310669\n",
            "epoch: 13, classification_loss: 1.762468934059143, Val Loss: 70.52763521671295, loss : 2.0107383728027344\n",
            "epoch: 14, classification_loss: 1.7731043100357056, Val Loss: 70.5350376367569, loss : 2.0172133445739746\n",
            "epoch: 15, classification_loss: 1.766090750694275, Val Loss: 70.5700466632843, loss : 2.007138252258301\n",
            "epoch: 16, classification_loss: 1.7688796520233154, Val Loss: 70.5700900554657, loss : 2.0148208141326904\n",
            "epoch: 17, classification_loss: 1.7696020603179932, Val Loss: 70.55538952350616, loss : 2.0244410037994385\n",
            "epoch: 18, classification_loss: 1.7817614078521729, Val Loss: 70.55636584758759, loss : 2.0313544273376465\n",
            "epoch: 19, classification_loss: 1.7775155305862427, Val Loss: 70.54846119880676, loss : 2.009514570236206\n",
            "Batch: 128, Test Acc: 0.5738181089743589\n",
            "Batch: 129:\n",
            "epoch: 0, classification_loss: 1.822592854499817, Val Loss: 70.52642238140106, loss : 1.822592854499817\n",
            "epoch: 1, classification_loss: 1.7840529680252075, Val Loss: 70.55208921432495, loss : 2.1329967975616455\n",
            "epoch: 2, classification_loss: 1.7618571519851685, Val Loss: 70.57106184959412, loss : 2.0467686653137207\n",
            "epoch: 3, classification_loss: 1.785431146621704, Val Loss: 70.53704261779785, loss : 2.0784244537353516\n",
            "epoch: 4, classification_loss: 1.8167766332626343, Val Loss: 70.5405375957489, loss : 2.0797617435455322\n",
            "epoch: 5, classification_loss: 1.8184759616851807, Val Loss: 70.52417290210724, loss : 2.0718812942504883\n",
            "epoch: 6, classification_loss: 1.7965151071548462, Val Loss: 70.54104721546173, loss : 2.0728821754455566\n",
            "epoch: 7, classification_loss: 1.7879444360733032, Val Loss: 70.57421946525574, loss : 2.0595366954803467\n",
            "epoch: 8, classification_loss: 1.7816030979156494, Val Loss: 70.56954824924469, loss : 2.0528812408447266\n",
            "epoch: 9, classification_loss: 1.8096052408218384, Val Loss: 70.60895895957947, loss : 2.0618181228637695\n",
            "epoch: 10, classification_loss: 1.8114597797393799, Val Loss: 70.58334064483643, loss : 2.065948724746704\n",
            "epoch: 11, classification_loss: 1.79927396774292, Val Loss: 70.58427059650421, loss : 2.04658842086792\n",
            "epoch: 12, classification_loss: 1.7960792779922485, Val Loss: 70.60436081886292, loss : 2.0520496368408203\n",
            "epoch: 13, classification_loss: 1.793246865272522, Val Loss: 70.63140726089478, loss : 2.042712450027466\n",
            "epoch: 14, classification_loss: 1.802832007408142, Val Loss: 70.60094141960144, loss : 2.044485569000244\n",
            "epoch: 15, classification_loss: 1.7909178733825684, Val Loss: 70.632204413414, loss : 2.0314743518829346\n",
            "epoch: 16, classification_loss: 1.8132227659225464, Val Loss: 70.63376653194427, loss : 2.04508900642395\n",
            "epoch: 17, classification_loss: 1.8069939613342285, Val Loss: 70.65002739429474, loss : 2.0386440753936768\n",
            "epoch: 18, classification_loss: 1.7976104021072388, Val Loss: 70.62121999263763, loss : 2.0286591053009033\n",
            "epoch: 19, classification_loss: 1.8035643100738525, Val Loss: 70.66686367988586, loss : 2.035475730895996\n",
            "Batch: 129, Test Acc: 0.5731169871794872\n",
            "Batch: 130:\n",
            "epoch: 0, classification_loss: 1.8084557056427002, Val Loss: 70.48627233505249, loss : 1.8084557056427002\n",
            "epoch: 1, classification_loss: 1.7653510570526123, Val Loss: 70.52213954925537, loss : 2.0840229988098145\n",
            "epoch: 2, classification_loss: 1.7609786987304688, Val Loss: 70.49869441986084, loss : 2.0031399726867676\n",
            "epoch: 3, classification_loss: 1.7863610982894897, Val Loss: 70.54105150699615, loss : 2.0460519790649414\n",
            "epoch: 4, classification_loss: 1.8051925897598267, Val Loss: 70.54051661491394, loss : 2.0245654582977295\n",
            "epoch: 5, classification_loss: 1.7868725061416626, Val Loss: 70.51063883304596, loss : 2.0535061359405518\n",
            "epoch: 6, classification_loss: 1.7829129695892334, Val Loss: 70.55684959888458, loss : 2.0639374256134033\n",
            "epoch: 7, classification_loss: 1.7750650644302368, Val Loss: 70.5881769657135, loss : 2.03230357170105\n",
            "epoch: 8, classification_loss: 1.7924766540527344, Val Loss: 70.590811252594, loss : 2.046867847442627\n",
            "epoch: 9, classification_loss: 1.7980314493179321, Val Loss: 70.53587746620178, loss : 2.0299601554870605\n",
            "epoch: 10, classification_loss: 1.7944047451019287, Val Loss: 70.57067000865936, loss : 2.043227195739746\n",
            "epoch: 11, classification_loss: 1.7777674198150635, Val Loss: 70.60502207279205, loss : 2.018301010131836\n",
            "epoch: 12, classification_loss: 1.7888948917388916, Val Loss: 70.60961043834686, loss : 2.0234322547912598\n",
            "epoch: 13, classification_loss: 1.7929577827453613, Val Loss: 70.6096556186676, loss : 2.03098201751709\n",
            "epoch: 14, classification_loss: 1.7920061349868774, Val Loss: 70.64355039596558, loss : 2.0191893577575684\n",
            "epoch: 15, classification_loss: 1.7962361574172974, Val Loss: 70.59673404693604, loss : 2.02655291557312\n",
            "epoch: 16, classification_loss: 1.7804845571517944, Val Loss: 70.58055579662323, loss : 2.0103304386138916\n",
            "epoch: 17, classification_loss: 1.784558653831482, Val Loss: 70.61886215209961, loss : 2.014000177383423\n",
            "epoch: 18, classification_loss: 1.7876733541488647, Val Loss: 70.63881480693817, loss : 2.015033006668091\n",
            "epoch: 19, classification_loss: 1.7797491550445557, Val Loss: 70.67568528652191, loss : 2.0142970085144043\n",
            "Batch: 130, Test Acc: 0.5748197115384616\n",
            "Batch: 131:\n",
            "epoch: 0, classification_loss: 1.878632664680481, Val Loss: 70.48543906211853, loss : 1.878632664680481\n",
            "epoch: 1, classification_loss: 1.8225270509719849, Val Loss: 70.49427151679993, loss : 2.1509246826171875\n",
            "epoch: 2, classification_loss: 1.8022079467773438, Val Loss: 70.52924180030823, loss : 2.066923141479492\n",
            "epoch: 3, classification_loss: 1.8219237327575684, Val Loss: 70.54030537605286, loss : 2.0981454849243164\n",
            "epoch: 4, classification_loss: 1.8533213138580322, Val Loss: 70.51581251621246, loss : 2.094581365585327\n",
            "epoch: 5, classification_loss: 1.8371227979660034, Val Loss: 70.54584181308746, loss : 2.0875885486602783\n",
            "epoch: 6, classification_loss: 1.8377386331558228, Val Loss: 70.56585443019867, loss : 2.0972254276275635\n",
            "epoch: 7, classification_loss: 1.8403619527816772, Val Loss: 70.62822711467743, loss : 2.090754985809326\n",
            "epoch: 8, classification_loss: 1.8409769535064697, Val Loss: 70.59764158725739, loss : 2.0984678268432617\n",
            "epoch: 9, classification_loss: 1.8328801393508911, Val Loss: 70.57199943065643, loss : 2.090604782104492\n",
            "epoch: 10, classification_loss: 1.8335052728652954, Val Loss: 70.6245025396347, loss : 2.092442035675049\n",
            "epoch: 11, classification_loss: 1.8428446054458618, Val Loss: 70.58838999271393, loss : 2.0977659225463867\n",
            "epoch: 12, classification_loss: 1.8408823013305664, Val Loss: 70.63164782524109, loss : 2.092197895050049\n",
            "epoch: 13, classification_loss: 1.836888313293457, Val Loss: 70.62864708900452, loss : 2.089569568634033\n",
            "epoch: 14, classification_loss: 1.846927523612976, Val Loss: 70.61533105373383, loss : 2.0892534255981445\n",
            "epoch: 15, classification_loss: 1.8307934999465942, Val Loss: 70.64683556556702, loss : 2.081103563308716\n",
            "epoch: 16, classification_loss: 1.8446162939071655, Val Loss: 70.60612380504608, loss : 2.086988687515259\n",
            "epoch: 17, classification_loss: 1.8479382991790771, Val Loss: 70.63219225406647, loss : 2.0942747592926025\n",
            "epoch: 18, classification_loss: 1.846789002418518, Val Loss: 70.60013580322266, loss : 2.0754573345184326\n",
            "epoch: 19, classification_loss: 1.8403209447860718, Val Loss: 70.6818459033966, loss : 2.0870819091796875\n",
            "Batch: 131, Test Acc: 0.5709134615384616\n",
            "Batch: 132:\n",
            "epoch: 0, classification_loss: 1.8246132135391235, Val Loss: 70.57713770866394, loss : 1.8246132135391235\n",
            "epoch: 1, classification_loss: 1.7663748264312744, Val Loss: 70.54347825050354, loss : 2.0884876251220703\n",
            "epoch: 2, classification_loss: 1.7568373680114746, Val Loss: 70.56089115142822, loss : 2.02003812789917\n",
            "epoch: 3, classification_loss: 1.7746014595031738, Val Loss: 70.53806900978088, loss : 2.0460569858551025\n",
            "epoch: 4, classification_loss: 1.8099322319030762, Val Loss: 70.5279346704483, loss : 2.041416645050049\n",
            "epoch: 5, classification_loss: 1.7897834777832031, Val Loss: 70.57840991020203, loss : 2.047032356262207\n",
            "epoch: 6, classification_loss: 1.7865351438522339, Val Loss: 70.56204867362976, loss : 2.05198335647583\n",
            "epoch: 7, classification_loss: 1.7899470329284668, Val Loss: 70.56547093391418, loss : 2.045414924621582\n",
            "epoch: 8, classification_loss: 1.7887800931930542, Val Loss: 70.56979048252106, loss : 2.045915126800537\n",
            "epoch: 9, classification_loss: 1.7926276922225952, Val Loss: 70.55732548236847, loss : 2.035491704940796\n",
            "epoch: 10, classification_loss: 1.7968918085098267, Val Loss: 70.59410393238068, loss : 2.0383405685424805\n",
            "epoch: 11, classification_loss: 1.7856543064117432, Val Loss: 70.59259867668152, loss : 2.0267207622528076\n",
            "epoch: 12, classification_loss: 1.7942206859588623, Val Loss: 70.58702230453491, loss : 2.0301074981689453\n",
            "epoch: 13, classification_loss: 1.792169213294983, Val Loss: 70.57985854148865, loss : 2.0305163860321045\n",
            "epoch: 14, classification_loss: 1.7979040145874023, Val Loss: 70.61475658416748, loss : 2.0391674041748047\n",
            "epoch: 15, classification_loss: 1.8018877506256104, Val Loss: 70.58765089511871, loss : 2.041532039642334\n",
            "epoch: 16, classification_loss: 1.7846237421035767, Val Loss: 70.6201924085617, loss : 2.017486572265625\n",
            "epoch: 17, classification_loss: 1.800244927406311, Val Loss: 70.6522319316864, loss : 2.0291948318481445\n",
            "epoch: 18, classification_loss: 1.8007025718688965, Val Loss: 70.60591888427734, loss : 2.0223641395568848\n",
            "epoch: 19, classification_loss: 1.8005608320236206, Val Loss: 70.61488449573517, loss : 2.034605026245117\n",
            "Batch: 132, Test Acc: 0.5746193910256411\n",
            "Batch: 133:\n",
            "epoch: 0, classification_loss: 1.8092349767684937, Val Loss: 70.48384654521942, loss : 1.8092349767684937\n",
            "epoch: 1, classification_loss: 1.7670024633407593, Val Loss: 70.58404541015625, loss : 2.0922627449035645\n",
            "epoch: 2, classification_loss: 1.7423595190048218, Val Loss: 70.58084332942963, loss : 2.000515937805176\n",
            "epoch: 3, classification_loss: 1.7573667764663696, Val Loss: 70.547816157341, loss : 2.0289487838745117\n",
            "epoch: 4, classification_loss: 1.8018405437469482, Val Loss: 70.52524185180664, loss : 2.039384603500366\n",
            "epoch: 5, classification_loss: 1.7928276062011719, Val Loss: 70.50483441352844, loss : 2.0524330139160156\n",
            "epoch: 6, classification_loss: 1.7604707479476929, Val Loss: 70.55433249473572, loss : 2.0298662185668945\n",
            "epoch: 7, classification_loss: 1.7704074382781982, Val Loss: 70.58232688903809, loss : 2.035443067550659\n",
            "epoch: 8, classification_loss: 1.7779935598373413, Val Loss: 70.56551468372345, loss : 2.041144847869873\n",
            "epoch: 9, classification_loss: 1.7816637754440308, Val Loss: 70.58658540248871, loss : 2.0342798233032227\n",
            "epoch: 10, classification_loss: 1.778618574142456, Val Loss: 70.52701258659363, loss : 2.034585952758789\n",
            "epoch: 11, classification_loss: 1.777421236038208, Val Loss: 70.57243394851685, loss : 2.027920722961426\n",
            "epoch: 12, classification_loss: 1.787432074546814, Val Loss: 70.57874417304993, loss : 2.0406134128570557\n",
            "epoch: 13, classification_loss: 1.7765494585037231, Val Loss: 70.65916574001312, loss : 2.0206093788146973\n",
            "epoch: 14, classification_loss: 1.7734553813934326, Val Loss: 70.55416893959045, loss : 2.0288102626800537\n",
            "epoch: 15, classification_loss: 1.7711210250854492, Val Loss: 70.53796207904816, loss : 2.022742509841919\n",
            "epoch: 16, classification_loss: 1.7727739810943604, Val Loss: 70.64754343032837, loss : 2.025331735610962\n",
            "epoch: 17, classification_loss: 1.782851219177246, Val Loss: 70.59265744686127, loss : 2.026965856552124\n",
            "epoch: 18, classification_loss: 1.784307837486267, Val Loss: 70.5618155002594, loss : 2.025346517562866\n",
            "epoch: 19, classification_loss: 1.7776356935501099, Val Loss: 70.59008800983429, loss : 2.024299144744873\n",
            "Batch: 133, Test Acc: 0.5735176282051282\n",
            "Batch: 134:\n",
            "epoch: 0, classification_loss: 1.7975859642028809, Val Loss: 70.56435739994049, loss : 1.7975859642028809\n",
            "epoch: 1, classification_loss: 1.7712119817733765, Val Loss: 70.47985088825226, loss : 2.0995635986328125\n",
            "epoch: 2, classification_loss: 1.7617568969726562, Val Loss: 70.54673326015472, loss : 2.015450954437256\n",
            "epoch: 3, classification_loss: 1.7660281658172607, Val Loss: 70.53512668609619, loss : 2.0489964485168457\n",
            "epoch: 4, classification_loss: 1.7997325658798218, Val Loss: 70.55582630634308, loss : 2.033538579940796\n",
            "epoch: 5, classification_loss: 1.7944332361221313, Val Loss: 70.516770362854, loss : 2.055835485458374\n",
            "epoch: 6, classification_loss: 1.771634578704834, Val Loss: 70.53716266155243, loss : 2.0430943965911865\n",
            "epoch: 7, classification_loss: 1.7814573049545288, Val Loss: 70.56667399406433, loss : 2.0434155464172363\n",
            "epoch: 8, classification_loss: 1.7871519327163696, Val Loss: 70.57090139389038, loss : 2.0480265617370605\n",
            "epoch: 9, classification_loss: 1.7826814651489258, Val Loss: 70.55070066452026, loss : 2.031663417816162\n",
            "epoch: 10, classification_loss: 1.7792271375656128, Val Loss: 70.56617975234985, loss : 2.023209810256958\n",
            "epoch: 11, classification_loss: 1.77777099609375, Val Loss: 70.5871970653534, loss : 2.021090030670166\n",
            "epoch: 12, classification_loss: 1.7849435806274414, Val Loss: 70.57498383522034, loss : 2.0230977535247803\n",
            "epoch: 13, classification_loss: 1.7778611183166504, Val Loss: 70.57013046741486, loss : 2.011018753051758\n",
            "epoch: 14, classification_loss: 1.7748676538467407, Val Loss: 70.56201672554016, loss : 2.004493236541748\n",
            "epoch: 15, classification_loss: 1.7724108695983887, Val Loss: 70.58556342124939, loss : 2.0088083744049072\n",
            "epoch: 16, classification_loss: 1.7779330015182495, Val Loss: 70.57822287082672, loss : 2.008075475692749\n",
            "epoch: 17, classification_loss: 1.777574896812439, Val Loss: 70.59303331375122, loss : 2.0079731941223145\n",
            "epoch: 18, classification_loss: 1.791420578956604, Val Loss: 70.61389780044556, loss : 2.012618064880371\n",
            "epoch: 19, classification_loss: 1.7800155878067017, Val Loss: 70.5823677778244, loss : 2.009317636489868\n",
            "Batch: 134, Test Acc: 0.5729166666666666\n",
            "Batch: 135:\n",
            "epoch: 0, classification_loss: 1.7944508790969849, Val Loss: 70.46428167819977, loss : 1.7944508790969849\n",
            "epoch: 1, classification_loss: 1.7652010917663574, Val Loss: 70.57772254943848, loss : 1.7652010917663574\n",
            "epoch: 2, classification_loss: 1.712708830833435, Val Loss: 70.57170724868774, loss : 2.0706911087036133\n",
            "epoch: 3, classification_loss: 1.7087949514389038, Val Loss: 70.51790153980255, loss : 2.021451473236084\n",
            "epoch: 4, classification_loss: 1.729508638381958, Val Loss: 70.55518555641174, loss : 2.0008132457733154\n",
            "epoch: 5, classification_loss: 1.774533748626709, Val Loss: 70.61946487426758, loss : 2.0133144855499268\n",
            "epoch: 6, classification_loss: 1.7663649320602417, Val Loss: 70.56189715862274, loss : 2.008553981781006\n",
            "epoch: 7, classification_loss: 1.7386292219161987, Val Loss: 70.56039094924927, loss : 2.007111072540283\n",
            "epoch: 8, classification_loss: 1.714384913444519, Val Loss: 70.56526792049408, loss : 1.9959962368011475\n",
            "epoch: 9, classification_loss: 1.7520029544830322, Val Loss: 70.60805010795593, loss : 2.0126686096191406\n",
            "epoch: 10, classification_loss: 1.7405692338943481, Val Loss: 70.6160979270935, loss : 1.9858084917068481\n",
            "epoch: 11, classification_loss: 1.7544296979904175, Val Loss: 70.66033065319061, loss : 1.9867699146270752\n",
            "epoch: 12, classification_loss: 1.744736909866333, Val Loss: 70.6439299583435, loss : 1.9857800006866455\n",
            "epoch: 13, classification_loss: 1.7325105667114258, Val Loss: 70.66419541835785, loss : 1.972835898399353\n",
            "epoch: 14, classification_loss: 1.7398358583450317, Val Loss: 70.65133571624756, loss : 1.9820222854614258\n",
            "epoch: 15, classification_loss: 1.752463459968567, Val Loss: 70.6508504152298, loss : 1.9752047061920166\n",
            "epoch: 16, classification_loss: 1.755506157875061, Val Loss: 70.66847229003906, loss : 1.983720064163208\n",
            "epoch: 17, classification_loss: 1.744116187095642, Val Loss: 70.7088223695755, loss : 1.9760031700134277\n",
            "epoch: 18, classification_loss: 1.7404818534851074, Val Loss: 70.68241548538208, loss : 1.9740309715270996\n",
            "epoch: 19, classification_loss: 1.7535400390625, Val Loss: 70.71437788009644, loss : 1.9769666194915771\n",
            "Batch: 135, Test Acc: 0.5743189102564102\n",
            "Batch: 136:\n",
            "epoch: 0, classification_loss: 1.8713531494140625, Val Loss: 70.49584341049194, loss : 1.8713531494140625\n",
            "epoch: 1, classification_loss: 1.8289321660995483, Val Loss: 70.49610722064972, loss : 2.174428939819336\n",
            "epoch: 2, classification_loss: 1.7905985116958618, Val Loss: 70.5539790391922, loss : 2.0636842250823975\n",
            "epoch: 3, classification_loss: 1.8096593618392944, Val Loss: 70.54027235507965, loss : 2.0929148197174072\n",
            "epoch: 4, classification_loss: 1.836919903755188, Val Loss: 70.55779349803925, loss : 2.091477632522583\n",
            "epoch: 5, classification_loss: 1.8434312343597412, Val Loss: 70.52759194374084, loss : 2.086909770965576\n",
            "epoch: 6, classification_loss: 1.8321198225021362, Val Loss: 70.52926659584045, loss : 2.098926544189453\n",
            "epoch: 7, classification_loss: 1.8169991970062256, Val Loss: 70.5588767528534, loss : 2.0777623653411865\n",
            "epoch: 8, classification_loss: 1.820263147354126, Val Loss: 70.55655610561371, loss : 2.076432228088379\n",
            "epoch: 9, classification_loss: 1.8341144323349, Val Loss: 70.53454053401947, loss : 2.076223373413086\n",
            "epoch: 10, classification_loss: 1.836366057395935, Val Loss: 70.53345024585724, loss : 2.082484722137451\n",
            "epoch: 11, classification_loss: 1.833771824836731, Val Loss: 70.58919823169708, loss : 2.0725178718566895\n",
            "epoch: 12, classification_loss: 1.825021743774414, Val Loss: 70.5816261768341, loss : 2.0734307765960693\n",
            "epoch: 13, classification_loss: 1.829685926437378, Val Loss: 70.55455267429352, loss : 2.0732240676879883\n",
            "epoch: 14, classification_loss: 1.8280106782913208, Val Loss: 70.54267752170563, loss : 2.077125310897827\n",
            "epoch: 15, classification_loss: 1.823287010192871, Val Loss: 70.58492982387543, loss : 2.0681815147399902\n",
            "epoch: 16, classification_loss: 1.8295520544052124, Val Loss: 70.60277390480042, loss : 2.0728278160095215\n",
            "epoch: 17, classification_loss: 1.8284767866134644, Val Loss: 70.58717286586761, loss : 2.0693655014038086\n",
            "epoch: 18, classification_loss: 1.8249070644378662, Val Loss: 70.54264736175537, loss : 2.0591440200805664\n",
            "epoch: 19, classification_loss: 1.8332240581512451, Val Loss: 70.55646204948425, loss : 2.0717756748199463\n",
            "Batch: 136, Test Acc: 0.5732171474358975\n",
            "Batch: 137:\n",
            "epoch: 0, classification_loss: 1.8248299360275269, Val Loss: 70.54321300983429, loss : 1.8248299360275269\n",
            "epoch: 1, classification_loss: 1.7898938655853271, Val Loss: 70.60615754127502, loss : 2.121403217315674\n",
            "epoch: 2, classification_loss: 1.7623343467712402, Val Loss: 70.52602446079254, loss : 2.0319950580596924\n",
            "epoch: 3, classification_loss: 1.7855620384216309, Val Loss: 70.53848314285278, loss : 2.0610766410827637\n",
            "epoch: 4, classification_loss: 1.812302589416504, Val Loss: 70.53215825557709, loss : 2.0607028007507324\n",
            "epoch: 5, classification_loss: 1.8106566667556763, Val Loss: 70.52203798294067, loss : 2.0683999061584473\n",
            "epoch: 6, classification_loss: 1.7975208759307861, Val Loss: 70.54994893074036, loss : 2.070727586746216\n",
            "epoch: 7, classification_loss: 1.7910815477371216, Val Loss: 70.53966152667999, loss : 2.049806594848633\n",
            "epoch: 8, classification_loss: 1.8122285604476929, Val Loss: 70.62555122375488, loss : 2.075322151184082\n",
            "epoch: 9, classification_loss: 1.8016797304153442, Val Loss: 70.5803884267807, loss : 2.057269811630249\n",
            "epoch: 10, classification_loss: 1.7943414449691772, Val Loss: 70.59136497974396, loss : 2.050611734390259\n",
            "epoch: 11, classification_loss: 1.799498438835144, Val Loss: 70.60949671268463, loss : 2.0452165603637695\n",
            "epoch: 12, classification_loss: 1.8067415952682495, Val Loss: 70.68121600151062, loss : 2.0560860633850098\n",
            "epoch: 13, classification_loss: 1.792909026145935, Val Loss: 70.61380863189697, loss : 2.0382704734802246\n",
            "epoch: 14, classification_loss: 1.8075700998306274, Val Loss: 70.58763360977173, loss : 2.045607328414917\n",
            "epoch: 15, classification_loss: 1.806463599205017, Val Loss: 70.55040574073792, loss : 2.043663263320923\n",
            "epoch: 16, classification_loss: 1.802626132965088, Val Loss: 70.57107698917389, loss : 2.0432217121124268\n",
            "epoch: 17, classification_loss: 1.8047542572021484, Val Loss: 70.64726507663727, loss : 2.0427610874176025\n",
            "epoch: 18, classification_loss: 1.805773377418518, Val Loss: 70.66144001483917, loss : 2.047611713409424\n",
            "epoch: 19, classification_loss: 1.7945666313171387, Val Loss: 70.619797706604, loss : 2.0388758182525635\n",
            "Batch: 137, Test Acc: 0.5714142628205128\n",
            "Batch: 138:\n",
            "epoch: 0, classification_loss: 1.8108618259429932, Val Loss: 70.66262567043304, loss : 1.8108618259429932\n",
            "epoch: 1, classification_loss: 1.777375340461731, Val Loss: 70.61413073539734, loss : 2.1053104400634766\n",
            "epoch: 2, classification_loss: 1.758119821548462, Val Loss: 70.55112147331238, loss : 2.0120887756347656\n",
            "epoch: 3, classification_loss: 1.7932894229888916, Val Loss: 70.57616519927979, loss : 2.0575222969055176\n",
            "epoch: 4, classification_loss: 1.803666353225708, Val Loss: 70.54255521297455, loss : 2.0386199951171875\n",
            "epoch: 5, classification_loss: 1.7909166812896729, Val Loss: 70.55519247055054, loss : 2.0451102256774902\n",
            "epoch: 6, classification_loss: 1.7787429094314575, Val Loss: 70.54296541213989, loss : 2.0549824237823486\n",
            "epoch: 7, classification_loss: 1.780214786529541, Val Loss: 70.61688137054443, loss : 2.0364491939544678\n",
            "epoch: 8, classification_loss: 1.7835923433303833, Val Loss: 70.57619166374207, loss : 2.0426597595214844\n",
            "epoch: 9, classification_loss: 1.7972238063812256, Val Loss: 70.49965810775757, loss : 2.0429463386535645\n",
            "epoch: 10, classification_loss: 1.7915197610855103, Val Loss: 70.53890466690063, loss : 2.0347836017608643\n",
            "epoch: 11, classification_loss: 1.7984822988510132, Val Loss: 70.63632106781006, loss : 2.0308773517608643\n",
            "epoch: 12, classification_loss: 1.7888737916946411, Val Loss: 70.59126770496368, loss : 2.02476167678833\n",
            "epoch: 13, classification_loss: 1.7937625646591187, Val Loss: 70.6373473405838, loss : 2.0317270755767822\n",
            "epoch: 14, classification_loss: 1.7793428897857666, Val Loss: 70.57938015460968, loss : 2.0153117179870605\n",
            "epoch: 15, classification_loss: 1.79204261302948, Val Loss: 70.60933554172516, loss : 2.0246129035949707\n",
            "epoch: 16, classification_loss: 1.8033040761947632, Val Loss: 70.6383718252182, loss : 2.0340094566345215\n",
            "epoch: 17, classification_loss: 1.8045098781585693, Val Loss: 70.62478017807007, loss : 2.0289316177368164\n",
            "epoch: 18, classification_loss: 1.7935024499893188, Val Loss: 70.58939027786255, loss : 2.0272369384765625\n",
            "epoch: 19, classification_loss: 1.793287754058838, Val Loss: 70.63659274578094, loss : 2.0242581367492676\n",
            "Batch: 138, Test Acc: 0.5715144230769231\n",
            "Batch: 139:\n",
            "epoch: 0, classification_loss: 1.7721292972564697, Val Loss: 70.63439846038818, loss : 1.7721292972564697\n",
            "epoch: 1, classification_loss: 1.745592713356018, Val Loss: 70.61734902858734, loss : 2.076533317565918\n",
            "epoch: 2, classification_loss: 1.7257604598999023, Val Loss: 70.51303744316101, loss : 1.9824341535568237\n",
            "epoch: 3, classification_loss: 1.7607824802398682, Val Loss: 70.5309339761734, loss : 2.0204920768737793\n",
            "epoch: 4, classification_loss: 1.779345989227295, Val Loss: 70.54424750804901, loss : 2.007115364074707\n",
            "epoch: 5, classification_loss: 1.7770556211471558, Val Loss: 70.53069424629211, loss : 2.0277810096740723\n",
            "epoch: 6, classification_loss: 1.7521144151687622, Val Loss: 70.59157180786133, loss : 2.0139918327331543\n",
            "epoch: 7, classification_loss: 1.7511759996414185, Val Loss: 70.7731876373291, loss : 2.0005950927734375\n",
            "epoch: 8, classification_loss: 1.753690242767334, Val Loss: 70.60587620735168, loss : 2.005411386489868\n",
            "epoch: 9, classification_loss: 1.7566707134246826, Val Loss: 70.58192074298859, loss : 1.9896955490112305\n",
            "epoch: 10, classification_loss: 1.7571412324905396, Val Loss: 70.57458567619324, loss : 1.9917958974838257\n",
            "epoch: 11, classification_loss: 1.7698559761047363, Val Loss: 70.62717473506927, loss : 2.002545118331909\n",
            "epoch: 12, classification_loss: 1.7536004781723022, Val Loss: 70.67854011058807, loss : 1.9896243810653687\n",
            "epoch: 13, classification_loss: 1.7618112564086914, Val Loss: 70.62566423416138, loss : 1.988416075706482\n",
            "epoch: 14, classification_loss: 1.7635796070098877, Val Loss: 70.6296728849411, loss : 1.9937119483947754\n",
            "epoch: 15, classification_loss: 1.7504905462265015, Val Loss: 70.62065172195435, loss : 1.9728282690048218\n",
            "epoch: 16, classification_loss: 1.758009672164917, Val Loss: 70.62029588222504, loss : 1.985036849975586\n",
            "epoch: 17, classification_loss: 1.7603174448013306, Val Loss: 70.65409708023071, loss : 1.98469877243042\n",
            "epoch: 18, classification_loss: 1.7562146186828613, Val Loss: 70.67545127868652, loss : 1.981285810470581\n",
            "epoch: 19, classification_loss: 1.758576512336731, Val Loss: 70.60264325141907, loss : 1.9834603071212769\n",
            "Batch: 139, Test Acc: 0.5729166666666666\n",
            "Batch: 140:\n",
            "epoch: 0, classification_loss: 1.845260500907898, Val Loss: 70.53591167926788, loss : 1.845260500907898\n",
            "epoch: 1, classification_loss: 1.8137030601501465, Val Loss: 70.53461527824402, loss : 2.1426384449005127\n",
            "epoch: 2, classification_loss: 1.793675422668457, Val Loss: 70.52801597118378, loss : 2.0577521324157715\n",
            "epoch: 3, classification_loss: 1.8258728981018066, Val Loss: 70.53691685199738, loss : 2.08478045463562\n",
            "epoch: 4, classification_loss: 1.8425647020339966, Val Loss: 70.5551723241806, loss : 2.0783586502075195\n",
            "epoch: 5, classification_loss: 1.8389626741409302, Val Loss: 70.5276370048523, loss : 2.0727741718292236\n",
            "epoch: 6, classification_loss: 1.819492220878601, Val Loss: 70.56025969982147, loss : 2.0739195346832275\n",
            "epoch: 7, classification_loss: 1.8018388748168945, Val Loss: 70.55274748802185, loss : 2.0506844520568848\n",
            "epoch: 8, classification_loss: 1.8243684768676758, Val Loss: 70.55654573440552, loss : 2.067620038986206\n",
            "epoch: 9, classification_loss: 1.8302485942840576, Val Loss: 70.64675283432007, loss : 2.060969352722168\n",
            "epoch: 10, classification_loss: 1.8349448442459106, Val Loss: 70.58488094806671, loss : 2.0684173107147217\n",
            "epoch: 11, classification_loss: 1.8205749988555908, Val Loss: 70.57578635215759, loss : 2.0450637340545654\n",
            "epoch: 12, classification_loss: 1.829628586769104, Val Loss: 70.58550882339478, loss : 2.0686113834381104\n",
            "epoch: 13, classification_loss: 1.8249977827072144, Val Loss: 70.60828530788422, loss : 2.051908493041992\n",
            "epoch: 14, classification_loss: 1.8327116966247559, Val Loss: 70.61263537406921, loss : 2.066570997238159\n",
            "epoch: 15, classification_loss: 1.8275036811828613, Val Loss: 70.61581289768219, loss : 2.0444884300231934\n",
            "epoch: 16, classification_loss: 1.82047700881958, Val Loss: 70.58896017074585, loss : 2.046659231185913\n",
            "epoch: 17, classification_loss: 1.8224200010299683, Val Loss: 70.59451365470886, loss : 2.0440797805786133\n",
            "epoch: 18, classification_loss: 1.82756769657135, Val Loss: 70.6180067062378, loss : 2.06205153465271\n",
            "epoch: 19, classification_loss: 1.8308062553405762, Val Loss: 70.60283410549164, loss : 2.0590648651123047\n",
            "Batch: 140, Test Acc: 0.5728165064102564\n",
            "Batch: 141:\n",
            "epoch: 0, classification_loss: 1.820901870727539, Val Loss: 70.60923624038696, loss : 1.820901870727539\n",
            "epoch: 1, classification_loss: 1.7885687351226807, Val Loss: 70.67950689792633, loss : 2.124951124191284\n",
            "epoch: 2, classification_loss: 1.7544280290603638, Val Loss: 70.5151606798172, loss : 2.0308027267456055\n",
            "epoch: 3, classification_loss: 1.7926435470581055, Val Loss: 70.56537902355194, loss : 2.0541670322418213\n",
            "epoch: 4, classification_loss: 1.8188605308532715, Val Loss: 70.4934732913971, loss : 2.0712080001831055\n",
            "epoch: 5, classification_loss: 1.8103911876678467, Val Loss: 70.5107913017273, loss : 2.0440831184387207\n",
            "epoch: 6, classification_loss: 1.7936549186706543, Val Loss: 70.5401269197464, loss : 2.050828695297241\n",
            "epoch: 7, classification_loss: 1.7884225845336914, Val Loss: 70.5748485326767, loss : 2.0441160202026367\n",
            "epoch: 8, classification_loss: 1.8015191555023193, Val Loss: 70.54547393321991, loss : 2.0509040355682373\n",
            "epoch: 9, classification_loss: 1.7981106042861938, Val Loss: 70.56116724014282, loss : 2.046612501144409\n",
            "epoch: 10, classification_loss: 1.796578288078308, Val Loss: 70.54978632926941, loss : 2.0363149642944336\n",
            "epoch: 11, classification_loss: 1.7964905500411987, Val Loss: 70.56384980678558, loss : 2.035733699798584\n",
            "epoch: 12, classification_loss: 1.8078253269195557, Val Loss: 70.55099642276764, loss : 2.0429043769836426\n",
            "epoch: 13, classification_loss: 1.7919044494628906, Val Loss: 70.55874979496002, loss : 2.0323433876037598\n",
            "epoch: 14, classification_loss: 1.7996556758880615, Val Loss: 70.57999110221863, loss : 2.0339255332946777\n",
            "epoch: 15, classification_loss: 1.7913143634796143, Val Loss: 70.56071138381958, loss : 2.0259807109832764\n",
            "epoch: 16, classification_loss: 1.8079044818878174, Val Loss: 70.56433033943176, loss : 2.0340020656585693\n",
            "epoch: 17, classification_loss: 1.7993841171264648, Val Loss: 70.54713249206543, loss : 2.03454327583313\n",
            "epoch: 18, classification_loss: 1.803604006767273, Val Loss: 70.53375554084778, loss : 2.033172130584717\n",
            "epoch: 19, classification_loss: 1.7932292222976685, Val Loss: 70.59067904949188, loss : 2.029024839401245\n",
            "Batch: 141, Test Acc: 0.5749198717948718\n",
            "Batch: 142:\n",
            "epoch: 0, classification_loss: 1.8673219680786133, Val Loss: 70.5466912984848, loss : 1.8673219680786133\n",
            "epoch: 1, classification_loss: 1.8173513412475586, Val Loss: 70.54272842407227, loss : 2.1457254886627197\n",
            "epoch: 2, classification_loss: 1.7920138835906982, Val Loss: 70.5150386095047, loss : 2.0630667209625244\n",
            "epoch: 3, classification_loss: 1.7993924617767334, Val Loss: 70.59540390968323, loss : 2.073570728302002\n",
            "epoch: 4, classification_loss: 1.8403016328811646, Val Loss: 70.45069181919098, loss : 2.0857179164886475\n",
            "epoch: 5, classification_loss: 1.8298618793487549, Val Loss: 70.59760963916779, loss : 1.8298618793487549\n",
            "epoch: 6, classification_loss: 1.7955447435379028, Val Loss: 70.59945380687714, loss : 2.183070182800293\n",
            "epoch: 7, classification_loss: 1.7743698358535767, Val Loss: 70.53415381908417, loss : 2.11419677734375\n",
            "epoch: 8, classification_loss: 1.7698745727539062, Val Loss: 70.5181633234024, loss : 2.0641629695892334\n",
            "epoch: 9, classification_loss: 1.8302905559539795, Val Loss: 70.48988509178162, loss : 2.128324270248413\n",
            "epoch: 10, classification_loss: 1.8167221546173096, Val Loss: 70.4918133020401, loss : 2.064439058303833\n",
            "epoch: 11, classification_loss: 1.7838752269744873, Val Loss: 70.62212240695953, loss : 2.097261905670166\n",
            "epoch: 12, classification_loss: 1.790982723236084, Val Loss: 70.49994266033173, loss : 2.1035547256469727\n",
            "epoch: 13, classification_loss: 1.8026705980300903, Val Loss: 70.48766803741455, loss : 2.0927834510803223\n",
            "epoch: 14, classification_loss: 1.805301547050476, Val Loss: 70.48834145069122, loss : 2.069730281829834\n",
            "epoch: 15, classification_loss: 1.8244770765304565, Val Loss: 70.47141492366791, loss : 2.0986878871917725\n",
            "epoch: 16, classification_loss: 1.789710283279419, Val Loss: 70.50784981250763, loss : 2.055138349533081\n",
            "epoch: 17, classification_loss: 1.8043997287750244, Val Loss: 70.52146184444427, loss : 2.072859525680542\n",
            "epoch: 18, classification_loss: 1.8044096231460571, Val Loss: 70.56662285327911, loss : 2.0593185424804688\n",
            "epoch: 19, classification_loss: 1.7995229959487915, Val Loss: 70.495929479599, loss : 2.0581283569335938\n",
            "Batch: 142, Test Acc: 0.5756209935897436\n",
            "Batch: 143:\n",
            "epoch: 0, classification_loss: 1.8360817432403564, Val Loss: 70.50121212005615, loss : 1.8360817432403564\n",
            "epoch: 1, classification_loss: 1.784119725227356, Val Loss: 70.50240767002106, loss : 2.1107017993927\n",
            "epoch: 2, classification_loss: 1.7787482738494873, Val Loss: 70.49908638000488, loss : 2.0296430587768555\n",
            "epoch: 3, classification_loss: 1.8033639192581177, Val Loss: 70.50971698760986, loss : 2.090819835662842\n",
            "epoch: 4, classification_loss: 1.831313967704773, Val Loss: 70.50195097923279, loss : 2.0808069705963135\n",
            "epoch: 5, classification_loss: 1.818626046180725, Val Loss: 70.47266483306885, loss : 2.0727076530456543\n",
            "epoch: 6, classification_loss: 1.7975572347640991, Val Loss: 70.4998346567154, loss : 2.060215711593628\n",
            "epoch: 7, classification_loss: 1.7886301279067993, Val Loss: 70.53235828876495, loss : 2.0472540855407715\n",
            "epoch: 8, classification_loss: 1.8084934949874878, Val Loss: 70.57903134822845, loss : 2.0662765502929688\n",
            "epoch: 9, classification_loss: 1.8034629821777344, Val Loss: 70.53725814819336, loss : 2.066531181335449\n",
            "epoch: 10, classification_loss: 1.8092081546783447, Val Loss: 70.48242163658142, loss : 2.0565130710601807\n",
            "epoch: 11, classification_loss: 1.805363416671753, Val Loss: 70.54650902748108, loss : 2.0563042163848877\n",
            "epoch: 12, classification_loss: 1.8148702383041382, Val Loss: 70.61904036998749, loss : 2.060338258743286\n",
            "epoch: 13, classification_loss: 1.8083206415176392, Val Loss: 70.53511655330658, loss : 2.066035270690918\n",
            "epoch: 14, classification_loss: 1.7945924997329712, Val Loss: 70.49095416069031, loss : 2.043219804763794\n",
            "epoch: 15, classification_loss: 1.8191075325012207, Val Loss: 70.53965508937836, loss : 2.066894292831421\n",
            "epoch: 16, classification_loss: 1.7992846965789795, Val Loss: 70.56711459159851, loss : 2.051889181137085\n",
            "epoch: 17, classification_loss: 1.8056350946426392, Val Loss: 70.51655983924866, loss : 2.0592548847198486\n",
            "epoch: 18, classification_loss: 1.8119374513626099, Val Loss: 70.51971232891083, loss : 2.047995090484619\n",
            "epoch: 19, classification_loss: 1.8115150928497314, Val Loss: 70.57679164409637, loss : 2.050684928894043\n",
            "Batch: 143, Test Acc: 0.5740184294871795\n",
            "Batch: 144:\n",
            "epoch: 0, classification_loss: 1.8029379844665527, Val Loss: 70.55628478527069, loss : 1.8029379844665527\n",
            "epoch: 1, classification_loss: 1.7794028520584106, Val Loss: 70.56843602657318, loss : 2.101811647415161\n",
            "epoch: 2, classification_loss: 1.752058982849121, Val Loss: 70.53056275844574, loss : 2.015101432800293\n",
            "epoch: 3, classification_loss: 1.7845134735107422, Val Loss: 70.47545909881592, loss : 2.0535831451416016\n",
            "epoch: 4, classification_loss: 1.812232494354248, Val Loss: 70.53007507324219, loss : 2.0460197925567627\n",
            "epoch: 5, classification_loss: 1.7867217063903809, Val Loss: 70.50992786884308, loss : 2.038517951965332\n",
            "epoch: 6, classification_loss: 1.7816730737686157, Val Loss: 70.52958154678345, loss : 2.0418426990509033\n",
            "epoch: 7, classification_loss: 1.7855474948883057, Val Loss: 70.51661777496338, loss : 2.027500629425049\n",
            "epoch: 8, classification_loss: 1.7838268280029297, Val Loss: 70.50946283340454, loss : 2.0289103984832764\n",
            "epoch: 9, classification_loss: 1.7820216417312622, Val Loss: 70.50781178474426, loss : 2.027003526687622\n",
            "epoch: 10, classification_loss: 1.788624882698059, Val Loss: 70.49267661571503, loss : 2.0254316329956055\n",
            "epoch: 11, classification_loss: 1.797864556312561, Val Loss: 70.52010989189148, loss : 2.0346384048461914\n",
            "epoch: 12, classification_loss: 1.794067144393921, Val Loss: 70.58515691757202, loss : 2.036123037338257\n",
            "epoch: 13, classification_loss: 1.781483769416809, Val Loss: 70.53231656551361, loss : 2.0196330547332764\n",
            "epoch: 14, classification_loss: 1.778135061264038, Val Loss: 70.50087869167328, loss : 2.016479015350342\n",
            "epoch: 15, classification_loss: 1.7990896701812744, Val Loss: 70.51375377178192, loss : 2.028960943222046\n",
            "epoch: 16, classification_loss: 1.7789040803909302, Val Loss: 70.5799411535263, loss : 2.0133180618286133\n",
            "epoch: 17, classification_loss: 1.7864153385162354, Val Loss: 70.53953647613525, loss : 2.0140116214752197\n",
            "epoch: 18, classification_loss: 1.7796825170516968, Val Loss: 70.5093445777893, loss : 2.007213830947876\n",
            "epoch: 19, classification_loss: 1.7871638536453247, Val Loss: 70.54778039455414, loss : 2.016697406768799\n",
            "Batch: 144, Test Acc: 0.5743189102564102\n",
            "Batch: 145:\n",
            "epoch: 0, classification_loss: 1.8488343954086304, Val Loss: 70.54347252845764, loss : 1.8488343954086304\n",
            "epoch: 1, classification_loss: 1.812747597694397, Val Loss: 70.66051256656647, loss : 2.1464881896972656\n",
            "epoch: 2, classification_loss: 1.7814360857009888, Val Loss: 70.67006325721741, loss : 2.050987958908081\n",
            "epoch: 3, classification_loss: 1.7998583316802979, Val Loss: 70.58102869987488, loss : 2.0706801414489746\n",
            "epoch: 4, classification_loss: 1.8357350826263428, Val Loss: 70.51605212688446, loss : 2.0803964138031006\n",
            "epoch: 5, classification_loss: 1.8287237882614136, Val Loss: 70.51228165626526, loss : 2.080277919769287\n",
            "epoch: 6, classification_loss: 1.8201810121536255, Val Loss: 70.54525077342987, loss : 2.087480306625366\n",
            "epoch: 7, classification_loss: 1.8097903728485107, Val Loss: 70.54022908210754, loss : 2.0616464614868164\n",
            "epoch: 8, classification_loss: 1.8211805820465088, Val Loss: 70.6077595949173, loss : 2.0665576457977295\n",
            "epoch: 9, classification_loss: 1.8302581310272217, Val Loss: 70.60381138324738, loss : 2.0722100734710693\n",
            "epoch: 10, classification_loss: 1.8297114372253418, Val Loss: 70.53411018848419, loss : 2.0686893463134766\n",
            "epoch: 11, classification_loss: 1.8235522508621216, Val Loss: 70.5317006111145, loss : 2.0645575523376465\n",
            "epoch: 12, classification_loss: 1.8202420473098755, Val Loss: 70.56839144229889, loss : 2.0564281940460205\n",
            "epoch: 13, classification_loss: 1.824842095375061, Val Loss: 70.63671624660492, loss : 2.0610275268554688\n",
            "epoch: 14, classification_loss: 1.8212785720825195, Val Loss: 70.62238144874573, loss : 2.0554301738739014\n",
            "epoch: 15, classification_loss: 1.8217556476593018, Val Loss: 70.61071634292603, loss : 2.0617716312408447\n",
            "epoch: 16, classification_loss: 1.8180793523788452, Val Loss: 70.55437386035919, loss : 2.046642780303955\n",
            "epoch: 17, classification_loss: 1.8302593231201172, Val Loss: 70.57101047039032, loss : 2.0677003860473633\n",
            "epoch: 18, classification_loss: 1.829698085784912, Val Loss: 70.58973729610443, loss : 2.05452299118042\n",
            "epoch: 19, classification_loss: 1.82499098777771, Val Loss: 70.59231412410736, loss : 2.0677530765533447\n",
            "Batch: 145, Test Acc: 0.5740184294871795\n",
            "Batch: 146:\n",
            "epoch: 0, classification_loss: 1.8214045763015747, Val Loss: 70.49293053150177, loss : 1.8214045763015747\n",
            "epoch: 1, classification_loss: 1.774730920791626, Val Loss: 70.5136057138443, loss : 2.086559534072876\n",
            "epoch: 2, classification_loss: 1.762587070465088, Val Loss: 70.49584460258484, loss : 2.012941360473633\n",
            "epoch: 3, classification_loss: 1.7926141023635864, Val Loss: 70.49323797225952, loss : 2.0482711791992188\n",
            "epoch: 4, classification_loss: 1.819563388824463, Val Loss: 70.48717129230499, loss : 2.046285390853882\n",
            "epoch: 5, classification_loss: 1.7958160638809204, Val Loss: 70.51702129840851, loss : 2.0477147102355957\n",
            "epoch: 6, classification_loss: 1.7906105518341064, Val Loss: 70.54800474643707, loss : 2.0456082820892334\n",
            "epoch: 7, classification_loss: 1.787556529045105, Val Loss: 70.48395788669586, loss : 2.041271924972534\n",
            "epoch: 8, classification_loss: 1.7869230508804321, Val Loss: 70.49100279808044, loss : 2.035607099533081\n",
            "epoch: 9, classification_loss: 1.7982863187789917, Val Loss: 70.50668370723724, loss : 2.0407636165618896\n",
            "epoch: 10, classification_loss: 1.800384283065796, Val Loss: 70.52427124977112, loss : 2.038137912750244\n",
            "epoch: 11, classification_loss: 1.7966517210006714, Val Loss: 70.5369861125946, loss : 2.0315768718719482\n",
            "epoch: 12, classification_loss: 1.7951096296310425, Val Loss: 70.50304532051086, loss : 2.03879451751709\n",
            "epoch: 13, classification_loss: 1.7926571369171143, Val Loss: 70.51071059703827, loss : 2.0256707668304443\n",
            "epoch: 14, classification_loss: 1.7914767265319824, Val Loss: 70.52924513816833, loss : 2.0324389934539795\n",
            "epoch: 15, classification_loss: 1.8032948970794678, Val Loss: 70.56837844848633, loss : 2.0327417850494385\n",
            "epoch: 16, classification_loss: 1.8005952835083008, Val Loss: 70.53432369232178, loss : 2.0413806438446045\n",
            "epoch: 17, classification_loss: 1.7897937297821045, Val Loss: 70.51001560688019, loss : 2.02260684967041\n",
            "epoch: 18, classification_loss: 1.8037749528884888, Val Loss: 70.59295845031738, loss : 2.036484956741333\n",
            "epoch: 19, classification_loss: 1.798546552658081, Val Loss: 70.56300091743469, loss : 2.028921365737915\n",
            "Batch: 146, Test Acc: 0.5725160256410257\n",
            "Batch: 147:\n",
            "epoch: 0, classification_loss: 1.832311987876892, Val Loss: 70.44311165809631, loss : 1.832311987876892\n",
            "epoch: 1, classification_loss: 1.7920643091201782, Val Loss: 70.65341556072235, loss : 1.7920643091201782\n",
            "epoch: 2, classification_loss: 1.7241603136062622, Val Loss: 70.66447865962982, loss : 2.096388816833496\n",
            "epoch: 3, classification_loss: 1.697356939315796, Val Loss: 70.60752844810486, loss : 2.0423226356506348\n",
            "epoch: 4, classification_loss: 1.7189316749572754, Val Loss: 70.53233051300049, loss : 2.028062343597412\n",
            "epoch: 5, classification_loss: 1.7847461700439453, Val Loss: 70.52311754226685, loss : 2.0651698112487793\n",
            "epoch: 6, classification_loss: 1.7754629850387573, Val Loss: 70.50331890583038, loss : 2.026782989501953\n",
            "epoch: 7, classification_loss: 1.7745671272277832, Val Loss: 70.47875797748566, loss : 2.0529603958129883\n",
            "epoch: 8, classification_loss: 1.7636162042617798, Val Loss: 70.50749492645264, loss : 2.0566227436065674\n",
            "epoch: 9, classification_loss: 1.747748613357544, Val Loss: 70.65091812610626, loss : 2.0351595878601074\n",
            "epoch: 10, classification_loss: 1.744019627571106, Val Loss: 70.59813678264618, loss : 2.029111385345459\n",
            "epoch: 11, classification_loss: 1.7602009773254395, Val Loss: 70.47861731052399, loss : 2.0284156799316406\n",
            "epoch: 12, classification_loss: 1.7607460021972656, Val Loss: 70.49970710277557, loss : 2.0296132564544678\n",
            "epoch: 13, classification_loss: 1.7670167684555054, Val Loss: 70.5413646697998, loss : 2.0271964073181152\n",
            "epoch: 14, classification_loss: 1.7622681856155396, Val Loss: 70.52916848659515, loss : 2.02146315574646\n",
            "epoch: 15, classification_loss: 1.7495278120040894, Val Loss: 70.4989162683487, loss : 2.003354549407959\n",
            "epoch: 16, classification_loss: 1.7691906690597534, Val Loss: 70.51026284694672, loss : 2.0196027755737305\n",
            "epoch: 17, classification_loss: 1.7648124694824219, Val Loss: 70.55867779254913, loss : 2.013413906097412\n",
            "epoch: 18, classification_loss: 1.7619619369506836, Val Loss: 70.51391208171844, loss : 2.019207715988159\n",
            "epoch: 19, classification_loss: 1.7589019536972046, Val Loss: 70.53314924240112, loss : 2.006194829940796\n",
            "Batch: 147, Test Acc: 0.5749198717948718\n",
            "Batch: 148:\n",
            "epoch: 0, classification_loss: 1.8102054595947266, Val Loss: 70.5161052942276, loss : 1.8102054595947266\n",
            "epoch: 1, classification_loss: 1.7895464897155762, Val Loss: 70.52932810783386, loss : 2.127847909927368\n",
            "epoch: 2, classification_loss: 1.7593202590942383, Val Loss: 70.5585607290268, loss : 2.0313355922698975\n",
            "epoch: 3, classification_loss: 1.794301152229309, Val Loss: 70.55837404727936, loss : 2.0735771656036377\n",
            "epoch: 4, classification_loss: 1.8193844556808472, Val Loss: 70.51888465881348, loss : 2.0665175914764404\n",
            "epoch: 5, classification_loss: 1.8046528100967407, Val Loss: 70.50937700271606, loss : 2.0564422607421875\n",
            "epoch: 6, classification_loss: 1.7925938367843628, Val Loss: 70.58484947681427, loss : 2.0699422359466553\n",
            "epoch: 7, classification_loss: 1.8007339239120483, Val Loss: 70.58289527893066, loss : 2.055675506591797\n",
            "epoch: 8, classification_loss: 1.802708625793457, Val Loss: 70.51688051223755, loss : 2.0517396926879883\n",
            "epoch: 9, classification_loss: 1.8041356801986694, Val Loss: 70.51793265342712, loss : 2.0452370643615723\n",
            "epoch: 10, classification_loss: 1.7862759828567505, Val Loss: 70.58572161197662, loss : 2.031013250350952\n",
            "epoch: 11, classification_loss: 1.7969683408737183, Val Loss: 70.59361600875854, loss : 2.0424938201904297\n",
            "epoch: 12, classification_loss: 1.8064398765563965, Val Loss: 70.5794757604599, loss : 2.045494318008423\n",
            "epoch: 13, classification_loss: 1.8043146133422852, Val Loss: 70.6035760641098, loss : 2.044633626937866\n",
            "epoch: 14, classification_loss: 1.8005412817001343, Val Loss: 70.5903948545456, loss : 2.0286104679107666\n",
            "epoch: 15, classification_loss: 1.7909497022628784, Val Loss: 70.57898736000061, loss : 2.0287179946899414\n",
            "epoch: 16, classification_loss: 1.8052295446395874, Val Loss: 70.59354078769684, loss : 2.0327038764953613\n",
            "epoch: 17, classification_loss: 1.8043750524520874, Val Loss: 70.62194156646729, loss : 2.032569408416748\n",
            "epoch: 18, classification_loss: 1.8025760650634766, Val Loss: 70.62069392204285, loss : 2.025407314300537\n",
            "epoch: 19, classification_loss: 1.7987010478973389, Val Loss: 70.57906877994537, loss : 2.0297768115997314\n",
            "Batch: 148, Test Acc: 0.5764222756410257\n",
            "Batch: 149:\n",
            "epoch: 0, classification_loss: 1.8572838306427002, Val Loss: 70.47772634029388, loss : 1.8572838306427002\n",
            "epoch: 1, classification_loss: 1.8126627206802368, Val Loss: 70.51565265655518, loss : 2.135349750518799\n",
            "epoch: 2, classification_loss: 1.7856690883636475, Val Loss: 70.51324772834778, loss : 2.0376992225646973\n",
            "epoch: 3, classification_loss: 1.8262501955032349, Val Loss: 70.47531592845917, loss : 2.0911622047424316\n",
            "epoch: 4, classification_loss: 1.8349634408950806, Val Loss: 70.47480547428131, loss : 2.069167137145996\n",
            "epoch: 5, classification_loss: 1.8193321228027344, Val Loss: 70.57039666175842, loss : 2.0792555809020996\n",
            "epoch: 6, classification_loss: 1.8090153932571411, Val Loss: 70.52022314071655, loss : 2.0781428813934326\n",
            "epoch: 7, classification_loss: 1.8042411804199219, Val Loss: 70.49459969997406, loss : 2.062019109725952\n",
            "epoch: 8, classification_loss: 1.8150473833084106, Val Loss: 70.52834153175354, loss : 2.068422317504883\n",
            "epoch: 9, classification_loss: 1.8158987760543823, Val Loss: 70.55693984031677, loss : 2.0709898471832275\n",
            "epoch: 10, classification_loss: 1.8238136768341064, Val Loss: 70.55005216598511, loss : 2.066631317138672\n",
            "epoch: 11, classification_loss: 1.8238778114318848, Val Loss: 70.49626660346985, loss : 2.0740528106689453\n",
            "epoch: 12, classification_loss: 1.8210766315460205, Val Loss: 70.52048718929291, loss : 2.057281970977783\n",
            "epoch: 13, classification_loss: 1.8180277347564697, Val Loss: 70.57437765598297, loss : 2.0691046714782715\n",
            "epoch: 14, classification_loss: 1.818705439567566, Val Loss: 70.54278838634491, loss : 2.0616202354431152\n",
            "epoch: 15, classification_loss: 1.8210006952285767, Val Loss: 70.508420586586, loss : 2.0629475116729736\n",
            "epoch: 16, classification_loss: 1.8333487510681152, Val Loss: 70.55916941165924, loss : 2.070847988128662\n",
            "epoch: 17, classification_loss: 1.8200948238372803, Val Loss: 70.52909886837006, loss : 2.0672361850738525\n",
            "epoch: 18, classification_loss: 1.8332982063293457, Val Loss: 70.522745013237, loss : 2.066823959350586\n",
            "epoch: 19, classification_loss: 1.8331117630004883, Val Loss: 70.49977481365204, loss : 2.0720937252044678\n",
            "Batch: 149, Test Acc: 0.5758213141025641\n",
            "Batch: 150:\n",
            "epoch: 0, classification_loss: 1.8011969327926636, Val Loss: 70.4664978981018, loss : 1.8011969327926636\n",
            "epoch: 1, classification_loss: 1.7809557914733887, Val Loss: 70.50523781776428, loss : 2.102745294570923\n",
            "epoch: 2, classification_loss: 1.7553743124008179, Val Loss: 70.51376950740814, loss : 2.013477087020874\n",
            "epoch: 3, classification_loss: 1.7694984674453735, Val Loss: 70.51395678520203, loss : 2.0399086475372314\n",
            "epoch: 4, classification_loss: 1.7989360094070435, Val Loss: 70.50917625427246, loss : 2.0402920246124268\n",
            "epoch: 5, classification_loss: 1.7890963554382324, Val Loss: 70.49168574810028, loss : 2.03933048248291\n",
            "epoch: 6, classification_loss: 1.7874608039855957, Val Loss: 70.52521848678589, loss : 2.0427725315093994\n",
            "epoch: 7, classification_loss: 1.778105616569519, Val Loss: 70.49082970619202, loss : 2.0348892211914062\n",
            "epoch: 8, classification_loss: 1.7788904905319214, Val Loss: 70.52685511112213, loss : 2.026812791824341\n",
            "epoch: 9, classification_loss: 1.778536319732666, Val Loss: 70.54476499557495, loss : 2.0285890102386475\n",
            "epoch: 10, classification_loss: 1.792219877243042, Val Loss: 70.53705871105194, loss : 2.0303754806518555\n",
            "epoch: 11, classification_loss: 1.7918506860733032, Val Loss: 70.49153089523315, loss : 2.033893346786499\n",
            "epoch: 12, classification_loss: 1.7823604345321655, Val Loss: 70.5358259677887, loss : 2.021847724914551\n",
            "epoch: 13, classification_loss: 1.7888835668563843, Val Loss: 70.56868970394135, loss : 2.031731128692627\n",
            "epoch: 14, classification_loss: 1.7930532693862915, Val Loss: 70.52942335605621, loss : 2.024313449859619\n",
            "epoch: 15, classification_loss: 1.7827391624450684, Val Loss: 70.53724646568298, loss : 2.0200750827789307\n",
            "epoch: 16, classification_loss: 1.7828658819198608, Val Loss: 70.5284982919693, loss : 2.0110065937042236\n",
            "epoch: 17, classification_loss: 1.7896218299865723, Val Loss: 70.55085515975952, loss : 2.0252161026000977\n",
            "epoch: 18, classification_loss: 1.7932236194610596, Val Loss: 70.53593790531158, loss : 2.022170305252075\n",
            "epoch: 19, classification_loss: 1.775158405303955, Val Loss: 70.55491268634796, loss : 2.0104482173919678\n",
            "Batch: 150, Test Acc: 0.5798277243589743\n",
            "Batch: 151:\n",
            "epoch: 0, classification_loss: 1.8105872869491577, Val Loss: 70.5481184720993, loss : 1.8105872869491577\n",
            "epoch: 1, classification_loss: 1.795548439025879, Val Loss: 70.55321490764618, loss : 2.1228692531585693\n",
            "epoch: 2, classification_loss: 1.7569712400436401, Val Loss: 70.4985636472702, loss : 2.0154504776000977\n",
            "epoch: 3, classification_loss: 1.7835391759872437, Val Loss: 70.58213150501251, loss : 2.050023317337036\n",
            "epoch: 4, classification_loss: 1.8091543912887573, Val Loss: 70.52756154537201, loss : 2.039099931716919\n",
            "epoch: 5, classification_loss: 1.7993232011795044, Val Loss: 70.52100121974945, loss : 2.059636354446411\n",
            "epoch: 6, classification_loss: 1.7886120080947876, Val Loss: 70.55756962299347, loss : 2.0613245964050293\n",
            "epoch: 7, classification_loss: 1.7954436540603638, Val Loss: 70.51259624958038, loss : 2.0523059368133545\n",
            "epoch: 8, classification_loss: 1.800580620765686, Val Loss: 70.59940421581268, loss : 2.046274423599243\n",
            "epoch: 9, classification_loss: 1.792796015739441, Val Loss: 70.54928469657898, loss : 2.0421981811523438\n",
            "epoch: 10, classification_loss: 1.7958519458770752, Val Loss: 70.56071615219116, loss : 2.038421154022217\n",
            "epoch: 11, classification_loss: 1.7915186882019043, Val Loss: 70.55713593959808, loss : 2.0372402667999268\n",
            "epoch: 12, classification_loss: 1.793319582939148, Val Loss: 70.54721319675446, loss : 2.025773286819458\n",
            "epoch: 13, classification_loss: 1.7877904176712036, Val Loss: 70.53550946712494, loss : 2.029872417449951\n",
            "epoch: 14, classification_loss: 1.796479344367981, Val Loss: 70.60135328769684, loss : 2.0289034843444824\n",
            "epoch: 15, classification_loss: 1.7951245307922363, Val Loss: 70.63435399532318, loss : 2.0328664779663086\n",
            "epoch: 16, classification_loss: 1.7944984436035156, Val Loss: 70.58628535270691, loss : 2.028850555419922\n",
            "epoch: 17, classification_loss: 1.7836883068084717, Val Loss: 70.59430193901062, loss : 2.02010178565979\n",
            "epoch: 18, classification_loss: 1.801318883895874, Val Loss: 70.54837703704834, loss : 2.023585081100464\n",
            "epoch: 19, classification_loss: 1.794703722000122, Val Loss: 70.57762002944946, loss : 2.0269665718078613\n",
            "Batch: 151, Test Acc: 0.5755208333333334\n",
            "Batch: 152:\n",
            "epoch: 0, classification_loss: 1.8166416883468628, Val Loss: 70.69792377948761, loss : 1.8166416883468628\n",
            "epoch: 1, classification_loss: 1.7833830118179321, Val Loss: 70.63403642177582, loss : 2.1157174110412598\n",
            "epoch: 2, classification_loss: 1.750799536705017, Val Loss: 70.50822114944458, loss : 2.021505117416382\n",
            "epoch: 3, classification_loss: 1.7955635786056519, Val Loss: 70.53624475002289, loss : 2.0663416385650635\n",
            "epoch: 4, classification_loss: 1.7965842485427856, Val Loss: 70.51229333877563, loss : 2.034877300262451\n",
            "epoch: 5, classification_loss: 1.7974828481674194, Val Loss: 70.52060186862946, loss : 2.0511386394500732\n",
            "epoch: 6, classification_loss: 1.7763416767120361, Val Loss: 70.58315134048462, loss : 2.03218412399292\n",
            "epoch: 7, classification_loss: 1.7731505632400513, Val Loss: 70.58437597751617, loss : 2.036552906036377\n",
            "epoch: 8, classification_loss: 1.777148962020874, Val Loss: 70.55925250053406, loss : 2.0257859230041504\n",
            "epoch: 9, classification_loss: 1.7874616384506226, Val Loss: 70.57192325592041, loss : 2.041738748550415\n",
            "epoch: 10, classification_loss: 1.7962474822998047, Val Loss: 70.5531873703003, loss : 2.0355138778686523\n",
            "epoch: 11, classification_loss: 1.7766261100769043, Val Loss: 70.58818805217743, loss : 2.023300886154175\n",
            "epoch: 12, classification_loss: 1.793001651763916, Val Loss: 70.59932386875153, loss : 2.028628349304199\n",
            "epoch: 13, classification_loss: 1.7968125343322754, Val Loss: 70.58762586116791, loss : 2.0446884632110596\n",
            "epoch: 14, classification_loss: 1.7920944690704346, Val Loss: 70.62226951122284, loss : 2.0279195308685303\n",
            "epoch: 15, classification_loss: 1.7959359884262085, Val Loss: 70.59189558029175, loss : 2.036076307296753\n",
            "epoch: 16, classification_loss: 1.7938657999038696, Val Loss: 70.59607410430908, loss : 2.026846170425415\n",
            "epoch: 17, classification_loss: 1.789497971534729, Val Loss: 70.63335490226746, loss : 2.0283477306365967\n",
            "epoch: 18, classification_loss: 1.7855781316757202, Val Loss: 70.59983599185944, loss : 2.0099871158599854\n",
            "epoch: 19, classification_loss: 1.7908248901367188, Val Loss: 70.5898402929306, loss : 2.025535821914673\n",
            "Batch: 152, Test Acc: 0.5743189102564102\n",
            "Batch: 153:\n",
            "epoch: 0, classification_loss: 1.8465079069137573, Val Loss: 70.58471393585205, loss : 1.8465079069137573\n",
            "epoch: 1, classification_loss: 1.8260531425476074, Val Loss: 70.52910602092743, loss : 2.1510791778564453\n",
            "epoch: 2, classification_loss: 1.794840931892395, Val Loss: 70.516277551651, loss : 2.054663896560669\n",
            "epoch: 3, classification_loss: 1.8285163640975952, Val Loss: 70.53479838371277, loss : 2.0941412448883057\n",
            "epoch: 4, classification_loss: 1.8497692346572876, Val Loss: 70.57051038742065, loss : 2.0766208171844482\n",
            "epoch: 5, classification_loss: 1.8158791065216064, Val Loss: 70.48285663127899, loss : 2.082981586456299\n",
            "epoch: 6, classification_loss: 1.8158656358718872, Val Loss: 70.52889394760132, loss : 2.0823142528533936\n",
            "epoch: 7, classification_loss: 1.8332005739212036, Val Loss: 70.59199941158295, loss : 2.086249351501465\n",
            "epoch: 8, classification_loss: 1.8383336067199707, Val Loss: 70.53367364406586, loss : 2.0927765369415283\n",
            "epoch: 9, classification_loss: 1.818713665008545, Val Loss: 70.49439752101898, loss : 2.061708450317383\n",
            "epoch: 10, classification_loss: 1.8327789306640625, Val Loss: 70.53935468196869, loss : 2.082728385925293\n",
            "epoch: 11, classification_loss: 1.8406842947006226, Val Loss: 70.62841820716858, loss : 2.0809409618377686\n",
            "epoch: 12, classification_loss: 1.8363631963729858, Val Loss: 70.54041945934296, loss : 2.080024480819702\n",
            "epoch: 13, classification_loss: 1.8268628120422363, Val Loss: 70.54534161090851, loss : 2.066394805908203\n",
            "epoch: 14, classification_loss: 1.8352457284927368, Val Loss: 70.56647050380707, loss : 2.0768227577209473\n",
            "epoch: 15, classification_loss: 1.8265981674194336, Val Loss: 70.61784029006958, loss : 2.0670158863067627\n",
            "epoch: 16, classification_loss: 1.8316290378570557, Val Loss: 70.55237340927124, loss : 2.0726287364959717\n",
            "epoch: 17, classification_loss: 1.8252509832382202, Val Loss: 70.5901597738266, loss : 2.0624279975891113\n",
            "epoch: 18, classification_loss: 1.8436834812164307, Val Loss: 70.67167401313782, loss : 2.0747697353363037\n",
            "epoch: 19, classification_loss: 1.8309999704360962, Val Loss: 70.61061656475067, loss : 2.0724151134490967\n",
            "Batch: 153, Test Acc: 0.57421875\n",
            "Batch: 154:\n",
            "epoch: 0, classification_loss: 1.8128108978271484, Val Loss: 70.44087719917297, loss : 1.8128108978271484\n",
            "epoch: 1, classification_loss: 1.7699718475341797, Val Loss: 70.44953954219818, loss : 2.0886971950531006\n",
            "epoch: 2, classification_loss: 1.7644217014312744, Val Loss: 70.50087034702301, loss : 2.015535593032837\n",
            "epoch: 3, classification_loss: 1.7865915298461914, Val Loss: 70.48764431476593, loss : 2.04541015625\n",
            "epoch: 4, classification_loss: 1.8019287586212158, Val Loss: 70.46696257591248, loss : 2.0250136852264404\n",
            "epoch: 5, classification_loss: 1.7962486743927002, Val Loss: 70.48950386047363, loss : 2.0487141609191895\n",
            "epoch: 6, classification_loss: 1.7846843004226685, Val Loss: 70.49793541431427, loss : 2.037461280822754\n",
            "epoch: 7, classification_loss: 1.7821098566055298, Val Loss: 70.49682676792145, loss : 2.027205228805542\n",
            "epoch: 8, classification_loss: 1.786230444908142, Val Loss: 70.49015784263611, loss : 2.0230374336242676\n",
            "epoch: 9, classification_loss: 1.7846113443374634, Val Loss: 70.51749646663666, loss : 2.0189380645751953\n",
            "epoch: 10, classification_loss: 1.7900264263153076, Val Loss: 70.50832033157349, loss : 2.021361827850342\n",
            "epoch: 11, classification_loss: 1.7816319465637207, Val Loss: 70.50768685340881, loss : 2.0186498165130615\n",
            "epoch: 12, classification_loss: 1.7902753353118896, Val Loss: 70.52096021175385, loss : 2.0222885608673096\n",
            "epoch: 13, classification_loss: 1.78941011428833, Val Loss: 70.57059669494629, loss : 2.0196969509124756\n",
            "epoch: 14, classification_loss: 1.7878642082214355, Val Loss: 70.51153898239136, loss : 2.010089635848999\n",
            "epoch: 15, classification_loss: 1.7868306636810303, Val Loss: 70.51291060447693, loss : 2.0159051418304443\n",
            "epoch: 16, classification_loss: 1.7894186973571777, Val Loss: 70.58906030654907, loss : 2.010469675064087\n",
            "epoch: 17, classification_loss: 1.7965198755264282, Val Loss: 70.50824856758118, loss : 2.026799440383911\n",
            "epoch: 18, classification_loss: 1.7806075811386108, Val Loss: 70.5144635438919, loss : 1.9988455772399902\n",
            "epoch: 19, classification_loss: 1.7869764566421509, Val Loss: 70.59092593193054, loss : 2.0110995769500732\n",
            "Batch: 154, Test Acc: 0.573417467948718\n",
            "Batch: 155:\n",
            "epoch: 0, classification_loss: 1.8520547151565552, Val Loss: 70.49799942970276, loss : 1.8520547151565552\n",
            "epoch: 1, classification_loss: 1.8033636808395386, Val Loss: 70.5039267539978, loss : 2.130491256713867\n",
            "epoch: 2, classification_loss: 1.7815765142440796, Val Loss: 70.48059487342834, loss : 2.0419669151306152\n",
            "epoch: 3, classification_loss: 1.8152296543121338, Val Loss: 70.49854218959808, loss : 2.0816712379455566\n",
            "epoch: 4, classification_loss: 1.8351579904556274, Val Loss: 70.49767625331879, loss : 2.06870174407959\n",
            "epoch: 5, classification_loss: 1.8134628534317017, Val Loss: 70.51739954948425, loss : 2.0686116218566895\n",
            "epoch: 6, classification_loss: 1.8139711618423462, Val Loss: 70.49823081493378, loss : 2.0756022930145264\n",
            "epoch: 7, classification_loss: 1.7987337112426758, Val Loss: 70.5062826871872, loss : 2.0541646480560303\n",
            "epoch: 8, classification_loss: 1.8186531066894531, Val Loss: 70.51357924938202, loss : 2.0694117546081543\n",
            "epoch: 9, classification_loss: 1.8132174015045166, Val Loss: 70.45975244045258, loss : 2.0646538734436035\n",
            "epoch: 10, classification_loss: 1.7956311702728271, Val Loss: 70.51359057426453, loss : 2.0385239124298096\n",
            "epoch: 11, classification_loss: 1.8202072381973267, Val Loss: 70.53892183303833, loss : 2.0549814701080322\n",
            "epoch: 12, classification_loss: 1.808577299118042, Val Loss: 70.53597021102905, loss : 2.047314405441284\n",
            "epoch: 13, classification_loss: 1.8150955438613892, Val Loss: 70.49547040462494, loss : 2.0499989986419678\n",
            "epoch: 14, classification_loss: 1.8099408149719238, Val Loss: 70.495432138443, loss : 2.0392472743988037\n",
            "epoch: 15, classification_loss: 1.8171929121017456, Val Loss: 70.515291929245, loss : 2.044461488723755\n",
            "epoch: 16, classification_loss: 1.8288458585739136, Val Loss: 70.52235388755798, loss : 2.0575833320617676\n",
            "epoch: 17, classification_loss: 1.8133950233459473, Val Loss: 70.53311145305634, loss : 2.0436205863952637\n",
            "epoch: 18, classification_loss: 1.811294436454773, Val Loss: 70.53523004055023, loss : 2.0297720432281494\n",
            "epoch: 19, classification_loss: 1.822204351425171, Val Loss: 70.52146708965302, loss : 2.0465848445892334\n",
            "Batch: 155, Test Acc: 0.575020032051282\n",
            "#########################################################\n",
            "Batch: 0:\n",
            "epoch: 0, classification_loss: 1.6439354419708252, Val Loss: 70.6682699918747, loss : 1.6439354419708252\n",
            "epoch: 1, classification_loss: 1.6192638874053955, Val Loss: 71.26180601119995, loss : 1.6192638874053955\n",
            "epoch: 2, classification_loss: 1.5853192806243896, Val Loss: 72.32113981246948, loss : 1.5853192806243896\n",
            "epoch: 3, classification_loss: 1.5536937713623047, Val Loss: 73.43840277194977, loss : 1.5536937713623047\n",
            "epoch: 4, classification_loss: 1.5362335443496704, Val Loss: 74.24525022506714, loss : 1.5362335443496704\n",
            "epoch: 5, classification_loss: 1.5190421342849731, Val Loss: 74.76180219650269, loss : 1.5190421342849731\n",
            "epoch: 6, classification_loss: 1.5111151933670044, Val Loss: 74.98181426525116, loss : 1.5111151933670044\n",
            "epoch: 7, classification_loss: 1.5067522525787354, Val Loss: 74.97674798965454, loss : 1.5067522525787354\n",
            "epoch: 8, classification_loss: 1.4953985214233398, Val Loss: 74.92489421367645, loss : 1.4953985214233398\n",
            "epoch: 9, classification_loss: 1.4855178594589233, Val Loss: 74.88116896152496, loss : 1.4855178594589233\n",
            "epoch: 10, classification_loss: 1.4790949821472168, Val Loss: 74.84493243694305, loss : 1.4790949821472168\n",
            "epoch: 11, classification_loss: 1.4765326976776123, Val Loss: 74.82671558856964, loss : 1.4765326976776123\n",
            "epoch: 12, classification_loss: 1.475345492362976, Val Loss: 74.85000491142273, loss : 1.475345492362976\n",
            "epoch: 13, classification_loss: 1.471103549003601, Val Loss: 74.9342131614685, loss : 1.471103549003601\n",
            "epoch: 14, classification_loss: 1.4704430103302002, Val Loss: 75.09601867198944, loss : 1.4704430103302002\n",
            "epoch: 15, classification_loss: 1.4706690311431885, Val Loss: 75.29631412029266, loss : 1.4706690311431885\n",
            "epoch: 16, classification_loss: 1.469482183456421, Val Loss: 75.4833232164383, loss : 1.469482183456421\n",
            "epoch: 17, classification_loss: 1.469897985458374, Val Loss: 75.63143050670624, loss : 1.469897985458374\n",
            "epoch: 18, classification_loss: 1.4697823524475098, Val Loss: 75.69331526756287, loss : 1.4697823524475098\n",
            "epoch: 19, classification_loss: 1.468409776687622, Val Loss: 75.68914353847504, loss : 1.468409776687622\n",
            "Batch: 0, Test Acc: 0.49278846153846156\n",
            "Batch: 1:\n",
            "epoch: 0, classification_loss: 1.505712866783142, Val Loss: 70.73239743709564, loss : 1.505712866783142\n",
            "epoch: 1, classification_loss: 1.498724102973938, Val Loss: 70.96156203746796, loss : 1.498724102973938\n",
            "epoch: 2, classification_loss: 1.4925122261047363, Val Loss: 71.3690470457077, loss : 1.4925122261047363\n",
            "epoch: 3, classification_loss: 1.4805502891540527, Val Loss: 71.84786534309387, loss : 1.4805502891540527\n",
            "epoch: 4, classification_loss: 1.4792369604110718, Val Loss: 72.29887390136719, loss : 1.4792369604110718\n",
            "epoch: 5, classification_loss: 1.4754929542541504, Val Loss: 72.72075283527374, loss : 1.4754929542541504\n",
            "epoch: 6, classification_loss: 1.4755650758743286, Val Loss: 73.11617529392242, loss : 1.4755650758743286\n",
            "epoch: 7, classification_loss: 1.4735140800476074, Val Loss: 73.4655030965805, loss : 1.4735140800476074\n",
            "epoch: 8, classification_loss: 1.4731497764587402, Val Loss: 73.7291431427002, loss : 1.4731497764587402\n",
            "epoch: 9, classification_loss: 1.4714784622192383, Val Loss: 73.92745888233185, loss : 1.4714784622192383\n",
            "epoch: 10, classification_loss: 1.4703667163848877, Val Loss: 74.06971025466919, loss : 1.4703667163848877\n",
            "epoch: 11, classification_loss: 1.4703317880630493, Val Loss: 74.17004442214966, loss : 1.4703317880630493\n",
            "epoch: 12, classification_loss: 1.4717762470245361, Val Loss: 74.23803460597992, loss : 1.4717762470245361\n",
            "epoch: 13, classification_loss: 1.4697587490081787, Val Loss: 74.2767186164856, loss : 1.4697587490081787\n",
            "epoch: 14, classification_loss: 1.4697424173355103, Val Loss: 74.29933476448059, loss : 1.4697424173355103\n",
            "epoch: 15, classification_loss: 1.4696389436721802, Val Loss: 74.29691112041473, loss : 1.4696389436721802\n",
            "epoch: 16, classification_loss: 1.4700697660446167, Val Loss: 74.26420283317566, loss : 1.4700697660446167\n",
            "epoch: 17, classification_loss: 1.4684107303619385, Val Loss: 74.21265804767609, loss : 1.4684107303619385\n",
            "epoch: 18, classification_loss: 1.4686651229858398, Val Loss: 74.14844799041748, loss : 1.4686651229858398\n",
            "epoch: 19, classification_loss: 1.469211459159851, Val Loss: 74.07340109348297, loss : 1.469211459159851\n",
            "Batch: 1, Test Acc: 0.5309495192307693\n",
            "Batch: 2:\n",
            "epoch: 0, classification_loss: 1.648368000984192, Val Loss: 70.68885660171509, loss : 1.648368000984192\n",
            "epoch: 1, classification_loss: 1.6259324550628662, Val Loss: 70.84295630455017, loss : 1.6259324550628662\n",
            "epoch: 2, classification_loss: 1.5922255516052246, Val Loss: 71.18816471099854, loss : 1.5922255516052246\n",
            "epoch: 3, classification_loss: 1.568482518196106, Val Loss: 71.81169736385345, loss : 1.568482518196106\n",
            "epoch: 4, classification_loss: 1.5523666143417358, Val Loss: 72.63269770145416, loss : 1.5523666143417358\n",
            "epoch: 5, classification_loss: 1.538596272468567, Val Loss: 73.5454638004303, loss : 1.538596272468567\n",
            "epoch: 6, classification_loss: 1.5306950807571411, Val Loss: 74.366854429245, loss : 1.5306950807571411\n",
            "epoch: 7, classification_loss: 1.5267409086227417, Val Loss: 75.04000246524811, loss : 1.5267409086227417\n",
            "epoch: 8, classification_loss: 1.522823691368103, Val Loss: 75.57096600532532, loss : 1.522823691368103\n",
            "epoch: 9, classification_loss: 1.5155858993530273, Val Loss: 75.93763947486877, loss : 1.5155858993530273\n",
            "epoch: 10, classification_loss: 1.5108509063720703, Val Loss: 76.15015876293182, loss : 1.5108509063720703\n",
            "epoch: 11, classification_loss: 1.507738709449768, Val Loss: 76.2879011631012, loss : 1.507738709449768\n",
            "epoch: 12, classification_loss: 1.502043604850769, Val Loss: 76.32866907119751, loss : 1.502043604850769\n",
            "epoch: 13, classification_loss: 1.4981639385223389, Val Loss: 76.32193851470947, loss : 1.4981639385223389\n",
            "epoch: 14, classification_loss: 1.4918107986450195, Val Loss: 76.22905731201172, loss : 1.4918107986450195\n",
            "epoch: 15, classification_loss: 1.4876363277435303, Val Loss: 76.09132051467896, loss : 1.4876363277435303\n",
            "epoch: 16, classification_loss: 1.4864835739135742, Val Loss: 75.97638988494873, loss : 1.4864835739135742\n",
            "epoch: 17, classification_loss: 1.4834704399108887, Val Loss: 75.90219414234161, loss : 1.4834704399108887\n",
            "epoch: 18, classification_loss: 1.4785963296890259, Val Loss: 75.84526836872101, loss : 1.4785963296890259\n",
            "epoch: 19, classification_loss: 1.4763922691345215, Val Loss: 75.73608064651489, loss : 1.4763922691345215\n",
            "Batch: 2, Test Acc: 0.5019030448717948\n",
            "Batch: 3:\n",
            "epoch: 0, classification_loss: 1.6467772722244263, Val Loss: 70.67635250091553, loss : 1.6467772722244263\n",
            "epoch: 1, classification_loss: 1.632472038269043, Val Loss: 70.7992296218872, loss : 1.632472038269043\n",
            "epoch: 2, classification_loss: 1.6129814386367798, Val Loss: 71.10321021080017, loss : 1.6129814386367798\n",
            "epoch: 3, classification_loss: 1.5954378843307495, Val Loss: 71.56556963920593, loss : 1.5954378843307495\n",
            "epoch: 4, classification_loss: 1.5720583200454712, Val Loss: 72.20117139816284, loss : 1.5720583200454712\n",
            "epoch: 5, classification_loss: 1.5542625188827515, Val Loss: 72.87900733947754, loss : 1.5542625188827515\n",
            "epoch: 6, classification_loss: 1.552856683731079, Val Loss: 73.47629797458649, loss : 1.552856683731079\n",
            "epoch: 7, classification_loss: 1.5377393960952759, Val Loss: 73.99505698680878, loss : 1.5377393960952759\n",
            "epoch: 8, classification_loss: 1.5264278650283813, Val Loss: 74.40122830867767, loss : 1.5264278650283813\n",
            "epoch: 9, classification_loss: 1.5146721601486206, Val Loss: 74.70106911659241, loss : 1.5146721601486206\n",
            "epoch: 10, classification_loss: 1.5062800645828247, Val Loss: 74.9191745519638, loss : 1.5062800645828247\n",
            "epoch: 11, classification_loss: 1.4999513626098633, Val Loss: 75.0783839225769, loss : 1.4999513626098633\n",
            "epoch: 12, classification_loss: 1.4983017444610596, Val Loss: 75.17803275585175, loss : 1.4983017444610596\n",
            "epoch: 13, classification_loss: 1.494491457939148, Val Loss: 75.18319773674011, loss : 1.494491457939148\n",
            "epoch: 14, classification_loss: 1.4917806386947632, Val Loss: 75.13803458213806, loss : 1.4917806386947632\n",
            "epoch: 15, classification_loss: 1.4904152154922485, Val Loss: 75.05471897125244, loss : 1.4904152154922485\n",
            "epoch: 16, classification_loss: 1.488981008529663, Val Loss: 74.94073450565338, loss : 1.488981008529663\n",
            "epoch: 17, classification_loss: 1.4889887571334839, Val Loss: 74.77074587345123, loss : 1.4889887571334839\n",
            "epoch: 18, classification_loss: 1.4866479635238647, Val Loss: 74.60885655879974, loss : 1.4866479635238647\n",
            "epoch: 19, classification_loss: 1.4835861921310425, Val Loss: 74.486283659935, loss : 1.4835861921310425\n",
            "Batch: 3, Test Acc: 0.5256410256410257\n",
            "Batch: 4:\n",
            "epoch: 0, classification_loss: 1.6565618515014648, Val Loss: 70.83361053466797, loss : 1.6565618515014648\n",
            "epoch: 1, classification_loss: 1.6329606771469116, Val Loss: 70.68117117881775, loss : 1.7892944812774658\n",
            "epoch: 2, classification_loss: 1.650094747543335, Val Loss: 70.60731422901154, loss : 1.900078535079956\n",
            "epoch: 3, classification_loss: 1.672095775604248, Val Loss: 70.78775417804718, loss : 1.9046162366867065\n",
            "epoch: 4, classification_loss: 1.646608829498291, Val Loss: 70.77111291885376, loss : 1.898698091506958\n",
            "epoch: 5, classification_loss: 1.6377660036087036, Val Loss: 70.63903450965881, loss : 1.9518662691116333\n",
            "epoch: 6, classification_loss: 1.645362138748169, Val Loss: 70.6200623512268, loss : 1.9051246643066406\n",
            "epoch: 7, classification_loss: 1.655937671661377, Val Loss: 70.61456298828125, loss : 1.926360845565796\n",
            "epoch: 8, classification_loss: 1.656298041343689, Val Loss: 70.64708614349365, loss : 1.9071087837219238\n",
            "epoch: 9, classification_loss: 1.6467232704162598, Val Loss: 70.64340686798096, loss : 1.8981671333312988\n",
            "epoch: 10, classification_loss: 1.636038899421692, Val Loss: 70.58900022506714, loss : 1.897566795349121\n",
            "epoch: 11, classification_loss: 1.6369993686676025, Val Loss: 70.61769378185272, loss : 1.8957891464233398\n",
            "epoch: 12, classification_loss: 1.6519001722335815, Val Loss: 70.59117376804352, loss : 1.8779932260513306\n",
            "epoch: 13, classification_loss: 1.655205488204956, Val Loss: 70.59347224235535, loss : 1.9054683446884155\n",
            "epoch: 14, classification_loss: 1.644904613494873, Val Loss: 70.6048493385315, loss : 1.8707197904586792\n",
            "epoch: 15, classification_loss: 1.6504230499267578, Val Loss: 70.60975313186646, loss : 1.9028306007385254\n",
            "epoch: 16, classification_loss: 1.6440931558609009, Val Loss: 70.61033320426941, loss : 1.8753204345703125\n",
            "epoch: 17, classification_loss: 1.6457427740097046, Val Loss: 70.59768688678741, loss : 1.8915523290634155\n",
            "epoch: 18, classification_loss: 1.6398382186889648, Val Loss: 70.60973048210144, loss : 1.8659067153930664\n",
            "epoch: 19, classification_loss: 1.6548656225204468, Val Loss: 70.61471509933472, loss : 1.8818410634994507\n",
            "Batch: 4, Test Acc: 0.5752203525641025\n",
            "Batch: 5:\n",
            "epoch: 0, classification_loss: 1.753760814666748, Val Loss: 70.82849931716919, loss : 1.753760814666748\n",
            "epoch: 1, classification_loss: 1.7162717580795288, Val Loss: 70.78322494029999, loss : 2.068283796310425\n",
            "epoch: 2, classification_loss: 1.6878706216812134, Val Loss: 70.69028460979462, loss : 1.9641752243041992\n",
            "epoch: 3, classification_loss: 1.7231559753417969, Val Loss: 70.73725473880768, loss : 2.0233137607574463\n",
            "epoch: 4, classification_loss: 1.749842882156372, Val Loss: 70.6312940120697, loss : 2.0024020671844482\n",
            "epoch: 5, classification_loss: 1.7439041137695312, Val Loss: 70.66310262680054, loss : 2.0153555870056152\n",
            "epoch: 6, classification_loss: 1.713436245918274, Val Loss: 70.6773647069931, loss : 2.0102908611297607\n",
            "epoch: 7, classification_loss: 1.7287349700927734, Val Loss: 70.70451998710632, loss : 2.0251121520996094\n",
            "epoch: 8, classification_loss: 1.7404992580413818, Val Loss: 70.62196588516235, loss : 2.0058486461639404\n",
            "epoch: 9, classification_loss: 1.7379158735275269, Val Loss: 70.5865136384964, loss : 2.0136280059814453\n",
            "epoch: 10, classification_loss: 1.7368550300598145, Val Loss: 70.68089401721954, loss : 2.000308036804199\n",
            "epoch: 11, classification_loss: 1.7266497611999512, Val Loss: 70.73010385036469, loss : 1.9959354400634766\n",
            "epoch: 12, classification_loss: 1.7343831062316895, Val Loss: 70.6636290550232, loss : 1.9900848865509033\n",
            "epoch: 13, classification_loss: 1.726179838180542, Val Loss: 70.62667977809906, loss : 1.9990047216415405\n",
            "epoch: 14, classification_loss: 1.7309643030166626, Val Loss: 70.65374195575714, loss : 1.9885848760604858\n",
            "epoch: 15, classification_loss: 1.741348385810852, Val Loss: 70.62936353683472, loss : 1.9995179176330566\n",
            "epoch: 16, classification_loss: 1.730358600616455, Val Loss: 70.66203510761261, loss : 1.9724724292755127\n",
            "epoch: 17, classification_loss: 1.7299288511276245, Val Loss: 70.70586335659027, loss : 1.9859007596969604\n",
            "epoch: 18, classification_loss: 1.7383698225021362, Val Loss: 70.6624116897583, loss : 1.9846214056015015\n",
            "epoch: 19, classification_loss: 1.7364212274551392, Val Loss: 70.65116798877716, loss : 1.9938055276870728\n",
            "Batch: 5, Test Acc: 0.5738181089743589\n",
            "Batch: 6:\n",
            "epoch: 0, classification_loss: 1.7773323059082031, Val Loss: 70.75549149513245, loss : 1.7773323059082031\n",
            "epoch: 1, classification_loss: 1.7468353509902954, Val Loss: 70.6705607175827, loss : 2.0651347637176514\n",
            "epoch: 2, classification_loss: 1.7408428192138672, Val Loss: 70.61870241165161, loss : 1.9635521173477173\n",
            "epoch: 3, classification_loss: 1.7740634679794312, Val Loss: 70.61192882061005, loss : 2.0462963581085205\n",
            "epoch: 4, classification_loss: 1.7728127241134644, Val Loss: 70.58104467391968, loss : 1.999514102935791\n",
            "epoch: 5, classification_loss: 1.7567936182022095, Val Loss: 70.67061913013458, loss : 1.7567936182022095\n",
            "epoch: 6, classification_loss: 1.7231385707855225, Val Loss: 70.7356983423233, loss : 2.1439220905303955\n",
            "epoch: 7, classification_loss: 1.7016645669937134, Val Loss: 70.6448757648468, loss : 2.0786452293395996\n",
            "epoch: 8, classification_loss: 1.7240723371505737, Val Loss: 70.53113842010498, loss : 2.0056867599487305\n",
            "epoch: 9, classification_loss: 1.7655025720596313, Val Loss: 70.6240404844284, loss : 2.09578800201416\n",
            "epoch: 10, classification_loss: 1.770465612411499, Val Loss: 70.73432958126068, loss : 2.0485217571258545\n",
            "epoch: 11, classification_loss: 1.7341853380203247, Val Loss: 70.56761515140533, loss : 2.0396018028259277\n",
            "epoch: 12, classification_loss: 1.7208718061447144, Val Loss: 70.56672668457031, loss : 2.0500242710113525\n",
            "epoch: 13, classification_loss: 1.7446452379226685, Val Loss: 70.52497863769531, loss : 2.041555643081665\n",
            "epoch: 14, classification_loss: 1.7551277875900269, Val Loss: 70.65234971046448, loss : 2.0171003341674805\n",
            "epoch: 15, classification_loss: 1.7449254989624023, Val Loss: 70.57462406158447, loss : 2.0272533893585205\n",
            "epoch: 16, classification_loss: 1.7367801666259766, Val Loss: 70.53368401527405, loss : 2.0016372203826904\n",
            "epoch: 17, classification_loss: 1.728576898574829, Val Loss: 70.55107927322388, loss : 1.9943418502807617\n",
            "epoch: 18, classification_loss: 1.7361857891082764, Val Loss: 70.5596376657486, loss : 1.9950668811798096\n",
            "epoch: 19, classification_loss: 1.7395943403244019, Val Loss: 70.57498705387115, loss : 1.993665337562561\n",
            "Batch: 6, Test Acc: 0.5737179487179487\n",
            "Batch: 7:\n",
            "epoch: 0, classification_loss: 1.785927414894104, Val Loss: 70.76446497440338, loss : 1.785927414894104\n",
            "epoch: 1, classification_loss: 1.7507951259613037, Val Loss: 70.7437961101532, loss : 2.112752914428711\n",
            "epoch: 2, classification_loss: 1.7248785495758057, Val Loss: 70.67165696620941, loss : 2.0176596641540527\n",
            "epoch: 3, classification_loss: 1.7549874782562256, Val Loss: 70.56386649608612, loss : 2.0469300746917725\n",
            "epoch: 4, classification_loss: 1.7800639867782593, Val Loss: 70.61702191829681, loss : 2.058321237564087\n",
            "epoch: 5, classification_loss: 1.769673466682434, Val Loss: 70.65963637828827, loss : 2.019770383834839\n",
            "epoch: 6, classification_loss: 1.7564939260482788, Val Loss: 70.66592073440552, loss : 2.047607421875\n",
            "epoch: 7, classification_loss: 1.7439560890197754, Val Loss: 70.6710512638092, loss : 2.0414445400238037\n",
            "epoch: 8, classification_loss: 1.7462034225463867, Val Loss: 70.6161036491394, loss : 2.0130841732025146\n",
            "epoch: 9, classification_loss: 1.7630326747894287, Val Loss: 70.5983316898346, loss : 2.0359315872192383\n",
            "epoch: 10, classification_loss: 1.7699815034866333, Val Loss: 70.57250213623047, loss : 2.0221023559570312\n",
            "epoch: 11, classification_loss: 1.7544187307357788, Val Loss: 70.57442808151245, loss : 2.0200071334838867\n",
            "epoch: 12, classification_loss: 1.7587783336639404, Val Loss: 70.61757743358612, loss : 2.015268325805664\n",
            "epoch: 13, classification_loss: 1.7580958604812622, Val Loss: 70.64623761177063, loss : 2.0162088871002197\n",
            "epoch: 14, classification_loss: 1.757804274559021, Val Loss: 70.59487581253052, loss : 2.0124382972717285\n",
            "epoch: 15, classification_loss: 1.7591631412506104, Val Loss: 70.56774294376373, loss : 2.0112171173095703\n",
            "epoch: 16, classification_loss: 1.7633986473083496, Val Loss: 70.54525005817413, loss : 2.0067293643951416\n",
            "epoch: 17, classification_loss: 1.759236216545105, Val Loss: 70.56864881515503, loss : 2.00834584236145\n",
            "epoch: 18, classification_loss: 1.7509859800338745, Val Loss: 70.58802247047424, loss : 1.9953670501708984\n",
            "epoch: 19, classification_loss: 1.759447455406189, Val Loss: 70.59984970092773, loss : 2.006051540374756\n",
            "Batch: 7, Test Acc: 0.5754206730769231\n",
            "Batch: 8:\n",
            "epoch: 0, classification_loss: 1.784775733947754, Val Loss: 70.61456441879272, loss : 1.784775733947754\n",
            "epoch: 1, classification_loss: 1.746153473854065, Val Loss: 70.6628897190094, loss : 2.0868523120880127\n",
            "epoch: 2, classification_loss: 1.7206311225891113, Val Loss: 70.58947825431824, loss : 1.99030601978302\n",
            "epoch: 3, classification_loss: 1.7670040130615234, Val Loss: 70.55613243579865, loss : 2.042151689529419\n",
            "epoch: 4, classification_loss: 1.7876954078674316, Val Loss: 70.58136141300201, loss : 2.0283637046813965\n",
            "epoch: 5, classification_loss: 1.773186445236206, Val Loss: 70.60030937194824, loss : 2.0451505184173584\n",
            "epoch: 6, classification_loss: 1.7559285163879395, Val Loss: 70.56723630428314, loss : 2.031264305114746\n",
            "epoch: 7, classification_loss: 1.758159875869751, Val Loss: 70.54315757751465, loss : 2.0154359340667725\n",
            "epoch: 8, classification_loss: 1.7594517469406128, Val Loss: 70.5638530254364, loss : 2.024296760559082\n",
            "epoch: 9, classification_loss: 1.7638344764709473, Val Loss: 70.5028840303421, loss : 2.017380952835083\n",
            "epoch: 10, classification_loss: 1.7706266641616821, Val Loss: 70.56855535507202, loss : 1.7706266641616821\n",
            "epoch: 11, classification_loss: 1.7461392879486084, Val Loss: 70.60860204696655, loss : 2.0850212574005127\n",
            "epoch: 12, classification_loss: 1.7307733297348022, Val Loss: 70.53054392337799, loss : 1.9840240478515625\n",
            "epoch: 13, classification_loss: 1.763357162475586, Val Loss: 70.50961816310883, loss : 2.0325145721435547\n",
            "epoch: 14, classification_loss: 1.7689117193222046, Val Loss: 70.53544676303864, loss : 2.0111522674560547\n",
            "epoch: 15, classification_loss: 1.7421784400939941, Val Loss: 70.57702600955963, loss : 2.0026426315307617\n",
            "epoch: 16, classification_loss: 1.7362940311431885, Val Loss: 70.52250039577484, loss : 2.022083282470703\n",
            "epoch: 17, classification_loss: 1.7620915174484253, Val Loss: 70.51473689079285, loss : 2.0040690898895264\n",
            "epoch: 18, classification_loss: 1.7659869194030762, Val Loss: 70.54250466823578, loss : 2.008662462234497\n",
            "epoch: 19, classification_loss: 1.7463397979736328, Val Loss: 70.54171657562256, loss : 1.9919211864471436\n",
            "Batch: 8, Test Acc: 0.5772235576923077\n",
            "Batch: 9:\n",
            "epoch: 0, classification_loss: 1.8348878622055054, Val Loss: 70.52736330032349, loss : 1.8348878622055054\n",
            "epoch: 1, classification_loss: 1.797702431678772, Val Loss: 70.55232548713684, loss : 2.145158290863037\n",
            "epoch: 2, classification_loss: 1.7803698778152466, Val Loss: 70.59287559986115, loss : 2.064610719680786\n",
            "epoch: 3, classification_loss: 1.8024232387542725, Val Loss: 70.59555232524872, loss : 2.0903191566467285\n",
            "epoch: 4, classification_loss: 1.8193590641021729, Val Loss: 70.51980149745941, loss : 2.0785465240478516\n",
            "epoch: 5, classification_loss: 1.8148822784423828, Val Loss: 70.53098678588867, loss : 2.0711307525634766\n",
            "epoch: 6, classification_loss: 1.813093900680542, Val Loss: 70.54837012290955, loss : 2.0767855644226074\n",
            "epoch: 7, classification_loss: 1.8090695142745972, Val Loss: 70.59580790996552, loss : 2.076556921005249\n",
            "epoch: 8, classification_loss: 1.804542064666748, Val Loss: 70.58417344093323, loss : 2.0557727813720703\n",
            "epoch: 9, classification_loss: 1.8129972219467163, Val Loss: 70.55324459075928, loss : 2.084582567214966\n",
            "epoch: 10, classification_loss: 1.8134244680404663, Val Loss: 70.5683958530426, loss : 2.064216375350952\n",
            "epoch: 11, classification_loss: 1.822709321975708, Val Loss: 70.554248213768, loss : 2.0859642028808594\n",
            "epoch: 12, classification_loss: 1.8108363151550293, Val Loss: 70.60063588619232, loss : 2.0625147819519043\n",
            "epoch: 13, classification_loss: 1.8036184310913086, Val Loss: 70.57931697368622, loss : 2.064105749130249\n",
            "epoch: 14, classification_loss: 1.8131227493286133, Val Loss: 70.53901040554047, loss : 2.0633630752563477\n",
            "epoch: 15, classification_loss: 1.8119012117385864, Val Loss: 70.56214582920074, loss : 2.062655448913574\n",
            "epoch: 16, classification_loss: 1.8131409883499146, Val Loss: 70.59656620025635, loss : 2.0566177368164062\n",
            "epoch: 17, classification_loss: 1.813665509223938, Val Loss: 70.57142627239227, loss : 2.0582520961761475\n",
            "epoch: 18, classification_loss: 1.8131448030471802, Val Loss: 70.56476354598999, loss : 2.046156167984009\n",
            "epoch: 19, classification_loss: 1.8192110061645508, Val Loss: 70.61613869667053, loss : 2.0640869140625\n",
            "Batch: 9, Test Acc: 0.5758213141025641\n",
            "Batch: 10:\n",
            "epoch: 0, classification_loss: 1.7555694580078125, Val Loss: 70.59494507312775, loss : 1.7555694580078125\n",
            "epoch: 1, classification_loss: 1.719975233078003, Val Loss: 70.54584169387817, loss : 2.0523288249969482\n",
            "epoch: 2, classification_loss: 1.7109185457229614, Val Loss: 70.52438616752625, loss : 1.965183973312378\n",
            "epoch: 3, classification_loss: 1.7328897714614868, Val Loss: 70.50283193588257, loss : 2.0100157260894775\n",
            "epoch: 4, classification_loss: 1.7613763809204102, Val Loss: 70.60718131065369, loss : 2.0014405250549316\n",
            "epoch: 5, classification_loss: 1.749598741531372, Val Loss: 70.52827548980713, loss : 2.0209972858428955\n",
            "epoch: 6, classification_loss: 1.7173759937286377, Val Loss: 70.50962173938751, loss : 2.0115582942962646\n",
            "epoch: 7, classification_loss: 1.735892415046692, Val Loss: 70.52064430713654, loss : 2.0033977031707764\n",
            "epoch: 8, classification_loss: 1.738215446472168, Val Loss: 70.52207779884338, loss : 1.9949463605880737\n",
            "epoch: 9, classification_loss: 1.7400219440460205, Val Loss: 70.49630963802338, loss : 1.999484896659851\n",
            "epoch: 10, classification_loss: 1.7240185737609863, Val Loss: 70.49118220806122, loss : 1.9737211465835571\n",
            "epoch: 11, classification_loss: 1.7348387241363525, Val Loss: 70.54211401939392, loss : 1.9889761209487915\n",
            "epoch: 12, classification_loss: 1.7452011108398438, Val Loss: 70.529421210289, loss : 1.9864232540130615\n",
            "epoch: 13, classification_loss: 1.7344740629196167, Val Loss: 70.5901391506195, loss : 1.9936045408248901\n",
            "epoch: 14, classification_loss: 1.7431857585906982, Val Loss: 70.5398360490799, loss : 1.995229721069336\n",
            "epoch: 15, classification_loss: 1.7397924661636353, Val Loss: 70.50407028198242, loss : 1.9911181926727295\n",
            "epoch: 16, classification_loss: 1.7434858083724976, Val Loss: 70.57609176635742, loss : 1.9830963611602783\n",
            "epoch: 17, classification_loss: 1.7449711561203003, Val Loss: 70.5295295715332, loss : 1.9989194869995117\n",
            "epoch: 18, classification_loss: 1.7491086721420288, Val Loss: 70.53227198123932, loss : 1.9898515939712524\n",
            "epoch: 19, classification_loss: 1.7549192905426025, Val Loss: 70.52001225948334, loss : 1.9998104572296143\n",
            "Batch: 10, Test Acc: 0.5740184294871795\n",
            "Batch: 11:\n",
            "epoch: 0, classification_loss: 1.8108153343200684, Val Loss: 70.55241692066193, loss : 1.8108153343200684\n",
            "epoch: 1, classification_loss: 1.777338981628418, Val Loss: 70.5847430229187, loss : 2.1136653423309326\n",
            "epoch: 2, classification_loss: 1.766048789024353, Val Loss: 70.57316648960114, loss : 2.0107595920562744\n",
            "epoch: 3, classification_loss: 1.7963331937789917, Val Loss: 70.53225982189178, loss : 2.0750975608825684\n",
            "epoch: 4, classification_loss: 1.804991602897644, Val Loss: 70.56114661693573, loss : 2.0482676029205322\n",
            "epoch: 5, classification_loss: 1.791878342628479, Val Loss: 70.58888804912567, loss : 2.060523509979248\n",
            "epoch: 6, classification_loss: 1.7742204666137695, Val Loss: 70.55039763450623, loss : 2.0488810539245605\n",
            "epoch: 7, classification_loss: 1.7985069751739502, Val Loss: 70.53481793403625, loss : 2.058324098587036\n",
            "epoch: 8, classification_loss: 1.796979308128357, Val Loss: 70.58554136753082, loss : 2.047173023223877\n",
            "epoch: 9, classification_loss: 1.8094550371170044, Val Loss: 70.51890313625336, loss : 2.057243824005127\n",
            "epoch: 10, classification_loss: 1.7892906665802002, Val Loss: 70.5234569311142, loss : 2.03993558883667\n",
            "epoch: 11, classification_loss: 1.7914458513259888, Val Loss: 70.55999481678009, loss : 2.039274215698242\n",
            "epoch: 12, classification_loss: 1.7848258018493652, Val Loss: 70.57217979431152, loss : 2.0218276977539062\n",
            "epoch: 13, classification_loss: 1.787803292274475, Val Loss: 70.54612970352173, loss : 2.0421411991119385\n",
            "epoch: 14, classification_loss: 1.7886539697647095, Val Loss: 70.54689085483551, loss : 2.027282238006592\n",
            "epoch: 15, classification_loss: 1.7903112173080444, Val Loss: 70.55001628398895, loss : 2.0337581634521484\n",
            "epoch: 16, classification_loss: 1.8008997440338135, Val Loss: 70.56026113033295, loss : 2.0361812114715576\n",
            "epoch: 17, classification_loss: 1.793351650238037, Val Loss: 70.55032026767731, loss : 2.0400171279907227\n",
            "epoch: 18, classification_loss: 1.7881320714950562, Val Loss: 70.55698299407959, loss : 2.022554874420166\n",
            "epoch: 19, classification_loss: 1.7898355722427368, Val Loss: 70.56204855442047, loss : 2.0342564582824707\n",
            "Batch: 11, Test Acc: 0.5738181089743589\n",
            "Batch: 12:\n",
            "epoch: 0, classification_loss: 1.8345813751220703, Val Loss: 70.498281955719, loss : 1.8345813751220703\n",
            "epoch: 1, classification_loss: 1.7826756238937378, Val Loss: 70.52297246456146, loss : 2.121880054473877\n",
            "epoch: 2, classification_loss: 1.783018708229065, Val Loss: 70.5532603263855, loss : 2.0520405769348145\n",
            "epoch: 3, classification_loss: 1.7931885719299316, Val Loss: 70.54678249359131, loss : 2.085634231567383\n",
            "epoch: 4, classification_loss: 1.8142242431640625, Val Loss: 70.48929238319397, loss : 2.080352783203125\n",
            "epoch: 5, classification_loss: 1.8160465955734253, Val Loss: 70.49565720558167, loss : 2.0813159942626953\n",
            "epoch: 6, classification_loss: 1.8178342580795288, Val Loss: 70.51568758487701, loss : 2.0830023288726807\n",
            "epoch: 7, classification_loss: 1.8127849102020264, Val Loss: 70.5181086063385, loss : 2.084050178527832\n",
            "epoch: 8, classification_loss: 1.8097254037857056, Val Loss: 70.54939377307892, loss : 2.0715794563293457\n",
            "epoch: 9, classification_loss: 1.8014500141143799, Val Loss: 70.52333617210388, loss : 2.073969841003418\n",
            "epoch: 10, classification_loss: 1.8118358850479126, Val Loss: 70.51032710075378, loss : 2.0719878673553467\n",
            "epoch: 11, classification_loss: 1.8148880004882812, Val Loss: 70.51059186458588, loss : 2.064056634902954\n",
            "epoch: 12, classification_loss: 1.81205153465271, Val Loss: 70.50711822509766, loss : 2.058831214904785\n",
            "epoch: 13, classification_loss: 1.8098702430725098, Val Loss: 70.48461174964905, loss : 2.065624713897705\n",
            "epoch: 14, classification_loss: 1.8100285530090332, Val Loss: 70.58487021923065, loss : 2.0616953372955322\n",
            "epoch: 15, classification_loss: 1.810977578163147, Val Loss: 70.57202517986298, loss : 2.0744707584381104\n",
            "epoch: 16, classification_loss: 1.7984987497329712, Val Loss: 70.56859600543976, loss : 2.0495853424072266\n",
            "epoch: 17, classification_loss: 1.8128571510314941, Val Loss: 70.52559781074524, loss : 2.073739528656006\n",
            "epoch: 18, classification_loss: 1.8070377111434937, Val Loss: 70.53825974464417, loss : 2.045123815536499\n",
            "epoch: 19, classification_loss: 1.8167859315872192, Val Loss: 70.54241693019867, loss : 2.069551706314087\n",
            "Batch: 12, Test Acc: 0.5751201923076923\n",
            "Batch: 13:\n",
            "epoch: 0, classification_loss: 1.7571145296096802, Val Loss: 70.55804014205933, loss : 1.7571145296096802\n",
            "epoch: 1, classification_loss: 1.7176766395568848, Val Loss: 70.53681087493896, loss : 2.040369987487793\n",
            "epoch: 2, classification_loss: 1.70539128780365, Val Loss: 70.53818237781525, loss : 1.9508169889450073\n",
            "epoch: 3, classification_loss: 1.7361714839935303, Val Loss: 70.55944693088531, loss : 2.00053334236145\n",
            "epoch: 4, classification_loss: 1.7576578855514526, Val Loss: 70.52319598197937, loss : 1.9770987033843994\n",
            "epoch: 5, classification_loss: 1.7422903776168823, Val Loss: 70.51944243907928, loss : 2.019592046737671\n",
            "epoch: 6, classification_loss: 1.7229106426239014, Val Loss: 70.5338203907013, loss : 2.010164976119995\n",
            "epoch: 7, classification_loss: 1.7390297651290894, Val Loss: 70.55916142463684, loss : 1.9920566082000732\n",
            "epoch: 8, classification_loss: 1.7472560405731201, Val Loss: 70.54819595813751, loss : 1.997877597808838\n",
            "epoch: 9, classification_loss: 1.7396774291992188, Val Loss: 70.52857613563538, loss : 1.9854495525360107\n",
            "epoch: 10, classification_loss: 1.7302535772323608, Val Loss: 70.49660658836365, loss : 1.9782285690307617\n",
            "epoch: 11, classification_loss: 1.7346669435501099, Val Loss: 70.51481425762177, loss : 1.9892644882202148\n",
            "epoch: 12, classification_loss: 1.7323451042175293, Val Loss: 70.55818617343903, loss : 1.9738870859146118\n",
            "epoch: 13, classification_loss: 1.7371842861175537, Val Loss: 70.51834404468536, loss : 1.9791587591171265\n",
            "epoch: 14, classification_loss: 1.7363682985305786, Val Loss: 70.52899324893951, loss : 1.9704320430755615\n",
            "epoch: 15, classification_loss: 1.740938425064087, Val Loss: 70.58231246471405, loss : 1.9770615100860596\n",
            "epoch: 16, classification_loss: 1.7390773296356201, Val Loss: 70.5475367307663, loss : 1.9673939943313599\n",
            "epoch: 17, classification_loss: 1.7340376377105713, Val Loss: 70.55134081840515, loss : 1.9697414636611938\n",
            "epoch: 18, classification_loss: 1.7310336828231812, Val Loss: 70.53356099128723, loss : 1.955188274383545\n",
            "epoch: 19, classification_loss: 1.7320104837417603, Val Loss: 70.52869200706482, loss : 1.9676434993743896\n",
            "Batch: 13, Test Acc: 0.5740184294871795\n",
            "Batch: 14:\n",
            "epoch: 0, classification_loss: 1.795117735862732, Val Loss: 70.63289368152618, loss : 1.795117735862732\n",
            "epoch: 1, classification_loss: 1.7552891969680786, Val Loss: 70.63474559783936, loss : 2.089325428009033\n",
            "epoch: 2, classification_loss: 1.734583854675293, Val Loss: 70.52144503593445, loss : 2.0010547637939453\n",
            "epoch: 3, classification_loss: 1.7673381567001343, Val Loss: 70.4662412405014, loss : 2.037874698638916\n",
            "epoch: 4, classification_loss: 1.7910408973693848, Val Loss: 70.55724930763245, loss : 1.7910408973693848\n",
            "epoch: 5, classification_loss: 1.7643831968307495, Val Loss: 70.49797546863556, loss : 2.0708627700805664\n",
            "epoch: 6, classification_loss: 1.7497648000717163, Val Loss: 70.51006150245667, loss : 1.9527108669281006\n",
            "epoch: 7, classification_loss: 1.7630739212036133, Val Loss: 70.4864217042923, loss : 2.0610413551330566\n",
            "epoch: 8, classification_loss: 1.763375163078308, Val Loss: 70.47448718547821, loss : 1.9922535419464111\n",
            "epoch: 9, classification_loss: 1.7534691095352173, Val Loss: 70.47676801681519, loss : 2.069291114807129\n",
            "epoch: 10, classification_loss: 1.7590264081954956, Val Loss: 70.54500734806061, loss : 2.0582242012023926\n",
            "epoch: 11, classification_loss: 1.7573871612548828, Val Loss: 70.488241314888, loss : 2.023942470550537\n",
            "epoch: 12, classification_loss: 1.780907392501831, Val Loss: 70.49206328392029, loss : 2.0597798824310303\n",
            "epoch: 13, classification_loss: 1.761063575744629, Val Loss: 70.51648116111755, loss : 2.0503973960876465\n",
            "epoch: 14, classification_loss: 1.7488117218017578, Val Loss: 70.56509637832642, loss : 2.0320751667022705\n",
            "epoch: 15, classification_loss: 1.7743161916732788, Val Loss: 70.47585380077362, loss : 2.0463647842407227\n",
            "epoch: 16, classification_loss: 1.7692029476165771, Val Loss: 70.47878968715668, loss : 2.030866861343384\n",
            "epoch: 17, classification_loss: 1.7616748809814453, Val Loss: 70.4728467464447, loss : 2.036520481109619\n",
            "epoch: 18, classification_loss: 1.758261799812317, Val Loss: 70.48462975025177, loss : 2.0257697105407715\n",
            "epoch: 19, classification_loss: 1.75330650806427, Val Loss: 70.53050065040588, loss : 2.01143741607666\n",
            "Batch: 14, Test Acc: 0.5740184294871795\n",
            "Batch: 15:\n",
            "epoch: 0, classification_loss: 1.7908505201339722, Val Loss: 70.53972685337067, loss : 1.7908505201339722\n",
            "epoch: 1, classification_loss: 1.735276460647583, Val Loss: 70.52721178531647, loss : 2.0819592475891113\n",
            "epoch: 2, classification_loss: 1.7291450500488281, Val Loss: 70.53164339065552, loss : 2.005849838256836\n",
            "epoch: 3, classification_loss: 1.7620680332183838, Val Loss: 70.55105769634247, loss : 2.0635735988616943\n",
            "epoch: 4, classification_loss: 1.7837311029434204, Val Loss: 70.5176852941513, loss : 2.0500824451446533\n",
            "epoch: 5, classification_loss: 1.7758709192276, Val Loss: 70.54676711559296, loss : 2.0526928901672363\n",
            "epoch: 6, classification_loss: 1.7558515071868896, Val Loss: 70.52344739437103, loss : 2.0536434650421143\n",
            "epoch: 7, classification_loss: 1.7537847757339478, Val Loss: 70.56132519245148, loss : 2.0275723934173584\n",
            "epoch: 8, classification_loss: 1.7703205347061157, Val Loss: 70.55715084075928, loss : 2.047970771789551\n",
            "epoch: 9, classification_loss: 1.75629460811615, Val Loss: 70.52136301994324, loss : 2.0311813354492188\n",
            "epoch: 10, classification_loss: 1.76995050907135, Val Loss: 70.53451383113861, loss : 2.0377814769744873\n",
            "epoch: 11, classification_loss: 1.7629579305648804, Val Loss: 70.57972764968872, loss : 2.023418426513672\n",
            "epoch: 12, classification_loss: 1.7560125589370728, Val Loss: 70.54315638542175, loss : 2.0050253868103027\n",
            "epoch: 13, classification_loss: 1.7680063247680664, Val Loss: 70.55416536331177, loss : 2.026132822036743\n",
            "epoch: 14, classification_loss: 1.7662945985794067, Val Loss: 70.57845044136047, loss : 2.022385597229004\n",
            "epoch: 15, classification_loss: 1.7488497495651245, Val Loss: 70.58375811576843, loss : 2.008068561553955\n",
            "epoch: 16, classification_loss: 1.7769999504089355, Val Loss: 70.5567318201065, loss : 2.023545980453491\n",
            "epoch: 17, classification_loss: 1.7611855268478394, Val Loss: 70.51963067054749, loss : 2.0163419246673584\n",
            "epoch: 18, classification_loss: 1.76133394241333, Val Loss: 70.62978851795197, loss : 2.0072531700134277\n",
            "epoch: 19, classification_loss: 1.7668036222457886, Val Loss: 70.59892809391022, loss : 2.011430501937866\n",
            "Batch: 15, Test Acc: 0.5747195512820513\n",
            "Batch: 16:\n",
            "epoch: 0, classification_loss: 1.7875906229019165, Val Loss: 70.45984625816345, loss : 1.7875906229019165\n",
            "epoch: 1, classification_loss: 1.7693365812301636, Val Loss: 70.47784519195557, loss : 2.1152868270874023\n",
            "epoch: 2, classification_loss: 1.7393769025802612, Val Loss: 70.52094531059265, loss : 2.0217158794403076\n",
            "epoch: 3, classification_loss: 1.7649033069610596, Val Loss: 70.52595829963684, loss : 2.0569534301757812\n",
            "epoch: 4, classification_loss: 1.7932932376861572, Val Loss: 70.48357939720154, loss : 2.050912618637085\n",
            "epoch: 5, classification_loss: 1.7773264646530151, Val Loss: 70.48476839065552, loss : 2.042879581451416\n",
            "epoch: 6, classification_loss: 1.7720043659210205, Val Loss: 70.50770688056946, loss : 2.046333074569702\n",
            "epoch: 7, classification_loss: 1.7657631635665894, Val Loss: 70.56156206130981, loss : 2.034156084060669\n",
            "epoch: 8, classification_loss: 1.7745763063430786, Val Loss: 70.51481235027313, loss : 2.0367069244384766\n",
            "epoch: 9, classification_loss: 1.7725353240966797, Val Loss: 70.5466628074646, loss : 2.0468099117279053\n",
            "epoch: 10, classification_loss: 1.7754621505737305, Val Loss: 70.55822432041168, loss : 2.0341947078704834\n",
            "epoch: 11, classification_loss: 1.7703522443771362, Val Loss: 70.51801538467407, loss : 2.0303053855895996\n",
            "epoch: 12, classification_loss: 1.775483250617981, Val Loss: 70.53014028072357, loss : 2.0208895206451416\n",
            "epoch: 13, classification_loss: 1.777446985244751, Val Loss: 70.51930093765259, loss : 2.0385823249816895\n",
            "epoch: 14, classification_loss: 1.7825654745101929, Val Loss: 70.54051113128662, loss : 2.02204966545105\n",
            "epoch: 15, classification_loss: 1.7714380025863647, Val Loss: 70.54731726646423, loss : 2.0230398178100586\n",
            "epoch: 16, classification_loss: 1.782997488975525, Val Loss: 70.53388285636902, loss : 2.0249295234680176\n",
            "epoch: 17, classification_loss: 1.7759805917739868, Val Loss: 70.53513622283936, loss : 2.0265278816223145\n",
            "epoch: 18, classification_loss: 1.7688910961151123, Val Loss: 70.55616235733032, loss : 2.010934591293335\n",
            "epoch: 19, classification_loss: 1.778069019317627, Val Loss: 70.54024827480316, loss : 2.0299832820892334\n",
            "Batch: 16, Test Acc: 0.5743189102564102\n",
            "Batch: 17:\n",
            "epoch: 0, classification_loss: 1.8163806200027466, Val Loss: 70.63481676578522, loss : 1.8163806200027466\n",
            "epoch: 1, classification_loss: 1.7792117595672607, Val Loss: 70.61129140853882, loss : 2.118217706680298\n",
            "epoch: 2, classification_loss: 1.746013879776001, Val Loss: 70.59656167030334, loss : 2.0270421504974365\n",
            "epoch: 3, classification_loss: 1.782090187072754, Val Loss: 70.55166530609131, loss : 2.0743751525878906\n",
            "epoch: 4, classification_loss: 1.798789143562317, Val Loss: 70.51346111297607, loss : 2.051788330078125\n",
            "epoch: 5, classification_loss: 1.7906856536865234, Val Loss: 70.53426277637482, loss : 2.055434226989746\n",
            "epoch: 6, classification_loss: 1.7785561084747314, Val Loss: 70.52800285816193, loss : 2.051658868789673\n",
            "epoch: 7, classification_loss: 1.7852948904037476, Val Loss: 70.53391492366791, loss : 2.054173469543457\n",
            "epoch: 8, classification_loss: 1.7898104190826416, Val Loss: 70.5297315120697, loss : 2.0532138347625732\n",
            "epoch: 9, classification_loss: 1.7858532667160034, Val Loss: 70.53912544250488, loss : 2.058637857437134\n",
            "epoch: 10, classification_loss: 1.7786247730255127, Val Loss: 70.60707986354828, loss : 2.040395736694336\n",
            "epoch: 11, classification_loss: 1.7986514568328857, Val Loss: 70.56751823425293, loss : 2.0566656589508057\n",
            "epoch: 12, classification_loss: 1.788073182106018, Val Loss: 70.53072094917297, loss : 2.045048713684082\n",
            "epoch: 13, classification_loss: 1.7894880771636963, Val Loss: 70.54840302467346, loss : 2.0535941123962402\n",
            "epoch: 14, classification_loss: 1.7740434408187866, Val Loss: 70.60071277618408, loss : 2.03230619430542\n",
            "epoch: 15, classification_loss: 1.795936107635498, Val Loss: 70.57564067840576, loss : 2.0483601093292236\n",
            "epoch: 16, classification_loss: 1.795875906944275, Val Loss: 70.55568325519562, loss : 2.0442020893096924\n",
            "epoch: 17, classification_loss: 1.7896794080734253, Val Loss: 70.5497533082962, loss : 2.045003890991211\n",
            "epoch: 18, classification_loss: 1.7862489223480225, Val Loss: 70.60524570941925, loss : 2.0305309295654297\n",
            "epoch: 19, classification_loss: 1.7915775775909424, Val Loss: 70.63482224941254, loss : 2.0430381298065186\n",
            "Batch: 17, Test Acc: 0.5730168269230769\n",
            "Batch: 18:\n",
            "epoch: 0, classification_loss: 1.7980760335922241, Val Loss: 70.5191091299057, loss : 1.7980760335922241\n",
            "epoch: 1, classification_loss: 1.7619504928588867, Val Loss: 70.54217219352722, loss : 2.096165180206299\n",
            "epoch: 2, classification_loss: 1.7461681365966797, Val Loss: 70.51173281669617, loss : 2.0120646953582764\n",
            "epoch: 3, classification_loss: 1.787156343460083, Val Loss: 70.52851021289825, loss : 2.0606284141540527\n",
            "epoch: 4, classification_loss: 1.8015763759613037, Val Loss: 70.48910021781921, loss : 2.0382394790649414\n",
            "epoch: 5, classification_loss: 1.7818282842636108, Val Loss: 70.4719066619873, loss : 2.043046236038208\n",
            "epoch: 6, classification_loss: 1.7752389907836914, Val Loss: 70.51908266544342, loss : 2.041360855102539\n",
            "epoch: 7, classification_loss: 1.7773300409317017, Val Loss: 70.5121887922287, loss : 2.0392847061157227\n",
            "epoch: 8, classification_loss: 1.7764520645141602, Val Loss: 70.47760283946991, loss : 2.0316028594970703\n",
            "epoch: 9, classification_loss: 1.779538631439209, Val Loss: 70.58377528190613, loss : 2.0344154834747314\n",
            "epoch: 10, classification_loss: 1.7772161960601807, Val Loss: 70.54150676727295, loss : 2.0308351516723633\n",
            "epoch: 11, classification_loss: 1.7829108238220215, Val Loss: 70.51453411579132, loss : 2.0306758880615234\n",
            "epoch: 12, classification_loss: 1.791327714920044, Val Loss: 70.51426017284393, loss : 2.0364294052124023\n",
            "epoch: 13, classification_loss: 1.7744823694229126, Val Loss: 70.5400937795639, loss : 2.0330851078033447\n",
            "epoch: 14, classification_loss: 1.7784589529037476, Val Loss: 70.50772833824158, loss : 2.0194454193115234\n",
            "epoch: 15, classification_loss: 1.7902346849441528, Val Loss: 70.5032308101654, loss : 2.0323638916015625\n",
            "epoch: 16, classification_loss: 1.7831228971481323, Val Loss: 70.51784980297089, loss : 2.0197465419769287\n",
            "epoch: 17, classification_loss: 1.7782602310180664, Val Loss: 70.54776751995087, loss : 2.0244133472442627\n",
            "epoch: 18, classification_loss: 1.7763385772705078, Val Loss: 70.49762308597565, loss : 2.0192039012908936\n",
            "epoch: 19, classification_loss: 1.7679105997085571, Val Loss: 70.50878262519836, loss : 2.0106077194213867\n",
            "Batch: 18, Test Acc: 0.5762219551282052\n",
            "Batch: 19:\n",
            "epoch: 0, classification_loss: 1.816551685333252, Val Loss: 70.58482074737549, loss : 1.816551685333252\n",
            "epoch: 1, classification_loss: 1.771027684211731, Val Loss: 70.61085796356201, loss : 2.118621826171875\n",
            "epoch: 2, classification_loss: 1.743080496788025, Val Loss: 70.49484550952911, loss : 2.0320305824279785\n",
            "epoch: 3, classification_loss: 1.7690179347991943, Val Loss: 70.49411225318909, loss : 2.0559773445129395\n",
            "epoch: 4, classification_loss: 1.8019068241119385, Val Loss: 70.52674508094788, loss : 2.0598442554473877\n",
            "epoch: 5, classification_loss: 1.7787100076675415, Val Loss: 70.5114004611969, loss : 2.041374444961548\n",
            "epoch: 6, classification_loss: 1.7748950719833374, Val Loss: 70.52289974689484, loss : 2.055861473083496\n",
            "epoch: 7, classification_loss: 1.7727617025375366, Val Loss: 70.56018340587616, loss : 2.0508060455322266\n",
            "epoch: 8, classification_loss: 1.7669994831085205, Val Loss: 70.52532386779785, loss : 2.033902406692505\n",
            "epoch: 9, classification_loss: 1.7690702676773071, Val Loss: 70.48206615447998, loss : 2.0311620235443115\n",
            "epoch: 10, classification_loss: 1.78622567653656, Val Loss: 70.50757551193237, loss : 2.041961669921875\n",
            "epoch: 11, classification_loss: 1.7798550128936768, Val Loss: 70.52665543556213, loss : 2.0338947772979736\n",
            "epoch: 12, classification_loss: 1.7726202011108398, Val Loss: 70.52146399021149, loss : 2.0352752208709717\n",
            "epoch: 13, classification_loss: 1.7840088605880737, Val Loss: 70.53065323829651, loss : 2.041088581085205\n",
            "epoch: 14, classification_loss: 1.7716648578643799, Val Loss: 70.49415850639343, loss : 2.0209903717041016\n",
            "epoch: 15, classification_loss: 1.7815135717391968, Val Loss: 70.50410103797913, loss : 2.03005313873291\n",
            "epoch: 16, classification_loss: 1.7789050340652466, Val Loss: 70.53406739234924, loss : 2.0237350463867188\n",
            "epoch: 17, classification_loss: 1.782246708869934, Val Loss: 70.51856899261475, loss : 2.0293662548065186\n",
            "epoch: 18, classification_loss: 1.7763935327529907, Val Loss: 70.49462747573853, loss : 2.0126583576202393\n",
            "epoch: 19, classification_loss: 1.787889838218689, Val Loss: 70.52597308158875, loss : 2.030168294906616\n",
            "Batch: 19, Test Acc: 0.57421875\n",
            "Batch: 20:\n",
            "epoch: 0, classification_loss: 1.7989100217819214, Val Loss: 70.51650977134705, loss : 1.7989100217819214\n",
            "epoch: 1, classification_loss: 1.7466800212860107, Val Loss: 70.48579049110413, loss : 2.0885839462280273\n",
            "epoch: 2, classification_loss: 1.7246794700622559, Val Loss: 70.50002896785736, loss : 2.0038862228393555\n",
            "epoch: 3, classification_loss: 1.7555718421936035, Val Loss: 70.48931932449341, loss : 2.0361790657043457\n",
            "epoch: 4, classification_loss: 1.7665964365005493, Val Loss: 70.51250982284546, loss : 2.0196006298065186\n",
            "epoch: 5, classification_loss: 1.778387188911438, Val Loss: 70.47709500789642, loss : 2.036968231201172\n",
            "epoch: 6, classification_loss: 1.759260892868042, Val Loss: 70.48503828048706, loss : 2.0303030014038086\n",
            "epoch: 7, classification_loss: 1.7613822221755981, Val Loss: 70.51431608200073, loss : 2.02650785446167\n",
            "epoch: 8, classification_loss: 1.763897180557251, Val Loss: 70.48057854175568, loss : 2.0229692459106445\n",
            "epoch: 9, classification_loss: 1.7545853853225708, Val Loss: 70.48824620246887, loss : 2.009498357772827\n",
            "epoch: 10, classification_loss: 1.764266848564148, Val Loss: 70.49832010269165, loss : 2.018698215484619\n",
            "epoch: 11, classification_loss: 1.7640964984893799, Val Loss: 70.50952506065369, loss : 2.0129096508026123\n",
            "epoch: 12, classification_loss: 1.7693933248519897, Val Loss: 70.51435589790344, loss : 2.0155813694000244\n",
            "epoch: 13, classification_loss: 1.760067105293274, Val Loss: 70.52893090248108, loss : 2.010241746902466\n",
            "epoch: 14, classification_loss: 1.7725214958190918, Val Loss: 70.51098990440369, loss : 2.016564130783081\n",
            "epoch: 15, classification_loss: 1.767175316810608, Val Loss: 70.48247969150543, loss : 2.012794017791748\n",
            "epoch: 16, classification_loss: 1.7632436752319336, Val Loss: 70.49660003185272, loss : 2.0118355751037598\n",
            "epoch: 17, classification_loss: 1.7814329862594604, Val Loss: 70.48335242271423, loss : 2.024585008621216\n",
            "epoch: 18, classification_loss: 1.7516882419586182, Val Loss: 70.49619197845459, loss : 1.9896365404129028\n",
            "epoch: 19, classification_loss: 1.7694514989852905, Val Loss: 70.51662409305573, loss : 2.012495994567871\n",
            "Batch: 20, Test Acc: 0.5748197115384616\n",
            "Batch: 21:\n",
            "epoch: 0, classification_loss: 1.808671236038208, Val Loss: 70.56742465496063, loss : 1.808671236038208\n",
            "epoch: 1, classification_loss: 1.7718510627746582, Val Loss: 70.56595504283905, loss : 2.1086440086364746\n",
            "epoch: 2, classification_loss: 1.7691330909729004, Val Loss: 70.52707052230835, loss : 2.0221669673919678\n",
            "epoch: 3, classification_loss: 1.781128168106079, Val Loss: 70.48696291446686, loss : 2.0508925914764404\n",
            "epoch: 4, classification_loss: 1.8150726556777954, Val Loss: 70.4916398525238, loss : 2.0504496097564697\n",
            "epoch: 5, classification_loss: 1.7875829935073853, Val Loss: 70.50064420700073, loss : 2.052924156188965\n",
            "epoch: 6, classification_loss: 1.7814439535140991, Val Loss: 70.49334955215454, loss : 2.054222583770752\n",
            "epoch: 7, classification_loss: 1.7843928337097168, Val Loss: 70.4997125864029, loss : 2.0428709983825684\n",
            "epoch: 8, classification_loss: 1.7932379245758057, Val Loss: 70.49019527435303, loss : 2.0423390865325928\n",
            "epoch: 9, classification_loss: 1.7919288873672485, Val Loss: 70.46639478206635, loss : 2.036301612854004\n",
            "epoch: 10, classification_loss: 1.793880820274353, Val Loss: 70.49910855293274, loss : 2.0332753658294678\n",
            "epoch: 11, classification_loss: 1.7857462167739868, Val Loss: 70.50758981704712, loss : 2.0356075763702393\n",
            "epoch: 12, classification_loss: 1.7912296056747437, Val Loss: 70.49043893814087, loss : 2.0233840942382812\n",
            "epoch: 13, classification_loss: 1.8044495582580566, Val Loss: 70.50872027873993, loss : 2.0491878986358643\n",
            "epoch: 14, classification_loss: 1.7924774885177612, Val Loss: 70.51036357879639, loss : 2.0277650356292725\n",
            "epoch: 15, classification_loss: 1.7956446409225464, Val Loss: 70.50417947769165, loss : 2.038999557495117\n",
            "epoch: 16, classification_loss: 1.792645812034607, Val Loss: 70.53255307674408, loss : 2.0262765884399414\n",
            "epoch: 17, classification_loss: 1.7944648265838623, Val Loss: 70.49743866920471, loss : 2.032578468322754\n",
            "epoch: 18, classification_loss: 1.7880936861038208, Val Loss: 70.53237581253052, loss : 2.014885902404785\n",
            "epoch: 19, classification_loss: 1.7944951057434082, Val Loss: 70.54364335536957, loss : 2.030639886856079\n",
            "Batch: 21, Test Acc: 0.5761217948717948\n",
            "Batch: 22:\n",
            "epoch: 0, classification_loss: 1.8152215480804443, Val Loss: 70.49416077136993, loss : 1.8152215480804443\n",
            "epoch: 1, classification_loss: 1.7809662818908691, Val Loss: 70.50990533828735, loss : 2.107836961746216\n",
            "epoch: 2, classification_loss: 1.7561187744140625, Val Loss: 70.61291193962097, loss : 2.0241403579711914\n",
            "epoch: 3, classification_loss: 1.7762703895568848, Val Loss: 70.58889174461365, loss : 2.0573434829711914\n",
            "epoch: 4, classification_loss: 1.7894642353057861, Val Loss: 70.47925126552582, loss : 2.0379176139831543\n",
            "epoch: 5, classification_loss: 1.8023523092269897, Val Loss: 70.51075577735901, loss : 2.0641679763793945\n",
            "epoch: 6, classification_loss: 1.7852330207824707, Val Loss: 70.54470491409302, loss : 2.059725761413574\n",
            "epoch: 7, classification_loss: 1.78605055809021, Val Loss: 70.53515946865082, loss : 2.048603057861328\n",
            "epoch: 8, classification_loss: 1.7923470735549927, Val Loss: 70.49510645866394, loss : 2.0482230186462402\n",
            "epoch: 9, classification_loss: 1.7789393663406372, Val Loss: 70.53662598133087, loss : 2.0445547103881836\n",
            "epoch: 10, classification_loss: 1.7671889066696167, Val Loss: 70.53755903244019, loss : 2.024320602416992\n",
            "epoch: 11, classification_loss: 1.798975944519043, Val Loss: 70.52323746681213, loss : 2.0496697425842285\n",
            "epoch: 12, classification_loss: 1.792818546295166, Val Loss: 70.54403424263, loss : 2.0370984077453613\n",
            "epoch: 13, classification_loss: 1.7932761907577515, Val Loss: 70.54239237308502, loss : 2.048067569732666\n",
            "epoch: 14, classification_loss: 1.789190649986267, Val Loss: 70.53799521923065, loss : 2.036330461502075\n",
            "epoch: 15, classification_loss: 1.7929660081863403, Val Loss: 70.55558025836945, loss : 2.047041654586792\n",
            "epoch: 16, classification_loss: 1.7894678115844727, Val Loss: 70.53936553001404, loss : 2.036630153656006\n",
            "epoch: 17, classification_loss: 1.8018935918807983, Val Loss: 70.5355476140976, loss : 2.0452115535736084\n",
            "epoch: 18, classification_loss: 1.8000292778015137, Val Loss: 70.53116548061371, loss : 2.042381525039673\n",
            "epoch: 19, classification_loss: 1.7979230880737305, Val Loss: 70.62793123722076, loss : 2.043347120285034\n",
            "Batch: 22, Test Acc: 0.5755208333333334\n",
            "Batch: 23:\n",
            "epoch: 0, classification_loss: 1.8131531476974487, Val Loss: 70.60854518413544, loss : 1.8131531476974487\n",
            "epoch: 1, classification_loss: 1.7797467708587646, Val Loss: 70.63423013687134, loss : 2.120093822479248\n",
            "epoch: 2, classification_loss: 1.74968421459198, Val Loss: 70.53157603740692, loss : 2.0344159603118896\n",
            "epoch: 3, classification_loss: 1.7778431177139282, Val Loss: 70.54844522476196, loss : 2.063053607940674\n",
            "epoch: 4, classification_loss: 1.8071621656417847, Val Loss: 70.53748393058777, loss : 2.0691211223602295\n",
            "epoch: 5, classification_loss: 1.7988154888153076, Val Loss: 70.56460011005402, loss : 2.0615952014923096\n",
            "epoch: 6, classification_loss: 1.789141058921814, Val Loss: 70.52993607521057, loss : 2.0607447624206543\n",
            "epoch: 7, classification_loss: 1.7796145677566528, Val Loss: 70.5428079366684, loss : 2.041855812072754\n",
            "epoch: 8, classification_loss: 1.7881200313568115, Val Loss: 70.5928293466568, loss : 2.0491671562194824\n",
            "epoch: 9, classification_loss: 1.783769130706787, Val Loss: 70.58288013935089, loss : 2.0499308109283447\n",
            "epoch: 10, classification_loss: 1.780161738395691, Val Loss: 70.56754219532013, loss : 2.040548801422119\n",
            "epoch: 11, classification_loss: 1.7826635837554932, Val Loss: 70.55589771270752, loss : 2.040146827697754\n",
            "epoch: 12, classification_loss: 1.7941607236862183, Val Loss: 70.55830705165863, loss : 2.0388948917388916\n",
            "epoch: 13, classification_loss: 1.7908060550689697, Val Loss: 70.57017052173615, loss : 2.042620897293091\n",
            "epoch: 14, classification_loss: 1.7918843030929565, Val Loss: 70.57797026634216, loss : 2.0306851863861084\n",
            "epoch: 15, classification_loss: 1.7947683334350586, Val Loss: 70.5638837814331, loss : 2.03995418548584\n",
            "epoch: 16, classification_loss: 1.7903975248336792, Val Loss: 70.60573554039001, loss : 2.0314252376556396\n",
            "epoch: 17, classification_loss: 1.7743210792541504, Val Loss: 70.5767422914505, loss : 2.0280442237854004\n",
            "epoch: 18, classification_loss: 1.777872920036316, Val Loss: 70.56909906864166, loss : 2.017157554626465\n",
            "epoch: 19, classification_loss: 1.798403024673462, Val Loss: 70.58891260623932, loss : 2.04376220703125\n",
            "Batch: 23, Test Acc: 0.5726161858974359\n",
            "Batch: 24:\n",
            "epoch: 0, classification_loss: 1.8612910509109497, Val Loss: 70.55914866924286, loss : 1.8612910509109497\n",
            "epoch: 1, classification_loss: 1.8119977712631226, Val Loss: 70.61112570762634, loss : 2.1510324478149414\n",
            "epoch: 2, classification_loss: 1.7945526838302612, Val Loss: 70.53065800666809, loss : 2.074887275695801\n",
            "epoch: 3, classification_loss: 1.8098047971725464, Val Loss: 70.56138575077057, loss : 2.094780445098877\n",
            "epoch: 4, classification_loss: 1.85249924659729, Val Loss: 70.62976586818695, loss : 2.1045632362365723\n",
            "epoch: 5, classification_loss: 1.8365073204040527, Val Loss: 70.5998706817627, loss : 2.1115710735321045\n",
            "epoch: 6, classification_loss: 1.821359634399414, Val Loss: 70.56880748271942, loss : 2.1000583171844482\n",
            "epoch: 7, classification_loss: 1.81746244430542, Val Loss: 70.57946169376373, loss : 2.0874266624450684\n",
            "epoch: 8, classification_loss: 1.832566738128662, Val Loss: 70.60732817649841, loss : 2.100966215133667\n",
            "epoch: 9, classification_loss: 1.8427367210388184, Val Loss: 70.5711168050766, loss : 2.1103827953338623\n",
            "epoch: 10, classification_loss: 1.8245792388916016, Val Loss: 70.6697211265564, loss : 2.0909628868103027\n",
            "epoch: 11, classification_loss: 1.8276383876800537, Val Loss: 70.66801083087921, loss : 2.0927469730377197\n",
            "epoch: 12, classification_loss: 1.820499062538147, Val Loss: 70.7106945514679, loss : 2.0779716968536377\n",
            "epoch: 13, classification_loss: 1.8360897302627563, Val Loss: 70.66012454032898, loss : 2.101515769958496\n",
            "epoch: 14, classification_loss: 1.826359748840332, Val Loss: 70.60207116603851, loss : 2.080767869949341\n",
            "epoch: 15, classification_loss: 1.8272223472595215, Val Loss: 70.6361927986145, loss : 2.0791616439819336\n",
            "epoch: 16, classification_loss: 1.8267868757247925, Val Loss: 70.6405965089798, loss : 2.081549882888794\n",
            "epoch: 17, classification_loss: 1.8233476877212524, Val Loss: 70.66513085365295, loss : 2.08571457862854\n",
            "epoch: 18, classification_loss: 1.8309098482131958, Val Loss: 70.6543253660202, loss : 2.0844151973724365\n",
            "epoch: 19, classification_loss: 1.821752905845642, Val Loss: 70.66150367259979, loss : 2.0715739727020264\n",
            "Batch: 24, Test Acc: 0.5716145833333334\n",
            "Batch: 25:\n",
            "epoch: 0, classification_loss: 1.7967846393585205, Val Loss: 70.5650110244751, loss : 1.7967846393585205\n",
            "epoch: 1, classification_loss: 1.7501280307769775, Val Loss: 70.57483923435211, loss : 2.0873842239379883\n",
            "epoch: 2, classification_loss: 1.7413138151168823, Val Loss: 70.50151097774506, loss : 2.0089919567108154\n",
            "epoch: 3, classification_loss: 1.7668297290802002, Val Loss: 70.49670851230621, loss : 2.053229808807373\n",
            "epoch: 4, classification_loss: 1.7823362350463867, Val Loss: 70.5344089269638, loss : 2.035585403442383\n",
            "epoch: 5, classification_loss: 1.7912672758102417, Val Loss: 70.503946185112, loss : 2.058040142059326\n",
            "epoch: 6, classification_loss: 1.7633368968963623, Val Loss: 70.4994969367981, loss : 2.02915620803833\n",
            "epoch: 7, classification_loss: 1.769572377204895, Val Loss: 70.5134928226471, loss : 2.033994674682617\n",
            "epoch: 8, classification_loss: 1.7808587551116943, Val Loss: 70.55459415912628, loss : 2.0345873832702637\n",
            "epoch: 9, classification_loss: 1.778219223022461, Val Loss: 70.50604951381683, loss : 2.0362303256988525\n",
            "epoch: 10, classification_loss: 1.758005142211914, Val Loss: 70.53420031070709, loss : 2.014082908630371\n",
            "epoch: 11, classification_loss: 1.783565640449524, Val Loss: 70.49850809574127, loss : 2.03194260597229\n",
            "epoch: 12, classification_loss: 1.7768834829330444, Val Loss: 70.52402436733246, loss : 2.024240255355835\n",
            "epoch: 13, classification_loss: 1.7760367393493652, Val Loss: 70.51331949234009, loss : 2.0246124267578125\n",
            "epoch: 14, classification_loss: 1.7682136297225952, Val Loss: 70.50919890403748, loss : 2.017408609390259\n",
            "epoch: 15, classification_loss: 1.7735387086868286, Val Loss: 70.53273022174835, loss : 2.0192604064941406\n",
            "epoch: 16, classification_loss: 1.7771022319793701, Val Loss: 70.53128731250763, loss : 2.014707326889038\n",
            "epoch: 17, classification_loss: 1.7800593376159668, Val Loss: 70.50179970264435, loss : 2.0198922157287598\n",
            "epoch: 18, classification_loss: 1.780440330505371, Val Loss: 70.5632711648941, loss : 2.0106048583984375\n",
            "epoch: 19, classification_loss: 1.7809221744537354, Val Loss: 70.54786336421967, loss : 2.0197057723999023\n",
            "Batch: 25, Test Acc: 0.5741185897435898\n",
            "Batch: 26:\n",
            "epoch: 0, classification_loss: 1.8114882707595825, Val Loss: 70.50799715518951, loss : 1.8114882707595825\n",
            "epoch: 1, classification_loss: 1.7887264490127563, Val Loss: 70.54009366035461, loss : 2.117234706878662\n",
            "epoch: 2, classification_loss: 1.7723941802978516, Val Loss: 70.54423725605011, loss : 2.0318002700805664\n",
            "epoch: 3, classification_loss: 1.7851152420043945, Val Loss: 70.53538870811462, loss : 2.0531222820281982\n",
            "epoch: 4, classification_loss: 1.8079715967178345, Val Loss: 70.52690613269806, loss : 2.0398855209350586\n",
            "epoch: 5, classification_loss: 1.803736686706543, Val Loss: 70.50925409793854, loss : 2.0641846656799316\n",
            "epoch: 6, classification_loss: 1.781323790550232, Val Loss: 70.5056893825531, loss : 2.051990270614624\n",
            "epoch: 7, classification_loss: 1.7932902574539185, Val Loss: 70.56869804859161, loss : 2.0467288494110107\n",
            "epoch: 8, classification_loss: 1.7988533973693848, Val Loss: 70.56937181949615, loss : 2.046436071395874\n",
            "epoch: 9, classification_loss: 1.7980657815933228, Val Loss: 70.53432130813599, loss : 2.043053388595581\n",
            "epoch: 10, classification_loss: 1.8006837368011475, Val Loss: 70.52257037162781, loss : 2.047475814819336\n",
            "epoch: 11, classification_loss: 1.8034889698028564, Val Loss: 70.56066501140594, loss : 2.05120849609375\n",
            "epoch: 12, classification_loss: 1.8020623922348022, Val Loss: 70.59348666667938, loss : 2.0494229793548584\n",
            "epoch: 13, classification_loss: 1.805484652519226, Val Loss: 70.54788136482239, loss : 2.0511720180511475\n",
            "epoch: 14, classification_loss: 1.8023254871368408, Val Loss: 70.5519231557846, loss : 2.048675775527954\n",
            "epoch: 15, classification_loss: 1.803125023841858, Val Loss: 70.68429899215698, loss : 2.0506889820098877\n",
            "epoch: 16, classification_loss: 1.7880750894546509, Val Loss: 70.63534164428711, loss : 2.0380845069885254\n",
            "epoch: 17, classification_loss: 1.811508297920227, Val Loss: 70.56658780574799, loss : 2.051765203475952\n",
            "epoch: 18, classification_loss: 1.7934174537658691, Val Loss: 70.56373953819275, loss : 2.0298733711242676\n",
            "epoch: 19, classification_loss: 1.812665581703186, Val Loss: 70.54452228546143, loss : 2.04502534866333\n",
            "Batch: 26, Test Acc: 0.5764222756410257\n",
            "Batch: 27:\n",
            "epoch: 0, classification_loss: 1.822150468826294, Val Loss: 70.62460362911224, loss : 1.822150468826294\n",
            "epoch: 1, classification_loss: 1.767124891281128, Val Loss: 70.61810803413391, loss : 2.118936538696289\n",
            "epoch: 2, classification_loss: 1.744131326675415, Val Loss: 70.51420247554779, loss : 2.0396273136138916\n",
            "epoch: 3, classification_loss: 1.7808295488357544, Val Loss: 70.55614852905273, loss : 2.0662944316864014\n",
            "epoch: 4, classification_loss: 1.8007594347000122, Val Loss: 70.49538576602936, loss : 2.069390296936035\n",
            "epoch: 5, classification_loss: 1.796444058418274, Val Loss: 70.53440451622009, loss : 2.0483579635620117\n",
            "epoch: 6, classification_loss: 1.7995326519012451, Val Loss: 70.55524921417236, loss : 2.075589895248413\n",
            "epoch: 7, classification_loss: 1.7707231044769287, Val Loss: 70.53479611873627, loss : 2.05020809173584\n",
            "epoch: 8, classification_loss: 1.7916454076766968, Val Loss: 70.52271258831024, loss : 2.062051296234131\n",
            "epoch: 9, classification_loss: 1.786851167678833, Val Loss: 70.50613868236542, loss : 2.052473783493042\n",
            "epoch: 10, classification_loss: 1.791313886642456, Val Loss: 70.53235673904419, loss : 2.0560970306396484\n",
            "epoch: 11, classification_loss: 1.7881989479064941, Val Loss: 70.52473294734955, loss : 2.0435731410980225\n",
            "epoch: 12, classification_loss: 1.781198263168335, Val Loss: 70.52955996990204, loss : 2.033236265182495\n",
            "epoch: 13, classification_loss: 1.797385334968567, Val Loss: 70.51343214511871, loss : 2.0466020107269287\n",
            "epoch: 14, classification_loss: 1.7849609851837158, Val Loss: 70.51169168949127, loss : 2.038271903991699\n",
            "epoch: 15, classification_loss: 1.7972383499145508, Val Loss: 70.51528108119965, loss : 2.047558307647705\n",
            "epoch: 16, classification_loss: 1.7980812788009644, Val Loss: 70.52438390254974, loss : 2.049095869064331\n",
            "epoch: 17, classification_loss: 1.7940921783447266, Val Loss: 70.51928699016571, loss : 2.039111375808716\n",
            "epoch: 18, classification_loss: 1.794850468635559, Val Loss: 70.53276824951172, loss : 2.042451858520508\n",
            "epoch: 19, classification_loss: 1.7933588027954102, Val Loss: 70.54619002342224, loss : 2.0435259342193604\n",
            "Batch: 27, Test Acc: 0.575020032051282\n",
            "Batch: 28:\n",
            "epoch: 0, classification_loss: 1.7719684839248657, Val Loss: 70.5571231842041, loss : 1.7719684839248657\n",
            "epoch: 1, classification_loss: 1.7280828952789307, Val Loss: 70.55110490322113, loss : 2.0659990310668945\n",
            "epoch: 2, classification_loss: 1.7076747417449951, Val Loss: 70.51998484134674, loss : 1.9647570848464966\n",
            "epoch: 3, classification_loss: 1.7407742738723755, Val Loss: 70.49945962429047, loss : 2.0144777297973633\n",
            "epoch: 4, classification_loss: 1.7466790676116943, Val Loss: 70.53913915157318, loss : 1.9905807971954346\n",
            "epoch: 5, classification_loss: 1.7554240226745605, Val Loss: 70.52384436130524, loss : 2.010409116744995\n",
            "epoch: 6, classification_loss: 1.725296974182129, Val Loss: 70.53108882904053, loss : 2.0040526390075684\n",
            "epoch: 7, classification_loss: 1.7400190830230713, Val Loss: 70.51533925533295, loss : 2.0004045963287354\n",
            "epoch: 8, classification_loss: 1.7295974493026733, Val Loss: 70.49023866653442, loss : 1.9859880208969116\n",
            "epoch: 9, classification_loss: 1.742434024810791, Val Loss: 70.51598536968231, loss : 1.9872971773147583\n",
            "epoch: 10, classification_loss: 1.7443547248840332, Val Loss: 70.52760577201843, loss : 1.9837141036987305\n",
            "epoch: 11, classification_loss: 1.738000512123108, Val Loss: 70.52682340145111, loss : 1.9778414964675903\n",
            "epoch: 12, classification_loss: 1.748355746269226, Val Loss: 70.53625524044037, loss : 1.985509991645813\n",
            "epoch: 13, classification_loss: 1.7359482049942017, Val Loss: 70.52796268463135, loss : 1.9715144634246826\n",
            "epoch: 14, classification_loss: 1.7322427034378052, Val Loss: 70.50475358963013, loss : 1.9666218757629395\n",
            "epoch: 15, classification_loss: 1.737754225730896, Val Loss: 70.52423143386841, loss : 1.967358112335205\n",
            "epoch: 16, classification_loss: 1.7461774349212646, Val Loss: 70.55306613445282, loss : 1.9777554273605347\n",
            "epoch: 17, classification_loss: 1.7497146129608154, Val Loss: 70.53262090682983, loss : 1.98525071144104\n",
            "epoch: 18, classification_loss: 1.7401869297027588, Val Loss: 70.52145385742188, loss : 1.9675637483596802\n",
            "epoch: 19, classification_loss: 1.7450661659240723, Val Loss: 70.50753772258759, loss : 1.9850471019744873\n",
            "Batch: 28, Test Acc: 0.5769230769230769\n",
            "Batch: 29:\n",
            "epoch: 0, classification_loss: 1.8012168407440186, Val Loss: 70.50890612602234, loss : 1.8012168407440186\n",
            "epoch: 1, classification_loss: 1.7673324346542358, Val Loss: 70.55630660057068, loss : 2.105185031890869\n",
            "epoch: 2, classification_loss: 1.7467381954193115, Val Loss: 70.49453687667847, loss : 2.022066354751587\n",
            "epoch: 3, classification_loss: 1.7816575765609741, Val Loss: 70.52760982513428, loss : 2.0527520179748535\n",
            "epoch: 4, classification_loss: 1.8067982196807861, Val Loss: 70.53558194637299, loss : 2.0581743717193604\n",
            "epoch: 5, classification_loss: 1.7875936031341553, Val Loss: 70.53304183483124, loss : 2.0391550064086914\n",
            "epoch: 6, classification_loss: 1.7670044898986816, Val Loss: 70.50661969184875, loss : 2.0372982025146484\n",
            "epoch: 7, classification_loss: 1.7771435976028442, Val Loss: 70.52331733703613, loss : 2.0466415882110596\n",
            "epoch: 8, classification_loss: 1.7811193466186523, Val Loss: 70.52699363231659, loss : 2.043165683746338\n",
            "epoch: 9, classification_loss: 1.7871534824371338, Val Loss: 70.49094450473785, loss : 2.0546979904174805\n",
            "epoch: 10, classification_loss: 1.796339511871338, Val Loss: 70.5165638923645, loss : 2.0506227016448975\n",
            "epoch: 11, classification_loss: 1.7914644479751587, Val Loss: 70.51901423931122, loss : 2.0497167110443115\n",
            "epoch: 12, classification_loss: 1.7853482961654663, Val Loss: 70.54639744758606, loss : 2.033456802368164\n",
            "epoch: 13, classification_loss: 1.7934566736221313, Val Loss: 70.50250709056854, loss : 2.050370931625366\n",
            "epoch: 14, classification_loss: 1.7934966087341309, Val Loss: 70.52643978595734, loss : 2.037335157394409\n",
            "epoch: 15, classification_loss: 1.77286958694458, Val Loss: 70.51924431324005, loss : 2.0287747383117676\n",
            "epoch: 16, classification_loss: 1.7928481101989746, Val Loss: 70.50835812091827, loss : 2.036250114440918\n",
            "epoch: 17, classification_loss: 1.7827465534210205, Val Loss: 70.51952517032623, loss : 2.0372207164764404\n",
            "epoch: 18, classification_loss: 1.7945995330810547, Val Loss: 70.51636040210724, loss : 2.0326311588287354\n",
            "epoch: 19, classification_loss: 1.7921385765075684, Val Loss: 70.53997206687927, loss : 2.0396952629089355\n",
            "Batch: 29, Test Acc: 0.5747195512820513\n",
            "Batch: 30:\n",
            "epoch: 0, classification_loss: 1.8341418504714966, Val Loss: 70.5380539894104, loss : 1.8341418504714966\n",
            "epoch: 1, classification_loss: 1.7940901517868042, Val Loss: 70.4941942691803, loss : 2.122859001159668\n",
            "epoch: 2, classification_loss: 1.775374412536621, Val Loss: 70.49713051319122, loss : 2.042633056640625\n",
            "epoch: 3, classification_loss: 1.7966923713684082, Val Loss: 70.52088463306427, loss : 2.083320379257202\n",
            "epoch: 4, classification_loss: 1.8252699375152588, Val Loss: 70.59457516670227, loss : 2.0776705741882324\n",
            "epoch: 5, classification_loss: 1.813024640083313, Val Loss: 70.54935145378113, loss : 2.082179069519043\n",
            "epoch: 6, classification_loss: 1.8135789632797241, Val Loss: 70.51144933700562, loss : 2.07572603225708\n",
            "epoch: 7, classification_loss: 1.8119022846221924, Val Loss: 70.54036867618561, loss : 2.0726442337036133\n",
            "epoch: 8, classification_loss: 1.8206450939178467, Val Loss: 70.53841924667358, loss : 2.074551582336426\n",
            "epoch: 9, classification_loss: 1.8116158246994019, Val Loss: 70.55464172363281, loss : 2.070904016494751\n",
            "epoch: 10, classification_loss: 1.8022483587265015, Val Loss: 70.567507147789, loss : 2.0596656799316406\n",
            "epoch: 11, classification_loss: 1.8167158365249634, Val Loss: 70.5854697227478, loss : 2.0692696571350098\n",
            "epoch: 12, classification_loss: 1.8156734704971313, Val Loss: 70.5050812959671, loss : 2.063127279281616\n",
            "epoch: 13, classification_loss: 1.8290717601776123, Val Loss: 70.50765073299408, loss : 2.0762381553649902\n",
            "epoch: 14, classification_loss: 1.805307149887085, Val Loss: 70.63309180736542, loss : 2.0532946586608887\n",
            "epoch: 15, classification_loss: 1.8050278425216675, Val Loss: 70.56476271152496, loss : 2.046748161315918\n",
            "epoch: 16, classification_loss: 1.815956950187683, Val Loss: 70.54724204540253, loss : 2.0622751712799072\n",
            "epoch: 17, classification_loss: 1.7996890544891357, Val Loss: 70.60649931430817, loss : 2.0493602752685547\n",
            "epoch: 18, classification_loss: 1.8095563650131226, Val Loss: 70.56575870513916, loss : 2.0451223850250244\n",
            "epoch: 19, classification_loss: 1.817449927330017, Val Loss: 70.53124237060547, loss : 2.0616438388824463\n",
            "Batch: 30, Test Acc: 0.5752203525641025\n",
            "Batch: 31:\n",
            "epoch: 0, classification_loss: 1.848464846611023, Val Loss: 70.51042032241821, loss : 1.848464846611023\n",
            "epoch: 1, classification_loss: 1.8088475465774536, Val Loss: 70.52263069152832, loss : 2.1340737342834473\n",
            "epoch: 2, classification_loss: 1.7966209650039673, Val Loss: 70.51926267147064, loss : 2.05869722366333\n",
            "epoch: 3, classification_loss: 1.821370005607605, Val Loss: 70.5112099647522, loss : 2.086855888366699\n",
            "epoch: 4, classification_loss: 1.849331021308899, Val Loss: 70.51120209693909, loss : 2.074755907058716\n",
            "epoch: 5, classification_loss: 1.821218490600586, Val Loss: 70.50344157218933, loss : 2.0906662940979004\n",
            "epoch: 6, classification_loss: 1.8088274002075195, Val Loss: 70.53302597999573, loss : 2.080695629119873\n",
            "epoch: 7, classification_loss: 1.8115744590759277, Val Loss: 70.53703999519348, loss : 2.0726490020751953\n",
            "epoch: 8, classification_loss: 1.8281928300857544, Val Loss: 70.50754487514496, loss : 2.0736613273620605\n",
            "epoch: 9, classification_loss: 1.8250479698181152, Val Loss: 70.51622867584229, loss : 2.0698020458221436\n",
            "epoch: 10, classification_loss: 1.8304997682571411, Val Loss: 70.53421938419342, loss : 2.0725886821746826\n",
            "epoch: 11, classification_loss: 1.8196971416473389, Val Loss: 70.55677676200867, loss : 2.069486618041992\n",
            "epoch: 12, classification_loss: 1.8197832107543945, Val Loss: 70.5646378993988, loss : 2.0573134422302246\n",
            "epoch: 13, classification_loss: 1.8301892280578613, Val Loss: 70.52733826637268, loss : 2.0719857215881348\n",
            "epoch: 14, classification_loss: 1.8188068866729736, Val Loss: 70.53533458709717, loss : 2.0527689456939697\n",
            "epoch: 15, classification_loss: 1.828203797340393, Val Loss: 70.57982885837555, loss : 2.061530590057373\n",
            "epoch: 16, classification_loss: 1.823581576347351, Val Loss: 70.55186784267426, loss : 2.0601751804351807\n",
            "epoch: 17, classification_loss: 1.824766993522644, Val Loss: 70.54002702236176, loss : 2.0600426197052\n",
            "epoch: 18, classification_loss: 1.8309662342071533, Val Loss: 70.55808508396149, loss : 2.0574212074279785\n",
            "epoch: 19, classification_loss: 1.8177645206451416, Val Loss: 70.56012558937073, loss : 2.052952766418457\n",
            "Batch: 31, Test Acc: 0.5749198717948718\n",
            "Batch: 32:\n",
            "epoch: 0, classification_loss: 1.8040496110916138, Val Loss: 70.55467224121094, loss : 1.8040496110916138\n",
            "epoch: 1, classification_loss: 1.773777723312378, Val Loss: 70.63316595554352, loss : 2.1064093112945557\n",
            "epoch: 2, classification_loss: 1.7496570348739624, Val Loss: 70.52963936328888, loss : 2.006296157836914\n",
            "epoch: 3, classification_loss: 1.7793647050857544, Val Loss: 70.51615738868713, loss : 2.0463342666625977\n",
            "epoch: 4, classification_loss: 1.7949789762496948, Val Loss: 70.49889731407166, loss : 2.0335450172424316\n",
            "epoch: 5, classification_loss: 1.789218783378601, Val Loss: 70.50157022476196, loss : 2.054292917251587\n",
            "epoch: 6, classification_loss: 1.7678899765014648, Val Loss: 70.5301661491394, loss : 2.0385382175445557\n",
            "epoch: 7, classification_loss: 1.783363699913025, Val Loss: 70.56450843811035, loss : 2.0408358573913574\n",
            "epoch: 8, classification_loss: 1.7847663164138794, Val Loss: 70.5058366060257, loss : 2.0334291458129883\n",
            "epoch: 9, classification_loss: 1.7772806882858276, Val Loss: 70.5155885219574, loss : 2.0270485877990723\n",
            "epoch: 10, classification_loss: 1.767513632774353, Val Loss: 70.54398000240326, loss : 2.020141839981079\n",
            "epoch: 11, classification_loss: 1.769395112991333, Val Loss: 70.54382753372192, loss : 2.018746852874756\n",
            "epoch: 12, classification_loss: 1.7717437744140625, Val Loss: 70.51119768619537, loss : 2.015702724456787\n",
            "epoch: 13, classification_loss: 1.786119818687439, Val Loss: 70.52687656879425, loss : 2.034496545791626\n",
            "epoch: 14, classification_loss: 1.76944100856781, Val Loss: 70.5312180519104, loss : 2.0075950622558594\n",
            "epoch: 15, classification_loss: 1.782546877861023, Val Loss: 70.5144213438034, loss : 2.0304741859436035\n",
            "epoch: 16, classification_loss: 1.7758787870407104, Val Loss: 70.49331295490265, loss : 2.00699520111084\n",
            "epoch: 17, classification_loss: 1.7847234010696411, Val Loss: 70.52470052242279, loss : 2.0253095626831055\n",
            "epoch: 18, classification_loss: 1.7860080003738403, Val Loss: 70.51901113986969, loss : 2.0153565406799316\n",
            "epoch: 19, classification_loss: 1.785164475440979, Val Loss: 70.51372444629669, loss : 2.0260300636291504\n",
            "Batch: 32, Test Acc: 0.5740184294871795\n",
            "Batch: 33:\n",
            "epoch: 0, classification_loss: 1.8439726829528809, Val Loss: 70.51561033725739, loss : 1.8439726829528809\n",
            "epoch: 1, classification_loss: 1.796020269393921, Val Loss: 70.51590323448181, loss : 2.1279873847961426\n",
            "epoch: 2, classification_loss: 1.7673466205596924, Val Loss: 70.50216865539551, loss : 2.047604560852051\n",
            "epoch: 3, classification_loss: 1.7875187397003174, Val Loss: 70.50453507900238, loss : 2.073139190673828\n",
            "epoch: 4, classification_loss: 1.8144079446792603, Val Loss: 70.55453908443451, loss : 2.0721328258514404\n",
            "epoch: 5, classification_loss: 1.8068888187408447, Val Loss: 70.52373445034027, loss : 2.0585219860076904\n",
            "epoch: 6, classification_loss: 1.8148633241653442, Val Loss: 70.53338432312012, loss : 2.07907772064209\n",
            "epoch: 7, classification_loss: 1.8162446022033691, Val Loss: 70.53285038471222, loss : 2.0835886001586914\n",
            "epoch: 8, classification_loss: 1.7916927337646484, Val Loss: 70.58013415336609, loss : 2.0491676330566406\n",
            "epoch: 9, classification_loss: 1.7984366416931152, Val Loss: 70.52779603004456, loss : 2.0582661628723145\n",
            "epoch: 10, classification_loss: 1.7988784313201904, Val Loss: 70.54284703731537, loss : 2.057830572128296\n",
            "epoch: 11, classification_loss: 1.8046902418136597, Val Loss: 70.53911006450653, loss : 2.0628225803375244\n",
            "epoch: 12, classification_loss: 1.7988497018814087, Val Loss: 70.57665932178497, loss : 2.047438144683838\n",
            "epoch: 13, classification_loss: 1.8192157745361328, Val Loss: 70.53885269165039, loss : 2.0659565925598145\n",
            "epoch: 14, classification_loss: 1.816043734550476, Val Loss: 70.5465657711029, loss : 2.0566277503967285\n",
            "epoch: 15, classification_loss: 1.8041530847549438, Val Loss: 70.5359982252121, loss : 2.0552268028259277\n",
            "epoch: 16, classification_loss: 1.7919567823410034, Val Loss: 70.58850932121277, loss : 2.0353987216949463\n",
            "epoch: 17, classification_loss: 1.8036166429519653, Val Loss: 70.60294210910797, loss : 2.0583913326263428\n",
            "epoch: 18, classification_loss: 1.812381625175476, Val Loss: 70.56091952323914, loss : 2.0522656440734863\n",
            "epoch: 19, classification_loss: 1.799902319908142, Val Loss: 70.61029839515686, loss : 2.051354169845581\n",
            "Batch: 33, Test Acc: 0.5740184294871795\n",
            "Batch: 34:\n",
            "epoch: 0, classification_loss: 1.8280411958694458, Val Loss: 70.4985386133194, loss : 1.8280411958694458\n",
            "epoch: 1, classification_loss: 1.7944682836532593, Val Loss: 70.49505972862244, loss : 2.126960277557373\n",
            "epoch: 2, classification_loss: 1.762523889541626, Val Loss: 70.55645704269409, loss : 2.0306878089904785\n",
            "epoch: 3, classification_loss: 1.796814203262329, Val Loss: 70.51061248779297, loss : 2.0761663913726807\n",
            "epoch: 4, classification_loss: 1.8202381134033203, Val Loss: 70.48184037208557, loss : 2.05781888961792\n",
            "epoch: 5, classification_loss: 1.80228590965271, Val Loss: 70.52645945549011, loss : 2.0704286098480225\n",
            "epoch: 6, classification_loss: 1.8042902946472168, Val Loss: 70.51819241046906, loss : 2.0759570598602295\n",
            "epoch: 7, classification_loss: 1.7944787740707397, Val Loss: 70.52154576778412, loss : 2.052558660507202\n",
            "epoch: 8, classification_loss: 1.795267939567566, Val Loss: 70.52134311199188, loss : 2.0536375045776367\n",
            "epoch: 9, classification_loss: 1.8098618984222412, Val Loss: 70.50807762145996, loss : 2.063002824783325\n",
            "epoch: 10, classification_loss: 1.7949457168579102, Val Loss: 70.51648795604706, loss : 2.048175811767578\n",
            "epoch: 11, classification_loss: 1.8090885877609253, Val Loss: 70.50626993179321, loss : 2.0591681003570557\n",
            "epoch: 12, classification_loss: 1.805802345275879, Val Loss: 70.52439498901367, loss : 2.0520825386047363\n",
            "epoch: 13, classification_loss: 1.8021684885025024, Val Loss: 70.56739246845245, loss : 2.0430564880371094\n",
            "epoch: 14, classification_loss: 1.8082499504089355, Val Loss: 70.52527272701263, loss : 2.0463380813598633\n",
            "epoch: 15, classification_loss: 1.796045184135437, Val Loss: 70.5332362651825, loss : 2.037844657897949\n",
            "epoch: 16, classification_loss: 1.8024636507034302, Val Loss: 70.53964972496033, loss : 2.0503649711608887\n",
            "epoch: 17, classification_loss: 1.8044580221176147, Val Loss: 70.54151117801666, loss : 2.0422375202178955\n",
            "epoch: 18, classification_loss: 1.8037199974060059, Val Loss: 70.5171229839325, loss : 2.0438313484191895\n",
            "epoch: 19, classification_loss: 1.786584496498108, Val Loss: 70.57958996295929, loss : 2.02744722366333\n",
            "Batch: 34, Test Acc: 0.5739182692307693\n",
            "Batch: 35:\n",
            "epoch: 0, classification_loss: 1.854238510131836, Val Loss: 70.51952838897705, loss : 1.854238510131836\n",
            "epoch: 1, classification_loss: 1.8249993324279785, Val Loss: 70.5968861579895, loss : 2.168944835662842\n",
            "epoch: 2, classification_loss: 1.8036713600158691, Val Loss: 70.59760010242462, loss : 2.0845861434936523\n",
            "epoch: 3, classification_loss: 1.8271149396896362, Val Loss: 70.49088943004608, loss : 2.101219415664673\n",
            "epoch: 4, classification_loss: 1.855865716934204, Val Loss: 70.48592746257782, loss : 2.1121997833251953\n",
            "epoch: 5, classification_loss: 1.846817135810852, Val Loss: 70.49576902389526, loss : 2.0934431552886963\n",
            "epoch: 6, classification_loss: 1.8147351741790771, Val Loss: 70.51044714450836, loss : 2.0788187980651855\n",
            "epoch: 7, classification_loss: 1.8197352886199951, Val Loss: 70.51613712310791, loss : 2.0778772830963135\n",
            "epoch: 8, classification_loss: 1.83095121383667, Val Loss: 70.54176807403564, loss : 2.0845119953155518\n",
            "epoch: 9, classification_loss: 1.8394320011138916, Val Loss: 70.49472284317017, loss : 2.0921998023986816\n",
            "epoch: 10, classification_loss: 1.846994400024414, Val Loss: 70.49344265460968, loss : 2.0774779319763184\n",
            "epoch: 11, classification_loss: 1.8334579467773438, Val Loss: 70.50656306743622, loss : 2.081789970397949\n",
            "epoch: 12, classification_loss: 1.835030436515808, Val Loss: 70.51650047302246, loss : 2.0746867656707764\n",
            "epoch: 13, classification_loss: 1.8261232376098633, Val Loss: 70.52770686149597, loss : 2.0717742443084717\n",
            "epoch: 14, classification_loss: 1.8438035249710083, Val Loss: 70.54171597957611, loss : 2.076029062271118\n",
            "epoch: 15, classification_loss: 1.8346731662750244, Val Loss: 70.52269625663757, loss : 2.080639123916626\n",
            "epoch: 16, classification_loss: 1.834454894065857, Val Loss: 70.51295864582062, loss : 2.067756175994873\n",
            "epoch: 17, classification_loss: 1.8373247385025024, Val Loss: 70.5125458240509, loss : 2.087883949279785\n",
            "epoch: 18, classification_loss: 1.8343653678894043, Val Loss: 70.51192450523376, loss : 2.0613858699798584\n",
            "epoch: 19, classification_loss: 1.8286882638931274, Val Loss: 70.52095246315002, loss : 2.0749964714050293\n",
            "Batch: 35, Test Acc: 0.5759214743589743\n",
            "Batch: 36:\n",
            "epoch: 0, classification_loss: 1.776265025138855, Val Loss: 70.519944190979, loss : 1.776265025138855\n",
            "epoch: 1, classification_loss: 1.7390668392181396, Val Loss: 70.55135011672974, loss : 2.0641064643859863\n",
            "epoch: 2, classification_loss: 1.7024189233779907, Val Loss: 70.53855502605438, loss : 1.967207670211792\n",
            "epoch: 3, classification_loss: 1.7447096109390259, Val Loss: 70.52555310726166, loss : 2.0155279636383057\n",
            "epoch: 4, classification_loss: 1.764691710472107, Val Loss: 70.54448199272156, loss : 1.9904346466064453\n",
            "epoch: 5, classification_loss: 1.7423192262649536, Val Loss: 70.53168427944183, loss : 2.012314796447754\n",
            "epoch: 6, classification_loss: 1.7334805727005005, Val Loss: 70.52870285511017, loss : 2.0007545948028564\n",
            "epoch: 7, classification_loss: 1.7335807085037231, Val Loss: 70.55101704597473, loss : 2.002317190170288\n",
            "epoch: 8, classification_loss: 1.7471280097961426, Val Loss: 70.5461368560791, loss : 1.9966932535171509\n",
            "epoch: 9, classification_loss: 1.7476252317428589, Val Loss: 70.58195924758911, loss : 2.0033955574035645\n",
            "epoch: 10, classification_loss: 1.7360553741455078, Val Loss: 70.54902815818787, loss : 1.9901617765426636\n",
            "epoch: 11, classification_loss: 1.7583891153335571, Val Loss: 70.57050812244415, loss : 2.003652572631836\n",
            "epoch: 12, classification_loss: 1.7494947910308838, Val Loss: 70.54196965694427, loss : 1.993382215499878\n",
            "epoch: 13, classification_loss: 1.741654634475708, Val Loss: 70.56547379493713, loss : 1.9860179424285889\n",
            "epoch: 14, classification_loss: 1.751358985900879, Val Loss: 70.5920021533966, loss : 1.9938703775405884\n",
            "epoch: 15, classification_loss: 1.7392830848693848, Val Loss: 70.54179918766022, loss : 1.9796594381332397\n",
            "epoch: 16, classification_loss: 1.7490146160125732, Val Loss: 70.5650041103363, loss : 1.9876917600631714\n",
            "epoch: 17, classification_loss: 1.7375041246414185, Val Loss: 70.61409330368042, loss : 1.9771130084991455\n",
            "epoch: 18, classification_loss: 1.7463408708572388, Val Loss: 70.60626232624054, loss : 1.979321837425232\n",
            "epoch: 19, classification_loss: 1.74944007396698, Val Loss: 70.59323954582214, loss : 1.9856868982315063\n",
            "Batch: 36, Test Acc: 0.5740184294871795\n",
            "Batch: 37:\n",
            "epoch: 0, classification_loss: 1.8196969032287598, Val Loss: 70.50696098804474, loss : 1.8196969032287598\n",
            "epoch: 1, classification_loss: 1.7745373249053955, Val Loss: 70.52091789245605, loss : 2.114772319793701\n",
            "epoch: 2, classification_loss: 1.7550855875015259, Val Loss: 70.51096820831299, loss : 2.0260300636291504\n",
            "epoch: 3, classification_loss: 1.7959729433059692, Val Loss: 70.50926113128662, loss : 2.0640225410461426\n",
            "epoch: 4, classification_loss: 1.8134605884552002, Val Loss: 70.49013447761536, loss : 2.053046941757202\n",
            "epoch: 5, classification_loss: 1.7942825555801392, Val Loss: 70.4900735616684, loss : 2.0478017330169678\n",
            "epoch: 6, classification_loss: 1.7884773015975952, Val Loss: 70.51005673408508, loss : 2.048264265060425\n",
            "epoch: 7, classification_loss: 1.794551134109497, Val Loss: 70.50566601753235, loss : 2.0504846572875977\n",
            "epoch: 8, classification_loss: 1.7797871828079224, Val Loss: 70.49096179008484, loss : 2.030242919921875\n",
            "epoch: 9, classification_loss: 1.799176573753357, Val Loss: 70.51508593559265, loss : 2.0450146198272705\n",
            "epoch: 10, classification_loss: 1.7979286909103394, Val Loss: 70.48917841911316, loss : 2.0308094024658203\n",
            "epoch: 11, classification_loss: 1.8056004047393799, Val Loss: 70.52365410327911, loss : 2.0423169136047363\n",
            "epoch: 12, classification_loss: 1.7953177690505981, Val Loss: 70.51358997821808, loss : 2.0341668128967285\n",
            "epoch: 13, classification_loss: 1.7844047546386719, Val Loss: 70.55497372150421, loss : 2.031557321548462\n",
            "epoch: 14, classification_loss: 1.8015565872192383, Val Loss: 70.51802897453308, loss : 2.028358221054077\n",
            "epoch: 15, classification_loss: 1.8041138648986816, Val Loss: 70.48591208457947, loss : 2.043795347213745\n",
            "epoch: 16, classification_loss: 1.7917362451553345, Val Loss: 70.52650463581085, loss : 2.01997447013855\n",
            "epoch: 17, classification_loss: 1.8028398752212524, Val Loss: 70.56056714057922, loss : 2.036302328109741\n",
            "epoch: 18, classification_loss: 1.7935222387313843, Val Loss: 70.50098490715027, loss : 2.0157296657562256\n",
            "epoch: 19, classification_loss: 1.7920417785644531, Val Loss: 70.53556680679321, loss : 2.030674934387207\n",
            "Batch: 37, Test Acc: 0.5761217948717948\n",
            "Batch: 38:\n",
            "epoch: 0, classification_loss: 1.827914834022522, Val Loss: 70.46453189849854, loss : 1.827914834022522\n",
            "epoch: 1, classification_loss: 1.780221700668335, Val Loss: 70.49696969985962, loss : 2.102076292037964\n",
            "epoch: 2, classification_loss: 1.7810744047164917, Val Loss: 70.51443088054657, loss : 2.0358099937438965\n",
            "epoch: 3, classification_loss: 1.81028413772583, Val Loss: 70.5004152059555, loss : 2.0772829055786133\n",
            "epoch: 4, classification_loss: 1.8240960836410522, Val Loss: 70.47950029373169, loss : 2.057861804962158\n",
            "epoch: 5, classification_loss: 1.8021622896194458, Val Loss: 70.44896984100342, loss : 2.0740435123443604\n",
            "epoch: 6, classification_loss: 1.7957910299301147, Val Loss: 70.51890063285828, loss : 1.7957910299301147\n",
            "epoch: 7, classification_loss: 1.7631926536560059, Val Loss: 70.49827146530151, loss : 2.082853317260742\n",
            "epoch: 8, classification_loss: 1.7453022003173828, Val Loss: 70.45128536224365, loss : 1.9983090162277222\n",
            "epoch: 9, classification_loss: 1.786877989768982, Val Loss: 70.46938288211823, loss : 2.0272111892700195\n",
            "epoch: 10, classification_loss: 1.8025548458099365, Val Loss: 70.50762403011322, loss : 2.016530752182007\n",
            "epoch: 11, classification_loss: 1.7679073810577393, Val Loss: 70.46737611293793, loss : 2.028686285018921\n",
            "epoch: 12, classification_loss: 1.7580957412719727, Val Loss: 70.52437210083008, loss : 2.0199942588806152\n",
            "epoch: 13, classification_loss: 1.7868233919143677, Val Loss: 70.50805127620697, loss : 2.030029773712158\n",
            "epoch: 14, classification_loss: 1.784213662147522, Val Loss: 70.48450195789337, loss : 2.0241007804870605\n",
            "epoch: 15, classification_loss: 1.7862250804901123, Val Loss: 70.44744312763214, loss : 2.018031120300293\n",
            "epoch: 16, classification_loss: 1.770580530166626, Val Loss: 70.44622528553009, loss : 2.011829376220703\n",
            "epoch: 17, classification_loss: 1.7680588960647583, Val Loss: 70.50977909564972, loss : 2.012140989303589\n",
            "epoch: 18, classification_loss: 1.780121922492981, Val Loss: 70.47287893295288, loss : 2.0214626789093018\n",
            "epoch: 19, classification_loss: 1.7776764631271362, Val Loss: 70.49220263957977, loss : 2.0136892795562744\n",
            "Batch: 38, Test Acc: 0.5765224358974359\n",
            "Batch: 39:\n",
            "epoch: 0, classification_loss: 1.8135826587677002, Val Loss: 70.540931224823, loss : 1.8135826587677002\n",
            "epoch: 1, classification_loss: 1.7719322443008423, Val Loss: 70.5706034898758, loss : 2.1106443405151367\n",
            "epoch: 2, classification_loss: 1.7476030588150024, Val Loss: 70.4687774181366, loss : 2.0242040157318115\n",
            "epoch: 3, classification_loss: 1.7645418643951416, Val Loss: 70.49100351333618, loss : 2.044647455215454\n",
            "epoch: 4, classification_loss: 1.8000190258026123, Val Loss: 70.50755333900452, loss : 2.057847023010254\n",
            "epoch: 5, classification_loss: 1.7813286781311035, Val Loss: 70.52813220024109, loss : 2.0268731117248535\n",
            "epoch: 6, classification_loss: 1.7801119089126587, Val Loss: 70.51560735702515, loss : 2.038254737854004\n",
            "epoch: 7, classification_loss: 1.7885764837265015, Val Loss: 70.51746559143066, loss : 2.0452728271484375\n",
            "epoch: 8, classification_loss: 1.7727299928665161, Val Loss: 70.49145245552063, loss : 2.029448986053467\n",
            "epoch: 9, classification_loss: 1.7945160865783691, Val Loss: 70.49552094936371, loss : 2.0429933071136475\n",
            "epoch: 10, classification_loss: 1.7851113080978394, Val Loss: 70.51592934131622, loss : 2.032437801361084\n",
            "epoch: 11, classification_loss: 1.7894774675369263, Val Loss: 70.57634508609772, loss : 2.0337414741516113\n",
            "epoch: 12, classification_loss: 1.781916856765747, Val Loss: 70.53675270080566, loss : 2.0264620780944824\n",
            "epoch: 13, classification_loss: 1.7788922786712646, Val Loss: 70.53993105888367, loss : 2.0210089683532715\n",
            "epoch: 14, classification_loss: 1.787315011024475, Val Loss: 70.51936435699463, loss : 2.0271263122558594\n",
            "epoch: 15, classification_loss: 1.7726562023162842, Val Loss: 70.54512643814087, loss : 2.017604351043701\n",
            "epoch: 16, classification_loss: 1.7865049839019775, Val Loss: 70.58298361301422, loss : 2.0264291763305664\n",
            "epoch: 17, classification_loss: 1.7867969274520874, Val Loss: 70.58407282829285, loss : 2.028874397277832\n",
            "epoch: 18, classification_loss: 1.7885223627090454, Val Loss: 70.5685225725174, loss : 2.01823353767395\n",
            "epoch: 19, classification_loss: 1.7663304805755615, Val Loss: 70.60905086994171, loss : 2.007266044616699\n",
            "Batch: 39, Test Acc: 0.5739182692307693\n",
            "Batch: 40:\n",
            "epoch: 0, classification_loss: 1.814537525177002, Val Loss: 70.55493116378784, loss : 1.814537525177002\n",
            "epoch: 1, classification_loss: 1.7768634557724, Val Loss: 70.5454969406128, loss : 2.0997495651245117\n",
            "epoch: 2, classification_loss: 1.7497268915176392, Val Loss: 70.4763240814209, loss : 2.0257601737976074\n",
            "epoch: 3, classification_loss: 1.770464301109314, Val Loss: 70.56076657772064, loss : 2.057155132293701\n",
            "epoch: 4, classification_loss: 1.8031672239303589, Val Loss: 70.54847598075867, loss : 2.0586493015289307\n",
            "epoch: 5, classification_loss: 1.7902107238769531, Val Loss: 70.51553118228912, loss : 2.0461034774780273\n",
            "epoch: 6, classification_loss: 1.7803030014038086, Val Loss: 70.4988112449646, loss : 2.0477261543273926\n",
            "epoch: 7, classification_loss: 1.7803969383239746, Val Loss: 70.52369856834412, loss : 2.0510404109954834\n",
            "epoch: 8, classification_loss: 1.7731297016143799, Val Loss: 70.57549893856049, loss : 2.0364694595336914\n",
            "epoch: 9, classification_loss: 1.7914434671401978, Val Loss: 70.52213132381439, loss : 2.05718731880188\n",
            "epoch: 10, classification_loss: 1.7825580835342407, Val Loss: 70.50698161125183, loss : 2.042363405227661\n",
            "epoch: 11, classification_loss: 1.7857354879379272, Val Loss: 70.53565084934235, loss : 2.043030023574829\n",
            "epoch: 12, classification_loss: 1.7934008836746216, Val Loss: 70.51159453392029, loss : 2.042668104171753\n",
            "epoch: 13, classification_loss: 1.781077265739441, Val Loss: 70.4978575706482, loss : 2.0472729206085205\n",
            "epoch: 14, classification_loss: 1.775820255279541, Val Loss: 70.53117454051971, loss : 2.0357937812805176\n",
            "epoch: 15, classification_loss: 1.7881503105163574, Val Loss: 70.54099035263062, loss : 2.048914670944214\n",
            "epoch: 16, classification_loss: 1.784430980682373, Val Loss: 70.52911794185638, loss : 2.0465993881225586\n",
            "epoch: 17, classification_loss: 1.7741347551345825, Val Loss: 70.54051077365875, loss : 2.040173053741455\n",
            "epoch: 18, classification_loss: 1.7862019538879395, Val Loss: 70.55049502849579, loss : 2.051060199737549\n",
            "epoch: 19, classification_loss: 1.7859716415405273, Val Loss: 70.5185261964798, loss : 2.0464603900909424\n",
            "Batch: 40, Test Acc: 0.5721153846153846\n",
            "Batch: 41:\n",
            "epoch: 0, classification_loss: 1.8488680124282837, Val Loss: 70.48355388641357, loss : 1.8488680124282837\n",
            "epoch: 1, classification_loss: 1.8275038003921509, Val Loss: 70.52038395404816, loss : 2.1718883514404297\n",
            "epoch: 2, classification_loss: 1.7983908653259277, Val Loss: 70.50573217868805, loss : 2.0827760696411133\n",
            "epoch: 3, classification_loss: 1.8283476829528809, Val Loss: 70.50726878643036, loss : 2.112175226211548\n",
            "epoch: 4, classification_loss: 1.85188627243042, Val Loss: 70.47427177429199, loss : 2.1051182746887207\n",
            "epoch: 5, classification_loss: 1.8321574926376343, Val Loss: 70.47311353683472, loss : 2.0905771255493164\n",
            "epoch: 6, classification_loss: 1.8251577615737915, Val Loss: 70.55178081989288, loss : 2.0989580154418945\n",
            "epoch: 7, classification_loss: 1.8292033672332764, Val Loss: 70.56624221801758, loss : 2.095726490020752\n",
            "epoch: 8, classification_loss: 1.8337340354919434, Val Loss: 70.53600561618805, loss : 2.098646640777588\n",
            "epoch: 9, classification_loss: 1.834833025932312, Val Loss: 70.47671473026276, loss : 2.0930168628692627\n",
            "epoch: 10, classification_loss: 1.822697639465332, Val Loss: 70.56107103824615, loss : 2.0812740325927734\n",
            "epoch: 11, classification_loss: 1.8278164863586426, Val Loss: 70.55039143562317, loss : 2.0822317600250244\n",
            "epoch: 12, classification_loss: 1.82566237449646, Val Loss: 70.54033493995667, loss : 2.080519676208496\n",
            "epoch: 13, classification_loss: 1.8331677913665771, Val Loss: 70.56948471069336, loss : 2.0882463455200195\n",
            "epoch: 14, classification_loss: 1.8150514364242554, Val Loss: 70.54969394207001, loss : 2.0727155208587646\n",
            "epoch: 15, classification_loss: 1.8192120790481567, Val Loss: 70.56193161010742, loss : 2.0691568851470947\n",
            "epoch: 16, classification_loss: 1.8322577476501465, Val Loss: 70.5612553358078, loss : 2.080763101577759\n",
            "epoch: 17, classification_loss: 1.825954556465149, Val Loss: 70.5388171672821, loss : 2.0824899673461914\n",
            "epoch: 18, classification_loss: 1.8311278820037842, Val Loss: 70.55250585079193, loss : 2.077967882156372\n",
            "epoch: 19, classification_loss: 1.8371840715408325, Val Loss: 70.53553247451782, loss : 2.0785512924194336\n",
            "Batch: 41, Test Acc: 0.5760216346153846\n",
            "Batch: 42:\n",
            "epoch: 0, classification_loss: 1.7717809677124023, Val Loss: 70.48806512355804, loss : 1.7717809677124023\n",
            "epoch: 1, classification_loss: 1.7416789531707764, Val Loss: 70.49303925037384, loss : 2.0726699829101562\n",
            "epoch: 2, classification_loss: 1.726030945777893, Val Loss: 70.49513399600983, loss : 1.9940085411071777\n",
            "epoch: 3, classification_loss: 1.7460572719573975, Val Loss: 70.4801174402237, loss : 2.0232934951782227\n",
            "epoch: 4, classification_loss: 1.7644684314727783, Val Loss: 70.4874814748764, loss : 2.00129771232605\n",
            "epoch: 5, classification_loss: 1.7559618949890137, Val Loss: 70.49228012561798, loss : 2.0240283012390137\n",
            "epoch: 6, classification_loss: 1.7468814849853516, Val Loss: 70.53738224506378, loss : 2.024501085281372\n",
            "epoch: 7, classification_loss: 1.7337862253189087, Val Loss: 70.514972448349, loss : 1.992121934890747\n",
            "epoch: 8, classification_loss: 1.7413098812103271, Val Loss: 70.50842809677124, loss : 2.0096404552459717\n",
            "epoch: 9, classification_loss: 1.7597681283950806, Val Loss: 70.56732654571533, loss : 2.00543475151062\n",
            "epoch: 10, classification_loss: 1.7544680833816528, Val Loss: 70.58887135982513, loss : 2.0134758949279785\n",
            "epoch: 11, classification_loss: 1.7588154077529907, Val Loss: 70.48813140392303, loss : 2.002727746963501\n",
            "epoch: 12, classification_loss: 1.7597301006317139, Val Loss: 70.50916755199432, loss : 2.007617712020874\n",
            "epoch: 13, classification_loss: 1.7577098608016968, Val Loss: 70.5658849477768, loss : 2.0072004795074463\n",
            "epoch: 14, classification_loss: 1.7360444068908691, Val Loss: 70.49831509590149, loss : 1.991944432258606\n",
            "epoch: 15, classification_loss: 1.7600756883621216, Val Loss: 70.5429984331131, loss : 2.0064892768859863\n",
            "epoch: 16, classification_loss: 1.7690285444259644, Val Loss: 70.57247221469879, loss : 2.010207176208496\n",
            "epoch: 17, classification_loss: 1.7577439546585083, Val Loss: 70.57793021202087, loss : 2.0002553462982178\n",
            "epoch: 18, classification_loss: 1.7507381439208984, Val Loss: 70.49420082569122, loss : 1.984049677848816\n",
            "epoch: 19, classification_loss: 1.7536134719848633, Val Loss: 70.51142191886902, loss : 1.9975210428237915\n",
            "Batch: 42, Test Acc: 0.5741185897435898\n",
            "Batch: 43:\n",
            "epoch: 0, classification_loss: 1.812018632888794, Val Loss: 70.5222738981247, loss : 1.812018632888794\n",
            "epoch: 1, classification_loss: 1.7894110679626465, Val Loss: 70.4693694114685, loss : 2.1087799072265625\n",
            "epoch: 2, classification_loss: 1.7679367065429688, Val Loss: 70.49945116043091, loss : 2.0182509422302246\n",
            "epoch: 3, classification_loss: 1.8059800863265991, Val Loss: 70.48598146438599, loss : 2.082986354827881\n",
            "epoch: 4, classification_loss: 1.8192574977874756, Val Loss: 70.53130662441254, loss : 2.055386543273926\n",
            "epoch: 5, classification_loss: 1.8060699701309204, Val Loss: 70.4812376499176, loss : 2.084028720855713\n",
            "epoch: 6, classification_loss: 1.7872790098190308, Val Loss: 70.48377287387848, loss : 2.077305316925049\n",
            "epoch: 7, classification_loss: 1.7956047058105469, Val Loss: 70.49347031116486, loss : 2.0516955852508545\n",
            "epoch: 8, classification_loss: 1.8103909492492676, Val Loss: 70.4985579252243, loss : 2.0677130222320557\n",
            "epoch: 9, classification_loss: 1.800753116607666, Val Loss: 70.51814496517181, loss : 2.0466480255126953\n",
            "epoch: 10, classification_loss: 1.7987130880355835, Val Loss: 70.49059641361237, loss : 2.057455062866211\n",
            "epoch: 11, classification_loss: 1.7912204265594482, Val Loss: 70.53510355949402, loss : 2.051931619644165\n",
            "epoch: 12, classification_loss: 1.804229736328125, Val Loss: 70.55614674091339, loss : 2.0553300380706787\n",
            "epoch: 13, classification_loss: 1.7991654872894287, Val Loss: 70.54039859771729, loss : 2.048794746398926\n",
            "epoch: 14, classification_loss: 1.8060393333435059, Val Loss: 70.50520956516266, loss : 2.044757127761841\n",
            "epoch: 15, classification_loss: 1.8165284395217896, Val Loss: 70.53291141986847, loss : 2.0597357749938965\n",
            "epoch: 16, classification_loss: 1.7953689098358154, Val Loss: 70.54567039012909, loss : 2.03482985496521\n",
            "epoch: 17, classification_loss: 1.80514657497406, Val Loss: 70.54186165332794, loss : 2.0495879650115967\n",
            "epoch: 18, classification_loss: 1.7933906316757202, Val Loss: 70.50916528701782, loss : 2.0306854248046875\n",
            "epoch: 19, classification_loss: 1.7990641593933105, Val Loss: 70.55158638954163, loss : 2.032803773880005\n",
            "Batch: 43, Test Acc: 0.5739182692307693\n",
            "Batch: 44:\n",
            "epoch: 0, classification_loss: 1.750266671180725, Val Loss: 70.57284927368164, loss : 1.750266671180725\n",
            "epoch: 1, classification_loss: 1.7227619886398315, Val Loss: 70.55160677433014, loss : 2.061828851699829\n",
            "epoch: 2, classification_loss: 1.7006738185882568, Val Loss: 70.47736406326294, loss : 1.9640241861343384\n",
            "epoch: 3, classification_loss: 1.7293648719787598, Val Loss: 70.47694551944733, loss : 2.0032591819763184\n",
            "epoch: 4, classification_loss: 1.7491735219955444, Val Loss: 70.46847426891327, loss : 1.985715389251709\n",
            "epoch: 5, classification_loss: 1.7416586875915527, Val Loss: 70.50157332420349, loss : 2.0131590366363525\n",
            "epoch: 6, classification_loss: 1.7231453657150269, Val Loss: 70.49562120437622, loss : 2.0103983879089355\n",
            "epoch: 7, classification_loss: 1.7273215055465698, Val Loss: 70.51915788650513, loss : 1.9906256198883057\n",
            "epoch: 8, classification_loss: 1.747856855392456, Val Loss: 70.4851301908493, loss : 2.0062856674194336\n",
            "epoch: 9, classification_loss: 1.7343112230300903, Val Loss: 70.50528621673584, loss : 1.9812358617782593\n",
            "epoch: 10, classification_loss: 1.7458630800247192, Val Loss: 70.52454340457916, loss : 1.994814157485962\n",
            "epoch: 11, classification_loss: 1.736302137374878, Val Loss: 70.48784124851227, loss : 1.9870076179504395\n",
            "epoch: 12, classification_loss: 1.7372244596481323, Val Loss: 70.53185677528381, loss : 1.9806822538375854\n",
            "epoch: 13, classification_loss: 1.7424076795578003, Val Loss: 70.54720330238342, loss : 1.9816625118255615\n",
            "epoch: 14, classification_loss: 1.737152099609375, Val Loss: 70.4837703704834, loss : 1.9800149202346802\n",
            "epoch: 15, classification_loss: 1.7237329483032227, Val Loss: 70.4951560497284, loss : 1.9630446434020996\n",
            "epoch: 16, classification_loss: 1.7412805557250977, Val Loss: 70.57388484477997, loss : 1.9773610830307007\n",
            "epoch: 17, classification_loss: 1.7422490119934082, Val Loss: 70.50107848644257, loss : 1.972372055053711\n",
            "epoch: 18, classification_loss: 1.7376900911331177, Val Loss: 70.49345052242279, loss : 1.9740248918533325\n",
            "epoch: 19, classification_loss: 1.7308893203735352, Val Loss: 70.5115715265274, loss : 1.9598654508590698\n",
            "Batch: 44, Test Acc: 0.5739182692307693\n",
            "Batch: 45:\n",
            "epoch: 0, classification_loss: 1.7839083671569824, Val Loss: 70.51437830924988, loss : 1.7839083671569824\n",
            "epoch: 1, classification_loss: 1.7308505773544312, Val Loss: 70.51016545295715, loss : 2.0698227882385254\n",
            "epoch: 2, classification_loss: 1.7186743021011353, Val Loss: 70.48710441589355, loss : 1.9987080097198486\n",
            "epoch: 3, classification_loss: 1.7473002672195435, Val Loss: 70.48106443881989, loss : 2.027740240097046\n",
            "epoch: 4, classification_loss: 1.7639737129211426, Val Loss: 70.51408815383911, loss : 2.0162785053253174\n",
            "epoch: 5, classification_loss: 1.7575136423110962, Val Loss: 70.4841091632843, loss : 2.0134880542755127\n",
            "epoch: 6, classification_loss: 1.7353990077972412, Val Loss: 70.48838114738464, loss : 2.006894826889038\n",
            "epoch: 7, classification_loss: 1.751592755317688, Val Loss: 70.49889028072357, loss : 2.009173631668091\n",
            "epoch: 8, classification_loss: 1.7460911273956299, Val Loss: 70.49498879909515, loss : 2.0101842880249023\n",
            "epoch: 9, classification_loss: 1.7574342489242554, Val Loss: 70.48804688453674, loss : 2.0098025798797607\n",
            "epoch: 10, classification_loss: 1.7402487993240356, Val Loss: 70.49918508529663, loss : 1.9995862245559692\n",
            "epoch: 11, classification_loss: 1.7495723962783813, Val Loss: 70.54294097423553, loss : 1.9919415712356567\n",
            "epoch: 12, classification_loss: 1.7572075128555298, Val Loss: 70.49869740009308, loss : 2.0040414333343506\n",
            "epoch: 13, classification_loss: 1.7469844818115234, Val Loss: 70.4745203256607, loss : 1.9921396970748901\n",
            "epoch: 14, classification_loss: 1.7499680519104004, Val Loss: 70.51469254493713, loss : 1.992228627204895\n",
            "epoch: 15, classification_loss: 1.7480370998382568, Val Loss: 70.54293954372406, loss : 1.99143385887146\n",
            "epoch: 16, classification_loss: 1.753390908241272, Val Loss: 70.4932233095169, loss : 1.9883286952972412\n",
            "epoch: 17, classification_loss: 1.7471625804901123, Val Loss: 70.47877204418182, loss : 1.980901837348938\n",
            "epoch: 18, classification_loss: 1.7540175914764404, Val Loss: 70.51126110553741, loss : 1.985134243965149\n",
            "epoch: 19, classification_loss: 1.7529683113098145, Val Loss: 70.49660778045654, loss : 1.9909181594848633\n",
            "Batch: 45, Test Acc: 0.5752203525641025\n",
            "Batch: 46:\n",
            "epoch: 0, classification_loss: 1.8271039724349976, Val Loss: 70.4833756685257, loss : 1.8271039724349976\n",
            "epoch: 1, classification_loss: 1.7960939407348633, Val Loss: 70.46833789348602, loss : 2.127385377883911\n",
            "epoch: 2, classification_loss: 1.764305830001831, Val Loss: 70.48599827289581, loss : 2.027224540710449\n",
            "epoch: 3, classification_loss: 1.7959948778152466, Val Loss: 70.4663952589035, loss : 2.086885452270508\n",
            "epoch: 4, classification_loss: 1.7965668439865112, Val Loss: 70.46068048477173, loss : 2.053143262863159\n",
            "epoch: 5, classification_loss: 1.7996236085891724, Val Loss: 70.45790958404541, loss : 2.0691659450531006\n",
            "epoch: 6, classification_loss: 1.7955083847045898, Val Loss: 70.44746446609497, loss : 2.0679914951324463\n",
            "epoch: 7, classification_loss: 1.7874717712402344, Val Loss: 70.4689474105835, loss : 2.050028085708618\n",
            "epoch: 8, classification_loss: 1.8058323860168457, Val Loss: 70.47635114192963, loss : 2.0593385696411133\n",
            "epoch: 9, classification_loss: 1.8109965324401855, Val Loss: 70.48993527889252, loss : 2.0643117427825928\n",
            "epoch: 10, classification_loss: 1.8075004816055298, Val Loss: 70.46991169452667, loss : 2.064957618713379\n",
            "epoch: 11, classification_loss: 1.79029381275177, Val Loss: 70.50583875179291, loss : 2.0491983890533447\n",
            "epoch: 12, classification_loss: 1.7971951961517334, Val Loss: 70.47933232784271, loss : 2.042715072631836\n",
            "epoch: 13, classification_loss: 1.8011432886123657, Val Loss: 70.48316359519958, loss : 2.0508217811584473\n",
            "epoch: 14, classification_loss: 1.7946574687957764, Val Loss: 70.49736738204956, loss : 2.039707899093628\n",
            "epoch: 15, classification_loss: 1.798457145690918, Val Loss: 70.49092853069305, loss : 2.0440263748168945\n",
            "epoch: 16, classification_loss: 1.7985444068908691, Val Loss: 70.49201321601868, loss : 2.050797939300537\n",
            "epoch: 17, classification_loss: 1.7970688343048096, Val Loss: 70.50217413902283, loss : 2.0520339012145996\n",
            "epoch: 18, classification_loss: 1.7991149425506592, Val Loss: 70.51640355587006, loss : 2.0492336750030518\n",
            "epoch: 19, classification_loss: 1.7986127138137817, Val Loss: 70.48535537719727, loss : 2.0355584621429443\n",
            "Batch: 46, Test Acc: 0.5746193910256411\n",
            "Batch: 47:\n",
            "epoch: 0, classification_loss: 1.8059312105178833, Val Loss: 70.47683560848236, loss : 1.8059312105178833\n",
            "epoch: 1, classification_loss: 1.7804176807403564, Val Loss: 70.47561752796173, loss : 2.1211700439453125\n",
            "epoch: 2, classification_loss: 1.7746553421020508, Val Loss: 70.50025618076324, loss : 2.0350613594055176\n",
            "epoch: 3, classification_loss: 1.8020399808883667, Val Loss: 70.50542771816254, loss : 2.0820565223693848\n",
            "epoch: 4, classification_loss: 1.818860411643982, Val Loss: 70.47335815429688, loss : 2.0709774494171143\n",
            "epoch: 5, classification_loss: 1.8026093244552612, Val Loss: 70.4364116191864, loss : 2.0642781257629395\n",
            "epoch: 6, classification_loss: 1.7942531108856201, Val Loss: 70.4693056344986, loss : 1.7942531108856201\n",
            "epoch: 7, classification_loss: 1.7661842107772827, Val Loss: 70.49066126346588, loss : 2.0952277183532715\n",
            "epoch: 8, classification_loss: 1.7452877759933472, Val Loss: 70.52117800712585, loss : 2.022014617919922\n",
            "epoch: 9, classification_loss: 1.7628401517868042, Val Loss: 70.47143220901489, loss : 2.01922869682312\n",
            "epoch: 10, classification_loss: 1.7919554710388184, Val Loss: 70.49840271472931, loss : 2.022756814956665\n",
            "epoch: 11, classification_loss: 1.7787175178527832, Val Loss: 70.4680243730545, loss : 2.021728754043579\n",
            "epoch: 12, classification_loss: 1.767162799835205, Val Loss: 70.46681344509125, loss : 2.026010274887085\n",
            "epoch: 13, classification_loss: 1.7668726444244385, Val Loss: 70.50806152820587, loss : 2.0026237964630127\n",
            "epoch: 14, classification_loss: 1.7713127136230469, Val Loss: 70.4845175743103, loss : 2.0403318405151367\n",
            "epoch: 15, classification_loss: 1.7824580669403076, Val Loss: 70.48601341247559, loss : 2.0129921436309814\n",
            "epoch: 16, classification_loss: 1.7776035070419312, Val Loss: 70.48328340053558, loss : 2.030518054962158\n",
            "epoch: 17, classification_loss: 1.7789034843444824, Val Loss: 70.46837341785431, loss : 2.018104314804077\n",
            "epoch: 18, classification_loss: 1.7840505838394165, Val Loss: 70.50026297569275, loss : 2.0229339599609375\n",
            "epoch: 19, classification_loss: 1.7832436561584473, Val Loss: 70.53223311901093, loss : 2.021925687789917\n",
            "Batch: 47, Test Acc: 0.5769230769230769\n",
            "Batch: 48:\n",
            "epoch: 0, classification_loss: 1.8188698291778564, Val Loss: 70.51524150371552, loss : 1.8188698291778564\n",
            "epoch: 1, classification_loss: 1.7737679481506348, Val Loss: 70.52062845230103, loss : 2.1180286407470703\n",
            "epoch: 2, classification_loss: 1.7592889070510864, Val Loss: 70.48705160617828, loss : 2.01770281791687\n",
            "epoch: 3, classification_loss: 1.794115662574768, Val Loss: 70.49583125114441, loss : 2.0737874507904053\n",
            "epoch: 4, classification_loss: 1.8104183673858643, Val Loss: 70.50155556201935, loss : 2.055997610092163\n",
            "epoch: 5, classification_loss: 1.7883492708206177, Val Loss: 70.5025315284729, loss : 2.043391704559326\n",
            "epoch: 6, classification_loss: 1.7754919528961182, Val Loss: 70.51323914527893, loss : 2.046825885772705\n",
            "epoch: 7, classification_loss: 1.7850661277770996, Val Loss: 70.50991809368134, loss : 2.030564308166504\n",
            "epoch: 8, classification_loss: 1.8049927949905396, Val Loss: 70.50107443332672, loss : 2.062875747680664\n",
            "epoch: 9, classification_loss: 1.795601487159729, Val Loss: 70.49381673336029, loss : 2.0380232334136963\n",
            "epoch: 10, classification_loss: 1.7963765859603882, Val Loss: 70.50066995620728, loss : 2.040175437927246\n",
            "epoch: 11, classification_loss: 1.7930622100830078, Val Loss: 70.49515092372894, loss : 2.0262694358825684\n",
            "epoch: 12, classification_loss: 1.800141453742981, Val Loss: 70.50588381290436, loss : 2.03879451751709\n",
            "epoch: 13, classification_loss: 1.7991821765899658, Val Loss: 70.54389953613281, loss : 2.0387043952941895\n",
            "epoch: 14, classification_loss: 1.7986667156219482, Val Loss: 70.52772784233093, loss : 2.0256175994873047\n",
            "epoch: 15, classification_loss: 1.7888414859771729, Val Loss: 70.52696371078491, loss : 2.0242531299591064\n",
            "epoch: 16, classification_loss: 1.7849178314208984, Val Loss: 70.56260800361633, loss : 2.005028009414673\n",
            "epoch: 17, classification_loss: 1.7916107177734375, Val Loss: 70.54805421829224, loss : 2.0293099880218506\n",
            "epoch: 18, classification_loss: 1.7963829040527344, Val Loss: 70.52779483795166, loss : 2.0164289474487305\n",
            "epoch: 19, classification_loss: 1.7947254180908203, Val Loss: 70.59269833564758, loss : 2.0333495140075684\n",
            "Batch: 48, Test Acc: 0.573417467948718\n",
            "Batch: 49:\n",
            "epoch: 0, classification_loss: 1.7856892347335815, Val Loss: 70.52519714832306, loss : 1.7856892347335815\n",
            "epoch: 1, classification_loss: 1.7575631141662598, Val Loss: 70.48149394989014, loss : 2.0695834159851074\n",
            "epoch: 2, classification_loss: 1.742282748222351, Val Loss: 70.51763164997101, loss : 1.9829425811767578\n",
            "epoch: 3, classification_loss: 1.761259913444519, Val Loss: 70.44189608097076, loss : 2.035383939743042\n",
            "epoch: 4, classification_loss: 1.7765209674835205, Val Loss: 70.44434022903442, loss : 2.0033750534057617\n",
            "epoch: 5, classification_loss: 1.7789783477783203, Val Loss: 70.50170588493347, loss : 2.031980276107788\n",
            "epoch: 6, classification_loss: 1.7556710243225098, Val Loss: 70.49700224399567, loss : 2.0203070640563965\n",
            "epoch: 7, classification_loss: 1.756610631942749, Val Loss: 70.48833131790161, loss : 2.0144333839416504\n",
            "epoch: 8, classification_loss: 1.7660568952560425, Val Loss: 70.44022727012634, loss : 2.0175278186798096\n",
            "epoch: 9, classification_loss: 1.7547388076782227, Val Loss: 70.46199822425842, loss : 1.9978886842727661\n",
            "epoch: 10, classification_loss: 1.7636990547180176, Val Loss: 70.47282493114471, loss : 1.9993541240692139\n",
            "epoch: 11, classification_loss: 1.7566776275634766, Val Loss: 70.47311425209045, loss : 2.000523567199707\n",
            "epoch: 12, classification_loss: 1.766411304473877, Val Loss: 70.4708161354065, loss : 2.0064291954040527\n",
            "epoch: 13, classification_loss: 1.7662941217422485, Val Loss: 70.46815156936646, loss : 2.0163047313690186\n",
            "epoch: 14, classification_loss: 1.765700101852417, Val Loss: 70.47390329837799, loss : 1.999192714691162\n",
            "epoch: 15, classification_loss: 1.7727984189987183, Val Loss: 70.46397650241852, loss : 2.0061256885528564\n",
            "epoch: 16, classification_loss: 1.771380066871643, Val Loss: 70.47382974624634, loss : 1.9951971769332886\n",
            "epoch: 17, classification_loss: 1.7573835849761963, Val Loss: 70.48418152332306, loss : 1.9959447383880615\n",
            "epoch: 18, classification_loss: 1.7627243995666504, Val Loss: 70.47078490257263, loss : 1.9840433597564697\n",
            "epoch: 19, classification_loss: 1.7609683275222778, Val Loss: 70.46327066421509, loss : 1.9967267513275146\n",
            "Batch: 49, Test Acc: 0.5764222756410257\n",
            "Batch: 50:\n",
            "epoch: 0, classification_loss: 1.758127212524414, Val Loss: 70.57293581962585, loss : 1.758127212524414\n",
            "epoch: 1, classification_loss: 1.715783715248108, Val Loss: 70.50442826747894, loss : 2.0473713874816895\n",
            "epoch: 2, classification_loss: 1.7054768800735474, Val Loss: 70.52827310562134, loss : 1.9646105766296387\n",
            "epoch: 3, classification_loss: 1.7280559539794922, Val Loss: 70.48531031608582, loss : 1.992103099822998\n",
            "epoch: 4, classification_loss: 1.7420743703842163, Val Loss: 70.46801352500916, loss : 1.975669026374817\n",
            "epoch: 5, classification_loss: 1.7333265542984009, Val Loss: 70.46776926517487, loss : 1.9892613887786865\n",
            "epoch: 6, classification_loss: 1.7189310789108276, Val Loss: 70.49396574497223, loss : 1.9862161874771118\n",
            "epoch: 7, classification_loss: 1.7263166904449463, Val Loss: 70.51717805862427, loss : 1.9859533309936523\n",
            "epoch: 8, classification_loss: 1.7359671592712402, Val Loss: 70.48459267616272, loss : 1.9813963174819946\n",
            "epoch: 9, classification_loss: 1.7281365394592285, Val Loss: 70.5135326385498, loss : 1.9789353609085083\n",
            "epoch: 10, classification_loss: 1.7169189453125, Val Loss: 70.51840496063232, loss : 1.9627538919448853\n",
            "epoch: 11, classification_loss: 1.729026198387146, Val Loss: 70.55107009410858, loss : 1.965415596961975\n",
            "epoch: 12, classification_loss: 1.730944037437439, Val Loss: 70.55176329612732, loss : 1.9673736095428467\n",
            "epoch: 13, classification_loss: 1.740026831626892, Val Loss: 70.55083131790161, loss : 1.9775055646896362\n",
            "epoch: 14, classification_loss: 1.7315473556518555, Val Loss: 70.53806042671204, loss : 1.971150279045105\n",
            "epoch: 15, classification_loss: 1.726008653640747, Val Loss: 70.51119470596313, loss : 1.9733309745788574\n",
            "epoch: 16, classification_loss: 1.7372024059295654, Val Loss: 70.52849841117859, loss : 1.9730472564697266\n",
            "epoch: 17, classification_loss: 1.7293355464935303, Val Loss: 70.56277799606323, loss : 1.9676601886749268\n",
            "epoch: 18, classification_loss: 1.7505395412445068, Val Loss: 70.5822457075119, loss : 1.9776394367218018\n",
            "epoch: 19, classification_loss: 1.7338693141937256, Val Loss: 70.53860175609589, loss : 1.9758338928222656\n",
            "Batch: 50, Test Acc: 0.5764222756410257\n",
            "Batch: 51:\n",
            "epoch: 0, classification_loss: 1.8051064014434814, Val Loss: 70.54466640949249, loss : 1.8051064014434814\n",
            "epoch: 1, classification_loss: 1.7745941877365112, Val Loss: 70.49739956855774, loss : 2.09860897064209\n",
            "epoch: 2, classification_loss: 1.7587217092514038, Val Loss: 70.54909944534302, loss : 2.0072643756866455\n",
            "epoch: 3, classification_loss: 1.7962932586669922, Val Loss: 70.45278775691986, loss : 2.0673747062683105\n",
            "epoch: 4, classification_loss: 1.8034495115280151, Val Loss: 70.52548670768738, loss : 2.028742790222168\n",
            "epoch: 5, classification_loss: 1.7828373908996582, Val Loss: 70.49191951751709, loss : 2.0569961071014404\n",
            "epoch: 6, classification_loss: 1.7701767683029175, Val Loss: 70.51728570461273, loss : 2.0461411476135254\n",
            "epoch: 7, classification_loss: 1.788412094116211, Val Loss: 70.49052476882935, loss : 2.044036865234375\n",
            "epoch: 8, classification_loss: 1.7936427593231201, Val Loss: 70.46685457229614, loss : 2.0568885803222656\n",
            "epoch: 9, classification_loss: 1.7791043519973755, Val Loss: 70.52082359790802, loss : 2.028528928756714\n",
            "epoch: 10, classification_loss: 1.7942490577697754, Val Loss: 70.49734222888947, loss : 2.044912576675415\n",
            "epoch: 11, classification_loss: 1.7932815551757812, Val Loss: 70.4891072511673, loss : 2.034893035888672\n",
            "epoch: 12, classification_loss: 1.794944167137146, Val Loss: 70.49563133716583, loss : 2.0359792709350586\n",
            "epoch: 13, classification_loss: 1.797730803489685, Val Loss: 70.53205823898315, loss : 2.0405774116516113\n",
            "epoch: 14, classification_loss: 1.7765604257583618, Val Loss: 70.49126386642456, loss : 2.0170507431030273\n",
            "epoch: 15, classification_loss: 1.7933166027069092, Val Loss: 70.50317585468292, loss : 2.0361745357513428\n",
            "epoch: 16, classification_loss: 1.7862838506698608, Val Loss: 70.68318843841553, loss : 2.0201597213745117\n",
            "epoch: 17, classification_loss: 1.7876722812652588, Val Loss: 70.52172827720642, loss : 2.0290071964263916\n",
            "epoch: 18, classification_loss: 1.7895179986953735, Val Loss: 70.50770127773285, loss : 2.0177807807922363\n",
            "epoch: 19, classification_loss: 1.7997132539749146, Val Loss: 70.53366160392761, loss : 2.0451908111572266\n",
            "Batch: 51, Test Acc: 0.5757211538461539\n",
            "Batch: 52:\n",
            "epoch: 0, classification_loss: 1.7652794122695923, Val Loss: 70.49686706066132, loss : 1.7652794122695923\n",
            "epoch: 1, classification_loss: 1.743638515472412, Val Loss: 70.50533878803253, loss : 2.0607991218566895\n",
            "epoch: 2, classification_loss: 1.731276512145996, Val Loss: 70.51205372810364, loss : 1.991646647453308\n",
            "epoch: 3, classification_loss: 1.7518553733825684, Val Loss: 70.49010574817657, loss : 2.01222562789917\n",
            "epoch: 4, classification_loss: 1.762563705444336, Val Loss: 70.49222993850708, loss : 1.9913737773895264\n",
            "epoch: 5, classification_loss: 1.754961371421814, Val Loss: 70.47329998016357, loss : 2.012453556060791\n",
            "epoch: 6, classification_loss: 1.7379603385925293, Val Loss: 70.49259996414185, loss : 1.999946117401123\n",
            "epoch: 7, classification_loss: 1.7424240112304688, Val Loss: 70.51432204246521, loss : 1.999114751815796\n",
            "epoch: 8, classification_loss: 1.75141179561615, Val Loss: 70.52905356884003, loss : 2.0024845600128174\n",
            "epoch: 9, classification_loss: 1.76514732837677, Val Loss: 70.51485896110535, loss : 2.006497621536255\n",
            "epoch: 10, classification_loss: 1.7449592351913452, Val Loss: 70.4961005449295, loss : 1.9907457828521729\n",
            "epoch: 11, classification_loss: 1.750156044960022, Val Loss: 70.56017696857452, loss : 1.9939574003219604\n",
            "epoch: 12, classification_loss: 1.7429157495498657, Val Loss: 70.51765179634094, loss : 1.9926837682724\n",
            "epoch: 13, classification_loss: 1.7564852237701416, Val Loss: 70.54551565647125, loss : 2.0013065338134766\n",
            "epoch: 14, classification_loss: 1.7521734237670898, Val Loss: 70.54414403438568, loss : 1.9875447750091553\n",
            "epoch: 15, classification_loss: 1.7600351572036743, Val Loss: 70.53693354129791, loss : 1.9942939281463623\n",
            "epoch: 16, classification_loss: 1.7559007406234741, Val Loss: 70.51033186912537, loss : 1.9926097393035889\n",
            "epoch: 17, classification_loss: 1.736089825630188, Val Loss: 70.55759644508362, loss : 1.9733061790466309\n",
            "epoch: 18, classification_loss: 1.7488799095153809, Val Loss: 70.52966117858887, loss : 1.9859247207641602\n",
            "epoch: 19, classification_loss: 1.756734013557434, Val Loss: 70.53853476047516, loss : 1.9948703050613403\n",
            "Batch: 52, Test Acc: 0.5736177884615384\n",
            "Batch: 53:\n",
            "epoch: 0, classification_loss: 1.813373327255249, Val Loss: 70.491037607193, loss : 1.813373327255249\n",
            "epoch: 1, classification_loss: 1.767147183418274, Val Loss: 70.50150215625763, loss : 2.098323345184326\n",
            "epoch: 2, classification_loss: 1.7537035942077637, Val Loss: 70.48970818519592, loss : 2.0265517234802246\n",
            "epoch: 3, classification_loss: 1.77162766456604, Val Loss: 70.48482978343964, loss : 2.048302412033081\n",
            "epoch: 4, classification_loss: 1.8053967952728271, Val Loss: 70.44190800189972, loss : 2.0537099838256836\n",
            "epoch: 5, classification_loss: 1.797398567199707, Val Loss: 70.43355464935303, loss : 2.0508334636688232\n",
            "epoch: 6, classification_loss: 1.780842900276184, Val Loss: 70.4870924949646, loss : 2.0476343631744385\n",
            "epoch: 7, classification_loss: 1.767686367034912, Val Loss: 70.5418871641159, loss : 2.0360965728759766\n",
            "epoch: 8, classification_loss: 1.7797887325286865, Val Loss: 70.5057725906372, loss : 2.044403553009033\n",
            "epoch: 9, classification_loss: 1.7695446014404297, Val Loss: 70.53844797611237, loss : 2.0379178524017334\n",
            "epoch: 10, classification_loss: 1.7889652252197266, Val Loss: 70.54977083206177, loss : 2.038982629776001\n",
            "epoch: 11, classification_loss: 1.7929461002349854, Val Loss: 70.52324724197388, loss : 2.0363667011260986\n",
            "epoch: 12, classification_loss: 1.7947020530700684, Val Loss: 70.54198157787323, loss : 2.0437324047088623\n",
            "epoch: 13, classification_loss: 1.7771902084350586, Val Loss: 70.54811024665833, loss : 2.0323033332824707\n",
            "epoch: 14, classification_loss: 1.7890135049819946, Val Loss: 70.52792179584503, loss : 2.0378241539001465\n",
            "epoch: 15, classification_loss: 1.780527949333191, Val Loss: 70.49909210205078, loss : 2.029151678085327\n",
            "epoch: 16, classification_loss: 1.7876079082489014, Val Loss: 70.62414741516113, loss : 2.0216691493988037\n",
            "epoch: 17, classification_loss: 1.7902666330337524, Val Loss: 70.64183938503265, loss : 2.0322611331939697\n",
            "epoch: 18, classification_loss: 1.7955034971237183, Val Loss: 70.54408359527588, loss : 2.0305018424987793\n",
            "epoch: 19, classification_loss: 1.7876062393188477, Val Loss: 70.53856658935547, loss : 2.036916494369507\n",
            "Batch: 53, Test Acc: 0.5737179487179487\n",
            "Batch: 54:\n",
            "epoch: 0, classification_loss: 1.8615914583206177, Val Loss: 70.47668123245239, loss : 1.8615914583206177\n",
            "epoch: 1, classification_loss: 1.827121376991272, Val Loss: 70.52774786949158, loss : 2.151831865310669\n",
            "epoch: 2, classification_loss: 1.7939139604568481, Val Loss: 70.54519641399384, loss : 2.0596046447753906\n",
            "epoch: 3, classification_loss: 1.8335922956466675, Val Loss: 70.52564263343811, loss : 2.10538387298584\n",
            "epoch: 4, classification_loss: 1.8437788486480713, Val Loss: 70.47031486034393, loss : 2.0809738636016846\n",
            "epoch: 5, classification_loss: 1.8434250354766846, Val Loss: 70.44617474079132, loss : 2.1023671627044678\n",
            "epoch: 6, classification_loss: 1.8249107599258423, Val Loss: 70.4983332157135, loss : 2.0991671085357666\n",
            "epoch: 7, classification_loss: 1.8234529495239258, Val Loss: 70.5161966085434, loss : 2.0821378231048584\n",
            "epoch: 8, classification_loss: 1.836984395980835, Val Loss: 70.48159551620483, loss : 2.0913562774658203\n",
            "epoch: 9, classification_loss: 1.8325841426849365, Val Loss: 70.47950100898743, loss : 2.0862154960632324\n",
            "epoch: 10, classification_loss: 1.8285266160964966, Val Loss: 70.5011967420578, loss : 2.0803794860839844\n",
            "epoch: 11, classification_loss: 1.832189917564392, Val Loss: 70.50279152393341, loss : 2.0795302391052246\n",
            "epoch: 12, classification_loss: 1.840315818786621, Val Loss: 70.47070527076721, loss : 2.0860373973846436\n",
            "epoch: 13, classification_loss: 1.8454283475875854, Val Loss: 70.46921241283417, loss : 2.083617925643921\n",
            "epoch: 14, classification_loss: 1.8495509624481201, Val Loss: 70.4927306175232, loss : 2.093136787414551\n",
            "epoch: 15, classification_loss: 1.8272093534469604, Val Loss: 70.46589040756226, loss : 2.0703704357147217\n",
            "epoch: 16, classification_loss: 1.8259836435317993, Val Loss: 70.47927129268646, loss : 2.067223072052002\n",
            "epoch: 17, classification_loss: 1.8333326578140259, Val Loss: 70.50294744968414, loss : 2.0795412063598633\n",
            "epoch: 18, classification_loss: 1.8274672031402588, Val Loss: 70.52537167072296, loss : 2.0703797340393066\n",
            "epoch: 19, classification_loss: 1.842372179031372, Val Loss: 70.51610350608826, loss : 2.0806820392608643\n",
            "Batch: 54, Test Acc: 0.573417467948718\n",
            "Batch: 55:\n",
            "epoch: 0, classification_loss: 1.8252525329589844, Val Loss: 70.58829057216644, loss : 1.8252525329589844\n",
            "epoch: 1, classification_loss: 1.7924036979675293, Val Loss: 70.57756888866425, loss : 2.132615089416504\n",
            "epoch: 2, classification_loss: 1.7622345685958862, Val Loss: 70.49927246570587, loss : 2.0418429374694824\n",
            "epoch: 3, classification_loss: 1.802833914756775, Val Loss: 70.49930918216705, loss : 2.0832037925720215\n",
            "epoch: 4, classification_loss: 1.8097018003463745, Val Loss: 70.48042118549347, loss : 2.0569911003112793\n",
            "epoch: 5, classification_loss: 1.812295913696289, Val Loss: 70.45732843875885, loss : 2.072899580001831\n",
            "epoch: 6, classification_loss: 1.7889132499694824, Val Loss: 70.51129353046417, loss : 2.06538724899292\n",
            "epoch: 7, classification_loss: 1.7850884199142456, Val Loss: 70.50856685638428, loss : 2.0534486770629883\n",
            "epoch: 8, classification_loss: 1.7844212055206299, Val Loss: 70.46608126163483, loss : 2.040172576904297\n",
            "epoch: 9, classification_loss: 1.7938001155853271, Val Loss: 70.52120864391327, loss : 2.0492467880249023\n",
            "epoch: 10, classification_loss: 1.7951416969299316, Val Loss: 70.50612723827362, loss : 2.0431036949157715\n",
            "epoch: 11, classification_loss: 1.7953661680221558, Val Loss: 70.48054850101471, loss : 2.046299934387207\n",
            "epoch: 12, classification_loss: 1.794442892074585, Val Loss: 70.5238870382309, loss : 2.0396528244018555\n",
            "epoch: 13, classification_loss: 1.7932519912719727, Val Loss: 70.55407404899597, loss : 2.0393621921539307\n",
            "epoch: 14, classification_loss: 1.7968112230300903, Val Loss: 70.52216124534607, loss : 2.039144992828369\n",
            "epoch: 15, classification_loss: 1.7938402891159058, Val Loss: 70.51307427883148, loss : 2.0333099365234375\n",
            "epoch: 16, classification_loss: 1.7945553064346313, Val Loss: 70.56367349624634, loss : 2.0404462814331055\n",
            "epoch: 17, classification_loss: 1.7943544387817383, Val Loss: 70.53946363925934, loss : 2.0272538661956787\n",
            "epoch: 18, classification_loss: 1.8104848861694336, Val Loss: 70.50846838951111, loss : 2.0490283966064453\n",
            "epoch: 19, classification_loss: 1.8105101585388184, Val Loss: 70.56314611434937, loss : 2.0404698848724365\n",
            "Batch: 55, Test Acc: 0.5733173076923077\n",
            "Batch: 56:\n",
            "epoch: 0, classification_loss: 1.8242990970611572, Val Loss: 70.48293137550354, loss : 1.8242990970611572\n",
            "epoch: 1, classification_loss: 1.781173825263977, Val Loss: 70.60100948810577, loss : 2.124805450439453\n",
            "epoch: 2, classification_loss: 1.75933039188385, Val Loss: 70.55243289470673, loss : 2.0493075847625732\n",
            "epoch: 3, classification_loss: 1.788811206817627, Val Loss: 70.48182165622711, loss : 2.0830278396606445\n",
            "epoch: 4, classification_loss: 1.8160163164138794, Val Loss: 70.45545148849487, loss : 2.083463668823242\n",
            "epoch: 5, classification_loss: 1.8172922134399414, Val Loss: 70.53968608379364, loss : 2.07832932472229\n",
            "epoch: 6, classification_loss: 1.7961046695709229, Val Loss: 70.57746183872223, loss : 2.0825257301330566\n",
            "epoch: 7, classification_loss: 1.780135989189148, Val Loss: 70.51298022270203, loss : 2.0592966079711914\n",
            "epoch: 8, classification_loss: 1.7960472106933594, Val Loss: 70.557457447052, loss : 2.0650601387023926\n",
            "epoch: 9, classification_loss: 1.7950173616409302, Val Loss: 70.59186553955078, loss : 2.055610179901123\n",
            "epoch: 10, classification_loss: 1.797097086906433, Val Loss: 70.51954305171967, loss : 2.0567493438720703\n",
            "epoch: 11, classification_loss: 1.7940471172332764, Val Loss: 70.53493523597717, loss : 2.043980598449707\n",
            "epoch: 12, classification_loss: 1.8006584644317627, Val Loss: 70.55348265171051, loss : 2.0583882331848145\n",
            "epoch: 13, classification_loss: 1.792782187461853, Val Loss: 70.51593613624573, loss : 2.052204132080078\n",
            "epoch: 14, classification_loss: 1.7933423519134521, Val Loss: 70.5495103597641, loss : 2.048841714859009\n",
            "epoch: 15, classification_loss: 1.8017793893814087, Val Loss: 70.52806532382965, loss : 2.051269292831421\n",
            "epoch: 16, classification_loss: 1.7966606616973877, Val Loss: 70.54669523239136, loss : 2.0452654361724854\n",
            "epoch: 17, classification_loss: 1.7938865423202515, Val Loss: 70.57044303417206, loss : 2.048947811126709\n",
            "epoch: 18, classification_loss: 1.791454553604126, Val Loss: 70.59067225456238, loss : 2.0386276245117188\n",
            "epoch: 19, classification_loss: 1.8057807683944702, Val Loss: 70.54287767410278, loss : 2.0519354343414307\n",
            "Batch: 56, Test Acc: 0.5729166666666666\n",
            "Batch: 57:\n",
            "epoch: 0, classification_loss: 1.785410761833191, Val Loss: 70.64866101741791, loss : 1.785410761833191\n",
            "epoch: 1, classification_loss: 1.742027759552002, Val Loss: 70.57129621505737, loss : 2.0759170055389404\n",
            "epoch: 2, classification_loss: 1.714137077331543, Val Loss: 70.4922354221344, loss : 1.9904794692993164\n",
            "epoch: 3, classification_loss: 1.758402943611145, Val Loss: 70.4480848312378, loss : 2.035271167755127\n",
            "epoch: 4, classification_loss: 1.7682145833969116, Val Loss: 70.50766682624817, loss : 2.010181188583374\n",
            "epoch: 5, classification_loss: 1.7578487396240234, Val Loss: 70.46798408031464, loss : 2.0261425971984863\n",
            "epoch: 6, classification_loss: 1.7552894353866577, Val Loss: 70.49231994152069, loss : 2.0239570140838623\n",
            "epoch: 7, classification_loss: 1.7467877864837646, Val Loss: 70.5006639957428, loss : 2.012674331665039\n",
            "epoch: 8, classification_loss: 1.7547563314437866, Val Loss: 70.44999420642853, loss : 2.013617515563965\n",
            "epoch: 9, classification_loss: 1.7576078176498413, Val Loss: 70.43413281440735, loss : 2.006412982940674\n",
            "epoch: 10, classification_loss: 1.7553162574768066, Val Loss: 70.46846377849579, loss : 2.001821517944336\n",
            "epoch: 11, classification_loss: 1.7590742111206055, Val Loss: 70.47356343269348, loss : 1.998066782951355\n",
            "epoch: 12, classification_loss: 1.760021686553955, Val Loss: 70.47876703739166, loss : 2.0056984424591064\n",
            "epoch: 13, classification_loss: 1.7460302114486694, Val Loss: 70.47913444042206, loss : 1.9896845817565918\n",
            "epoch: 14, classification_loss: 1.7592289447784424, Val Loss: 70.4885892868042, loss : 2.0034468173980713\n",
            "epoch: 15, classification_loss: 1.7627884149551392, Val Loss: 70.45515370368958, loss : 2.0015573501586914\n",
            "epoch: 16, classification_loss: 1.7477399110794067, Val Loss: 70.49906587600708, loss : 1.9870649576187134\n",
            "epoch: 17, classification_loss: 1.7466717958450317, Val Loss: 70.50952625274658, loss : 1.9881718158721924\n",
            "epoch: 18, classification_loss: 1.7621805667877197, Val Loss: 70.49946677684784, loss : 2.002525806427002\n",
            "epoch: 19, classification_loss: 1.7647045850753784, Val Loss: 70.47687470912933, loss : 2.009075164794922\n",
            "Batch: 57, Test Acc: 0.5761217948717948\n",
            "Batch: 58:\n",
            "epoch: 0, classification_loss: 1.7607812881469727, Val Loss: 70.62625348567963, loss : 1.7607812881469727\n",
            "epoch: 1, classification_loss: 1.720215916633606, Val Loss: 70.57858407497406, loss : 2.0555481910705566\n",
            "epoch: 2, classification_loss: 1.709073781967163, Val Loss: 70.57463920116425, loss : 1.9775562286376953\n",
            "epoch: 3, classification_loss: 1.7296710014343262, Val Loss: 70.51597106456757, loss : 2.0135855674743652\n",
            "epoch: 4, classification_loss: 1.7485698461532593, Val Loss: 70.50278007984161, loss : 1.998304009437561\n",
            "epoch: 5, classification_loss: 1.7501710653305054, Val Loss: 70.49236333370209, loss : 2.0138516426086426\n",
            "epoch: 6, classification_loss: 1.7320352792739868, Val Loss: 70.44828748703003, loss : 2.014909505844116\n",
            "epoch: 7, classification_loss: 1.7268569469451904, Val Loss: 70.4967451095581, loss : 1.990052342414856\n",
            "epoch: 8, classification_loss: 1.7428867816925049, Val Loss: 70.46583116054535, loss : 2.0099639892578125\n",
            "epoch: 9, classification_loss: 1.7432193756103516, Val Loss: 70.51356446743011, loss : 1.9948604106903076\n",
            "epoch: 10, classification_loss: 1.7449439764022827, Val Loss: 70.48135483264923, loss : 1.9996800422668457\n",
            "epoch: 11, classification_loss: 1.7359135150909424, Val Loss: 70.4776451587677, loss : 1.9910249710083008\n",
            "epoch: 12, classification_loss: 1.7383980751037598, Val Loss: 70.45692491531372, loss : 1.9851245880126953\n",
            "epoch: 13, classification_loss: 1.7405422925949097, Val Loss: 70.48263168334961, loss : 1.997216820716858\n",
            "epoch: 14, classification_loss: 1.7321546077728271, Val Loss: 70.45142328739166, loss : 1.979083776473999\n",
            "epoch: 15, classification_loss: 1.7316296100616455, Val Loss: 70.46452128887177, loss : 1.9851139783859253\n",
            "epoch: 16, classification_loss: 1.7354811429977417, Val Loss: 70.5122629404068, loss : 1.9834362268447876\n",
            "epoch: 17, classification_loss: 1.734254002571106, Val Loss: 70.43843281269073, loss : 1.9824435710906982\n",
            "epoch: 18, classification_loss: 1.733468770980835, Val Loss: 70.48713600635529, loss : 1.974750280380249\n",
            "epoch: 19, classification_loss: 1.7362395524978638, Val Loss: 70.50969398021698, loss : 1.983176827430725\n",
            "Batch: 58, Test Acc: 0.5751201923076923\n",
            "Batch: 59:\n",
            "epoch: 0, classification_loss: 1.803117275238037, Val Loss: 70.71036195755005, loss : 1.803117275238037\n",
            "epoch: 1, classification_loss: 1.7844046354293823, Val Loss: 70.5356457233429, loss : 2.117307186126709\n",
            "epoch: 2, classification_loss: 1.7520484924316406, Val Loss: 70.65184533596039, loss : 2.022812843322754\n",
            "epoch: 3, classification_loss: 1.7761255502700806, Val Loss: 70.4795548915863, loss : 2.0685081481933594\n",
            "epoch: 4, classification_loss: 1.7977222204208374, Val Loss: 70.61058247089386, loss : 2.0500741004943848\n",
            "epoch: 5, classification_loss: 1.8012281656265259, Val Loss: 70.50607204437256, loss : 2.0666110515594482\n",
            "epoch: 6, classification_loss: 1.7849031686782837, Val Loss: 70.52183055877686, loss : 2.0586934089660645\n",
            "epoch: 7, classification_loss: 1.7834599018096924, Val Loss: 70.48734188079834, loss : 2.044375419616699\n",
            "epoch: 8, classification_loss: 1.7896370887756348, Val Loss: 70.55468451976776, loss : 2.0465869903564453\n",
            "epoch: 9, classification_loss: 1.7745968103408813, Val Loss: 70.50058603286743, loss : 2.043766498565674\n",
            "epoch: 10, classification_loss: 1.7921189069747925, Val Loss: 70.497589468956, loss : 2.0408918857574463\n",
            "epoch: 11, classification_loss: 1.7974544763565063, Val Loss: 70.546138048172, loss : 2.0546560287475586\n",
            "epoch: 12, classification_loss: 1.7931833267211914, Val Loss: 70.58490455150604, loss : 2.0469706058502197\n",
            "epoch: 13, classification_loss: 1.8071708679199219, Val Loss: 70.53101623058319, loss : 2.053281784057617\n",
            "epoch: 14, classification_loss: 1.8006106615066528, Val Loss: 70.48473381996155, loss : 2.0460734367370605\n",
            "epoch: 15, classification_loss: 1.7857141494750977, Val Loss: 70.50720262527466, loss : 2.025761127471924\n",
            "epoch: 16, classification_loss: 1.7922554016113281, Val Loss: 70.5125823020935, loss : 2.0359315872192383\n",
            "epoch: 17, classification_loss: 1.7905434370040894, Val Loss: 70.50659000873566, loss : 2.0360422134399414\n",
            "epoch: 18, classification_loss: 1.7907671928405762, Val Loss: 70.53133618831635, loss : 2.028244733810425\n",
            "epoch: 19, classification_loss: 1.802152156829834, Val Loss: 70.50687277317047, loss : 2.0450525283813477\n",
            "Batch: 59, Test Acc: 0.5752203525641025\n",
            "Batch: 60:\n",
            "epoch: 0, classification_loss: 1.7571760416030884, Val Loss: 70.54617822170258, loss : 1.7571760416030884\n",
            "epoch: 1, classification_loss: 1.7231508493423462, Val Loss: 70.55413281917572, loss : 2.0383141040802\n",
            "epoch: 2, classification_loss: 1.710762619972229, Val Loss: 70.48428046703339, loss : 1.9493663311004639\n",
            "epoch: 3, classification_loss: 1.749851107597351, Val Loss: 70.44560480117798, loss : 2.0161123275756836\n",
            "epoch: 4, classification_loss: 1.7571406364440918, Val Loss: 70.49363481998444, loss : 1.981123924255371\n",
            "epoch: 5, classification_loss: 1.7409650087356567, Val Loss: 70.45645403862, loss : 2.015146255493164\n",
            "epoch: 6, classification_loss: 1.7317661046981812, Val Loss: 70.48444950580597, loss : 2.016288995742798\n",
            "epoch: 7, classification_loss: 1.727911114692688, Val Loss: 70.44547474384308, loss : 1.9755550622940063\n",
            "epoch: 8, classification_loss: 1.739253282546997, Val Loss: 70.44419384002686, loss : 1.9911028146743774\n",
            "epoch: 9, classification_loss: 1.7416268587112427, Val Loss: 70.46068358421326, loss : 1.9808344841003418\n",
            "epoch: 10, classification_loss: 1.7383304834365845, Val Loss: 70.4699319601059, loss : 1.9828166961669922\n",
            "epoch: 11, classification_loss: 1.7365491390228271, Val Loss: 70.4555070400238, loss : 1.9824031591415405\n",
            "epoch: 12, classification_loss: 1.7433390617370605, Val Loss: 70.47863841056824, loss : 1.981683373451233\n",
            "epoch: 13, classification_loss: 1.7400486469268799, Val Loss: 70.55296289920807, loss : 1.986156940460205\n",
            "epoch: 14, classification_loss: 1.7400747537612915, Val Loss: 70.50248908996582, loss : 1.9753754138946533\n",
            "epoch: 15, classification_loss: 1.7379438877105713, Val Loss: 70.43426263332367, loss : 1.9781708717346191\n",
            "epoch: 16, classification_loss: 1.7264310121536255, Val Loss: 70.46874415874481, loss : 1.959169864654541\n",
            "epoch: 17, classification_loss: 1.7391926050186157, Val Loss: 70.4688229560852, loss : 1.9764405488967896\n",
            "epoch: 18, classification_loss: 1.7344342470169067, Val Loss: 70.44193804264069, loss : 1.9717721939086914\n",
            "epoch: 19, classification_loss: 1.7267783880233765, Val Loss: 70.45737159252167, loss : 1.9680461883544922\n",
            "Batch: 60, Test Acc: 0.5759214743589743\n",
            "Batch: 61:\n",
            "epoch: 0, classification_loss: 1.8104608058929443, Val Loss: 70.5040647983551, loss : 1.8104608058929443\n",
            "epoch: 1, classification_loss: 1.773453950881958, Val Loss: 70.51983630657196, loss : 2.1097066402435303\n",
            "epoch: 2, classification_loss: 1.7460092306137085, Val Loss: 70.44981276988983, loss : 2.0154054164886475\n",
            "epoch: 3, classification_loss: 1.7928062677383423, Val Loss: 70.49346125125885, loss : 2.0612432956695557\n",
            "epoch: 4, classification_loss: 1.8136125802993774, Val Loss: 70.49144172668457, loss : 2.051865816116333\n",
            "epoch: 5, classification_loss: 1.7866891622543335, Val Loss: 70.45716345310211, loss : 2.0542640686035156\n",
            "epoch: 6, classification_loss: 1.7828906774520874, Val Loss: 70.47438883781433, loss : 2.0549888610839844\n",
            "epoch: 7, classification_loss: 1.7777519226074219, Val Loss: 70.52658104896545, loss : 2.036423683166504\n",
            "epoch: 8, classification_loss: 1.7942488193511963, Val Loss: 70.50177156925201, loss : 2.0551633834838867\n",
            "epoch: 9, classification_loss: 1.7848633527755737, Val Loss: 70.4572126865387, loss : 2.040959596633911\n",
            "epoch: 10, classification_loss: 1.786581039428711, Val Loss: 70.49687623977661, loss : 2.0368716716766357\n",
            "epoch: 11, classification_loss: 1.793086051940918, Val Loss: 70.4903039932251, loss : 2.046700954437256\n",
            "epoch: 12, classification_loss: 1.7868355512619019, Val Loss: 70.47584903240204, loss : 2.0259478092193604\n",
            "epoch: 13, classification_loss: 1.790102243423462, Val Loss: 70.46554064750671, loss : 2.0391733646392822\n",
            "epoch: 14, classification_loss: 1.7744512557983398, Val Loss: 70.57412695884705, loss : 2.0199947357177734\n",
            "epoch: 15, classification_loss: 1.7790647745132446, Val Loss: 70.5735832452774, loss : 2.025355339050293\n",
            "epoch: 16, classification_loss: 1.7780629396438599, Val Loss: 70.47560811042786, loss : 2.0198068618774414\n",
            "epoch: 17, classification_loss: 1.7826225757598877, Val Loss: 70.46472442150116, loss : 2.0266027450561523\n",
            "epoch: 18, classification_loss: 1.7830842733383179, Val Loss: 70.52534806728363, loss : 2.017883062362671\n",
            "epoch: 19, classification_loss: 1.7826828956604004, Val Loss: 70.49991738796234, loss : 2.02286958694458\n",
            "Batch: 61, Test Acc: 0.5746193910256411\n",
            "Batch: 62:\n",
            "epoch: 0, classification_loss: 1.8318275213241577, Val Loss: 70.47871768474579, loss : 1.8318275213241577\n",
            "epoch: 1, classification_loss: 1.8007172346115112, Val Loss: 70.49963390827179, loss : 2.1285207271575928\n",
            "epoch: 2, classification_loss: 1.763476848602295, Val Loss: 70.48051285743713, loss : 2.0385327339172363\n",
            "epoch: 3, classification_loss: 1.7989377975463867, Val Loss: 70.48965954780579, loss : 2.0759940147399902\n",
            "epoch: 4, classification_loss: 1.8119243383407593, Val Loss: 70.5045006275177, loss : 2.0578255653381348\n",
            "epoch: 5, classification_loss: 1.8128414154052734, Val Loss: 70.44244039058685, loss : 2.07817006111145\n",
            "epoch: 6, classification_loss: 1.7923457622528076, Val Loss: 70.46418035030365, loss : 2.0534064769744873\n",
            "epoch: 7, classification_loss: 1.805846929550171, Val Loss: 70.47446978092194, loss : 2.0690486431121826\n",
            "epoch: 8, classification_loss: 1.804341197013855, Val Loss: 70.49817109107971, loss : 2.0570740699768066\n",
            "epoch: 9, classification_loss: 1.7981799840927124, Val Loss: 70.45513045787811, loss : 2.0567727088928223\n",
            "epoch: 10, classification_loss: 1.8103359937667847, Val Loss: 70.4595513343811, loss : 2.055087089538574\n",
            "epoch: 11, classification_loss: 1.8049635887145996, Val Loss: 70.4601172208786, loss : 2.0470211505889893\n",
            "epoch: 12, classification_loss: 1.8077871799468994, Val Loss: 70.47424459457397, loss : 2.0475707054138184\n",
            "epoch: 13, classification_loss: 1.8032293319702148, Val Loss: 70.46253967285156, loss : 2.0469436645507812\n",
            "epoch: 14, classification_loss: 1.8041446208953857, Val Loss: 70.45897221565247, loss : 2.0428757667541504\n",
            "epoch: 15, classification_loss: 1.8044700622558594, Val Loss: 70.46532022953033, loss : 2.044952869415283\n",
            "epoch: 16, classification_loss: 1.8077512979507446, Val Loss: 70.47059082984924, loss : 2.0464823246002197\n",
            "epoch: 17, classification_loss: 1.7978816032409668, Val Loss: 70.486745595932, loss : 2.039536476135254\n",
            "epoch: 18, classification_loss: 1.8060003519058228, Val Loss: 70.49036824703217, loss : 2.034301519393921\n",
            "epoch: 19, classification_loss: 1.8077442646026611, Val Loss: 70.45372438430786, loss : 2.0479748249053955\n",
            "Batch: 62, Test Acc: 0.5767227564102564\n",
            "Batch: 63:\n",
            "epoch: 0, classification_loss: 1.7668310403823853, Val Loss: 70.4610093832016, loss : 1.7668310403823853\n",
            "epoch: 1, classification_loss: 1.7276583909988403, Val Loss: 70.44905662536621, loss : 2.046048879623413\n",
            "epoch: 2, classification_loss: 1.7065150737762451, Val Loss: 70.48267590999603, loss : 1.9485846757888794\n",
            "epoch: 3, classification_loss: 1.7432206869125366, Val Loss: 70.45452988147736, loss : 2.002995491027832\n",
            "epoch: 4, classification_loss: 1.7546931505203247, Val Loss: 70.45674872398376, loss : 1.973494291305542\n",
            "epoch: 5, classification_loss: 1.7381609678268433, Val Loss: 70.45938110351562, loss : 2.007172107696533\n",
            "epoch: 6, classification_loss: 1.720697045326233, Val Loss: 70.45863378047943, loss : 1.9947432279586792\n",
            "epoch: 7, classification_loss: 1.729496717453003, Val Loss: 70.45375323295593, loss : 1.9787415266036987\n",
            "epoch: 8, classification_loss: 1.747718095779419, Val Loss: 70.46109104156494, loss : 1.9944795370101929\n",
            "epoch: 9, classification_loss: 1.7391058206558228, Val Loss: 70.504021525383, loss : 1.9833264350891113\n",
            "epoch: 10, classification_loss: 1.7392200231552124, Val Loss: 70.4520435333252, loss : 1.9907524585723877\n",
            "epoch: 11, classification_loss: 1.7579001188278198, Val Loss: 70.48330271244049, loss : 2.0032272338867188\n",
            "epoch: 12, classification_loss: 1.7413289546966553, Val Loss: 70.44925057888031, loss : 1.9815521240234375\n",
            "epoch: 13, classification_loss: 1.7383650541305542, Val Loss: 70.46734261512756, loss : 1.9821357727050781\n",
            "epoch: 14, classification_loss: 1.7379169464111328, Val Loss: 70.47223210334778, loss : 1.9780304431915283\n",
            "epoch: 15, classification_loss: 1.7348350286483765, Val Loss: 70.47970497608185, loss : 1.9779256582260132\n",
            "epoch: 16, classification_loss: 1.752089023590088, Val Loss: 70.46447670459747, loss : 1.9844425916671753\n",
            "epoch: 17, classification_loss: 1.7527086734771729, Val Loss: 70.46824049949646, loss : 1.9933605194091797\n",
            "epoch: 18, classification_loss: 1.7360339164733887, Val Loss: 70.44870281219482, loss : 1.9620198011398315\n",
            "epoch: 19, classification_loss: 1.7424696683883667, Val Loss: 70.46660363674164, loss : 1.9791882038116455\n",
            "Batch: 63, Test Acc: 0.5731169871794872\n",
            "Batch: 64:\n",
            "epoch: 0, classification_loss: 1.8249320983886719, Val Loss: 70.47968065738678, loss : 1.8249320983886719\n",
            "epoch: 1, classification_loss: 1.7908165454864502, Val Loss: 70.50420272350311, loss : 2.121819496154785\n",
            "epoch: 2, classification_loss: 1.7630077600479126, Val Loss: 70.48469519615173, loss : 2.0190887451171875\n",
            "epoch: 3, classification_loss: 1.7875301837921143, Val Loss: 70.47782790660858, loss : 2.050868034362793\n",
            "epoch: 4, classification_loss: 1.8081660270690918, Val Loss: 70.46229159832001, loss : 2.034217596054077\n",
            "epoch: 5, classification_loss: 1.7873482704162598, Val Loss: 70.51141703128815, loss : 2.0488126277923584\n",
            "epoch: 6, classification_loss: 1.784929871559143, Val Loss: 70.48169040679932, loss : 2.055389404296875\n",
            "epoch: 7, classification_loss: 1.7973320484161377, Val Loss: 70.47414994239807, loss : 2.052611827850342\n",
            "epoch: 8, classification_loss: 1.800713300704956, Val Loss: 70.46909618377686, loss : 2.0536489486694336\n",
            "epoch: 9, classification_loss: 1.7848774194717407, Val Loss: 70.52916467189789, loss : 2.025874137878418\n",
            "epoch: 10, classification_loss: 1.7911421060562134, Val Loss: 70.47577619552612, loss : 2.0400710105895996\n",
            "epoch: 11, classification_loss: 1.7920787334442139, Val Loss: 70.4819039106369, loss : 2.0407166481018066\n",
            "epoch: 12, classification_loss: 1.7844101190567017, Val Loss: 70.50327098369598, loss : 2.022726058959961\n",
            "epoch: 13, classification_loss: 1.800240397453308, Val Loss: 70.51752233505249, loss : 2.0449953079223633\n",
            "epoch: 14, classification_loss: 1.7886041402816772, Val Loss: 70.4794100522995, loss : 2.024902105331421\n",
            "epoch: 15, classification_loss: 1.7961293458938599, Val Loss: 70.5249502658844, loss : 2.034090518951416\n",
            "epoch: 16, classification_loss: 1.7952300310134888, Val Loss: 70.46036851406097, loss : 2.0255069732666016\n",
            "epoch: 17, classification_loss: 1.783898949623108, Val Loss: 70.49061679840088, loss : 2.0205624103546143\n",
            "epoch: 18, classification_loss: 1.7947522401809692, Val Loss: 70.52181077003479, loss : 2.019437551498413\n",
            "epoch: 19, classification_loss: 1.7870770692825317, Val Loss: 70.46086204051971, loss : 2.0275347232818604\n",
            "Batch: 64, Test Acc: 0.5758213141025641\n",
            "Batch: 65:\n",
            "epoch: 0, classification_loss: 1.7914140224456787, Val Loss: 70.52438223361969, loss : 1.7914140224456787\n",
            "epoch: 1, classification_loss: 1.755600094795227, Val Loss: 70.50022566318512, loss : 2.084707736968994\n",
            "epoch: 2, classification_loss: 1.7319341897964478, Val Loss: 70.5229252576828, loss : 1.997374176979065\n",
            "epoch: 3, classification_loss: 1.760909914970398, Val Loss: 70.4737319946289, loss : 2.025146007537842\n",
            "epoch: 4, classification_loss: 1.7827715873718262, Val Loss: 70.43133437633514, loss : 2.0188987255096436\n",
            "epoch: 5, classification_loss: 1.7747255563735962, Val Loss: 70.47318196296692, loss : 2.0345780849456787\n",
            "epoch: 6, classification_loss: 1.7480719089508057, Val Loss: 70.4564733505249, loss : 2.0191290378570557\n",
            "epoch: 7, classification_loss: 1.7610642910003662, Val Loss: 70.46709740161896, loss : 2.02435302734375\n",
            "epoch: 8, classification_loss: 1.7621190547943115, Val Loss: 70.46238088607788, loss : 2.020256996154785\n",
            "epoch: 9, classification_loss: 1.7636748552322388, Val Loss: 70.46319735050201, loss : 2.012484073638916\n",
            "epoch: 10, classification_loss: 1.764976978302002, Val Loss: 70.44475984573364, loss : 2.0240514278411865\n",
            "epoch: 11, classification_loss: 1.7685978412628174, Val Loss: 70.48205363750458, loss : 2.020969867706299\n",
            "epoch: 12, classification_loss: 1.7496519088745117, Val Loss: 70.44895815849304, loss : 1.9967905282974243\n",
            "epoch: 13, classification_loss: 1.7665005922317505, Val Loss: 70.44423377513885, loss : 2.0131330490112305\n",
            "epoch: 14, classification_loss: 1.7716141939163208, Val Loss: 70.45429396629333, loss : 2.0085110664367676\n",
            "epoch: 15, classification_loss: 1.7571626901626587, Val Loss: 70.44375252723694, loss : 2.008714199066162\n",
            "epoch: 16, classification_loss: 1.7552905082702637, Val Loss: 70.46132123470306, loss : 2.000793695449829\n",
            "epoch: 17, classification_loss: 1.7641222476959229, Val Loss: 70.47739708423615, loss : 2.010354995727539\n",
            "epoch: 18, classification_loss: 1.7678226232528687, Val Loss: 70.45611882209778, loss : 1.9973835945129395\n",
            "epoch: 19, classification_loss: 1.7603408098220825, Val Loss: 70.40424466133118, loss : 2.0002686977386475\n",
            "Batch: 65, Test Acc: 0.5772235576923077\n",
            "Batch: 66:\n",
            "epoch: 0, classification_loss: 1.8090726137161255, Val Loss: 70.44227969646454, loss : 1.8090726137161255\n",
            "epoch: 1, classification_loss: 1.7732638120651245, Val Loss: 70.4428163766861, loss : 2.1014299392700195\n",
            "epoch: 2, classification_loss: 1.7516558170318604, Val Loss: 70.48375415802002, loss : 2.027749538421631\n",
            "epoch: 3, classification_loss: 1.7770012617111206, Val Loss: 70.46168267726898, loss : 2.055859327316284\n",
            "epoch: 4, classification_loss: 1.7987706661224365, Val Loss: 70.45734357833862, loss : 2.0404155254364014\n",
            "epoch: 5, classification_loss: 1.8061641454696655, Val Loss: 70.49500024318695, loss : 2.066807508468628\n",
            "epoch: 6, classification_loss: 1.780979871749878, Val Loss: 70.46019566059113, loss : 2.036864757537842\n",
            "epoch: 7, classification_loss: 1.7878155708312988, Val Loss: 70.46178328990936, loss : 2.048950433731079\n",
            "epoch: 8, classification_loss: 1.7778822183609009, Val Loss: 70.43543839454651, loss : 2.0430781841278076\n",
            "epoch: 9, classification_loss: 1.7752883434295654, Val Loss: 70.49389624595642, loss : 2.0412185192108154\n",
            "epoch: 10, classification_loss: 1.7819936275482178, Val Loss: 70.4829432964325, loss : 2.043217897415161\n",
            "epoch: 11, classification_loss: 1.7771294116973877, Val Loss: 70.4686541557312, loss : 2.0312275886535645\n",
            "epoch: 12, classification_loss: 1.788022518157959, Val Loss: 70.57711172103882, loss : 2.028381109237671\n",
            "epoch: 13, classification_loss: 1.7775545120239258, Val Loss: 70.48926138877869, loss : 2.0331919193267822\n",
            "epoch: 14, classification_loss: 1.781051516532898, Val Loss: 70.4535653591156, loss : 2.022493600845337\n",
            "epoch: 15, classification_loss: 1.7914963960647583, Val Loss: 70.44841933250427, loss : 2.0449185371398926\n",
            "epoch: 16, classification_loss: 1.7814369201660156, Val Loss: 70.48112750053406, loss : 2.0177998542785645\n",
            "epoch: 17, classification_loss: 1.7859090566635132, Val Loss: 70.50320100784302, loss : 2.035419225692749\n",
            "epoch: 18, classification_loss: 1.7847568988800049, Val Loss: 70.46959364414215, loss : 2.0281102657318115\n",
            "epoch: 19, classification_loss: 1.7873096466064453, Val Loss: 70.48829460144043, loss : 2.035907745361328\n",
            "Batch: 66, Test Acc: 0.5735176282051282\n",
            "Batch: 67:\n",
            "epoch: 0, classification_loss: 1.8036867380142212, Val Loss: 70.42764329910278, loss : 1.8036867380142212\n",
            "epoch: 1, classification_loss: 1.7585229873657227, Val Loss: 70.45824837684631, loss : 2.0973386764526367\n",
            "epoch: 2, classification_loss: 1.7427127361297607, Val Loss: 70.49503433704376, loss : 2.02531099319458\n",
            "epoch: 3, classification_loss: 1.7715421915054321, Val Loss: 70.44209432601929, loss : 2.0542991161346436\n",
            "epoch: 4, classification_loss: 1.8000706434249878, Val Loss: 70.39285910129547, loss : 2.0401997566223145\n",
            "epoch: 5, classification_loss: 1.7846218347549438, Val Loss: 70.57961213588715, loss : 1.7846218347549438\n",
            "epoch: 6, classification_loss: 1.7538119554519653, Val Loss: 70.54935586452484, loss : 2.153813600540161\n",
            "epoch: 7, classification_loss: 1.7220287322998047, Val Loss: 70.43783748149872, loss : 2.0712695121765137\n",
            "epoch: 8, classification_loss: 1.7442350387573242, Val Loss: 70.51946377754211, loss : 2.0042243003845215\n",
            "epoch: 9, classification_loss: 1.8057668209075928, Val Loss: 70.45147168636322, loss : 2.112907648086548\n",
            "epoch: 10, classification_loss: 1.7730977535247803, Val Loss: 70.46393239498138, loss : 2.0354602336883545\n",
            "epoch: 11, classification_loss: 1.749638557434082, Val Loss: 70.45633375644684, loss : 2.0737593173980713\n",
            "epoch: 12, classification_loss: 1.7432208061218262, Val Loss: 70.45686984062195, loss : 2.061326742172241\n",
            "epoch: 13, classification_loss: 1.7529438734054565, Val Loss: 70.47774183750153, loss : 2.0430853366851807\n",
            "epoch: 14, classification_loss: 1.7628856897354126, Val Loss: 70.49318444728851, loss : 2.0334198474884033\n",
            "epoch: 15, classification_loss: 1.7719640731811523, Val Loss: 70.53621017932892, loss : 2.045088291168213\n",
            "epoch: 16, classification_loss: 1.7542645931243896, Val Loss: 70.47904753684998, loss : 2.031186819076538\n",
            "epoch: 17, classification_loss: 1.7556105852127075, Val Loss: 70.46176362037659, loss : 2.0301108360290527\n",
            "epoch: 18, classification_loss: 1.7729138135910034, Val Loss: 70.4567666053772, loss : 2.0344455242156982\n",
            "epoch: 19, classification_loss: 1.7620078325271606, Val Loss: 70.48929846286774, loss : 2.009770631790161\n",
            "Batch: 67, Test Acc: 0.5751201923076923\n",
            "Batch: 68:\n",
            "epoch: 0, classification_loss: 1.863077998161316, Val Loss: 70.49544072151184, loss : 1.863077998161316\n",
            "epoch: 1, classification_loss: 1.815712571144104, Val Loss: 70.49903535842896, loss : 2.1712586879730225\n",
            "epoch: 2, classification_loss: 1.8028924465179443, Val Loss: 70.48549175262451, loss : 2.0991945266723633\n",
            "epoch: 3, classification_loss: 1.8230589628219604, Val Loss: 70.49369370937347, loss : 2.1245033740997314\n",
            "epoch: 4, classification_loss: 1.8501365184783936, Val Loss: 70.48909628391266, loss : 2.128804922103882\n",
            "epoch: 5, classification_loss: 1.8507754802703857, Val Loss: 70.5328152179718, loss : 2.1072535514831543\n",
            "epoch: 6, classification_loss: 1.8273835182189941, Val Loss: 70.51212215423584, loss : 2.0988893508911133\n",
            "epoch: 7, classification_loss: 1.828261375427246, Val Loss: 70.4915099143982, loss : 2.101806879043579\n",
            "epoch: 8, classification_loss: 1.8243510723114014, Val Loss: 70.48132729530334, loss : 2.081467866897583\n",
            "epoch: 9, classification_loss: 1.8344911336898804, Val Loss: 70.50501036643982, loss : 2.099905014038086\n",
            "epoch: 10, classification_loss: 1.8422374725341797, Val Loss: 70.51169538497925, loss : 2.096909523010254\n",
            "epoch: 11, classification_loss: 1.8450175523757935, Val Loss: 70.59347188472748, loss : 2.102931261062622\n",
            "epoch: 12, classification_loss: 1.835981845855713, Val Loss: 70.57364869117737, loss : 2.086919069290161\n",
            "epoch: 13, classification_loss: 1.8385642766952515, Val Loss: 70.5054601430893, loss : 2.090345859527588\n",
            "epoch: 14, classification_loss: 1.8431715965270996, Val Loss: 70.53083193302155, loss : 2.0930542945861816\n",
            "epoch: 15, classification_loss: 1.8292584419250488, Val Loss: 70.53269028663635, loss : 2.082000255584717\n",
            "epoch: 16, classification_loss: 1.8328804969787598, Val Loss: 70.55462908744812, loss : 2.0790231227874756\n",
            "epoch: 17, classification_loss: 1.838260293006897, Val Loss: 70.55239486694336, loss : 2.086681365966797\n",
            "epoch: 18, classification_loss: 1.8465676307678223, Val Loss: 70.55109977722168, loss : 2.087240219116211\n",
            "epoch: 19, classification_loss: 1.8313723802566528, Val Loss: 70.55634367465973, loss : 2.0701639652252197\n",
            "Batch: 68, Test Acc: 0.5738181089743589\n",
            "Batch: 69:\n",
            "epoch: 0, classification_loss: 1.759665608406067, Val Loss: 70.48309051990509, loss : 1.759665608406067\n",
            "epoch: 1, classification_loss: 1.7344988584518433, Val Loss: 70.44174122810364, loss : 2.0547597408294678\n",
            "epoch: 2, classification_loss: 1.7241662740707397, Val Loss: 70.4152125120163, loss : 1.9689512252807617\n",
            "epoch: 3, classification_loss: 1.7584307193756104, Val Loss: 70.42783796787262, loss : 2.0183069705963135\n",
            "epoch: 4, classification_loss: 1.7841274738311768, Val Loss: 70.44301676750183, loss : 2.008267879486084\n",
            "epoch: 5, classification_loss: 1.756028652191162, Val Loss: 70.43603980541229, loss : 2.017616033554077\n",
            "epoch: 6, classification_loss: 1.7536486387252808, Val Loss: 70.42660665512085, loss : 2.021925449371338\n",
            "epoch: 7, classification_loss: 1.7554984092712402, Val Loss: 70.4393299818039, loss : 2.000145435333252\n",
            "epoch: 8, classification_loss: 1.7473901510238647, Val Loss: 70.43286037445068, loss : 2.0004806518554688\n",
            "epoch: 9, classification_loss: 1.7555782794952393, Val Loss: 70.43050646781921, loss : 1.9866259098052979\n",
            "epoch: 10, classification_loss: 1.7503670454025269, Val Loss: 70.42519974708557, loss : 1.9938029050827026\n",
            "epoch: 11, classification_loss: 1.7483112812042236, Val Loss: 70.44577300548553, loss : 1.9834880828857422\n",
            "epoch: 12, classification_loss: 1.7612197399139404, Val Loss: 70.43301403522491, loss : 1.994510531425476\n",
            "epoch: 13, classification_loss: 1.7570204734802246, Val Loss: 70.4282546043396, loss : 1.9832475185394287\n",
            "epoch: 14, classification_loss: 1.7557541131973267, Val Loss: 70.45365703105927, loss : 1.986959457397461\n",
            "epoch: 15, classification_loss: 1.7485542297363281, Val Loss: 70.44307732582092, loss : 1.980603575706482\n",
            "epoch: 16, classification_loss: 1.758354663848877, Val Loss: 70.44853138923645, loss : 1.9767752885818481\n",
            "epoch: 17, classification_loss: 1.7529780864715576, Val Loss: 70.4465663433075, loss : 1.9781293869018555\n",
            "epoch: 18, classification_loss: 1.7434381246566772, Val Loss: 70.46384155750275, loss : 1.9612762928009033\n",
            "epoch: 19, classification_loss: 1.752549648284912, Val Loss: 70.45249879360199, loss : 1.9870967864990234\n",
            "Batch: 69, Test Acc: 0.577323717948718\n",
            "Batch: 70:\n",
            "epoch: 0, classification_loss: 1.8323713541030884, Val Loss: 70.51274716854095, loss : 1.8323713541030884\n",
            "epoch: 1, classification_loss: 1.7892117500305176, Val Loss: 70.44723296165466, loss : 2.112910747528076\n",
            "epoch: 2, classification_loss: 1.7733464241027832, Val Loss: 70.51702773571014, loss : 2.020235776901245\n",
            "epoch: 3, classification_loss: 1.8303747177124023, Val Loss: 70.47519671916962, loss : 2.0804834365844727\n",
            "epoch: 4, classification_loss: 1.826852798461914, Val Loss: 70.45471739768982, loss : 2.045396089553833\n",
            "epoch: 5, classification_loss: 1.8222839832305908, Val Loss: 70.46612679958344, loss : 2.0737380981445312\n",
            "epoch: 6, classification_loss: 1.7911401987075806, Val Loss: 70.47232711315155, loss : 2.034295082092285\n",
            "epoch: 7, classification_loss: 1.807833194732666, Val Loss: 70.50148677825928, loss : 2.048830032348633\n",
            "epoch: 8, classification_loss: 1.8218199014663696, Val Loss: 70.5262142419815, loss : 2.0652384757995605\n",
            "epoch: 9, classification_loss: 1.8102350234985352, Val Loss: 70.50589537620544, loss : 2.039961099624634\n",
            "epoch: 10, classification_loss: 1.8188672065734863, Val Loss: 70.48678874969482, loss : 2.057537794113159\n",
            "epoch: 11, classification_loss: 1.8050802946090698, Val Loss: 70.53040683269501, loss : 2.0314698219299316\n",
            "epoch: 12, classification_loss: 1.815845012664795, Val Loss: 70.58001112937927, loss : 2.0512936115264893\n",
            "epoch: 13, classification_loss: 1.8167059421539307, Val Loss: 70.52290880680084, loss : 2.048273801803589\n",
            "epoch: 14, classification_loss: 1.8074275255203247, Val Loss: 70.51674246788025, loss : 2.042327642440796\n",
            "epoch: 15, classification_loss: 1.8216992616653442, Val Loss: 70.6013594865799, loss : 2.04301381111145\n",
            "epoch: 16, classification_loss: 1.809202790260315, Val Loss: 70.60476469993591, loss : 2.041193962097168\n",
            "epoch: 17, classification_loss: 1.8060392141342163, Val Loss: 70.53889167308807, loss : 2.0366485118865967\n",
            "epoch: 18, classification_loss: 1.8098766803741455, Val Loss: 70.54046094417572, loss : 2.0427632331848145\n",
            "epoch: 19, classification_loss: 1.822411298751831, Val Loss: 70.61751818656921, loss : 2.049311399459839\n",
            "Batch: 70, Test Acc: 0.5721153846153846\n",
            "Batch: 71:\n",
            "epoch: 0, classification_loss: 1.8124768733978271, Val Loss: 70.45098352432251, loss : 1.8124768733978271\n",
            "epoch: 1, classification_loss: 1.7766255140304565, Val Loss: 70.45078480243683, loss : 2.112818479537964\n",
            "epoch: 2, classification_loss: 1.759483814239502, Val Loss: 70.4306845664978, loss : 2.0248122215270996\n",
            "epoch: 3, classification_loss: 1.7884197235107422, Val Loss: 70.40864276885986, loss : 2.0603647232055664\n",
            "epoch: 4, classification_loss: 1.8146045207977295, Val Loss: 70.41388702392578, loss : 2.0641183853149414\n",
            "epoch: 5, classification_loss: 1.7988442182540894, Val Loss: 70.46907758712769, loss : 2.048903226852417\n",
            "epoch: 6, classification_loss: 1.7822753190994263, Val Loss: 70.40403091907501, loss : 2.0544538497924805\n",
            "epoch: 7, classification_loss: 1.7857298851013184, Val Loss: 70.4182870388031, loss : 2.0487043857574463\n",
            "epoch: 8, classification_loss: 1.7970924377441406, Val Loss: 70.43836236000061, loss : 2.0561447143554688\n",
            "epoch: 9, classification_loss: 1.7860710620880127, Val Loss: 70.41448938846588, loss : 2.045285701751709\n",
            "epoch: 10, classification_loss: 1.7844717502593994, Val Loss: 70.43234157562256, loss : 2.0320682525634766\n",
            "epoch: 11, classification_loss: 1.8002190589904785, Val Loss: 70.41978406906128, loss : 2.039851188659668\n",
            "epoch: 12, classification_loss: 1.79591703414917, Val Loss: 70.4130368232727, loss : 2.03509783744812\n",
            "epoch: 13, classification_loss: 1.7945713996887207, Val Loss: 70.43496799468994, loss : 2.0410048961639404\n",
            "epoch: 14, classification_loss: 1.779350757598877, Val Loss: 70.5447119474411, loss : 2.0233261585235596\n",
            "epoch: 15, classification_loss: 1.7953965663909912, Val Loss: 70.42351853847504, loss : 2.033217430114746\n",
            "epoch: 16, classification_loss: 1.7935924530029297, Val Loss: 70.40219640731812, loss : 2.0327131748199463\n",
            "epoch: 17, classification_loss: 1.786059856414795, Val Loss: 70.5038036108017, loss : 2.0230095386505127\n",
            "epoch: 18, classification_loss: 1.7901360988616943, Val Loss: 70.46086955070496, loss : 2.0290112495422363\n",
            "epoch: 19, classification_loss: 1.7895898818969727, Val Loss: 70.42146825790405, loss : 2.021393060684204\n",
            "Batch: 71, Test Acc: 0.5776241987179487\n",
            "Batch: 72:\n",
            "epoch: 0, classification_loss: 1.839364767074585, Val Loss: 70.44639325141907, loss : 1.839364767074585\n",
            "epoch: 1, classification_loss: 1.818284034729004, Val Loss: 70.44601392745972, loss : 2.159489393234253\n",
            "epoch: 2, classification_loss: 1.788000226020813, Val Loss: 70.50871241092682, loss : 2.046948194503784\n",
            "epoch: 3, classification_loss: 1.8191418647766113, Val Loss: 70.52189981937408, loss : 2.0922062397003174\n",
            "epoch: 4, classification_loss: 1.8458235263824463, Val Loss: 70.50878036022186, loss : 2.0880379676818848\n",
            "epoch: 5, classification_loss: 1.8210790157318115, Val Loss: 70.42759656906128, loss : 2.0802907943725586\n",
            "epoch: 6, classification_loss: 1.802269458770752, Val Loss: 70.45233535766602, loss : 2.074376106262207\n",
            "epoch: 7, classification_loss: 1.8001246452331543, Val Loss: 70.4516054391861, loss : 2.070324182510376\n",
            "epoch: 8, classification_loss: 1.8109056949615479, Val Loss: 70.52064573764801, loss : 2.0597805976867676\n",
            "epoch: 9, classification_loss: 1.8270819187164307, Val Loss: 70.4459867477417, loss : 2.0748701095581055\n",
            "epoch: 10, classification_loss: 1.8088409900665283, Val Loss: 70.49915969371796, loss : 2.053086042404175\n",
            "epoch: 11, classification_loss: 1.805734634399414, Val Loss: 70.46455156803131, loss : 2.050161123275757\n",
            "epoch: 12, classification_loss: 1.8132164478302002, Val Loss: 70.50615429878235, loss : 2.0604166984558105\n",
            "epoch: 13, classification_loss: 1.804017186164856, Val Loss: 70.46782517433167, loss : 2.053349018096924\n",
            "epoch: 14, classification_loss: 1.8219835758209229, Val Loss: 70.48872816562653, loss : 2.055663585662842\n",
            "epoch: 15, classification_loss: 1.8194838762283325, Val Loss: 70.51457214355469, loss : 2.0573346614837646\n",
            "epoch: 16, classification_loss: 1.8226863145828247, Val Loss: 70.49573993682861, loss : 2.051318645477295\n",
            "epoch: 17, classification_loss: 1.830251693725586, Val Loss: 70.4718610048294, loss : 2.0634751319885254\n",
            "epoch: 18, classification_loss: 1.8264634609222412, Val Loss: 70.46675682067871, loss : 2.051011562347412\n",
            "epoch: 19, classification_loss: 1.8199880123138428, Val Loss: 70.51670253276825, loss : 2.0545654296875\n",
            "Batch: 72, Test Acc: 0.5761217948717948\n",
            "Batch: 73:\n",
            "epoch: 0, classification_loss: 1.8461140394210815, Val Loss: 70.48298442363739, loss : 1.8461140394210815\n",
            "epoch: 1, classification_loss: 1.817778468132019, Val Loss: 70.51295602321625, loss : 2.1507182121276855\n",
            "epoch: 2, classification_loss: 1.7922135591506958, Val Loss: 70.47553610801697, loss : 2.0712673664093018\n",
            "epoch: 3, classification_loss: 1.8066385984420776, Val Loss: 70.47385823726654, loss : 2.08335018157959\n",
            "epoch: 4, classification_loss: 1.8231418132781982, Val Loss: 70.43017256259918, loss : 2.0748934745788574\n",
            "epoch: 5, classification_loss: 1.828644871711731, Val Loss: 70.430424451828, loss : 2.081866502761841\n",
            "epoch: 6, classification_loss: 1.8229910135269165, Val Loss: 70.4504736661911, loss : 2.090542793273926\n",
            "epoch: 7, classification_loss: 1.8099727630615234, Val Loss: 70.47156870365143, loss : 2.0776047706604004\n",
            "epoch: 8, classification_loss: 1.8227825164794922, Val Loss: 70.42010533809662, loss : 2.0740835666656494\n",
            "epoch: 9, classification_loss: 1.826035499572754, Val Loss: 70.44613909721375, loss : 2.0865254402160645\n",
            "epoch: 10, classification_loss: 1.815685749053955, Val Loss: 70.43211209774017, loss : 2.0753562450408936\n",
            "epoch: 11, classification_loss: 1.8275213241577148, Val Loss: 70.4558914899826, loss : 2.078450918197632\n",
            "epoch: 12, classification_loss: 1.8174575567245483, Val Loss: 70.43467628955841, loss : 2.063075065612793\n",
            "epoch: 13, classification_loss: 1.8253718614578247, Val Loss: 70.43701028823853, loss : 2.0804591178894043\n",
            "epoch: 14, classification_loss: 1.8155055046081543, Val Loss: 70.52754974365234, loss : 2.057180166244507\n",
            "epoch: 15, classification_loss: 1.8287739753723145, Val Loss: 70.52010071277618, loss : 2.0766775608062744\n",
            "epoch: 16, classification_loss: 1.826171636581421, Val Loss: 70.44695484638214, loss : 2.0706989765167236\n",
            "epoch: 17, classification_loss: 1.814268708229065, Val Loss: 70.45750546455383, loss : 2.064837694168091\n",
            "epoch: 18, classification_loss: 1.8254938125610352, Val Loss: 70.51350677013397, loss : 2.0639307498931885\n",
            "epoch: 19, classification_loss: 1.8334846496582031, Val Loss: 70.44241714477539, loss : 2.0741803646087646\n",
            "Batch: 73, Test Acc: 0.5762219551282052\n",
            "Batch: 74:\n",
            "epoch: 0, classification_loss: 1.845855951309204, Val Loss: 70.49505245685577, loss : 1.845855951309204\n",
            "epoch: 1, classification_loss: 1.8145310878753662, Val Loss: 70.46997201442719, loss : 2.1459641456604004\n",
            "epoch: 2, classification_loss: 1.7803301811218262, Val Loss: 70.47885048389435, loss : 2.051330327987671\n",
            "epoch: 3, classification_loss: 1.81136953830719, Val Loss: 70.4551249742508, loss : 2.0938806533813477\n",
            "epoch: 4, classification_loss: 1.8257113695144653, Val Loss: 70.51853466033936, loss : 2.074939727783203\n",
            "epoch: 5, classification_loss: 1.823331356048584, Val Loss: 70.44592952728271, loss : 2.081697702407837\n",
            "epoch: 6, classification_loss: 1.8254374265670776, Val Loss: 70.50715792179108, loss : 2.0910966396331787\n",
            "epoch: 7, classification_loss: 1.8091182708740234, Val Loss: 70.48487436771393, loss : 2.078383207321167\n",
            "epoch: 8, classification_loss: 1.8149334192276, Val Loss: 70.46069693565369, loss : 2.070309638977051\n",
            "epoch: 9, classification_loss: 1.8240032196044922, Val Loss: 70.4499636888504, loss : 2.0915699005126953\n",
            "epoch: 10, classification_loss: 1.8154847621917725, Val Loss: 70.48296189308167, loss : 2.0671536922454834\n",
            "epoch: 11, classification_loss: 1.8278084993362427, Val Loss: 70.49933290481567, loss : 2.07123064994812\n",
            "epoch: 12, classification_loss: 1.829318881034851, Val Loss: 70.47544038295746, loss : 2.0687472820281982\n",
            "epoch: 13, classification_loss: 1.8336403369903564, Val Loss: 70.48714435100555, loss : 2.0782275199890137\n",
            "epoch: 14, classification_loss: 1.8241078853607178, Val Loss: 70.4884113073349, loss : 2.0560879707336426\n",
            "epoch: 15, classification_loss: 1.8284512758255005, Val Loss: 70.47118008136749, loss : 2.0694963932037354\n",
            "epoch: 16, classification_loss: 1.821866512298584, Val Loss: 70.48784565925598, loss : 2.0556840896606445\n",
            "epoch: 17, classification_loss: 1.8235114812850952, Val Loss: 70.49057328701019, loss : 2.0675010681152344\n",
            "epoch: 18, classification_loss: 1.8240101337432861, Val Loss: 70.52780878543854, loss : 2.0504531860351562\n",
            "epoch: 19, classification_loss: 1.8337931632995605, Val Loss: 70.52226293087006, loss : 2.073824882507324\n",
            "Batch: 74, Test Acc: 0.5749198717948718\n",
            "Batch: 75:\n",
            "epoch: 0, classification_loss: 1.8295621871948242, Val Loss: 70.44675803184509, loss : 1.8295621871948242\n",
            "epoch: 1, classification_loss: 1.789975881576538, Val Loss: 70.43970119953156, loss : 2.1057822704315186\n",
            "epoch: 2, classification_loss: 1.7695280313491821, Val Loss: 70.4896080493927, loss : 2.0263633728027344\n",
            "epoch: 3, classification_loss: 1.8042175769805908, Val Loss: 70.51448822021484, loss : 2.069783926010132\n",
            "epoch: 4, classification_loss: 1.8135831356048584, Val Loss: 70.40743637084961, loss : 2.0448477268218994\n",
            "epoch: 5, classification_loss: 1.8071452379226685, Val Loss: 70.44157779216766, loss : 2.0737802982330322\n",
            "epoch: 6, classification_loss: 1.8011789321899414, Val Loss: 70.47940266132355, loss : 2.0681614875793457\n",
            "epoch: 7, classification_loss: 1.7881027460098267, Val Loss: 70.54487609863281, loss : 2.0541257858276367\n",
            "epoch: 8, classification_loss: 1.802042007446289, Val Loss: 70.44961297512054, loss : 2.062385320663452\n",
            "epoch: 9, classification_loss: 1.8125128746032715, Val Loss: 70.45784211158752, loss : 2.0651445388793945\n",
            "epoch: 10, classification_loss: 1.8061186075210571, Val Loss: 70.54013872146606, loss : 2.06542706489563\n",
            "epoch: 11, classification_loss: 1.799203634262085, Val Loss: 70.4908457994461, loss : 2.045876979827881\n",
            "epoch: 12, classification_loss: 1.8052457571029663, Val Loss: 70.46611404418945, loss : 2.053109884262085\n",
            "epoch: 13, classification_loss: 1.8164098262786865, Val Loss: 70.47501873970032, loss : 2.0613064765930176\n",
            "epoch: 14, classification_loss: 1.7989463806152344, Val Loss: 70.54723930358887, loss : 2.0482964515686035\n",
            "epoch: 15, classification_loss: 1.8059751987457275, Val Loss: 70.56582653522491, loss : 2.0469417572021484\n",
            "epoch: 16, classification_loss: 1.7984728813171387, Val Loss: 70.48590755462646, loss : 2.042473077774048\n",
            "epoch: 17, classification_loss: 1.796650767326355, Val Loss: 70.47349333763123, loss : 2.0423176288604736\n",
            "epoch: 18, classification_loss: 1.8062076568603516, Val Loss: 70.51217031478882, loss : 2.044947862625122\n",
            "epoch: 19, classification_loss: 1.8110018968582153, Val Loss: 70.48718535900116, loss : 2.051877021789551\n",
            "Batch: 75, Test Acc: 0.5760216346153846\n",
            "Batch: 76:\n",
            "epoch: 0, classification_loss: 1.8192496299743652, Val Loss: 70.44041752815247, loss : 1.8192496299743652\n",
            "epoch: 1, classification_loss: 1.7744392156600952, Val Loss: 70.46967077255249, loss : 2.1077980995178223\n",
            "epoch: 2, classification_loss: 1.7489651441574097, Val Loss: 70.43917298316956, loss : 2.0325753688812256\n",
            "epoch: 3, classification_loss: 1.7816704511642456, Val Loss: 70.48205995559692, loss : 2.0592892169952393\n",
            "epoch: 4, classification_loss: 1.8080813884735107, Val Loss: 70.43676102161407, loss : 2.055023670196533\n",
            "epoch: 5, classification_loss: 1.7985239028930664, Val Loss: 70.45440471172333, loss : 2.0550384521484375\n",
            "epoch: 6, classification_loss: 1.7838643789291382, Val Loss: 70.4625324010849, loss : 2.057149887084961\n",
            "epoch: 7, classification_loss: 1.7765482664108276, Val Loss: 70.48937356472015, loss : 2.0441513061523438\n",
            "epoch: 8, classification_loss: 1.799591064453125, Val Loss: 70.44853341579437, loss : 2.060483694076538\n",
            "epoch: 9, classification_loss: 1.7938077449798584, Val Loss: 70.49046170711517, loss : 2.0515871047973633\n",
            "epoch: 10, classification_loss: 1.7874425649642944, Val Loss: 70.45416331291199, loss : 2.041354179382324\n",
            "epoch: 11, classification_loss: 1.7910990715026855, Val Loss: 70.47137570381165, loss : 2.043086528778076\n",
            "epoch: 12, classification_loss: 1.8117880821228027, Val Loss: 70.5000890493393, loss : 2.0507307052612305\n",
            "epoch: 13, classification_loss: 1.8107151985168457, Val Loss: 70.51036083698273, loss : 2.0561695098876953\n",
            "epoch: 14, classification_loss: 1.78773033618927, Val Loss: 70.49709093570709, loss : 2.0258262157440186\n",
            "epoch: 15, classification_loss: 1.7997218370437622, Val Loss: 70.47047138214111, loss : 2.047464370727539\n",
            "epoch: 16, classification_loss: 1.794456958770752, Val Loss: 70.50277888774872, loss : 2.0455808639526367\n",
            "epoch: 17, classification_loss: 1.7873033285140991, Val Loss: 70.5161064863205, loss : 2.038990020751953\n",
            "epoch: 18, classification_loss: 1.7947719097137451, Val Loss: 70.51146006584167, loss : 2.0350608825683594\n",
            "epoch: 19, classification_loss: 1.801306962966919, Val Loss: 70.50341141223907, loss : 2.044355630874634\n",
            "Batch: 76, Test Acc: 0.5741185897435898\n",
            "Batch: 77:\n",
            "epoch: 0, classification_loss: 1.8403033018112183, Val Loss: 70.52005290985107, loss : 1.8403033018112183\n",
            "epoch: 1, classification_loss: 1.7993544340133667, Val Loss: 70.6136828660965, loss : 2.134160041809082\n",
            "epoch: 2, classification_loss: 1.7771275043487549, Val Loss: 70.49338412284851, loss : 2.052746057510376\n",
            "epoch: 3, classification_loss: 1.785446286201477, Val Loss: 70.49816417694092, loss : 2.063204288482666\n",
            "epoch: 4, classification_loss: 1.817395567893982, Val Loss: 70.47839772701263, loss : 2.0588555335998535\n",
            "epoch: 5, classification_loss: 1.80787193775177, Val Loss: 70.45528221130371, loss : 2.0755364894866943\n",
            "epoch: 6, classification_loss: 1.797197699546814, Val Loss: 70.43985724449158, loss : 2.0756993293762207\n",
            "epoch: 7, classification_loss: 1.7895406484603882, Val Loss: 70.50506007671356, loss : 2.062181234359741\n",
            "epoch: 8, classification_loss: 1.7840602397918701, Val Loss: 70.58654284477234, loss : 2.0475668907165527\n",
            "epoch: 9, classification_loss: 1.798442006111145, Val Loss: 70.46975696086884, loss : 2.059340000152588\n",
            "epoch: 10, classification_loss: 1.8024582862854004, Val Loss: 70.45368528366089, loss : 2.056828260421753\n",
            "epoch: 11, classification_loss: 1.8049492835998535, Val Loss: 70.4807677268982, loss : 2.0561251640319824\n",
            "epoch: 12, classification_loss: 1.8106871843338013, Val Loss: 70.50122559070587, loss : 2.0576274394989014\n",
            "epoch: 13, classification_loss: 1.8042103052139282, Val Loss: 70.50550413131714, loss : 2.054438352584839\n",
            "epoch: 14, classification_loss: 1.800586223602295, Val Loss: 70.47216737270355, loss : 2.050150156021118\n",
            "epoch: 15, classification_loss: 1.8000752925872803, Val Loss: 70.50149941444397, loss : 2.051438570022583\n",
            "epoch: 16, classification_loss: 1.8041868209838867, Val Loss: 70.48434019088745, loss : 2.046595335006714\n",
            "epoch: 17, classification_loss: 1.7998220920562744, Val Loss: 70.47851729393005, loss : 2.0500340461730957\n",
            "epoch: 18, classification_loss: 1.8086307048797607, Val Loss: 70.5105299949646, loss : 2.0411558151245117\n",
            "epoch: 19, classification_loss: 1.81087064743042, Val Loss: 70.55455100536346, loss : 2.0555789470672607\n",
            "Batch: 77, Test Acc: 0.5716145833333334\n",
            "Batch: 78:\n",
            "epoch: 0, classification_loss: 1.8043522834777832, Val Loss: 70.49999165534973, loss : 1.8043522834777832\n",
            "epoch: 1, classification_loss: 1.7746411561965942, Val Loss: 70.55740797519684, loss : 2.1030054092407227\n",
            "epoch: 2, classification_loss: 1.7589448690414429, Val Loss: 70.49701702594757, loss : 2.031815767288208\n",
            "epoch: 3, classification_loss: 1.7822617292404175, Val Loss: 70.44067358970642, loss : 2.042402744293213\n",
            "epoch: 4, classification_loss: 1.8057925701141357, Val Loss: 70.4333701133728, loss : 2.0348689556121826\n",
            "epoch: 5, classification_loss: 1.7944557666778564, Val Loss: 70.44445359706879, loss : 2.0486273765563965\n",
            "epoch: 6, classification_loss: 1.7731252908706665, Val Loss: 70.46929264068604, loss : 2.0380194187164307\n",
            "epoch: 7, classification_loss: 1.7666728496551514, Val Loss: 70.45743989944458, loss : 2.020054578781128\n",
            "epoch: 8, classification_loss: 1.7990586757659912, Val Loss: 70.44398701190948, loss : 2.0399997234344482\n",
            "epoch: 9, classification_loss: 1.7789736986160278, Val Loss: 70.47617638111115, loss : 2.024578094482422\n",
            "epoch: 10, classification_loss: 1.8008235692977905, Val Loss: 70.46823215484619, loss : 2.0473103523254395\n",
            "epoch: 11, classification_loss: 1.7825043201446533, Val Loss: 70.44872486591339, loss : 2.0345940589904785\n",
            "epoch: 12, classification_loss: 1.778976559638977, Val Loss: 70.52622854709625, loss : 2.0198025703430176\n",
            "epoch: 13, classification_loss: 1.7818318605422974, Val Loss: 70.48964607715607, loss : 2.0265305042266846\n",
            "epoch: 14, classification_loss: 1.7683417797088623, Val Loss: 70.47820019721985, loss : 2.004364490509033\n",
            "epoch: 15, classification_loss: 1.777294397354126, Val Loss: 70.49204313755035, loss : 2.020169973373413\n",
            "epoch: 16, classification_loss: 1.7829303741455078, Val Loss: 70.4845962524414, loss : 2.0173826217651367\n",
            "epoch: 17, classification_loss: 1.7903227806091309, Val Loss: 70.48602092266083, loss : 2.0259852409362793\n",
            "epoch: 18, classification_loss: 1.7829341888427734, Val Loss: 70.53513967990875, loss : 2.014739751815796\n",
            "epoch: 19, classification_loss: 1.7786107063293457, Val Loss: 70.4977605342865, loss : 2.0140230655670166\n",
            "Batch: 78, Test Acc: 0.575020032051282\n",
            "Batch: 79:\n",
            "epoch: 0, classification_loss: 1.8046176433563232, Val Loss: 70.41474652290344, loss : 1.8046176433563232\n",
            "epoch: 1, classification_loss: 1.7634996175765991, Val Loss: 70.42948698997498, loss : 2.0923662185668945\n",
            "epoch: 2, classification_loss: 1.7452366352081299, Val Loss: 70.47127079963684, loss : 2.006305694580078\n",
            "epoch: 3, classification_loss: 1.7797569036483765, Val Loss: 70.47193837165833, loss : 2.049469470977783\n",
            "epoch: 4, classification_loss: 1.793689250946045, Val Loss: 70.42842590808868, loss : 2.023003101348877\n",
            "epoch: 5, classification_loss: 1.7874140739440918, Val Loss: 70.4273943901062, loss : 2.0486695766448975\n",
            "epoch: 6, classification_loss: 1.774662971496582, Val Loss: 70.47705292701721, loss : 2.0398614406585693\n",
            "epoch: 7, classification_loss: 1.7827824354171753, Val Loss: 70.49652647972107, loss : 2.0369482040405273\n",
            "epoch: 8, classification_loss: 1.789798617362976, Val Loss: 70.52730321884155, loss : 2.032743215560913\n",
            "epoch: 9, classification_loss: 1.7734073400497437, Val Loss: 70.4446839094162, loss : 2.0202643871307373\n",
            "epoch: 10, classification_loss: 1.7853742837905884, Val Loss: 70.51335322856903, loss : 2.0304081439971924\n",
            "epoch: 11, classification_loss: 1.776391625404358, Val Loss: 70.57082831859589, loss : 2.017559766769409\n",
            "epoch: 12, classification_loss: 1.7769176959991455, Val Loss: 70.49968338012695, loss : 2.020328998565674\n",
            "epoch: 13, classification_loss: 1.785416841506958, Val Loss: 70.54929828643799, loss : 2.0263864994049072\n",
            "epoch: 14, classification_loss: 1.7708911895751953, Val Loss: 70.49983048439026, loss : 2.013633966445923\n",
            "epoch: 15, classification_loss: 1.779972791671753, Val Loss: 70.5066020488739, loss : 2.0180392265319824\n",
            "epoch: 16, classification_loss: 1.784521460533142, Val Loss: 70.47452914714813, loss : 2.0193138122558594\n",
            "epoch: 17, classification_loss: 1.7821173667907715, Val Loss: 70.48867583274841, loss : 2.0153603553771973\n",
            "epoch: 18, classification_loss: 1.780269980430603, Val Loss: 70.54923367500305, loss : 2.0134079456329346\n",
            "epoch: 19, classification_loss: 1.772086501121521, Val Loss: 70.4812833070755, loss : 2.013779878616333\n",
            "Batch: 79, Test Acc: 0.5748197115384616\n",
            "Batch: 80:\n",
            "epoch: 0, classification_loss: 1.7988030910491943, Val Loss: 70.39618527889252, loss : 1.7988030910491943\n",
            "epoch: 1, classification_loss: 1.774341344833374, Val Loss: 70.4298609495163, loss : 2.1042542457580566\n",
            "epoch: 2, classification_loss: 1.7481945753097534, Val Loss: 70.45985472202301, loss : 2.0159285068511963\n",
            "epoch: 3, classification_loss: 1.7763001918792725, Val Loss: 70.45225024223328, loss : 2.0434317588806152\n",
            "epoch: 4, classification_loss: 1.785441517829895, Val Loss: 70.42520236968994, loss : 2.0182836055755615\n",
            "epoch: 5, classification_loss: 1.7909797430038452, Val Loss: 70.47598218917847, loss : 2.0455727577209473\n",
            "epoch: 6, classification_loss: 1.7761448621749878, Val Loss: 70.4475679397583, loss : 2.041740894317627\n",
            "epoch: 7, classification_loss: 1.7729251384735107, Val Loss: 70.47693622112274, loss : 2.03633189201355\n",
            "epoch: 8, classification_loss: 1.773053526878357, Val Loss: 70.42793560028076, loss : 2.0297555923461914\n",
            "epoch: 9, classification_loss: 1.8011287450790405, Val Loss: 70.46561276912689, loss : 2.0448243618011475\n",
            "epoch: 10, classification_loss: 1.7898707389831543, Val Loss: 70.46536874771118, loss : 2.0390143394470215\n",
            "epoch: 11, classification_loss: 1.7864314317703247, Val Loss: 70.46795511245728, loss : 2.027012586593628\n",
            "epoch: 12, classification_loss: 1.773718237876892, Val Loss: 70.47778797149658, loss : 2.0228469371795654\n",
            "epoch: 13, classification_loss: 1.7884085178375244, Val Loss: 70.46769189834595, loss : 2.031532049179077\n",
            "epoch: 14, classification_loss: 1.784780502319336, Val Loss: 70.46837663650513, loss : 2.0345401763916016\n",
            "epoch: 15, classification_loss: 1.7908761501312256, Val Loss: 70.44368016719818, loss : 2.040224552154541\n",
            "epoch: 16, classification_loss: 1.7933154106140137, Val Loss: 70.45062446594238, loss : 2.0347700119018555\n",
            "epoch: 17, classification_loss: 1.791131615638733, Val Loss: 70.46922361850739, loss : 2.030902624130249\n",
            "epoch: 18, classification_loss: 1.798433780670166, Val Loss: 70.49419820308685, loss : 2.030687093734741\n",
            "epoch: 19, classification_loss: 1.7902860641479492, Val Loss: 70.49116086959839, loss : 2.031961679458618\n",
            "Batch: 80, Test Acc: 0.5749198717948718\n",
            "Batch: 81:\n",
            "epoch: 0, classification_loss: 1.7819006443023682, Val Loss: 70.71517825126648, loss : 1.7819006443023682\n",
            "epoch: 1, classification_loss: 1.734094500541687, Val Loss: 70.57709240913391, loss : 2.0695908069610596\n",
            "epoch: 2, classification_loss: 1.7193738222122192, Val Loss: 70.47669804096222, loss : 1.9805973768234253\n",
            "epoch: 3, classification_loss: 1.7349671125411987, Val Loss: 70.42021048069, loss : 2.018890619277954\n",
            "epoch: 4, classification_loss: 1.7606019973754883, Val Loss: 70.55662095546722, loss : 2.003547191619873\n",
            "epoch: 5, classification_loss: 1.7612792253494263, Val Loss: 70.49366760253906, loss : 2.0334818363189697\n",
            "epoch: 6, classification_loss: 1.7440781593322754, Val Loss: 70.48597705364227, loss : 2.0286595821380615\n",
            "epoch: 7, classification_loss: 1.7507612705230713, Val Loss: 70.49751126766205, loss : 2.012791156768799\n",
            "epoch: 8, classification_loss: 1.7555032968521118, Val Loss: 70.51549291610718, loss : 2.01924729347229\n",
            "epoch: 9, classification_loss: 1.749435305595398, Val Loss: 70.44637548923492, loss : 2.010246515274048\n",
            "epoch: 10, classification_loss: 1.7442727088928223, Val Loss: 70.46120321750641, loss : 2.0023202896118164\n",
            "epoch: 11, classification_loss: 1.749057650566101, Val Loss: 70.49918103218079, loss : 2.005434989929199\n",
            "epoch: 12, classification_loss: 1.7489639520645142, Val Loss: 70.57563197612762, loss : 2.0008997917175293\n",
            "epoch: 13, classification_loss: 1.7627507448196411, Val Loss: 70.47709214687347, loss : 2.014131784439087\n",
            "epoch: 14, classification_loss: 1.7436288595199585, Val Loss: 70.46486246585846, loss : 1.9965498447418213\n",
            "epoch: 15, classification_loss: 1.7535854578018188, Val Loss: 70.48952734470367, loss : 1.9975907802581787\n",
            "epoch: 16, classification_loss: 1.7516686916351318, Val Loss: 70.46356558799744, loss : 2.001258373260498\n",
            "epoch: 17, classification_loss: 1.751749873161316, Val Loss: 70.50305831432343, loss : 1.9933125972747803\n",
            "epoch: 18, classification_loss: 1.76642644405365, Val Loss: 70.49523866176605, loss : 2.005747079849243\n",
            "epoch: 19, classification_loss: 1.753372311592102, Val Loss: 70.47891843318939, loss : 1.9901540279388428\n",
            "Batch: 81, Test Acc: 0.5774238782051282\n",
            "Batch: 82:\n",
            "epoch: 0, classification_loss: 1.8343877792358398, Val Loss: 70.4141491651535, loss : 1.8343877792358398\n",
            "epoch: 1, classification_loss: 1.792367935180664, Val Loss: 70.56805038452148, loss : 2.1273341178894043\n",
            "epoch: 2, classification_loss: 1.7584960460662842, Val Loss: 70.52649652957916, loss : 2.0340001583099365\n",
            "epoch: 3, classification_loss: 1.7887320518493652, Val Loss: 70.4691516160965, loss : 2.0715277194976807\n",
            "epoch: 4, classification_loss: 1.816334843635559, Val Loss: 70.41820991039276, loss : 2.0721499919891357\n",
            "epoch: 5, classification_loss: 1.813645839691162, Val Loss: 70.44437873363495, loss : 2.065122604370117\n",
            "epoch: 6, classification_loss: 1.8031799793243408, Val Loss: 70.46433389186859, loss : 2.0716323852539062\n",
            "epoch: 7, classification_loss: 1.7868345975875854, Val Loss: 70.47945499420166, loss : 2.0490219593048096\n",
            "epoch: 8, classification_loss: 1.8026565313339233, Val Loss: 70.4437974691391, loss : 2.06457781791687\n",
            "epoch: 9, classification_loss: 1.8056384325027466, Val Loss: 70.48227298259735, loss : 2.063293218612671\n",
            "epoch: 10, classification_loss: 1.8035032749176025, Val Loss: 70.47179758548737, loss : 2.0602874755859375\n",
            "epoch: 11, classification_loss: 1.796154260635376, Val Loss: 70.45512533187866, loss : 2.0518364906311035\n",
            "epoch: 12, classification_loss: 1.8013696670532227, Val Loss: 70.44761276245117, loss : 2.0548911094665527\n",
            "epoch: 13, classification_loss: 1.806279182434082, Val Loss: 70.48293554782867, loss : 2.0610463619232178\n",
            "epoch: 14, classification_loss: 1.795828104019165, Val Loss: 70.48690354824066, loss : 2.0473504066467285\n",
            "epoch: 15, classification_loss: 1.812230110168457, Val Loss: 70.50143897533417, loss : 2.0606799125671387\n",
            "epoch: 16, classification_loss: 1.8144792318344116, Val Loss: 70.47073996067047, loss : 2.061424493789673\n",
            "epoch: 17, classification_loss: 1.801072597503662, Val Loss: 70.46859300136566, loss : 2.055002212524414\n",
            "epoch: 18, classification_loss: 1.8062814474105835, Val Loss: 70.4598935842514, loss : 2.058116912841797\n",
            "epoch: 19, classification_loss: 1.8062160015106201, Val Loss: 70.49566626548767, loss : 2.056485652923584\n",
            "Batch: 82, Test Acc: 0.5754206730769231\n",
            "Batch: 83:\n",
            "epoch: 0, classification_loss: 1.7989979982376099, Val Loss: 70.42750978469849, loss : 1.7989979982376099\n",
            "epoch: 1, classification_loss: 1.7652419805526733, Val Loss: 70.46593141555786, loss : 2.0967202186584473\n",
            "epoch: 2, classification_loss: 1.7462763786315918, Val Loss: 70.43473601341248, loss : 2.0182390213012695\n",
            "epoch: 3, classification_loss: 1.7722302675247192, Val Loss: 70.44788324832916, loss : 2.0449905395507812\n",
            "epoch: 4, classification_loss: 1.7886561155319214, Val Loss: 70.44095408916473, loss : 2.0421109199523926\n",
            "epoch: 5, classification_loss: 1.7905381917953491, Val Loss: 70.47673070430756, loss : 2.047771692276001\n",
            "epoch: 6, classification_loss: 1.7649716138839722, Val Loss: 70.4656549692154, loss : 2.0411617755889893\n",
            "epoch: 7, classification_loss: 1.7706940174102783, Val Loss: 70.49796104431152, loss : 2.0330939292907715\n",
            "epoch: 8, classification_loss: 1.7816766500473022, Val Loss: 70.51505184173584, loss : 2.0320281982421875\n",
            "epoch: 9, classification_loss: 1.7863812446594238, Val Loss: 70.49786949157715, loss : 2.04472017288208\n",
            "epoch: 10, classification_loss: 1.782232403755188, Val Loss: 70.45427680015564, loss : 2.0330843925476074\n",
            "epoch: 11, classification_loss: 1.777881383895874, Val Loss: 70.461678981781, loss : 2.0316643714904785\n",
            "epoch: 12, classification_loss: 1.7622076272964478, Val Loss: 70.51627147197723, loss : 2.0181031227111816\n",
            "epoch: 13, classification_loss: 1.7788552045822144, Val Loss: 70.50197207927704, loss : 2.031157970428467\n",
            "epoch: 14, classification_loss: 1.7843331098556519, Val Loss: 70.465891122818, loss : 2.0350234508514404\n",
            "epoch: 15, classification_loss: 1.7753242254257202, Val Loss: 70.48716449737549, loss : 2.025038957595825\n",
            "epoch: 16, classification_loss: 1.7641282081604004, Val Loss: 70.52690947055817, loss : 2.009146213531494\n",
            "epoch: 17, classification_loss: 1.7653049230575562, Val Loss: 70.48727571964264, loss : 2.016044855117798\n",
            "epoch: 18, classification_loss: 1.7860734462738037, Val Loss: 70.48986780643463, loss : 2.0271642208099365\n",
            "epoch: 19, classification_loss: 1.7949548959732056, Val Loss: 70.54071855545044, loss : 2.042731285095215\n",
            "Batch: 83, Test Acc: 0.5762219551282052\n",
            "Batch: 84:\n",
            "epoch: 0, classification_loss: 1.8072906732559204, Val Loss: 70.45395219326019, loss : 1.8072906732559204\n",
            "epoch: 1, classification_loss: 1.7532258033752441, Val Loss: 70.4695417881012, loss : 2.0806849002838135\n",
            "epoch: 2, classification_loss: 1.751941204071045, Val Loss: 70.42279529571533, loss : 2.005258560180664\n",
            "epoch: 3, classification_loss: 1.786305546760559, Val Loss: 70.44657623767853, loss : 2.0457897186279297\n",
            "epoch: 4, classification_loss: 1.7994123697280884, Val Loss: 70.47822487354279, loss : 2.0213303565979004\n",
            "epoch: 5, classification_loss: 1.7763864994049072, Val Loss: 70.46836721897125, loss : 2.0432369709014893\n",
            "epoch: 6, classification_loss: 1.7609981298446655, Val Loss: 70.45174288749695, loss : 2.0403356552124023\n",
            "epoch: 7, classification_loss: 1.7685459852218628, Val Loss: 70.47256636619568, loss : 2.0245096683502197\n",
            "epoch: 8, classification_loss: 1.772534966468811, Val Loss: 70.53492057323456, loss : 2.0285892486572266\n",
            "epoch: 9, classification_loss: 1.7804723978042603, Val Loss: 70.47292006015778, loss : 2.0242626667022705\n",
            "epoch: 10, classification_loss: 1.774049997329712, Val Loss: 70.42897164821625, loss : 2.0203206539154053\n",
            "epoch: 11, classification_loss: 1.7744133472442627, Val Loss: 70.4907613992691, loss : 2.0134520530700684\n",
            "epoch: 12, classification_loss: 1.7768131494522095, Val Loss: 70.50943756103516, loss : 2.0118930339813232\n",
            "epoch: 13, classification_loss: 1.7758570909500122, Val Loss: 70.48235642910004, loss : 2.0179998874664307\n",
            "epoch: 14, classification_loss: 1.7829667329788208, Val Loss: 70.49818122386932, loss : 2.018078088760376\n",
            "epoch: 15, classification_loss: 1.781493067741394, Val Loss: 70.47687304019928, loss : 2.0157418251037598\n",
            "epoch: 16, classification_loss: 1.7860627174377441, Val Loss: 70.45791912078857, loss : 2.012885570526123\n",
            "epoch: 17, classification_loss: 1.7709366083145142, Val Loss: 70.49359571933746, loss : 2.0050899982452393\n",
            "epoch: 18, classification_loss: 1.7870291471481323, Val Loss: 70.54363822937012, loss : 2.014638900756836\n",
            "epoch: 19, classification_loss: 1.7855541706085205, Val Loss: 70.53939878940582, loss : 2.016073703765869\n",
            "Batch: 84, Test Acc: 0.5756209935897436\n",
            "Batch: 85:\n",
            "epoch: 0, classification_loss: 1.8042546510696411, Val Loss: 70.39464282989502, loss : 1.8042546510696411\n",
            "epoch: 1, classification_loss: 1.7684561014175415, Val Loss: 70.44611632823944, loss : 2.092155933380127\n",
            "epoch: 2, classification_loss: 1.7611521482467651, Val Loss: 70.4566011428833, loss : 2.0112476348876953\n",
            "epoch: 3, classification_loss: 1.7809906005859375, Val Loss: 70.43625581264496, loss : 2.0410468578338623\n",
            "epoch: 4, classification_loss: 1.806381106376648, Val Loss: 70.41481924057007, loss : 2.033294677734375\n",
            "epoch: 5, classification_loss: 1.7946174144744873, Val Loss: 70.4366979598999, loss : 2.055508613586426\n",
            "epoch: 6, classification_loss: 1.7682240009307861, Val Loss: 70.4979555606842, loss : 2.0356860160827637\n",
            "epoch: 7, classification_loss: 1.7695796489715576, Val Loss: 70.46596431732178, loss : 2.029453754425049\n",
            "epoch: 8, classification_loss: 1.7955137491226196, Val Loss: 70.4571897983551, loss : 2.0410261154174805\n",
            "epoch: 9, classification_loss: 1.7827427387237549, Val Loss: 70.45236480236053, loss : 2.0337672233581543\n",
            "epoch: 10, classification_loss: 1.7879682779312134, Val Loss: 70.44875407218933, loss : 2.0307881832122803\n",
            "epoch: 11, classification_loss: 1.7835572957992554, Val Loss: 70.45664095878601, loss : 2.0246729850769043\n",
            "epoch: 12, classification_loss: 1.7761945724487305, Val Loss: 70.47804379463196, loss : 2.0148556232452393\n",
            "epoch: 13, classification_loss: 1.7790366411209106, Val Loss: 70.46749651432037, loss : 2.0200302600860596\n",
            "epoch: 14, classification_loss: 1.791425347328186, Val Loss: 70.46107137203217, loss : 2.0294384956359863\n",
            "epoch: 15, classification_loss: 1.7894551753997803, Val Loss: 70.47626793384552, loss : 2.021909236907959\n",
            "epoch: 16, classification_loss: 1.7926671504974365, Val Loss: 70.51168668270111, loss : 2.018819808959961\n",
            "epoch: 17, classification_loss: 1.7883120775222778, Val Loss: 70.4795550107956, loss : 2.0242624282836914\n",
            "epoch: 18, classification_loss: 1.7821153402328491, Val Loss: 70.45029628276825, loss : 2.009272813796997\n",
            "epoch: 19, classification_loss: 1.7878881692886353, Val Loss: 70.47490739822388, loss : 2.0253188610076904\n",
            "Batch: 85, Test Acc: 0.5770232371794872\n",
            "Batch: 86:\n",
            "epoch: 0, classification_loss: 1.8326526880264282, Val Loss: 70.44541156291962, loss : 1.8326526880264282\n",
            "epoch: 1, classification_loss: 1.7952717542648315, Val Loss: 70.42798042297363, loss : 2.118691921234131\n",
            "epoch: 2, classification_loss: 1.7812315225601196, Val Loss: 70.42363798618317, loss : 2.0424723625183105\n",
            "epoch: 3, classification_loss: 1.8066293001174927, Val Loss: 70.42797982692719, loss : 2.074117660522461\n",
            "epoch: 4, classification_loss: 1.8194934129714966, Val Loss: 70.48739290237427, loss : 2.0490219593048096\n",
            "epoch: 5, classification_loss: 1.822026014328003, Val Loss: 70.44921541213989, loss : 2.0784451961517334\n",
            "epoch: 6, classification_loss: 1.8079296350479126, Val Loss: 70.44656527042389, loss : 2.0640313625335693\n",
            "epoch: 7, classification_loss: 1.8182098865509033, Val Loss: 70.43717002868652, loss : 2.0756967067718506\n",
            "epoch: 8, classification_loss: 1.8085377216339111, Val Loss: 70.48974919319153, loss : 2.066409111022949\n",
            "epoch: 9, classification_loss: 1.8151453733444214, Val Loss: 70.48716843128204, loss : 2.0684680938720703\n",
            "epoch: 10, classification_loss: 1.813705563545227, Val Loss: 70.42706143856049, loss : 2.0673623085021973\n",
            "epoch: 11, classification_loss: 1.812132477760315, Val Loss: 70.470383644104, loss : 2.0601091384887695\n",
            "epoch: 12, classification_loss: 1.8095344305038452, Val Loss: 70.51690316200256, loss : 2.051987886428833\n",
            "epoch: 13, classification_loss: 1.820619821548462, Val Loss: 70.44932508468628, loss : 2.06457257270813\n",
            "epoch: 14, classification_loss: 1.8185495138168335, Val Loss: 70.4367094039917, loss : 2.067674160003662\n",
            "epoch: 15, classification_loss: 1.8147943019866943, Val Loss: 70.48234510421753, loss : 2.068898916244507\n",
            "epoch: 16, classification_loss: 1.8048723936080933, Val Loss: 70.57983732223511, loss : 2.057607889175415\n",
            "epoch: 17, classification_loss: 1.8065197467803955, Val Loss: 70.5300886631012, loss : 2.0577125549316406\n",
            "epoch: 18, classification_loss: 1.8101625442504883, Val Loss: 70.50744116306305, loss : 2.0463459491729736\n",
            "epoch: 19, classification_loss: 1.8088678121566772, Val Loss: 70.48753488063812, loss : 2.053356409072876\n",
            "Batch: 86, Test Acc: 0.5710136217948718\n",
            "Batch: 87:\n",
            "epoch: 0, classification_loss: 1.796505331993103, Val Loss: 70.47042310237885, loss : 1.796505331993103\n",
            "epoch: 1, classification_loss: 1.7593978643417358, Val Loss: 70.46038126945496, loss : 2.0860979557037354\n",
            "epoch: 2, classification_loss: 1.7293990850448608, Val Loss: 70.4930135011673, loss : 1.9869046211242676\n",
            "epoch: 3, classification_loss: 1.7484012842178345, Val Loss: 70.45346868038177, loss : 2.017521619796753\n",
            "epoch: 4, classification_loss: 1.7887938022613525, Val Loss: 70.48009240627289, loss : 2.0254688262939453\n",
            "epoch: 5, classification_loss: 1.7684980630874634, Val Loss: 70.48535549640656, loss : 2.03399658203125\n",
            "epoch: 6, classification_loss: 1.745666265487671, Val Loss: 70.51273548603058, loss : 2.019108772277832\n",
            "epoch: 7, classification_loss: 1.749693751335144, Val Loss: 70.50500547885895, loss : 2.0087051391601562\n",
            "epoch: 8, classification_loss: 1.7596287727355957, Val Loss: 70.49789023399353, loss : 2.019864082336426\n",
            "epoch: 9, classification_loss: 1.762425184249878, Val Loss: 70.50846815109253, loss : 2.011129379272461\n",
            "epoch: 10, classification_loss: 1.7607007026672363, Val Loss: 70.54140889644623, loss : 2.011453628540039\n",
            "epoch: 11, classification_loss: 1.7702592611312866, Val Loss: 70.51133751869202, loss : 2.007956027984619\n",
            "epoch: 12, classification_loss: 1.755542516708374, Val Loss: 70.50463473796844, loss : 1.9933207035064697\n",
            "epoch: 13, classification_loss: 1.7496384382247925, Val Loss: 70.51337194442749, loss : 1.9915134906768799\n",
            "epoch: 14, classification_loss: 1.7502961158752441, Val Loss: 70.54155230522156, loss : 1.9892929792404175\n",
            "epoch: 15, classification_loss: 1.7633631229400635, Val Loss: 70.55220866203308, loss : 2.003460645675659\n",
            "epoch: 16, classification_loss: 1.7553571462631226, Val Loss: 70.51996183395386, loss : 1.9913954734802246\n",
            "epoch: 17, classification_loss: 1.762008547782898, Val Loss: 70.50135099887848, loss : 1.9937503337860107\n",
            "epoch: 18, classification_loss: 1.7612872123718262, Val Loss: 70.53161585330963, loss : 1.9919832944869995\n",
            "epoch: 19, classification_loss: 1.7575715780258179, Val Loss: 70.58164238929749, loss : 1.9913573265075684\n",
            "Batch: 87, Test Acc: 0.5741185897435898\n",
            "Batch: 88:\n",
            "epoch: 0, classification_loss: 1.8489800691604614, Val Loss: 70.50784933567047, loss : 1.8489800691604614\n",
            "epoch: 1, classification_loss: 1.8074073791503906, Val Loss: 70.47507214546204, loss : 2.137478828430176\n",
            "epoch: 2, classification_loss: 1.7885338068008423, Val Loss: 70.43433213233948, loss : 2.0454630851745605\n",
            "epoch: 3, classification_loss: 1.8316349983215332, Val Loss: 70.42371785640717, loss : 2.1003286838531494\n",
            "epoch: 4, classification_loss: 1.851068139076233, Val Loss: 70.45862352848053, loss : 2.090866804122925\n",
            "epoch: 5, classification_loss: 1.8373433351516724, Val Loss: 70.47018909454346, loss : 2.0937118530273438\n",
            "epoch: 6, classification_loss: 1.8360446691513062, Val Loss: 70.42088222503662, loss : 2.099489450454712\n",
            "epoch: 7, classification_loss: 1.8160243034362793, Val Loss: 70.42416763305664, loss : 2.0744082927703857\n",
            "epoch: 8, classification_loss: 1.823446273803711, Val Loss: 70.43392241001129, loss : 2.071291446685791\n",
            "epoch: 9, classification_loss: 1.837148666381836, Val Loss: 70.44602477550507, loss : 2.076598644256592\n",
            "epoch: 10, classification_loss: 1.8255023956298828, Val Loss: 70.43321180343628, loss : 2.0667128562927246\n",
            "epoch: 11, classification_loss: 1.830864667892456, Val Loss: 70.4442846775055, loss : 2.0651943683624268\n",
            "epoch: 12, classification_loss: 1.831487774848938, Val Loss: 70.45083284378052, loss : 2.0649008750915527\n",
            "epoch: 13, classification_loss: 1.8285982608795166, Val Loss: 70.45158767700195, loss : 2.065431833267212\n",
            "epoch: 14, classification_loss: 1.833312749862671, Val Loss: 70.434112906456, loss : 2.0664327144622803\n",
            "epoch: 15, classification_loss: 1.8332942724227905, Val Loss: 70.46660959720612, loss : 2.0643508434295654\n",
            "epoch: 16, classification_loss: 1.8367242813110352, Val Loss: 70.48091971874237, loss : 2.063913106918335\n",
            "epoch: 17, classification_loss: 1.8256027698516846, Val Loss: 70.43533873558044, loss : 2.058967113494873\n",
            "epoch: 18, classification_loss: 1.8251434564590454, Val Loss: 70.44021606445312, loss : 2.0510497093200684\n",
            "epoch: 19, classification_loss: 1.8298852443695068, Val Loss: 70.50431728363037, loss : 2.064758539199829\n",
            "Batch: 88, Test Acc: 0.5764222756410257\n",
            "Batch: 89:\n",
            "epoch: 0, classification_loss: 1.7644507884979248, Val Loss: 70.4672920703888, loss : 1.7644507884979248\n",
            "epoch: 1, classification_loss: 1.7410515546798706, Val Loss: 70.4423977136612, loss : 2.060734987258911\n",
            "epoch: 2, classification_loss: 1.7298673391342163, Val Loss: 70.47350907325745, loss : 1.9822527170181274\n",
            "epoch: 3, classification_loss: 1.7487819194793701, Val Loss: 70.47423315048218, loss : 2.0171468257904053\n",
            "epoch: 4, classification_loss: 1.7757360935211182, Val Loss: 70.44281566143036, loss : 2.0089433193206787\n",
            "epoch: 5, classification_loss: 1.7628759145736694, Val Loss: 70.44605779647827, loss : 2.010432004928589\n",
            "epoch: 6, classification_loss: 1.7554848194122314, Val Loss: 70.51377093791962, loss : 2.015178680419922\n",
            "epoch: 7, classification_loss: 1.74114191532135, Val Loss: 70.46974980831146, loss : 1.9959051609039307\n",
            "epoch: 8, classification_loss: 1.7555692195892334, Val Loss: 70.50200128555298, loss : 2.003675699234009\n",
            "epoch: 9, classification_loss: 1.749002456665039, Val Loss: 70.50035321712494, loss : 1.9921839237213135\n",
            "epoch: 10, classification_loss: 1.7662488222122192, Val Loss: 70.48500788211823, loss : 2.0065758228302\n",
            "epoch: 11, classification_loss: 1.7634063959121704, Val Loss: 70.44620323181152, loss : 2.0066936016082764\n",
            "epoch: 12, classification_loss: 1.7531980276107788, Val Loss: 70.54635846614838, loss : 1.990494728088379\n",
            "epoch: 13, classification_loss: 1.7616814374923706, Val Loss: 70.4974924325943, loss : 2.0050199031829834\n",
            "epoch: 14, classification_loss: 1.7515050172805786, Val Loss: 70.49222409725189, loss : 1.9893871545791626\n",
            "epoch: 15, classification_loss: 1.7580938339233398, Val Loss: 70.50447225570679, loss : 2.0001566410064697\n",
            "epoch: 16, classification_loss: 1.7523585557937622, Val Loss: 70.49902606010437, loss : 1.9903621673583984\n",
            "epoch: 17, classification_loss: 1.7547001838684082, Val Loss: 70.4764803647995, loss : 1.996167540550232\n",
            "epoch: 18, classification_loss: 1.7543542385101318, Val Loss: 70.5055011510849, loss : 1.9896169900894165\n",
            "epoch: 19, classification_loss: 1.7627265453338623, Val Loss: 70.49162709712982, loss : 2.0013046264648438\n",
            "Batch: 89, Test Acc: 0.5747195512820513\n",
            "Batch: 90:\n",
            "epoch: 0, classification_loss: 1.8230204582214355, Val Loss: 70.4503413438797, loss : 1.8230204582214355\n",
            "epoch: 1, classification_loss: 1.7936620712280273, Val Loss: 70.52179610729218, loss : 2.116039276123047\n",
            "epoch: 2, classification_loss: 1.7692673206329346, Val Loss: 70.47721433639526, loss : 2.028475522994995\n",
            "epoch: 3, classification_loss: 1.7953075170516968, Val Loss: 70.46159076690674, loss : 2.0639588832855225\n",
            "epoch: 4, classification_loss: 1.80681574344635, Val Loss: 70.40793919563293, loss : 2.035501718521118\n",
            "epoch: 5, classification_loss: 1.798221468925476, Val Loss: 70.41926836967468, loss : 2.0669000148773193\n",
            "epoch: 6, classification_loss: 1.795092225074768, Val Loss: 70.4418592453003, loss : 2.0734915733337402\n",
            "epoch: 7, classification_loss: 1.8026025295257568, Val Loss: 70.48209023475647, loss : 2.049839735031128\n",
            "epoch: 8, classification_loss: 1.8013468980789185, Val Loss: 70.47080898284912, loss : 2.054689884185791\n",
            "epoch: 9, classification_loss: 1.8098337650299072, Val Loss: 70.43664586544037, loss : 2.04581880569458\n",
            "epoch: 10, classification_loss: 1.8105462789535522, Val Loss: 70.41952121257782, loss : 2.0623090267181396\n",
            "epoch: 11, classification_loss: 1.795362114906311, Val Loss: 70.43705415725708, loss : 2.0442347526550293\n",
            "epoch: 12, classification_loss: 1.8022538423538208, Val Loss: 70.4608017206192, loss : 2.042895793914795\n",
            "epoch: 13, classification_loss: 1.7898294925689697, Val Loss: 70.46097159385681, loss : 2.0289459228515625\n",
            "epoch: 14, classification_loss: 1.802436113357544, Val Loss: 70.4867000579834, loss : 2.040262222290039\n",
            "epoch: 15, classification_loss: 1.798391342163086, Val Loss: 70.46603333950043, loss : 2.0365657806396484\n",
            "epoch: 16, classification_loss: 1.7977609634399414, Val Loss: 70.45686542987823, loss : 2.029306173324585\n",
            "epoch: 17, classification_loss: 1.8037786483764648, Val Loss: 70.44985401630402, loss : 2.0401971340179443\n",
            "epoch: 18, classification_loss: 1.801010012626648, Val Loss: 70.4696717262268, loss : 2.0235955715179443\n",
            "epoch: 19, classification_loss: 1.7956504821777344, Val Loss: 70.54806649684906, loss : 2.038130521774292\n",
            "Batch: 90, Test Acc: 0.5744190705128205\n",
            "Batch: 91:\n",
            "epoch: 0, classification_loss: 1.7959016561508179, Val Loss: 70.57665205001831, loss : 1.7959016561508179\n",
            "epoch: 1, classification_loss: 1.7509093284606934, Val Loss: 70.5467141866684, loss : 2.070383071899414\n",
            "epoch: 2, classification_loss: 1.733302116394043, Val Loss: 70.44800126552582, loss : 1.9979368448257446\n",
            "epoch: 3, classification_loss: 1.7589255571365356, Val Loss: 70.43845784664154, loss : 2.018834114074707\n",
            "epoch: 4, classification_loss: 1.789952039718628, Val Loss: 70.44207692146301, loss : 2.019775629043579\n",
            "epoch: 5, classification_loss: 1.7729066610336304, Val Loss: 70.47090864181519, loss : 2.0246646404266357\n",
            "epoch: 6, classification_loss: 1.7676173448562622, Val Loss: 70.45995950698853, loss : 2.026435375213623\n",
            "epoch: 7, classification_loss: 1.7587441205978394, Val Loss: 70.46302783489227, loss : 2.0122251510620117\n",
            "epoch: 8, classification_loss: 1.763735294342041, Val Loss: 70.46106278896332, loss : 2.0046753883361816\n",
            "epoch: 9, classification_loss: 1.7704699039459229, Val Loss: 70.49749147891998, loss : 2.021087646484375\n",
            "epoch: 10, classification_loss: 1.7613096237182617, Val Loss: 70.4543788433075, loss : 2.0012292861938477\n",
            "epoch: 11, classification_loss: 1.7650961875915527, Val Loss: 70.47647678852081, loss : 2.005469799041748\n",
            "epoch: 12, classification_loss: 1.766925573348999, Val Loss: 70.46875846385956, loss : 2.0040180683135986\n",
            "epoch: 13, classification_loss: 1.7677611112594604, Val Loss: 70.45285737514496, loss : 2.0103015899658203\n",
            "epoch: 14, classification_loss: 1.761924147605896, Val Loss: 70.48325431346893, loss : 2.009455680847168\n",
            "epoch: 15, classification_loss: 1.7642139196395874, Val Loss: 70.52412354946136, loss : 2.00337815284729\n",
            "epoch: 16, classification_loss: 1.7619608640670776, Val Loss: 70.49476611614227, loss : 1.9964388608932495\n",
            "epoch: 17, classification_loss: 1.7738656997680664, Val Loss: 70.42418944835663, loss : 2.011322498321533\n",
            "epoch: 18, classification_loss: 1.7693263292312622, Val Loss: 70.46924722194672, loss : 2.0036256313323975\n",
            "epoch: 19, classification_loss: 1.7731047868728638, Val Loss: 70.61932122707367, loss : 2.0193066596984863\n",
            "Batch: 91, Test Acc: 0.5717147435897436\n",
            "Batch: 92:\n",
            "epoch: 0, classification_loss: 1.783247470855713, Val Loss: 70.59390962123871, loss : 1.783247470855713\n",
            "epoch: 1, classification_loss: 1.7401241064071655, Val Loss: 70.49839401245117, loss : 2.064622402191162\n",
            "epoch: 2, classification_loss: 1.7249104976654053, Val Loss: 70.48584711551666, loss : 1.9736111164093018\n",
            "epoch: 3, classification_loss: 1.7586086988449097, Val Loss: 70.4400247335434, loss : 2.032364845275879\n",
            "epoch: 4, classification_loss: 1.7838410139083862, Val Loss: 70.49547326564789, loss : 2.0157079696655273\n",
            "epoch: 5, classification_loss: 1.743500828742981, Val Loss: 70.49393832683563, loss : 2.01017689704895\n",
            "epoch: 6, classification_loss: 1.7390354871749878, Val Loss: 70.4746835231781, loss : 2.012726306915283\n",
            "epoch: 7, classification_loss: 1.7433940172195435, Val Loss: 70.47311913967133, loss : 2.000857353210449\n",
            "epoch: 8, classification_loss: 1.7496328353881836, Val Loss: 70.46261835098267, loss : 2.0080525875091553\n",
            "epoch: 9, classification_loss: 1.754784345626831, Val Loss: 70.47773718833923, loss : 1.9980612993240356\n",
            "epoch: 10, classification_loss: 1.7509078979492188, Val Loss: 70.5103497505188, loss : 1.9961901903152466\n",
            "epoch: 11, classification_loss: 1.767199993133545, Val Loss: 70.52715849876404, loss : 2.006044387817383\n",
            "epoch: 12, classification_loss: 1.7530101537704468, Val Loss: 70.46331107616425, loss : 1.989628553390503\n",
            "epoch: 13, classification_loss: 1.7591835260391235, Val Loss: 70.4999371767044, loss : 1.9965537786483765\n",
            "epoch: 14, classification_loss: 1.7563512325286865, Val Loss: 70.52460956573486, loss : 1.9836466312408447\n",
            "epoch: 15, classification_loss: 1.7501832246780396, Val Loss: 70.53022742271423, loss : 1.9812935590744019\n",
            "epoch: 16, classification_loss: 1.7470489740371704, Val Loss: 70.50468182563782, loss : 1.9737977981567383\n",
            "epoch: 17, classification_loss: 1.7490532398223877, Val Loss: 70.5057647228241, loss : 1.9828174114227295\n",
            "epoch: 18, classification_loss: 1.7575467824935913, Val Loss: 70.51446866989136, loss : 1.9775124788284302\n",
            "epoch: 19, classification_loss: 1.758771538734436, Val Loss: 70.57267189025879, loss : 1.9899139404296875\n",
            "Batch: 92, Test Acc: 0.5732171474358975\n",
            "Batch: 93:\n",
            "epoch: 0, classification_loss: 1.7948400974273682, Val Loss: 70.49182486534119, loss : 1.7948400974273682\n",
            "epoch: 1, classification_loss: 1.747506856918335, Val Loss: 70.48819780349731, loss : 2.074495315551758\n",
            "epoch: 2, classification_loss: 1.738973617553711, Val Loss: 70.45738530158997, loss : 1.986769199371338\n",
            "epoch: 3, classification_loss: 1.7676606178283691, Val Loss: 70.4447934627533, loss : 2.0298709869384766\n",
            "epoch: 4, classification_loss: 1.7943564653396606, Val Loss: 70.4810254573822, loss : 2.0154497623443604\n",
            "epoch: 5, classification_loss: 1.7751305103302002, Val Loss: 70.47093296051025, loss : 2.035264730453491\n",
            "epoch: 6, classification_loss: 1.7614363431930542, Val Loss: 70.50990927219391, loss : 2.0252976417541504\n",
            "epoch: 7, classification_loss: 1.7605504989624023, Val Loss: 70.48093104362488, loss : 2.016268253326416\n",
            "epoch: 8, classification_loss: 1.7473032474517822, Val Loss: 70.51912844181061, loss : 2.0090997219085693\n",
            "epoch: 9, classification_loss: 1.7717961072921753, Val Loss: 70.56213760375977, loss : 2.0117807388305664\n",
            "epoch: 10, classification_loss: 1.7462053298950195, Val Loss: 70.50784873962402, loss : 1.9929441213607788\n",
            "epoch: 11, classification_loss: 1.7671550512313843, Val Loss: 70.49658870697021, loss : 2.0066967010498047\n",
            "epoch: 12, classification_loss: 1.771843433380127, Val Loss: 70.53648030757904, loss : 2.0121097564697266\n",
            "epoch: 13, classification_loss: 1.7616429328918457, Val Loss: 70.54413652420044, loss : 1.998624324798584\n",
            "epoch: 14, classification_loss: 1.7734367847442627, Val Loss: 70.5086077451706, loss : 2.012716293334961\n",
            "epoch: 15, classification_loss: 1.7606191635131836, Val Loss: 70.52962982654572, loss : 1.9904416799545288\n",
            "epoch: 16, classification_loss: 1.7773511409759521, Val Loss: 70.5283831357956, loss : 2.011099338531494\n",
            "epoch: 17, classification_loss: 1.7710095643997192, Val Loss: 70.53988742828369, loss : 1.9986803531646729\n",
            "epoch: 18, classification_loss: 1.7813771963119507, Val Loss: 70.56088936328888, loss : 2.0085859298706055\n",
            "epoch: 19, classification_loss: 1.7788455486297607, Val Loss: 70.51458156108856, loss : 2.009610414505005\n",
            "Batch: 93, Test Acc: 0.57421875\n",
            "Batch: 94:\n",
            "epoch: 0, classification_loss: 1.8079242706298828, Val Loss: 70.5037168264389, loss : 1.8079242706298828\n",
            "epoch: 1, classification_loss: 1.7800590991973877, Val Loss: 70.47042226791382, loss : 2.1117372512817383\n",
            "epoch: 2, classification_loss: 1.7544467449188232, Val Loss: 70.46440291404724, loss : 2.009366989135742\n",
            "epoch: 3, classification_loss: 1.7837612628936768, Val Loss: 70.43803453445435, loss : 2.0528531074523926\n",
            "epoch: 4, classification_loss: 1.7971388101577759, Val Loss: 70.46713578701019, loss : 2.0332260131835938\n",
            "epoch: 5, classification_loss: 1.7921228408813477, Val Loss: 70.46937143802643, loss : 2.043992042541504\n",
            "epoch: 6, classification_loss: 1.7879486083984375, Val Loss: 70.48981940746307, loss : 2.0492658615112305\n",
            "epoch: 7, classification_loss: 1.7859734296798706, Val Loss: 70.52194726467133, loss : 2.036120891571045\n",
            "epoch: 8, classification_loss: 1.7883628606796265, Val Loss: 70.475799202919, loss : 2.0396835803985596\n",
            "epoch: 9, classification_loss: 1.7897887229919434, Val Loss: 70.52474117279053, loss : 2.027747392654419\n",
            "epoch: 10, classification_loss: 1.7887392044067383, Val Loss: 70.55867457389832, loss : 2.0341289043426514\n",
            "epoch: 11, classification_loss: 1.7976454496383667, Val Loss: 70.53940713405609, loss : 2.032163143157959\n",
            "epoch: 12, classification_loss: 1.785048246383667, Val Loss: 70.58038413524628, loss : 2.024388074874878\n",
            "epoch: 13, classification_loss: 1.8004357814788818, Val Loss: 70.51978015899658, loss : 2.0348753929138184\n",
            "epoch: 14, classification_loss: 1.7819451093673706, Val Loss: 70.52394580841064, loss : 2.015334367752075\n",
            "epoch: 15, classification_loss: 1.7919901609420776, Val Loss: 70.561683177948, loss : 2.021608591079712\n",
            "epoch: 16, classification_loss: 1.7834068536758423, Val Loss: 70.57266438007355, loss : 2.010392189025879\n",
            "epoch: 17, classification_loss: 1.7903069257736206, Val Loss: 70.54979050159454, loss : 2.0180859565734863\n",
            "epoch: 18, classification_loss: 1.7765520811080933, Val Loss: 70.53422486782074, loss : 1.9933006763458252\n",
            "epoch: 19, classification_loss: 1.7946573495864868, Val Loss: 70.56720423698425, loss : 2.0270895957946777\n",
            "Batch: 94, Test Acc: 0.5752203525641025\n",
            "Batch: 95:\n",
            "epoch: 0, classification_loss: 1.8381634950637817, Val Loss: 70.43358945846558, loss : 1.8381634950637817\n",
            "epoch: 1, classification_loss: 1.7963240146636963, Val Loss: 70.47068810462952, loss : 2.120952606201172\n",
            "epoch: 2, classification_loss: 1.7975720167160034, Val Loss: 70.47488749027252, loss : 2.0474400520324707\n",
            "epoch: 3, classification_loss: 1.8078441619873047, Val Loss: 70.47058820724487, loss : 2.07307505607605\n",
            "epoch: 4, classification_loss: 1.8299258947372437, Val Loss: 70.50165021419525, loss : 2.061506748199463\n",
            "epoch: 5, classification_loss: 1.8227087259292603, Val Loss: 70.47552800178528, loss : 2.082787036895752\n",
            "epoch: 6, classification_loss: 1.8116109371185303, Val Loss: 70.51809573173523, loss : 2.0758094787597656\n",
            "epoch: 7, classification_loss: 1.810217022895813, Val Loss: 70.51529705524445, loss : 2.073549509048462\n",
            "epoch: 8, classification_loss: 1.824718713760376, Val Loss: 70.5366997718811, loss : 2.077022075653076\n",
            "epoch: 9, classification_loss: 1.823021650314331, Val Loss: 70.47876417636871, loss : 2.0710182189941406\n",
            "epoch: 10, classification_loss: 1.8216098546981812, Val Loss: 70.49383413791656, loss : 2.0677430629730225\n",
            "epoch: 11, classification_loss: 1.8273770809173584, Val Loss: 70.53721988201141, loss : 2.0668559074401855\n",
            "epoch: 12, classification_loss: 1.8293427228927612, Val Loss: 70.57082092761993, loss : 2.064310073852539\n",
            "epoch: 13, classification_loss: 1.8151462078094482, Val Loss: 70.54959738254547, loss : 2.0564188957214355\n",
            "epoch: 14, classification_loss: 1.8109216690063477, Val Loss: 70.53766977787018, loss : 2.051222562789917\n",
            "epoch: 15, classification_loss: 1.823988676071167, Val Loss: 70.5565869808197, loss : 2.062563419342041\n",
            "epoch: 16, classification_loss: 1.8171709775924683, Val Loss: 70.56737637519836, loss : 2.0502724647521973\n",
            "epoch: 17, classification_loss: 1.821108102798462, Val Loss: 70.56851732730865, loss : 2.053834915161133\n",
            "epoch: 18, classification_loss: 1.8211311101913452, Val Loss: 70.5628113746643, loss : 2.0504093170166016\n",
            "epoch: 19, classification_loss: 1.8133714199066162, Val Loss: 70.53827345371246, loss : 2.050825834274292\n",
            "Batch: 95, Test Acc: 0.5755208333333334\n",
            "Batch: 96:\n",
            "epoch: 0, classification_loss: 1.8186748027801514, Val Loss: 70.46732425689697, loss : 1.8186748027801514\n",
            "epoch: 1, classification_loss: 1.7967543601989746, Val Loss: 70.57858085632324, loss : 2.1286916732788086\n",
            "epoch: 2, classification_loss: 1.7592071294784546, Val Loss: 70.52953124046326, loss : 2.0294206142425537\n",
            "epoch: 3, classification_loss: 1.7835583686828613, Val Loss: 70.4133528470993, loss : 2.062458038330078\n",
            "epoch: 4, classification_loss: 1.8160408735275269, Val Loss: 70.48533487319946, loss : 2.068023920059204\n",
            "epoch: 5, classification_loss: 1.8095881938934326, Val Loss: 70.42577075958252, loss : 2.074657917022705\n",
            "epoch: 6, classification_loss: 1.8045176267623901, Val Loss: 70.5072580575943, loss : 2.0750722885131836\n",
            "epoch: 7, classification_loss: 1.7951499223709106, Val Loss: 70.4353779554367, loss : 2.0545995235443115\n",
            "epoch: 8, classification_loss: 1.797062873840332, Val Loss: 70.40314316749573, loss : 2.060410261154175\n",
            "epoch: 9, classification_loss: 1.802700400352478, Val Loss: 70.42485475540161, loss : 2.051640033721924\n",
            "epoch: 10, classification_loss: 1.794332504272461, Val Loss: 70.43134903907776, loss : 2.0457539558410645\n",
            "epoch: 11, classification_loss: 1.7960288524627686, Val Loss: 70.4338618516922, loss : 2.0362753868103027\n",
            "epoch: 12, classification_loss: 1.809815526008606, Val Loss: 70.45038175582886, loss : 2.051478385925293\n",
            "epoch: 13, classification_loss: 1.8003754615783691, Val Loss: 70.4416151046753, loss : 2.0454275608062744\n",
            "epoch: 14, classification_loss: 1.7935044765472412, Val Loss: 70.42925941944122, loss : 2.0417330265045166\n",
            "epoch: 15, classification_loss: 1.792075514793396, Val Loss: 70.42769491672516, loss : 2.040332078933716\n",
            "epoch: 16, classification_loss: 1.7894558906555176, Val Loss: 70.48398983478546, loss : 2.0328831672668457\n",
            "epoch: 17, classification_loss: 1.8127505779266357, Val Loss: 70.52942633628845, loss : 2.051086902618408\n",
            "epoch: 18, classification_loss: 1.801577091217041, Val Loss: 70.43061578273773, loss : 2.0369691848754883\n",
            "epoch: 19, classification_loss: 1.7945667505264282, Val Loss: 70.41792523860931, loss : 2.0323846340179443\n",
            "Batch: 96, Test Acc: 0.577323717948718\n",
            "Batch: 97:\n",
            "epoch: 0, classification_loss: 1.8305062055587769, Val Loss: 70.53560590744019, loss : 1.8305062055587769\n",
            "epoch: 1, classification_loss: 1.785852313041687, Val Loss: 70.55413365364075, loss : 2.1171858310699463\n",
            "epoch: 2, classification_loss: 1.7778500318527222, Val Loss: 70.40455031394958, loss : 2.0583159923553467\n",
            "epoch: 3, classification_loss: 1.7966108322143555, Val Loss: 70.41786062717438, loss : 2.0737571716308594\n",
            "epoch: 4, classification_loss: 1.8263617753982544, Val Loss: 70.44418597221375, loss : 2.065690040588379\n",
            "epoch: 5, classification_loss: 1.8123633861541748, Val Loss: 70.45646488666534, loss : 2.0744121074676514\n",
            "epoch: 6, classification_loss: 1.8032697439193726, Val Loss: 70.44149816036224, loss : 2.0682718753814697\n",
            "epoch: 7, classification_loss: 1.7956980466842651, Val Loss: 70.46995723247528, loss : 2.0601494312286377\n",
            "epoch: 8, classification_loss: 1.79816472530365, Val Loss: 70.44155550003052, loss : 2.0644748210906982\n",
            "epoch: 9, classification_loss: 1.7985631227493286, Val Loss: 70.46828019618988, loss : 2.0575714111328125\n",
            "epoch: 10, classification_loss: 1.8073465824127197, Val Loss: 70.45787715911865, loss : 2.065459966659546\n",
            "epoch: 11, classification_loss: 1.8128339052200317, Val Loss: 70.47387087345123, loss : 2.057983875274658\n",
            "epoch: 12, classification_loss: 1.8133255243301392, Val Loss: 70.44744777679443, loss : 2.05481219291687\n",
            "epoch: 13, classification_loss: 1.8145794868469238, Val Loss: 70.41942238807678, loss : 2.0601401329040527\n",
            "epoch: 14, classification_loss: 1.8047348260879517, Val Loss: 70.40245449542999, loss : 2.0515482425689697\n",
            "epoch: 15, classification_loss: 1.7953355312347412, Val Loss: 70.45767998695374, loss : 2.0380451679229736\n",
            "epoch: 16, classification_loss: 1.8048378229141235, Val Loss: 70.48112761974335, loss : 2.046203374862671\n",
            "epoch: 17, classification_loss: 1.8038588762283325, Val Loss: 70.42733025550842, loss : 2.042116165161133\n",
            "epoch: 18, classification_loss: 1.8050230741500854, Val Loss: 70.42178022861481, loss : 2.0452866554260254\n",
            "epoch: 19, classification_loss: 1.8011317253112793, Val Loss: 70.49364233016968, loss : 2.0394067764282227\n",
            "Batch: 97, Test Acc: 0.5788261217948718\n",
            "Batch: 98:\n",
            "epoch: 0, classification_loss: 1.8486427068710327, Val Loss: 70.43554472923279, loss : 1.8486427068710327\n",
            "epoch: 1, classification_loss: 1.8045333623886108, Val Loss: 70.46622145175934, loss : 2.134129762649536\n",
            "epoch: 2, classification_loss: 1.7902101278305054, Val Loss: 70.46553862094879, loss : 2.0581531524658203\n",
            "epoch: 3, classification_loss: 1.812603235244751, Val Loss: 70.42144119739532, loss : 2.094435930252075\n",
            "epoch: 4, classification_loss: 1.844704031944275, Val Loss: 70.45673036575317, loss : 2.0939645767211914\n",
            "epoch: 5, classification_loss: 1.8356263637542725, Val Loss: 70.45892298221588, loss : 2.0946762561798096\n",
            "epoch: 6, classification_loss: 1.8200187683105469, Val Loss: 70.43895924091339, loss : 2.098797559738159\n",
            "epoch: 7, classification_loss: 1.8215759992599487, Val Loss: 70.44475841522217, loss : 2.0757529735565186\n",
            "epoch: 8, classification_loss: 1.8408492803573608, Val Loss: 70.4558914899826, loss : 2.0985054969787598\n",
            "epoch: 9, classification_loss: 1.8235739469528198, Val Loss: 70.47419822216034, loss : 2.068373203277588\n",
            "epoch: 10, classification_loss: 1.8329354524612427, Val Loss: 70.48946857452393, loss : 2.080007314682007\n",
            "epoch: 11, classification_loss: 1.8269788026809692, Val Loss: 70.4446474313736, loss : 2.068284511566162\n",
            "epoch: 12, classification_loss: 1.8283137083053589, Val Loss: 70.48497438430786, loss : 2.0650556087493896\n",
            "epoch: 13, classification_loss: 1.8189823627471924, Val Loss: 70.51507759094238, loss : 2.0590124130249023\n",
            "epoch: 14, classification_loss: 1.8198519945144653, Val Loss: 70.46345734596252, loss : 2.0613691806793213\n",
            "epoch: 15, classification_loss: 1.8065061569213867, Val Loss: 70.49333798885345, loss : 2.045815944671631\n",
            "epoch: 16, classification_loss: 1.8201494216918945, Val Loss: 70.50091516971588, loss : 2.0499825477600098\n",
            "epoch: 17, classification_loss: 1.82965886592865, Val Loss: 70.50085306167603, loss : 2.06658935546875\n",
            "epoch: 18, classification_loss: 1.8351858854293823, Val Loss: 70.48774671554565, loss : 2.059297561645508\n",
            "epoch: 19, classification_loss: 1.8297441005706787, Val Loss: 70.47631120681763, loss : 2.0685017108917236\n",
            "Batch: 98, Test Acc: 0.5764222756410257\n",
            "Batch: 99:\n",
            "epoch: 0, classification_loss: 1.8162384033203125, Val Loss: 70.57080733776093, loss : 1.8162384033203125\n",
            "epoch: 1, classification_loss: 1.7678302526474, Val Loss: 70.47860193252563, loss : 2.1001875400543213\n",
            "epoch: 2, classification_loss: 1.7446260452270508, Val Loss: 70.44510233402252, loss : 2.01579213142395\n",
            "epoch: 3, classification_loss: 1.7787398099899292, Val Loss: 70.46355211734772, loss : 2.048819065093994\n",
            "epoch: 4, classification_loss: 1.7971659898757935, Val Loss: 70.47075939178467, loss : 2.0364890098571777\n",
            "epoch: 5, classification_loss: 1.7914068698883057, Val Loss: 70.45600354671478, loss : 2.0542848110198975\n",
            "epoch: 6, classification_loss: 1.7741669416427612, Val Loss: 70.45234215259552, loss : 2.0432205200195312\n",
            "epoch: 7, classification_loss: 1.7763043642044067, Val Loss: 70.48112154006958, loss : 2.040990114212036\n",
            "epoch: 8, classification_loss: 1.7926613092422485, Val Loss: 70.47087943553925, loss : 2.0482120513916016\n",
            "epoch: 9, classification_loss: 1.7830719947814941, Val Loss: 70.46804237365723, loss : 2.0374059677124023\n",
            "epoch: 10, classification_loss: 1.78325617313385, Val Loss: 70.49806809425354, loss : 2.031043767929077\n",
            "epoch: 11, classification_loss: 1.790007472038269, Val Loss: 70.50500404834747, loss : 2.032392740249634\n",
            "epoch: 12, classification_loss: 1.7840871810913086, Val Loss: 70.53542566299438, loss : 2.0305354595184326\n",
            "epoch: 13, classification_loss: 1.7853145599365234, Val Loss: 70.48945534229279, loss : 2.035897731781006\n",
            "epoch: 14, classification_loss: 1.7735520601272583, Val Loss: 70.48415875434875, loss : 2.0269882678985596\n",
            "epoch: 15, classification_loss: 1.7846581935882568, Val Loss: 70.50893902778625, loss : 2.029001474380493\n",
            "epoch: 16, classification_loss: 1.7849406003952026, Val Loss: 70.5506706237793, loss : 2.027967691421509\n",
            "epoch: 17, classification_loss: 1.7782888412475586, Val Loss: 70.52803707122803, loss : 2.0209693908691406\n",
            "epoch: 18, classification_loss: 1.7905558347702026, Val Loss: 70.55298459529877, loss : 2.029808521270752\n",
            "epoch: 19, classification_loss: 1.779308557510376, Val Loss: 70.51610696315765, loss : 2.024460792541504\n",
            "Batch: 99, Test Acc: 0.5760216346153846\n",
            "Batch: 100:\n",
            "epoch: 0, classification_loss: 1.852648138999939, Val Loss: 70.44503498077393, loss : 1.852648138999939\n",
            "epoch: 1, classification_loss: 1.8179619312286377, Val Loss: 70.46503520011902, loss : 2.148138999938965\n",
            "epoch: 2, classification_loss: 1.79899001121521, Val Loss: 70.42945873737335, loss : 2.053196668624878\n",
            "epoch: 3, classification_loss: 1.8342845439910889, Val Loss: 70.44301521778107, loss : 2.1028666496276855\n",
            "epoch: 4, classification_loss: 1.8460907936096191, Val Loss: 70.43313658237457, loss : 2.0769755840301514\n",
            "epoch: 5, classification_loss: 1.835087537765503, Val Loss: 70.4011549949646, loss : 2.0936667919158936\n",
            "epoch: 6, classification_loss: 1.8193049430847168, Val Loss: 70.4156277179718, loss : 2.080338716506958\n",
            "epoch: 7, classification_loss: 1.825823187828064, Val Loss: 70.45073413848877, loss : 2.073460102081299\n",
            "epoch: 8, classification_loss: 1.8240002393722534, Val Loss: 70.43386578559875, loss : 2.0752851963043213\n",
            "epoch: 9, classification_loss: 1.8252493143081665, Val Loss: 70.44080853462219, loss : 2.061018705368042\n",
            "epoch: 10, classification_loss: 1.8336832523345947, Val Loss: 70.46125113964081, loss : 2.0741400718688965\n",
            "epoch: 11, classification_loss: 1.8371294736862183, Val Loss: 70.46298122406006, loss : 2.0641651153564453\n",
            "epoch: 12, classification_loss: 1.8435969352722168, Val Loss: 70.42718803882599, loss : 2.0794594287872314\n",
            "epoch: 13, classification_loss: 1.8261873722076416, Val Loss: 70.43417418003082, loss : 2.060297966003418\n",
            "epoch: 14, classification_loss: 1.8390191793441772, Val Loss: 70.4630401134491, loss : 2.0712413787841797\n",
            "epoch: 15, classification_loss: 1.832883358001709, Val Loss: 70.52460253238678, loss : 2.0600852966308594\n",
            "epoch: 16, classification_loss: 1.836194634437561, Val Loss: 70.47139239311218, loss : 2.0606462955474854\n",
            "epoch: 17, classification_loss: 1.8322516679763794, Val Loss: 70.46563303470612, loss : 2.0600757598876953\n",
            "epoch: 18, classification_loss: 1.8230572938919067, Val Loss: 70.45926177501678, loss : 2.0457351207733154\n",
            "epoch: 19, classification_loss: 1.8319100141525269, Val Loss: 70.49620699882507, loss : 2.062490701675415\n",
            "Batch: 100, Test Acc: 0.5766225961538461\n",
            "Batch: 101:\n",
            "epoch: 0, classification_loss: 1.813081979751587, Val Loss: 70.3842784166336, loss : 1.813081979751587\n",
            "epoch: 1, classification_loss: 1.774186134338379, Val Loss: 70.57850432395935, loss : 1.774186134338379\n",
            "epoch: 2, classification_loss: 1.7332985401153564, Val Loss: 70.54599833488464, loss : 2.0827863216400146\n",
            "epoch: 3, classification_loss: 1.7114806175231934, Val Loss: 70.43733143806458, loss : 2.0157713890075684\n",
            "epoch: 4, classification_loss: 1.7264423370361328, Val Loss: 70.42017209529877, loss : 1.9966633319854736\n",
            "epoch: 5, classification_loss: 1.7837235927581787, Val Loss: 70.40043580532074, loss : 2.0258688926696777\n",
            "epoch: 6, classification_loss: 1.7513935565948486, Val Loss: 70.39461553096771, loss : 2.0081937313079834\n",
            "epoch: 7, classification_loss: 1.7373671531677246, Val Loss: 70.40561735630035, loss : 2.0144622325897217\n",
            "epoch: 8, classification_loss: 1.729608178138733, Val Loss: 70.42672455310822, loss : 2.008685827255249\n",
            "epoch: 9, classification_loss: 1.7532159090042114, Val Loss: 70.39488649368286, loss : 2.0141024589538574\n",
            "epoch: 10, classification_loss: 1.757949948310852, Val Loss: 70.38849544525146, loss : 2.0125763416290283\n",
            "epoch: 11, classification_loss: 1.7578014135360718, Val Loss: 70.45199477672577, loss : 2.0119357109069824\n",
            "epoch: 12, classification_loss: 1.7536436319351196, Val Loss: 70.45487856864929, loss : 2.0007612705230713\n",
            "epoch: 13, classification_loss: 1.7409660816192627, Val Loss: 70.4162015914917, loss : 1.9880516529083252\n",
            "epoch: 14, classification_loss: 1.7562294006347656, Val Loss: 70.47638130187988, loss : 2.0015065670013428\n",
            "epoch: 15, classification_loss: 1.7516928911209106, Val Loss: 70.45780098438263, loss : 1.9905204772949219\n",
            "epoch: 16, classification_loss: 1.744468092918396, Val Loss: 70.4394862651825, loss : 1.985622763633728\n",
            "epoch: 17, classification_loss: 1.7582536935806274, Val Loss: 70.4565657377243, loss : 1.9882172346115112\n",
            "epoch: 18, classification_loss: 1.7542006969451904, Val Loss: 70.46700167655945, loss : 1.993726372718811\n",
            "epoch: 19, classification_loss: 1.7619317770004272, Val Loss: 70.4899274110794, loss : 1.9858051538467407\n",
            "Batch: 101, Test Acc: 0.5771233974358975\n",
            "Batch: 102:\n",
            "epoch: 0, classification_loss: 1.8403630256652832, Val Loss: 70.46635103225708, loss : 1.8403630256652832\n",
            "epoch: 1, classification_loss: 1.8023755550384521, Val Loss: 70.48204243183136, loss : 2.1538684368133545\n",
            "epoch: 2, classification_loss: 1.7765098810195923, Val Loss: 70.47314989566803, loss : 2.0507724285125732\n",
            "epoch: 3, classification_loss: 1.7892446517944336, Val Loss: 70.41588759422302, loss : 2.074558734893799\n",
            "epoch: 4, classification_loss: 1.835269808769226, Val Loss: 70.4145758152008, loss : 2.1032321453094482\n",
            "epoch: 5, classification_loss: 1.812206745147705, Val Loss: 70.40619027614594, loss : 2.0638678073883057\n",
            "epoch: 6, classification_loss: 1.803826093673706, Val Loss: 70.44031572341919, loss : 2.079693078994751\n",
            "epoch: 7, classification_loss: 1.8080427646636963, Val Loss: 70.44353187084198, loss : 2.0678162574768066\n",
            "epoch: 8, classification_loss: 1.8041558265686035, Val Loss: 70.45877397060394, loss : 2.0783863067626953\n",
            "epoch: 9, classification_loss: 1.8160984516143799, Val Loss: 70.43360459804535, loss : 2.073838472366333\n",
            "epoch: 10, classification_loss: 1.8115521669387817, Val Loss: 70.42526292800903, loss : 2.070394515991211\n",
            "epoch: 11, classification_loss: 1.8081835508346558, Val Loss: 70.44046640396118, loss : 2.0635719299316406\n",
            "epoch: 12, classification_loss: 1.8017317056655884, Val Loss: 70.52505171298981, loss : 2.058499574661255\n",
            "epoch: 13, classification_loss: 1.814799189567566, Val Loss: 70.52835261821747, loss : 2.062563180923462\n",
            "epoch: 14, classification_loss: 1.8147144317626953, Val Loss: 70.41738164424896, loss : 2.0592243671417236\n",
            "epoch: 15, classification_loss: 1.8093976974487305, Val Loss: 70.43154311180115, loss : 2.057572364807129\n",
            "epoch: 16, classification_loss: 1.8096235990524292, Val Loss: 70.48429524898529, loss : 2.0548036098480225\n",
            "epoch: 17, classification_loss: 1.80785071849823, Val Loss: 70.46627020835876, loss : 2.0539824962615967\n",
            "epoch: 18, classification_loss: 1.8103708028793335, Val Loss: 70.52582430839539, loss : 2.0411221981048584\n",
            "epoch: 19, classification_loss: 1.8191994428634644, Val Loss: 70.49064183235168, loss : 2.0590178966522217\n",
            "Batch: 102, Test Acc: 0.5737179487179487\n",
            "Batch: 103:\n",
            "epoch: 0, classification_loss: 1.8277338743209839, Val Loss: 70.48327827453613, loss : 1.8277338743209839\n",
            "epoch: 1, classification_loss: 1.789772629737854, Val Loss: 70.4762088060379, loss : 2.131521463394165\n",
            "epoch: 2, classification_loss: 1.7745743989944458, Val Loss: 70.42091071605682, loss : 2.0404717922210693\n",
            "epoch: 3, classification_loss: 1.8038182258605957, Val Loss: 70.44609940052032, loss : 2.08969783782959\n",
            "epoch: 4, classification_loss: 1.8227920532226562, Val Loss: 70.44317138195038, loss : 2.0632731914520264\n",
            "epoch: 5, classification_loss: 1.815869688987732, Val Loss: 70.42384457588196, loss : 2.0871593952178955\n",
            "epoch: 6, classification_loss: 1.7879345417022705, Val Loss: 70.41288352012634, loss : 2.0763027667999268\n",
            "epoch: 7, classification_loss: 1.7966479063034058, Val Loss: 70.50008404254913, loss : 2.0614774227142334\n",
            "epoch: 8, classification_loss: 1.8079853057861328, Val Loss: 70.45519375801086, loss : 2.0661163330078125\n",
            "epoch: 9, classification_loss: 1.812255620956421, Val Loss: 70.39550840854645, loss : 2.0688745975494385\n",
            "epoch: 10, classification_loss: 1.8047621250152588, Val Loss: 70.43289244174957, loss : 2.0573408603668213\n",
            "epoch: 11, classification_loss: 1.8004735708236694, Val Loss: 70.4959248304367, loss : 2.0545313358306885\n",
            "epoch: 12, classification_loss: 1.8083207607269287, Val Loss: 70.43035125732422, loss : 2.046278715133667\n",
            "epoch: 13, classification_loss: 1.8103883266448975, Val Loss: 70.43071258068085, loss : 2.063323497772217\n",
            "epoch: 14, classification_loss: 1.800062894821167, Val Loss: 70.5020409822464, loss : 2.0437488555908203\n",
            "epoch: 15, classification_loss: 1.8090039491653442, Val Loss: 70.47499716281891, loss : 2.0667836666107178\n",
            "epoch: 16, classification_loss: 1.8053462505340576, Val Loss: 70.45305073261261, loss : 2.050626516342163\n",
            "epoch: 17, classification_loss: 1.8038486242294312, Val Loss: 70.49007296562195, loss : 2.0509634017944336\n",
            "epoch: 18, classification_loss: 1.8229047060012817, Val Loss: 70.44372415542603, loss : 2.053910732269287\n",
            "epoch: 19, classification_loss: 1.8177787065505981, Val Loss: 70.51091063022614, loss : 2.0610530376434326\n",
            "Batch: 103, Test Acc: 0.5754206730769231\n",
            "Batch: 104:\n",
            "epoch: 0, classification_loss: 1.7797759771347046, Val Loss: 70.4769879579544, loss : 1.7797759771347046\n",
            "epoch: 1, classification_loss: 1.7472853660583496, Val Loss: 70.41305553913116, loss : 2.0704092979431152\n",
            "epoch: 2, classification_loss: 1.7351248264312744, Val Loss: 70.46540939807892, loss : 1.977068305015564\n",
            "epoch: 3, classification_loss: 1.7679060697555542, Val Loss: 70.49378263950348, loss : 2.0409698486328125\n",
            "epoch: 4, classification_loss: 1.7730238437652588, Val Loss: 70.41537475585938, loss : 2.0029966831207275\n",
            "epoch: 5, classification_loss: 1.7714442014694214, Val Loss: 70.42359149456024, loss : 2.0375542640686035\n",
            "epoch: 6, classification_loss: 1.7588380575180054, Val Loss: 70.4541140794754, loss : 2.0376274585723877\n",
            "epoch: 7, classification_loss: 1.7559125423431396, Val Loss: 70.53171229362488, loss : 2.0188357830047607\n",
            "epoch: 8, classification_loss: 1.769464135169983, Val Loss: 70.4627548456192, loss : 2.025988817214966\n",
            "epoch: 9, classification_loss: 1.7628626823425293, Val Loss: 70.43601906299591, loss : 2.009803295135498\n",
            "epoch: 10, classification_loss: 1.7655504941940308, Val Loss: 70.44851243495941, loss : 2.00636625289917\n",
            "epoch: 11, classification_loss: 1.7559618949890137, Val Loss: 70.45779860019684, loss : 2.0037906169891357\n",
            "epoch: 12, classification_loss: 1.7618637084960938, Val Loss: 70.51334249973297, loss : 2.0005874633789062\n",
            "epoch: 13, classification_loss: 1.7597081661224365, Val Loss: 70.57534801959991, loss : 2.0074892044067383\n",
            "epoch: 14, classification_loss: 1.768653392791748, Val Loss: 70.4820865392685, loss : 2.0054352283477783\n",
            "epoch: 15, classification_loss: 1.7730553150177002, Val Loss: 70.42284762859344, loss : 2.012035608291626\n",
            "epoch: 16, classification_loss: 1.7616941928863525, Val Loss: 70.4960069656372, loss : 1.9977378845214844\n",
            "epoch: 17, classification_loss: 1.7567451000213623, Val Loss: 70.55128955841064, loss : 1.9940598011016846\n",
            "epoch: 18, classification_loss: 1.765298843383789, Val Loss: 70.48304438591003, loss : 1.998373031616211\n",
            "epoch: 19, classification_loss: 1.7660589218139648, Val Loss: 70.46498513221741, loss : 1.9961873292922974\n",
            "Batch: 104, Test Acc: 0.5727163461538461\n",
            "Batch: 105:\n",
            "epoch: 0, classification_loss: 1.8607268333435059, Val Loss: 70.39223575592041, loss : 1.8607268333435059\n",
            "epoch: 1, classification_loss: 1.830059289932251, Val Loss: 70.46603977680206, loss : 2.1508302688598633\n",
            "epoch: 2, classification_loss: 1.794711947441101, Val Loss: 70.4524337053299, loss : 2.066166639328003\n",
            "epoch: 3, classification_loss: 1.824620246887207, Val Loss: 70.42749559879303, loss : 2.1031689643859863\n",
            "epoch: 4, classification_loss: 1.8569356203079224, Val Loss: 70.3889491558075, loss : 2.1017494201660156\n",
            "epoch: 5, classification_loss: 1.8434666395187378, Val Loss: 70.41129398345947, loss : 2.0973386764526367\n",
            "epoch: 6, classification_loss: 1.8344136476516724, Val Loss: 70.40496170520782, loss : 2.0976665019989014\n",
            "epoch: 7, classification_loss: 1.8272467851638794, Val Loss: 70.48209571838379, loss : 2.083350896835327\n",
            "epoch: 8, classification_loss: 1.8241276741027832, Val Loss: 70.47645938396454, loss : 2.0887324810028076\n",
            "epoch: 9, classification_loss: 1.8274821043014526, Val Loss: 70.43743681907654, loss : 2.0804693698883057\n",
            "epoch: 10, classification_loss: 1.8393182754516602, Val Loss: 70.4142529964447, loss : 2.090571403503418\n",
            "epoch: 11, classification_loss: 1.8318829536437988, Val Loss: 70.46574282646179, loss : 2.073364734649658\n",
            "epoch: 12, classification_loss: 1.8414607048034668, Val Loss: 70.46641254425049, loss : 2.0865185260772705\n",
            "epoch: 13, classification_loss: 1.8336429595947266, Val Loss: 70.42400419712067, loss : 2.0764639377593994\n",
            "epoch: 14, classification_loss: 1.8392152786254883, Val Loss: 70.42579102516174, loss : 2.0816588401794434\n",
            "epoch: 15, classification_loss: 1.8341927528381348, Val Loss: 70.42789733409882, loss : 2.0775976181030273\n",
            "epoch: 16, classification_loss: 1.8400322198867798, Val Loss: 70.42351365089417, loss : 2.0853219032287598\n",
            "epoch: 17, classification_loss: 1.820769190788269, Val Loss: 70.47794735431671, loss : 2.0608229637145996\n",
            "epoch: 18, classification_loss: 1.8331867456436157, Val Loss: 70.48254835605621, loss : 2.0744972229003906\n",
            "epoch: 19, classification_loss: 1.835364818572998, Val Loss: 70.45388603210449, loss : 2.0688724517822266\n",
            "Batch: 105, Test Acc: 0.5759214743589743\n",
            "Batch: 106:\n",
            "epoch: 0, classification_loss: 1.807357668876648, Val Loss: 70.38897180557251, loss : 1.807357668876648\n",
            "epoch: 1, classification_loss: 1.7766941785812378, Val Loss: 70.41975975036621, loss : 2.1080193519592285\n",
            "epoch: 2, classification_loss: 1.7613428831100464, Val Loss: 70.44970834255219, loss : 2.014535903930664\n",
            "epoch: 3, classification_loss: 1.7931506633758545, Val Loss: 70.43003928661346, loss : 2.0713894367218018\n",
            "epoch: 4, classification_loss: 1.816664218902588, Val Loss: 70.38729918003082, loss : 2.0575106143951416\n",
            "epoch: 5, classification_loss: 1.8043649196624756, Val Loss: 70.39751923084259, loss : 2.0709803104400635\n",
            "epoch: 6, classification_loss: 1.7857838869094849, Val Loss: 70.43302404880524, loss : 2.06677508354187\n",
            "epoch: 7, classification_loss: 1.781163215637207, Val Loss: 70.44069314002991, loss : 2.034227132797241\n",
            "epoch: 8, classification_loss: 1.7918344736099243, Val Loss: 70.45665371417999, loss : 2.0469369888305664\n",
            "epoch: 9, classification_loss: 1.7952147722244263, Val Loss: 70.41854858398438, loss : 2.0434038639068604\n",
            "epoch: 10, classification_loss: 1.7957837581634521, Val Loss: 70.43884301185608, loss : 2.0449438095092773\n",
            "epoch: 11, classification_loss: 1.7844972610473633, Val Loss: 70.47340631484985, loss : 2.0277726650238037\n",
            "epoch: 12, classification_loss: 1.801210641860962, Val Loss: 70.44704687595367, loss : 2.032731771469116\n",
            "epoch: 13, classification_loss: 1.7919386625289917, Val Loss: 70.43562805652618, loss : 2.030087947845459\n",
            "epoch: 14, classification_loss: 1.7925664186477661, Val Loss: 70.46990931034088, loss : 2.025498867034912\n",
            "epoch: 15, classification_loss: 1.787783145904541, Val Loss: 70.48866271972656, loss : 2.0222227573394775\n",
            "epoch: 16, classification_loss: 1.7967793941497803, Val Loss: 70.4942137002945, loss : 2.0221548080444336\n",
            "epoch: 17, classification_loss: 1.7940102815628052, Val Loss: 70.45304024219513, loss : 2.0290346145629883\n",
            "epoch: 18, classification_loss: 1.7865145206451416, Val Loss: 70.49118709564209, loss : 2.010448455810547\n",
            "epoch: 19, classification_loss: 1.7934943437576294, Val Loss: 70.50018346309662, loss : 2.0231142044067383\n",
            "Batch: 106, Test Acc: 0.575020032051282\n",
            "Batch: 107:\n",
            "epoch: 0, classification_loss: 1.833487629890442, Val Loss: 70.41032326221466, loss : 1.833487629890442\n",
            "epoch: 1, classification_loss: 1.7821213006973267, Val Loss: 70.48345625400543, loss : 2.1093058586120605\n",
            "epoch: 2, classification_loss: 1.7742704153060913, Val Loss: 70.44460034370422, loss : 2.0302906036376953\n",
            "epoch: 3, classification_loss: 1.7951810359954834, Val Loss: 70.41863179206848, loss : 2.0590808391571045\n",
            "epoch: 4, classification_loss: 1.8283225297927856, Val Loss: 70.3771380186081, loss : 2.064936399459839\n",
            "epoch: 5, classification_loss: 1.812152624130249, Val Loss: 70.45474588871002, loss : 1.812152624130249\n",
            "epoch: 6, classification_loss: 1.7944507598876953, Val Loss: 70.45746195316315, loss : 2.181379556655884\n",
            "epoch: 7, classification_loss: 1.754483938217163, Val Loss: 70.41032087802887, loss : 2.064260959625244\n",
            "epoch: 8, classification_loss: 1.7820574045181274, Val Loss: 70.48017680644989, loss : 2.048600196838379\n",
            "epoch: 9, classification_loss: 1.825478434562683, Val Loss: 70.42974841594696, loss : 2.120532751083374\n",
            "epoch: 10, classification_loss: 1.8015302419662476, Val Loss: 70.48572170734406, loss : 2.051339626312256\n",
            "epoch: 11, classification_loss: 1.783408522605896, Val Loss: 70.52390253543854, loss : 2.0970685482025146\n",
            "epoch: 12, classification_loss: 1.7794828414916992, Val Loss: 70.42288184165955, loss : 2.082235813140869\n",
            "epoch: 13, classification_loss: 1.8003298044204712, Val Loss: 70.46414601802826, loss : 2.0771663188934326\n",
            "epoch: 14, classification_loss: 1.802719235420227, Val Loss: 70.45152497291565, loss : 2.069911479949951\n",
            "epoch: 15, classification_loss: 1.7941752672195435, Val Loss: 70.44904220104218, loss : 2.075603485107422\n",
            "epoch: 16, classification_loss: 1.7938052415847778, Val Loss: 70.4239262342453, loss : 2.063502550125122\n",
            "epoch: 17, classification_loss: 1.7864203453063965, Val Loss: 70.45641243457794, loss : 2.0613107681274414\n",
            "epoch: 18, classification_loss: 1.7918989658355713, Val Loss: 70.45100426673889, loss : 2.0475454330444336\n",
            "epoch: 19, classification_loss: 1.7887601852416992, Val Loss: 70.44809830188751, loss : 2.0437045097351074\n",
            "Batch: 107, Test Acc: 0.5753205128205128\n",
            "Batch: 108:\n",
            "epoch: 0, classification_loss: 1.8111944198608398, Val Loss: 70.42624568939209, loss : 1.8111944198608398\n",
            "epoch: 1, classification_loss: 1.7788394689559937, Val Loss: 70.45420181751251, loss : 2.1219265460968018\n",
            "epoch: 2, classification_loss: 1.7659417390823364, Val Loss: 70.41602206230164, loss : 2.0493736267089844\n",
            "epoch: 3, classification_loss: 1.775817632675171, Val Loss: 70.44059681892395, loss : 2.0632729530334473\n",
            "epoch: 4, classification_loss: 1.8064405918121338, Val Loss: 70.44256925582886, loss : 2.0640926361083984\n",
            "epoch: 5, classification_loss: 1.7945067882537842, Val Loss: 70.47796380519867, loss : 2.0507588386535645\n",
            "epoch: 6, classification_loss: 1.787177324295044, Val Loss: 70.44315671920776, loss : 2.062873363494873\n",
            "epoch: 7, classification_loss: 1.786598563194275, Val Loss: 70.45856082439423, loss : 2.0572171211242676\n",
            "epoch: 8, classification_loss: 1.7759504318237305, Val Loss: 70.43485045433044, loss : 2.034409523010254\n",
            "epoch: 9, classification_loss: 1.782142162322998, Val Loss: 70.41411292552948, loss : 2.040203332901001\n",
            "epoch: 10, classification_loss: 1.7948085069656372, Val Loss: 70.47030103206635, loss : 2.052914619445801\n",
            "epoch: 11, classification_loss: 1.7894940376281738, Val Loss: 70.50147366523743, loss : 2.04526686668396\n",
            "epoch: 12, classification_loss: 1.783825159072876, Val Loss: 70.45383942127228, loss : 2.040668249130249\n",
            "epoch: 13, classification_loss: 1.791222095489502, Val Loss: 70.48586356639862, loss : 2.0441904067993164\n",
            "epoch: 14, classification_loss: 1.7796931266784668, Val Loss: 70.48792624473572, loss : 2.0237619876861572\n",
            "epoch: 15, classification_loss: 1.7933616638183594, Val Loss: 70.47089636325836, loss : 2.0466694831848145\n",
            "epoch: 16, classification_loss: 1.789039969444275, Val Loss: 70.43618512153625, loss : 2.0400798320770264\n",
            "epoch: 17, classification_loss: 1.7846603393554688, Val Loss: 70.52990186214447, loss : 2.0429039001464844\n",
            "epoch: 18, classification_loss: 1.7873082160949707, Val Loss: 70.46104598045349, loss : 2.039499521255493\n",
            "epoch: 19, classification_loss: 1.7971341609954834, Val Loss: 70.45523250102997, loss : 2.0457639694213867\n",
            "Batch: 108, Test Acc: 0.5783253205128205\n",
            "Batch: 109:\n",
            "epoch: 0, classification_loss: 1.8121144771575928, Val Loss: 70.43740427494049, loss : 1.8121144771575928\n",
            "epoch: 1, classification_loss: 1.7574388980865479, Val Loss: 70.49931561946869, loss : 2.0905580520629883\n",
            "epoch: 2, classification_loss: 1.7372462749481201, Val Loss: 70.43267965316772, loss : 2.004443407058716\n",
            "epoch: 3, classification_loss: 1.7722305059432983, Val Loss: 70.43945384025574, loss : 2.063751697540283\n",
            "epoch: 4, classification_loss: 1.7840912342071533, Val Loss: 70.40149104595184, loss : 2.041889190673828\n",
            "epoch: 5, classification_loss: 1.7831101417541504, Val Loss: 70.45133757591248, loss : 2.0486948490142822\n",
            "epoch: 6, classification_loss: 1.770790457725525, Val Loss: 70.44135546684265, loss : 2.054241895675659\n",
            "epoch: 7, classification_loss: 1.763338327407837, Val Loss: 70.45024800300598, loss : 2.041444778442383\n",
            "epoch: 8, classification_loss: 1.7789334058761597, Val Loss: 70.42799913883209, loss : 2.0355587005615234\n",
            "epoch: 9, classification_loss: 1.77487313747406, Val Loss: 70.43294513225555, loss : 2.044149160385132\n",
            "epoch: 10, classification_loss: 1.7709057331085205, Val Loss: 70.38779842853546, loss : 2.0339016914367676\n",
            "epoch: 11, classification_loss: 1.7694100141525269, Val Loss: 70.40618813037872, loss : 2.024599552154541\n",
            "epoch: 12, classification_loss: 1.77523934841156, Val Loss: 70.43614780902863, loss : 2.0203194618225098\n",
            "epoch: 13, classification_loss: 1.7823750972747803, Val Loss: 70.47272825241089, loss : 2.032405376434326\n",
            "epoch: 14, classification_loss: 1.782989263534546, Val Loss: 70.40708422660828, loss : 2.031568765640259\n",
            "epoch: 15, classification_loss: 1.7880215644836426, Val Loss: 70.38477218151093, loss : 2.0389678478240967\n",
            "epoch: 16, classification_loss: 1.7819416522979736, Val Loss: 70.4111328125, loss : 2.0283141136169434\n",
            "epoch: 17, classification_loss: 1.7810664176940918, Val Loss: 70.48115766048431, loss : 2.030468225479126\n",
            "epoch: 18, classification_loss: 1.7903281450271606, Val Loss: 70.45065212249756, loss : 2.028146743774414\n",
            "epoch: 19, classification_loss: 1.7813783884048462, Val Loss: 70.40312123298645, loss : 2.026555061340332\n",
            "Batch: 109, Test Acc: 0.5776241987179487\n",
            "Batch: 110:\n",
            "epoch: 0, classification_loss: 1.8289902210235596, Val Loss: 70.39457368850708, loss : 1.8289902210235596\n",
            "epoch: 1, classification_loss: 1.7951829433441162, Val Loss: 70.43892288208008, loss : 2.1155738830566406\n",
            "epoch: 2, classification_loss: 1.7724316120147705, Val Loss: 70.46376538276672, loss : 2.0247156620025635\n",
            "epoch: 3, classification_loss: 1.7935513257980347, Val Loss: 70.43153095245361, loss : 2.0636680126190186\n",
            "epoch: 4, classification_loss: 1.8265677690505981, Val Loss: 70.40662288665771, loss : 2.0528204441070557\n",
            "epoch: 5, classification_loss: 1.8076684474945068, Val Loss: 70.45981240272522, loss : 2.0667312145233154\n",
            "epoch: 6, classification_loss: 1.8017823696136475, Val Loss: 70.46753597259521, loss : 2.070742130279541\n",
            "epoch: 7, classification_loss: 1.8001304864883423, Val Loss: 70.44461667537689, loss : 2.0483505725860596\n",
            "epoch: 8, classification_loss: 1.8032392263412476, Val Loss: 70.442498087883, loss : 2.0510268211364746\n",
            "epoch: 9, classification_loss: 1.8000119924545288, Val Loss: 70.43620324134827, loss : 2.040210485458374\n",
            "epoch: 10, classification_loss: 1.812036156654358, Val Loss: 70.46493351459503, loss : 2.049031972885132\n",
            "epoch: 11, classification_loss: 1.8094311952590942, Val Loss: 70.46145415306091, loss : 2.0423994064331055\n",
            "epoch: 12, classification_loss: 1.8051949739456177, Val Loss: 70.46730661392212, loss : 2.0402450561523438\n",
            "epoch: 13, classification_loss: 1.8090863227844238, Val Loss: 70.45683252811432, loss : 2.037520408630371\n",
            "epoch: 14, classification_loss: 1.80431067943573, Val Loss: 70.43623113632202, loss : 2.0352442264556885\n",
            "epoch: 15, classification_loss: 1.8017297983169556, Val Loss: 70.45393431186676, loss : 2.026773452758789\n",
            "epoch: 16, classification_loss: 1.798944354057312, Val Loss: 70.49237370491028, loss : 2.0240774154663086\n",
            "epoch: 17, classification_loss: 1.8105535507202148, Val Loss: 70.49544990062714, loss : 2.0305607318878174\n",
            "epoch: 18, classification_loss: 1.805553674697876, Val Loss: 70.46642398834229, loss : 2.0307769775390625\n",
            "epoch: 19, classification_loss: 1.804411768913269, Val Loss: 70.50603020191193, loss : 2.0232887268066406\n",
            "Batch: 110, Test Acc: 0.5752203525641025\n",
            "Batch: 111:\n",
            "epoch: 0, classification_loss: 1.7811596393585205, Val Loss: 70.57750177383423, loss : 1.7811596393585205\n",
            "epoch: 1, classification_loss: 1.754211187362671, Val Loss: 70.50127017498016, loss : 2.0914766788482666\n",
            "epoch: 2, classification_loss: 1.7257399559020996, Val Loss: 70.45033061504364, loss : 2.0085952281951904\n",
            "epoch: 3, classification_loss: 1.7429219484329224, Val Loss: 70.45355892181396, loss : 2.012678861618042\n",
            "epoch: 4, classification_loss: 1.7756061553955078, Val Loss: 70.48098123073578, loss : 2.0320587158203125\n",
            "epoch: 5, classification_loss: 1.784683346748352, Val Loss: 70.44472336769104, loss : 2.018861770629883\n",
            "epoch: 6, classification_loss: 1.7620365619659424, Val Loss: 70.46354234218597, loss : 2.0256223678588867\n",
            "epoch: 7, classification_loss: 1.7499945163726807, Val Loss: 70.52127683162689, loss : 2.0243828296661377\n",
            "epoch: 8, classification_loss: 1.7619763612747192, Val Loss: 70.52961432933807, loss : 2.014298915863037\n",
            "epoch: 9, classification_loss: 1.7517586946487427, Val Loss: 70.50837481021881, loss : 2.008664608001709\n",
            "epoch: 10, classification_loss: 1.7683993577957153, Val Loss: 70.47940611839294, loss : 2.006394386291504\n",
            "epoch: 11, classification_loss: 1.7645461559295654, Val Loss: 70.51047742366791, loss : 2.011991024017334\n",
            "epoch: 12, classification_loss: 1.7620223760604858, Val Loss: 70.5239725112915, loss : 2.0162575244903564\n",
            "epoch: 13, classification_loss: 1.757259488105774, Val Loss: 70.48683536052704, loss : 2.006598472595215\n",
            "epoch: 14, classification_loss: 1.7672313451766968, Val Loss: 70.51936030387878, loss : 2.01161789894104\n",
            "epoch: 15, classification_loss: 1.7725491523742676, Val Loss: 70.55837500095367, loss : 2.016744613647461\n",
            "epoch: 16, classification_loss: 1.765442967414856, Val Loss: 70.5367476940155, loss : 2.004384994506836\n",
            "epoch: 17, classification_loss: 1.76328444480896, Val Loss: 70.51458287239075, loss : 2.0146677494049072\n",
            "epoch: 18, classification_loss: 1.7576982975006104, Val Loss: 70.50479459762573, loss : 1.992730975151062\n",
            "epoch: 19, classification_loss: 1.7675167322158813, Val Loss: 70.5421085357666, loss : 2.0095322132110596\n",
            "Batch: 111, Test Acc: 0.5738181089743589\n",
            "Batch: 112:\n",
            "epoch: 0, classification_loss: 1.7964699268341064, Val Loss: 70.51712000370026, loss : 1.7964699268341064\n",
            "epoch: 1, classification_loss: 1.7525115013122559, Val Loss: 70.44960737228394, loss : 2.08040714263916\n",
            "epoch: 2, classification_loss: 1.7281725406646729, Val Loss: 70.44154036045074, loss : 2.0028457641601562\n",
            "epoch: 3, classification_loss: 1.7512142658233643, Val Loss: 70.40035927295685, loss : 2.025020122528076\n",
            "epoch: 4, classification_loss: 1.7779273986816406, Val Loss: 70.55044567584991, loss : 2.0225327014923096\n",
            "epoch: 5, classification_loss: 1.774115800857544, Val Loss: 70.42340004444122, loss : 2.033534288406372\n",
            "epoch: 6, classification_loss: 1.7638704776763916, Val Loss: 70.46629893779755, loss : 2.023064613342285\n",
            "epoch: 7, classification_loss: 1.7626904249191284, Val Loss: 70.50882649421692, loss : 2.023021697998047\n",
            "epoch: 8, classification_loss: 1.7565276622772217, Val Loss: 70.50963139533997, loss : 2.01228666305542\n",
            "epoch: 9, classification_loss: 1.772548794746399, Val Loss: 70.45153832435608, loss : 2.024832248687744\n",
            "epoch: 10, classification_loss: 1.7700756788253784, Val Loss: 70.43303334712982, loss : 2.0192461013793945\n",
            "epoch: 11, classification_loss: 1.7734198570251465, Val Loss: 70.53387475013733, loss : 2.023080825805664\n",
            "epoch: 12, classification_loss: 1.7664885520935059, Val Loss: 70.45196318626404, loss : 2.014235734939575\n",
            "epoch: 13, classification_loss: 1.7652868032455444, Val Loss: 70.47256863117218, loss : 2.004791021347046\n",
            "epoch: 14, classification_loss: 1.7652215957641602, Val Loss: 70.43186616897583, loss : 2.0112366676330566\n",
            "epoch: 15, classification_loss: 1.7624255418777466, Val Loss: 70.49206399917603, loss : 2.008053779602051\n",
            "epoch: 16, classification_loss: 1.77093505859375, Val Loss: 70.47907090187073, loss : 2.02468204498291\n",
            "epoch: 17, classification_loss: 1.764845609664917, Val Loss: 70.46026313304901, loss : 2.015662670135498\n",
            "epoch: 18, classification_loss: 1.7710907459259033, Val Loss: 70.44075131416321, loss : 2.0094048976898193\n",
            "epoch: 19, classification_loss: 1.7734899520874023, Val Loss: 70.50183141231537, loss : 2.012747287750244\n",
            "Batch: 112, Test Acc: 0.5753205128205128\n",
            "Batch: 113:\n",
            "epoch: 0, classification_loss: 1.8064239025115967, Val Loss: 70.45464015007019, loss : 1.8064239025115967\n",
            "epoch: 1, classification_loss: 1.7493736743927002, Val Loss: 70.458540558815, loss : 2.088017702102661\n",
            "epoch: 2, classification_loss: 1.7433505058288574, Val Loss: 70.39439308643341, loss : 2.0061938762664795\n",
            "epoch: 3, classification_loss: 1.7631633281707764, Val Loss: 70.41699969768524, loss : 2.0420725345611572\n",
            "epoch: 4, classification_loss: 1.8000197410583496, Val Loss: 70.38197898864746, loss : 2.0458285808563232\n",
            "epoch: 5, classification_loss: 1.7856706380844116, Val Loss: 70.41990506649017, loss : 2.0540261268615723\n",
            "epoch: 6, classification_loss: 1.763357162475586, Val Loss: 70.40478420257568, loss : 2.041186571121216\n",
            "epoch: 7, classification_loss: 1.774746298789978, Val Loss: 70.3943110704422, loss : 2.0288450717926025\n",
            "epoch: 8, classification_loss: 1.7725977897644043, Val Loss: 70.41546285152435, loss : 2.037252902984619\n",
            "epoch: 9, classification_loss: 1.7733428478240967, Val Loss: 70.49904584884644, loss : 2.0280072689056396\n",
            "epoch: 10, classification_loss: 1.7620699405670166, Val Loss: 70.39764451980591, loss : 2.0189874172210693\n",
            "epoch: 11, classification_loss: 1.7750838994979858, Val Loss: 70.38612508773804, loss : 2.017745018005371\n",
            "epoch: 12, classification_loss: 1.7776174545288086, Val Loss: 70.40195333957672, loss : 2.023259401321411\n",
            "epoch: 13, classification_loss: 1.7758880853652954, Val Loss: 70.42970323562622, loss : 2.029578685760498\n",
            "epoch: 14, classification_loss: 1.7718876600265503, Val Loss: 70.39417052268982, loss : 2.020641565322876\n",
            "epoch: 15, classification_loss: 1.7690399885177612, Val Loss: 70.40071249008179, loss : 2.0103414058685303\n",
            "epoch: 16, classification_loss: 1.776787519454956, Val Loss: 70.37638700008392, loss : 2.0093536376953125\n",
            "epoch: 17, classification_loss: 1.773749589920044, Val Loss: 70.38171195983887, loss : 2.0166268348693848\n",
            "epoch: 18, classification_loss: 1.7766296863555908, Val Loss: 70.42849469184875, loss : 2.009774684906006\n",
            "epoch: 19, classification_loss: 1.7785930633544922, Val Loss: 70.43089950084686, loss : 2.0213074684143066\n",
            "Batch: 113, Test Acc: 0.5753205128205128\n",
            "Batch: 114:\n",
            "epoch: 0, classification_loss: 1.80778169631958, Val Loss: 70.44239068031311, loss : 1.80778169631958\n",
            "epoch: 1, classification_loss: 1.7783613204956055, Val Loss: 70.4573005437851, loss : 2.097712516784668\n",
            "epoch: 2, classification_loss: 1.7507632970809937, Val Loss: 70.4717036485672, loss : 2.01204252243042\n",
            "epoch: 3, classification_loss: 1.7874306440353394, Val Loss: 70.46282935142517, loss : 2.0532045364379883\n",
            "epoch: 4, classification_loss: 1.7924764156341553, Val Loss: 70.43648397922516, loss : 2.023261785507202\n",
            "epoch: 5, classification_loss: 1.8050720691680908, Val Loss: 70.46520709991455, loss : 2.0607285499572754\n",
            "epoch: 6, classification_loss: 1.7738386392593384, Val Loss: 70.51674711704254, loss : 2.04215145111084\n",
            "epoch: 7, classification_loss: 1.7736272811889648, Val Loss: 70.52132594585419, loss : 2.035360813140869\n",
            "epoch: 8, classification_loss: 1.7817637920379639, Val Loss: 70.42896592617035, loss : 2.029301643371582\n",
            "epoch: 9, classification_loss: 1.7887969017028809, Val Loss: 70.44560623168945, loss : 2.031968832015991\n",
            "epoch: 10, classification_loss: 1.789218544960022, Val Loss: 70.5061799287796, loss : 2.0390374660491943\n",
            "epoch: 11, classification_loss: 1.7905397415161133, Val Loss: 70.50290942192078, loss : 2.0389442443847656\n",
            "epoch: 12, classification_loss: 1.7805589437484741, Val Loss: 70.50278770923615, loss : 2.0246076583862305\n",
            "epoch: 13, classification_loss: 1.7945951223373413, Val Loss: 70.4977742433548, loss : 2.040341854095459\n",
            "epoch: 14, classification_loss: 1.7806588411331177, Val Loss: 70.50685739517212, loss : 2.018235683441162\n",
            "epoch: 15, classification_loss: 1.7911499738693237, Val Loss: 70.51518785953522, loss : 2.030217170715332\n",
            "epoch: 16, classification_loss: 1.786180853843689, Val Loss: 70.46929574012756, loss : 2.022459030151367\n",
            "epoch: 17, classification_loss: 1.7751624584197998, Val Loss: 70.48836100101471, loss : 2.0099563598632812\n",
            "epoch: 18, classification_loss: 1.7881662845611572, Val Loss: 70.52567684650421, loss : 2.021265745162964\n",
            "epoch: 19, classification_loss: 1.792356014251709, Val Loss: 70.56329822540283, loss : 2.0256147384643555\n",
            "Batch: 114, Test Acc: 0.573417467948718\n",
            "Batch: 115:\n",
            "epoch: 0, classification_loss: 1.8216480016708374, Val Loss: 70.48534548282623, loss : 1.8216480016708374\n",
            "epoch: 1, classification_loss: 1.7763268947601318, Val Loss: 70.45130264759064, loss : 2.1063990592956543\n",
            "epoch: 2, classification_loss: 1.7653818130493164, Val Loss: 70.42536664009094, loss : 2.03171706199646\n",
            "epoch: 3, classification_loss: 1.7816855907440186, Val Loss: 70.43757104873657, loss : 2.059924364089966\n",
            "epoch: 4, classification_loss: 1.8061498403549194, Val Loss: 70.47453558444977, loss : 2.0531039237976074\n",
            "epoch: 5, classification_loss: 1.7949706315994263, Val Loss: 70.4131727218628, loss : 2.0564613342285156\n",
            "epoch: 6, classification_loss: 1.7875025272369385, Val Loss: 70.42075634002686, loss : 2.056605100631714\n",
            "epoch: 7, classification_loss: 1.7847639322280884, Val Loss: 70.43549942970276, loss : 2.041168689727783\n",
            "epoch: 8, classification_loss: 1.8045424222946167, Val Loss: 70.47388863563538, loss : 2.061469078063965\n",
            "epoch: 9, classification_loss: 1.794457197189331, Val Loss: 70.44741487503052, loss : 2.0426268577575684\n",
            "epoch: 10, classification_loss: 1.7975573539733887, Val Loss: 70.4205572605133, loss : 2.045140504837036\n",
            "epoch: 11, classification_loss: 1.8002136945724487, Val Loss: 70.43772530555725, loss : 2.0417861938476562\n",
            "epoch: 12, classification_loss: 1.7942864894866943, Val Loss: 70.44951796531677, loss : 2.0379717350006104\n",
            "epoch: 13, classification_loss: 1.7852100133895874, Val Loss: 70.44438028335571, loss : 2.0309178829193115\n",
            "epoch: 14, classification_loss: 1.8047600984573364, Val Loss: 70.43246126174927, loss : 2.042924642562866\n",
            "epoch: 15, classification_loss: 1.7916123867034912, Val Loss: 70.44394505023956, loss : 2.0341176986694336\n",
            "epoch: 16, classification_loss: 1.7859874963760376, Val Loss: 70.41007947921753, loss : 2.016192674636841\n",
            "epoch: 17, classification_loss: 1.8091671466827393, Val Loss: 70.41973912715912, loss : 2.0394368171691895\n",
            "epoch: 18, classification_loss: 1.7956808805465698, Val Loss: 70.45680856704712, loss : 2.0220890045166016\n",
            "epoch: 19, classification_loss: 1.7894662618637085, Val Loss: 70.48557531833649, loss : 2.033482551574707\n",
            "Batch: 115, Test Acc: 0.5741185897435898\n",
            "Batch: 116:\n",
            "epoch: 0, classification_loss: 1.7733184099197388, Val Loss: 70.40098428726196, loss : 1.7733184099197388\n",
            "epoch: 1, classification_loss: 1.7526087760925293, Val Loss: 70.41560566425323, loss : 2.0664267539978027\n",
            "epoch: 2, classification_loss: 1.7339767217636108, Val Loss: 70.42820453643799, loss : 1.9836293458938599\n",
            "epoch: 3, classification_loss: 1.7638697624206543, Val Loss: 70.4145016670227, loss : 2.0151751041412354\n",
            "epoch: 4, classification_loss: 1.7764244079589844, Val Loss: 70.3913688659668, loss : 1.995294451713562\n",
            "epoch: 5, classification_loss: 1.7515522241592407, Val Loss: 70.42955434322357, loss : 2.0130364894866943\n",
            "epoch: 6, classification_loss: 1.7586086988449097, Val Loss: 70.45506191253662, loss : 2.019223928451538\n",
            "epoch: 7, classification_loss: 1.7584877014160156, Val Loss: 70.4291684627533, loss : 2.010798931121826\n",
            "epoch: 8, classification_loss: 1.7626878023147583, Val Loss: 70.41926431655884, loss : 2.002239942550659\n",
            "epoch: 9, classification_loss: 1.7662025690078735, Val Loss: 70.47205829620361, loss : 2.0011415481567383\n",
            "epoch: 10, classification_loss: 1.7686086893081665, Val Loss: 70.49390733242035, loss : 2.006397008895874\n",
            "epoch: 11, classification_loss: 1.760558009147644, Val Loss: 70.46402513980865, loss : 1.989642858505249\n",
            "epoch: 12, classification_loss: 1.77061128616333, Val Loss: 70.48219287395477, loss : 2.0018606185913086\n",
            "epoch: 13, classification_loss: 1.773463249206543, Val Loss: 70.48379504680634, loss : 2.0063507556915283\n",
            "epoch: 14, classification_loss: 1.7707833051681519, Val Loss: 70.46036398410797, loss : 1.9959876537322998\n",
            "epoch: 15, classification_loss: 1.752908706665039, Val Loss: 70.48936021327972, loss : 1.9829485416412354\n",
            "epoch: 16, classification_loss: 1.7775837182998657, Val Loss: 70.56507086753845, loss : 2.0017430782318115\n",
            "epoch: 17, classification_loss: 1.7648109197616577, Val Loss: 70.47437477111816, loss : 2.0012168884277344\n",
            "epoch: 18, classification_loss: 1.7599809169769287, Val Loss: 70.47791922092438, loss : 1.9786450862884521\n",
            "epoch: 19, classification_loss: 1.7754364013671875, Val Loss: 70.50593376159668, loss : 2.0089426040649414\n",
            "Batch: 116, Test Acc: 0.5749198717948718\n",
            "Batch: 117:\n",
            "epoch: 0, classification_loss: 1.8373137712478638, Val Loss: 70.4587242603302, loss : 1.8373137712478638\n",
            "epoch: 1, classification_loss: 1.78758704662323, Val Loss: 70.47677528858185, loss : 2.1217877864837646\n",
            "epoch: 2, classification_loss: 1.7530772686004639, Val Loss: 70.4250853061676, loss : 2.0303001403808594\n",
            "epoch: 3, classification_loss: 1.7856251001358032, Val Loss: 70.40821516513824, loss : 2.064694881439209\n",
            "epoch: 4, classification_loss: 1.8257384300231934, Val Loss: 70.41684353351593, loss : 2.0683250427246094\n",
            "epoch: 5, classification_loss: 1.809180736541748, Val Loss: 70.42794954776764, loss : 2.0741429328918457\n",
            "epoch: 6, classification_loss: 1.7989003658294678, Val Loss: 70.43311834335327, loss : 2.063832998275757\n",
            "epoch: 7, classification_loss: 1.7992953062057495, Val Loss: 70.3957496881485, loss : 2.0690417289733887\n",
            "epoch: 8, classification_loss: 1.8072147369384766, Val Loss: 70.3823231458664, loss : 2.0648727416992188\n",
            "epoch: 9, classification_loss: 1.8051809072494507, Val Loss: 70.38745856285095, loss : 2.070516586303711\n",
            "epoch: 10, classification_loss: 1.8047285079956055, Val Loss: 70.3762708902359, loss : 2.0622715950012207\n",
            "epoch: 11, classification_loss: 1.8184287548065186, Val Loss: 70.40490543842316, loss : 2.07136607170105\n",
            "epoch: 12, classification_loss: 1.8102625608444214, Val Loss: 70.37613189220428, loss : 2.0566112995147705\n",
            "epoch: 13, classification_loss: 1.806196928024292, Val Loss: 70.37482476234436, loss : 2.0541229248046875\n",
            "epoch: 14, classification_loss: 1.803318738937378, Val Loss: 70.39776229858398, loss : 2.046185255050659\n",
            "epoch: 15, classification_loss: 1.794622540473938, Val Loss: 70.3892731666565, loss : 2.0464353561401367\n",
            "epoch: 16, classification_loss: 1.803505539894104, Val Loss: 70.3797059059143, loss : 2.045593738555908\n",
            "epoch: 17, classification_loss: 1.8157589435577393, Val Loss: 70.39778912067413, loss : 2.0673398971557617\n",
            "epoch: 18, classification_loss: 1.810158371925354, Val Loss: 70.40329039096832, loss : 2.045973539352417\n",
            "epoch: 19, classification_loss: 1.8138798475265503, Val Loss: 70.41420447826385, loss : 2.0582902431488037\n",
            "Batch: 117, Test Acc: 0.5760216346153846\n",
            "Batch: 118:\n",
            "epoch: 0, classification_loss: 1.832484483718872, Val Loss: 70.4293942451477, loss : 1.832484483718872\n",
            "epoch: 1, classification_loss: 1.7912921905517578, Val Loss: 70.44986355304718, loss : 2.12428879737854\n",
            "epoch: 2, classification_loss: 1.758088231086731, Val Loss: 70.51020765304565, loss : 2.0366482734680176\n",
            "epoch: 3, classification_loss: 1.7817485332489014, Val Loss: 70.43378567695618, loss : 2.065595865249634\n",
            "epoch: 4, classification_loss: 1.8124666213989258, Val Loss: 70.39040422439575, loss : 2.054028272628784\n",
            "epoch: 5, classification_loss: 1.7993512153625488, Val Loss: 70.38129806518555, loss : 2.066692352294922\n",
            "epoch: 6, classification_loss: 1.7756860256195068, Val Loss: 70.41768825054169, loss : 2.053676128387451\n",
            "epoch: 7, classification_loss: 1.7848732471466064, Val Loss: 70.4524554014206, loss : 2.0572409629821777\n",
            "epoch: 8, classification_loss: 1.8063246011734009, Val Loss: 70.4119359254837, loss : 2.0658247470855713\n",
            "epoch: 9, classification_loss: 1.8045376539230347, Val Loss: 70.43705034255981, loss : 2.066828966140747\n",
            "epoch: 10, classification_loss: 1.7817940711975098, Val Loss: 70.44521045684814, loss : 2.0390777587890625\n",
            "epoch: 11, classification_loss: 1.7974506616592407, Val Loss: 70.48742437362671, loss : 2.044945478439331\n",
            "epoch: 12, classification_loss: 1.7997299432754517, Val Loss: 70.43606293201447, loss : 2.0481441020965576\n",
            "epoch: 13, classification_loss: 1.7927676439285278, Val Loss: 70.4263858795166, loss : 2.0510056018829346\n",
            "epoch: 14, classification_loss: 1.78898286819458, Val Loss: 70.47232627868652, loss : 2.0391757488250732\n",
            "epoch: 15, classification_loss: 1.7941755056381226, Val Loss: 70.4386339187622, loss : 2.0433349609375\n",
            "epoch: 16, classification_loss: 1.789008617401123, Val Loss: 70.42907786369324, loss : 2.03462553024292\n",
            "epoch: 17, classification_loss: 1.7835036516189575, Val Loss: 70.43229043483734, loss : 2.031978130340576\n",
            "epoch: 18, classification_loss: 1.7962874174118042, Val Loss: 70.56585264205933, loss : 2.03537917137146\n",
            "epoch: 19, classification_loss: 1.802931547164917, Val Loss: 70.46912837028503, loss : 2.0453603267669678\n",
            "Batch: 118, Test Acc: 0.5780248397435898\n",
            "Batch: 119:\n",
            "epoch: 0, classification_loss: 1.8019089698791504, Val Loss: 70.55322074890137, loss : 1.8019089698791504\n",
            "epoch: 1, classification_loss: 1.7730858325958252, Val Loss: 70.48742043972015, loss : 2.0988593101501465\n",
            "epoch: 2, classification_loss: 1.737015724182129, Val Loss: 70.44458508491516, loss : 2.007183790206909\n",
            "epoch: 3, classification_loss: 1.7805439233779907, Val Loss: 70.4260938167572, loss : 2.056485652923584\n",
            "epoch: 4, classification_loss: 1.796143651008606, Val Loss: 70.4704657793045, loss : 2.0431854724884033\n",
            "epoch: 5, classification_loss: 1.784720778465271, Val Loss: 70.4439709186554, loss : 2.0426390171051025\n",
            "epoch: 6, classification_loss: 1.7795039415359497, Val Loss: 70.47370624542236, loss : 2.0472829341888428\n",
            "epoch: 7, classification_loss: 1.7767109870910645, Val Loss: 70.45799744129181, loss : 2.0429458618164062\n",
            "epoch: 8, classification_loss: 1.7827883958816528, Val Loss: 70.46807301044464, loss : 2.0360336303710938\n",
            "epoch: 9, classification_loss: 1.7763363122940063, Val Loss: 70.47109341621399, loss : 2.024123191833496\n",
            "epoch: 10, classification_loss: 1.7798889875411987, Val Loss: 70.45121800899506, loss : 2.022150993347168\n",
            "epoch: 11, classification_loss: 1.7846919298171997, Val Loss: 70.4910979270935, loss : 2.025237560272217\n",
            "epoch: 12, classification_loss: 1.779179334640503, Val Loss: 70.46135413646698, loss : 2.0211730003356934\n",
            "epoch: 13, classification_loss: 1.7817447185516357, Val Loss: 70.46896862983704, loss : 2.024244785308838\n",
            "epoch: 14, classification_loss: 1.7851372957229614, Val Loss: 70.48627555370331, loss : 2.0208792686462402\n",
            "epoch: 15, classification_loss: 1.7937458753585815, Val Loss: 70.45795977115631, loss : 2.0295517444610596\n",
            "epoch: 16, classification_loss: 1.7820920944213867, Val Loss: 70.47053861618042, loss : 2.0114638805389404\n",
            "epoch: 17, classification_loss: 1.7816803455352783, Val Loss: 70.48699676990509, loss : 2.016357660293579\n",
            "epoch: 18, classification_loss: 1.7787760496139526, Val Loss: 70.4826078414917, loss : 2.0118937492370605\n",
            "epoch: 19, classification_loss: 1.7793365716934204, Val Loss: 70.46884655952454, loss : 2.0120034217834473\n",
            "Batch: 119, Test Acc: 0.5777243589743589\n",
            "Batch: 120:\n",
            "epoch: 0, classification_loss: 1.7929718494415283, Val Loss: 70.41552829742432, loss : 1.7929718494415283\n",
            "epoch: 1, classification_loss: 1.7577273845672607, Val Loss: 70.4051775932312, loss : 2.082418203353882\n",
            "epoch: 2, classification_loss: 1.7322582006454468, Val Loss: 70.41059589385986, loss : 1.9983015060424805\n",
            "epoch: 3, classification_loss: 1.7564619779586792, Val Loss: 70.42994046211243, loss : 2.02755069732666\n",
            "epoch: 4, classification_loss: 1.7782340049743652, Val Loss: 70.45400750637054, loss : 2.019320487976074\n",
            "epoch: 5, classification_loss: 1.7793056964874268, Val Loss: 70.465611577034, loss : 2.0393030643463135\n",
            "epoch: 6, classification_loss: 1.7467732429504395, Val Loss: 70.45059132575989, loss : 2.0147948265075684\n",
            "epoch: 7, classification_loss: 1.7639068365097046, Val Loss: 70.43405330181122, loss : 2.023747682571411\n",
            "epoch: 8, classification_loss: 1.7638672590255737, Val Loss: 70.40386116504669, loss : 2.018375873565674\n",
            "epoch: 9, classification_loss: 1.775624394416809, Val Loss: 70.4081860780716, loss : 2.023205041885376\n",
            "epoch: 10, classification_loss: 1.766746997833252, Val Loss: 70.44715893268585, loss : 2.01234769821167\n",
            "epoch: 11, classification_loss: 1.7694098949432373, Val Loss: 70.46908509731293, loss : 2.0063939094543457\n",
            "epoch: 12, classification_loss: 1.7525376081466675, Val Loss: 70.46139943599701, loss : 1.9903604984283447\n",
            "epoch: 13, classification_loss: 1.7593543529510498, Val Loss: 70.4621274471283, loss : 2.000471353530884\n",
            "epoch: 14, classification_loss: 1.7576013803482056, Val Loss: 70.44088232517242, loss : 1.9998619556427002\n",
            "epoch: 15, classification_loss: 1.7686090469360352, Val Loss: 70.43932545185089, loss : 2.0027482509613037\n",
            "epoch: 16, classification_loss: 1.767133116722107, Val Loss: 70.47025740146637, loss : 1.999763011932373\n",
            "epoch: 17, classification_loss: 1.7593371868133545, Val Loss: 70.48657023906708, loss : 1.9924266338348389\n",
            "epoch: 18, classification_loss: 1.7665684223175049, Val Loss: 70.47838723659515, loss : 1.9970650672912598\n",
            "epoch: 19, classification_loss: 1.765160083770752, Val Loss: 70.44631230831146, loss : 2.0004754066467285\n",
            "Batch: 120, Test Acc: 0.5768229166666666\n",
            "Batch: 121:\n",
            "epoch: 0, classification_loss: 1.8111846446990967, Val Loss: 70.39499509334564, loss : 1.8111846446990967\n",
            "epoch: 1, classification_loss: 1.7712222337722778, Val Loss: 70.44624888896942, loss : 2.0934460163116455\n",
            "epoch: 2, classification_loss: 1.7451834678649902, Val Loss: 70.46581852436066, loss : 2.0059421062469482\n",
            "epoch: 3, classification_loss: 1.7732694149017334, Val Loss: 70.43500804901123, loss : 2.0403072834014893\n",
            "epoch: 4, classification_loss: 1.793955683708191, Val Loss: 70.41841053962708, loss : 2.030942440032959\n",
            "epoch: 5, classification_loss: 1.795467734336853, Val Loss: 70.39342284202576, loss : 2.0455727577209473\n",
            "epoch: 6, classification_loss: 1.7740607261657715, Val Loss: 70.43926358222961, loss : 2.0343825817108154\n",
            "epoch: 7, classification_loss: 1.770910382270813, Val Loss: 70.44157528877258, loss : 2.0231523513793945\n",
            "epoch: 8, classification_loss: 1.7782797813415527, Val Loss: 70.43261003494263, loss : 2.0212080478668213\n",
            "epoch: 9, classification_loss: 1.7743586301803589, Val Loss: 70.4363843202591, loss : 2.0203559398651123\n",
            "epoch: 10, classification_loss: 1.7719475030899048, Val Loss: 70.43580770492554, loss : 2.0128703117370605\n",
            "epoch: 11, classification_loss: 1.775604009628296, Val Loss: 70.47032082080841, loss : 2.0132968425750732\n",
            "epoch: 12, classification_loss: 1.776320457458496, Val Loss: 70.43320655822754, loss : 2.015841007232666\n",
            "epoch: 13, classification_loss: 1.768265724182129, Val Loss: 70.47546136379242, loss : 2.0051608085632324\n",
            "epoch: 14, classification_loss: 1.78766667842865, Val Loss: 70.45110094547272, loss : 2.0325045585632324\n",
            "epoch: 15, classification_loss: 1.7854546308517456, Val Loss: 70.46002352237701, loss : 2.0180246829986572\n",
            "epoch: 16, classification_loss: 1.782966136932373, Val Loss: 70.44569659233093, loss : 2.021482229232788\n",
            "epoch: 17, classification_loss: 1.7959064245224, Val Loss: 70.53186023235321, loss : 2.029252052307129\n",
            "epoch: 18, classification_loss: 1.787445306777954, Val Loss: 70.45984220504761, loss : 2.0254158973693848\n",
            "epoch: 19, classification_loss: 1.7774760723114014, Val Loss: 70.46658861637115, loss : 2.0139660835266113\n",
            "Batch: 121, Test Acc: 0.5790264423076923\n",
            "Batch: 122:\n",
            "epoch: 0, classification_loss: 1.8555264472961426, Val Loss: 70.49269247055054, loss : 1.8555264472961426\n",
            "epoch: 1, classification_loss: 1.8151072263717651, Val Loss: 70.56474113464355, loss : 2.1447157859802246\n",
            "epoch: 2, classification_loss: 1.7918778657913208, Val Loss: 70.44098544120789, loss : 2.0559961795806885\n",
            "epoch: 3, classification_loss: 1.8240362405776978, Val Loss: 70.43204915523529, loss : 2.0879178047180176\n",
            "epoch: 4, classification_loss: 1.843786597251892, Val Loss: 70.42625784873962, loss : 2.0882787704467773\n",
            "epoch: 5, classification_loss: 1.8315978050231934, Val Loss: 70.5017718076706, loss : 2.08341908454895\n",
            "epoch: 6, classification_loss: 1.81964111328125, Val Loss: 70.47686541080475, loss : 2.0923898220062256\n",
            "epoch: 7, classification_loss: 1.8138824701309204, Val Loss: 70.4013741016388, loss : 2.0676655769348145\n",
            "epoch: 8, classification_loss: 1.831290364265442, Val Loss: 70.40723860263824, loss : 2.080380916595459\n",
            "epoch: 9, classification_loss: 1.8280631303787231, Val Loss: 70.45558142662048, loss : 2.075385570526123\n",
            "epoch: 10, classification_loss: 1.8227276802062988, Val Loss: 70.46737289428711, loss : 2.071920156478882\n",
            "epoch: 11, classification_loss: 1.8155945539474487, Val Loss: 70.44741439819336, loss : 2.0639610290527344\n",
            "epoch: 12, classification_loss: 1.8209096193313599, Val Loss: 70.41858685016632, loss : 2.0607211589813232\n",
            "epoch: 13, classification_loss: 1.8325371742248535, Val Loss: 70.44685852527618, loss : 2.0760610103607178\n",
            "epoch: 14, classification_loss: 1.8247939348220825, Val Loss: 70.45462703704834, loss : 2.063539981842041\n",
            "epoch: 15, classification_loss: 1.826684832572937, Val Loss: 70.4327712059021, loss : 2.0730957984924316\n",
            "epoch: 16, classification_loss: 1.8274779319763184, Val Loss: 70.45865559577942, loss : 2.0644476413726807\n",
            "epoch: 17, classification_loss: 1.8361425399780273, Val Loss: 70.44536781311035, loss : 2.0785672664642334\n",
            "epoch: 18, classification_loss: 1.8285492658615112, Val Loss: 70.44514060020447, loss : 2.059121608734131\n",
            "epoch: 19, classification_loss: 1.830068826675415, Val Loss: 70.47987520694733, loss : 2.0674753189086914\n",
            "Batch: 122, Test Acc: 0.5757211538461539\n",
            "Batch: 123:\n",
            "epoch: 0, classification_loss: 1.8025339841842651, Val Loss: 70.4462274312973, loss : 1.8025339841842651\n",
            "epoch: 1, classification_loss: 1.7703893184661865, Val Loss: 70.47309839725494, loss : 2.097813606262207\n",
            "epoch: 2, classification_loss: 1.742542028427124, Val Loss: 70.42482876777649, loss : 2.0141139030456543\n",
            "epoch: 3, classification_loss: 1.7631736993789673, Val Loss: 70.40910995006561, loss : 2.0297465324401855\n",
            "epoch: 4, classification_loss: 1.7903491258621216, Val Loss: 70.40880608558655, loss : 2.0231828689575195\n",
            "epoch: 5, classification_loss: 1.7912424802780151, Val Loss: 70.41741061210632, loss : 2.0504589080810547\n",
            "epoch: 6, classification_loss: 1.7754156589508057, Val Loss: 70.4106171131134, loss : 2.03678822517395\n",
            "epoch: 7, classification_loss: 1.7560625076293945, Val Loss: 70.43895637989044, loss : 2.022703170776367\n",
            "epoch: 8, classification_loss: 1.7636055946350098, Val Loss: 70.45011150836945, loss : 2.0215489864349365\n",
            "epoch: 9, classification_loss: 1.773753046989441, Val Loss: 70.42085266113281, loss : 2.028191566467285\n",
            "epoch: 10, classification_loss: 1.7736958265304565, Val Loss: 70.44740116596222, loss : 2.021243095397949\n",
            "epoch: 11, classification_loss: 1.7734235525131226, Val Loss: 70.42527270317078, loss : 2.0210959911346436\n",
            "epoch: 12, classification_loss: 1.7731528282165527, Val Loss: 70.42907011508942, loss : 2.0159006118774414\n",
            "epoch: 13, classification_loss: 1.7738984823226929, Val Loss: 70.44829058647156, loss : 2.0206797122955322\n",
            "epoch: 14, classification_loss: 1.7623363733291626, Val Loss: 70.44217956066132, loss : 2.0079593658447266\n",
            "epoch: 15, classification_loss: 1.7800624370574951, Val Loss: 70.42627167701721, loss : 2.0214409828186035\n",
            "epoch: 16, classification_loss: 1.7809237241744995, Val Loss: 70.43257403373718, loss : 2.017000436782837\n",
            "epoch: 17, classification_loss: 1.7779566049575806, Val Loss: 70.43810594081879, loss : 2.0196449756622314\n",
            "epoch: 18, classification_loss: 1.7759346961975098, Val Loss: 70.45259630680084, loss : 2.010660171508789\n",
            "epoch: 19, classification_loss: 1.7716413736343384, Val Loss: 70.40883040428162, loss : 2.0096428394317627\n",
            "Batch: 123, Test Acc: 0.5785256410256411\n",
            "Batch: 124:\n",
            "epoch: 0, classification_loss: 1.8218942880630493, Val Loss: 70.56891560554504, loss : 1.8218942880630493\n",
            "epoch: 1, classification_loss: 1.7918792963027954, Val Loss: 70.45225298404694, loss : 2.1326823234558105\n",
            "epoch: 2, classification_loss: 1.7715716361999512, Val Loss: 70.50806200504303, loss : 2.033780574798584\n",
            "epoch: 3, classification_loss: 1.809294581413269, Val Loss: 70.45124340057373, loss : 2.09125018119812\n",
            "epoch: 4, classification_loss: 1.8220961093902588, Val Loss: 70.54693686962128, loss : 2.0688586235046387\n",
            "epoch: 5, classification_loss: 1.8028963804244995, Val Loss: 70.45534074306488, loss : 2.06829833984375\n",
            "epoch: 6, classification_loss: 1.7960443496704102, Val Loss: 70.4610401391983, loss : 2.071586847305298\n",
            "epoch: 7, classification_loss: 1.7971168756484985, Val Loss: 70.45630526542664, loss : 2.056333541870117\n",
            "epoch: 8, classification_loss: 1.8164299726486206, Val Loss: 70.55475521087646, loss : 2.0689268112182617\n",
            "epoch: 9, classification_loss: 1.807200312614441, Val Loss: 70.44760048389435, loss : 2.060213565826416\n",
            "epoch: 10, classification_loss: 1.8006678819656372, Val Loss: 70.46775007247925, loss : 2.0505337715148926\n",
            "epoch: 11, classification_loss: 1.8006421327590942, Val Loss: 70.51547646522522, loss : 2.043734312057495\n",
            "epoch: 12, classification_loss: 1.8010796308517456, Val Loss: 70.45904493331909, loss : 2.0417253971099854\n",
            "epoch: 13, classification_loss: 1.7948017120361328, Val Loss: 70.45295262336731, loss : 2.0373568534851074\n",
            "epoch: 14, classification_loss: 1.8015961647033691, Val Loss: 70.4820830821991, loss : 2.0462663173675537\n",
            "epoch: 15, classification_loss: 1.8052369356155396, Val Loss: 70.54848170280457, loss : 2.041624069213867\n",
            "epoch: 16, classification_loss: 1.8037405014038086, Val Loss: 70.47380530834198, loss : 2.0399885177612305\n",
            "epoch: 17, classification_loss: 1.8209748268127441, Val Loss: 70.43098556995392, loss : 2.050626039505005\n",
            "epoch: 18, classification_loss: 1.8039004802703857, Val Loss: 70.51818418502808, loss : 2.03786301612854\n",
            "epoch: 19, classification_loss: 1.798887014389038, Val Loss: 70.51625561714172, loss : 2.0345466136932373\n",
            "Batch: 124, Test Acc: 0.5768229166666666\n",
            "Batch: 125:\n",
            "epoch: 0, classification_loss: 1.8139314651489258, Val Loss: 70.53472864627838, loss : 1.8139314651489258\n",
            "epoch: 1, classification_loss: 1.791638731956482, Val Loss: 70.50527560710907, loss : 2.1198008060455322\n",
            "epoch: 2, classification_loss: 1.768893837928772, Val Loss: 70.50895154476166, loss : 2.0141372680664062\n",
            "epoch: 3, classification_loss: 1.8058120012283325, Val Loss: 70.45779621601105, loss : 2.0764694213867188\n",
            "epoch: 4, classification_loss: 1.8170067071914673, Val Loss: 70.51627135276794, loss : 2.0508060455322266\n",
            "epoch: 5, classification_loss: 1.8142776489257812, Val Loss: 70.53476083278656, loss : 2.0671231746673584\n",
            "epoch: 6, classification_loss: 1.7931158542633057, Val Loss: 70.49533820152283, loss : 2.0664751529693604\n",
            "epoch: 7, classification_loss: 1.8003876209259033, Val Loss: 70.50541079044342, loss : 2.0567190647125244\n",
            "epoch: 8, classification_loss: 1.8049230575561523, Val Loss: 70.46348166465759, loss : 2.0489089488983154\n",
            "epoch: 9, classification_loss: 1.8105345964431763, Val Loss: 70.46753180027008, loss : 2.052190065383911\n",
            "epoch: 10, classification_loss: 1.795833945274353, Val Loss: 70.5374847650528, loss : 2.0392041206359863\n",
            "epoch: 11, classification_loss: 1.81011962890625, Val Loss: 70.50826048851013, loss : 2.059941530227661\n",
            "epoch: 12, classification_loss: 1.8064242601394653, Val Loss: 70.47352838516235, loss : 2.042473077774048\n",
            "epoch: 13, classification_loss: 1.8071309328079224, Val Loss: 70.49554812908173, loss : 2.044375419616699\n",
            "epoch: 14, classification_loss: 1.7950751781463623, Val Loss: 70.52972483634949, loss : 2.0270872116088867\n",
            "epoch: 15, classification_loss: 1.7938222885131836, Val Loss: 70.5268886089325, loss : 2.033355951309204\n",
            "epoch: 16, classification_loss: 1.800212025642395, Val Loss: 70.51634871959686, loss : 2.0340704917907715\n",
            "epoch: 17, classification_loss: 1.8146167993545532, Val Loss: 70.49964725971222, loss : 2.0448718070983887\n",
            "epoch: 18, classification_loss: 1.7930724620819092, Val Loss: 70.52272343635559, loss : 2.016511917114258\n",
            "epoch: 19, classification_loss: 1.8084648847579956, Val Loss: 70.53067886829376, loss : 2.032360792160034\n",
            "Batch: 125, Test Acc: 0.5748197115384616\n",
            "Batch: 126:\n",
            "epoch: 0, classification_loss: 1.7688195705413818, Val Loss: 70.41793811321259, loss : 1.7688195705413818\n",
            "epoch: 1, classification_loss: 1.7363269329071045, Val Loss: 70.44246590137482, loss : 2.0699243545532227\n",
            "epoch: 2, classification_loss: 1.7291427850723267, Val Loss: 70.46316874027252, loss : 1.9840631484985352\n",
            "epoch: 3, classification_loss: 1.754638910293579, Val Loss: 70.51406359672546, loss : 2.0220088958740234\n",
            "epoch: 4, classification_loss: 1.7583184242248535, Val Loss: 70.46371412277222, loss : 1.9974628686904907\n",
            "epoch: 5, classification_loss: 1.7415834665298462, Val Loss: 70.49020552635193, loss : 1.99595046043396\n",
            "epoch: 6, classification_loss: 1.7361044883728027, Val Loss: 70.5364761352539, loss : 1.9971165657043457\n",
            "epoch: 7, classification_loss: 1.7401163578033447, Val Loss: 70.54465246200562, loss : 1.9989123344421387\n",
            "epoch: 8, classification_loss: 1.748181939125061, Val Loss: 70.5470974445343, loss : 1.99542236328125\n",
            "epoch: 9, classification_loss: 1.742884635925293, Val Loss: 70.51164770126343, loss : 1.986376404762268\n",
            "epoch: 10, classification_loss: 1.7412052154541016, Val Loss: 70.5267562866211, loss : 1.9781417846679688\n",
            "epoch: 11, classification_loss: 1.7458735704421997, Val Loss: 70.55633270740509, loss : 1.9862853288650513\n",
            "epoch: 12, classification_loss: 1.7540242671966553, Val Loss: 70.55781090259552, loss : 1.9821319580078125\n",
            "epoch: 13, classification_loss: 1.745483636856079, Val Loss: 70.63963341712952, loss : 1.9829250574111938\n",
            "epoch: 14, classification_loss: 1.752047061920166, Val Loss: 70.61375224590302, loss : 1.9816502332687378\n",
            "epoch: 15, classification_loss: 1.7409040927886963, Val Loss: 70.55648529529572, loss : 1.9747068881988525\n",
            "epoch: 16, classification_loss: 1.7521824836730957, Val Loss: 70.58369266986847, loss : 1.9778289794921875\n",
            "epoch: 17, classification_loss: 1.7402936220169067, Val Loss: 70.6254290342331, loss : 1.9767054319381714\n",
            "epoch: 18, classification_loss: 1.7523889541625977, Val Loss: 70.61129593849182, loss : 1.97395920753479\n",
            "epoch: 19, classification_loss: 1.7570486068725586, Val Loss: 70.61448383331299, loss : 1.9945350885391235\n",
            "Batch: 126, Test Acc: 0.5733173076923077\n",
            "Batch: 127:\n",
            "epoch: 0, classification_loss: 1.88666570186615, Val Loss: 70.47990190982819, loss : 1.88666570186615\n",
            "epoch: 1, classification_loss: 1.8368483781814575, Val Loss: 70.51017653942108, loss : 2.1749303340911865\n",
            "epoch: 2, classification_loss: 1.8181754350662231, Val Loss: 70.46672475337982, loss : 2.0892555713653564\n",
            "epoch: 3, classification_loss: 1.8502094745635986, Val Loss: 70.40845251083374, loss : 2.128082513809204\n",
            "epoch: 4, classification_loss: 1.8573918342590332, Val Loss: 70.45851802825928, loss : 2.1000757217407227\n",
            "epoch: 5, classification_loss: 1.8650051355361938, Val Loss: 70.44031369686127, loss : 2.1347358226776123\n",
            "epoch: 6, classification_loss: 1.8395812511444092, Val Loss: 70.4222629070282, loss : 2.119769811630249\n",
            "epoch: 7, classification_loss: 1.8623499870300293, Val Loss: 70.46369731426239, loss : 2.1265063285827637\n",
            "epoch: 8, classification_loss: 1.8583943843841553, Val Loss: 70.47660648822784, loss : 2.1239805221557617\n",
            "epoch: 9, classification_loss: 1.8530175685882568, Val Loss: 70.41613936424255, loss : 2.125765800476074\n",
            "epoch: 10, classification_loss: 1.8560816049575806, Val Loss: 70.44762814044952, loss : 2.1208157539367676\n",
            "epoch: 11, classification_loss: 1.85565984249115, Val Loss: 70.48254013061523, loss : 2.114138126373291\n",
            "epoch: 12, classification_loss: 1.865580677986145, Val Loss: 70.48264145851135, loss : 2.117642641067505\n",
            "epoch: 13, classification_loss: 1.8523094654083252, Val Loss: 70.47004640102386, loss : 2.112187147140503\n",
            "epoch: 14, classification_loss: 1.8543659448623657, Val Loss: 70.444211602211, loss : 2.1054301261901855\n",
            "epoch: 15, classification_loss: 1.8619904518127441, Val Loss: 70.46500062942505, loss : 2.1149849891662598\n",
            "epoch: 16, classification_loss: 1.8572571277618408, Val Loss: 70.4637520313263, loss : 2.110072374343872\n",
            "epoch: 17, classification_loss: 1.8526915311813354, Val Loss: 70.474862575531, loss : 2.1009554862976074\n",
            "epoch: 18, classification_loss: 1.866146445274353, Val Loss: 70.50360405445099, loss : 2.1090075969696045\n",
            "epoch: 19, classification_loss: 1.86570405960083, Val Loss: 70.49108362197876, loss : 2.114520311355591\n",
            "Batch: 127, Test Acc: 0.5765224358974359\n",
            "Batch: 128:\n",
            "epoch: 0, classification_loss: 1.8041247129440308, Val Loss: 70.45511186122894, loss : 1.8041247129440308\n",
            "epoch: 1, classification_loss: 1.7554348707199097, Val Loss: 70.42982339859009, loss : 2.088390350341797\n",
            "epoch: 2, classification_loss: 1.7408802509307861, Val Loss: 70.3980917930603, loss : 2.010730266571045\n",
            "epoch: 3, classification_loss: 1.7625441551208496, Val Loss: 70.40429067611694, loss : 2.0376970767974854\n",
            "epoch: 4, classification_loss: 1.7918506860733032, Val Loss: 70.4554169178009, loss : 2.0273020267486572\n",
            "epoch: 5, classification_loss: 1.782239317893982, Val Loss: 70.42570996284485, loss : 2.050300359725952\n",
            "epoch: 6, classification_loss: 1.7539058923721313, Val Loss: 70.4548888206482, loss : 2.033823251724243\n",
            "epoch: 7, classification_loss: 1.7668378353118896, Val Loss: 70.42699348926544, loss : 2.0327534675598145\n",
            "epoch: 8, classification_loss: 1.7887400388717651, Val Loss: 70.42356276512146, loss : 2.042046070098877\n",
            "epoch: 9, classification_loss: 1.7710578441619873, Val Loss: 70.41740703582764, loss : 2.0298032760620117\n",
            "epoch: 10, classification_loss: 1.786325216293335, Val Loss: 70.42252051830292, loss : 2.0337929725646973\n",
            "epoch: 11, classification_loss: 1.7846559286117554, Val Loss: 70.44171106815338, loss : 2.030973434448242\n",
            "epoch: 12, classification_loss: 1.7863857746124268, Val Loss: 70.37968575954437, loss : 2.026528835296631\n",
            "epoch: 13, classification_loss: 1.7776051759719849, Val Loss: 70.40143477916718, loss : 2.026540756225586\n",
            "epoch: 14, classification_loss: 1.7748606204986572, Val Loss: 70.44525909423828, loss : 2.016468048095703\n",
            "epoch: 15, classification_loss: 1.7909107208251953, Val Loss: 70.46718347072601, loss : 2.0361828804016113\n",
            "epoch: 16, classification_loss: 1.7820210456848145, Val Loss: 70.43133759498596, loss : 2.0227773189544678\n",
            "epoch: 17, classification_loss: 1.7789264917373657, Val Loss: 70.39152300357819, loss : 2.0231173038482666\n",
            "epoch: 18, classification_loss: 1.7784377336502075, Val Loss: 70.41928124427795, loss : 2.0115344524383545\n",
            "epoch: 19, classification_loss: 1.7795746326446533, Val Loss: 70.42411756515503, loss : 2.017979860305786\n",
            "Batch: 128, Test Acc: 0.5756209935897436\n",
            "Batch: 129:\n",
            "epoch: 0, classification_loss: 1.8190840482711792, Val Loss: 70.44818449020386, loss : 1.8190840482711792\n",
            "epoch: 1, classification_loss: 1.783707857131958, Val Loss: 70.42811095714569, loss : 2.1173789501190186\n",
            "epoch: 2, classification_loss: 1.7667919397354126, Val Loss: 70.4538459777832, loss : 2.031370162963867\n",
            "epoch: 3, classification_loss: 1.7883542776107788, Val Loss: 70.4755848646164, loss : 2.0678796768188477\n",
            "epoch: 4, classification_loss: 1.8061219453811646, Val Loss: 70.45430779457092, loss : 2.052072525024414\n",
            "epoch: 5, classification_loss: 1.8076369762420654, Val Loss: 70.45671510696411, loss : 2.0707015991210938\n",
            "epoch: 6, classification_loss: 1.790092945098877, Val Loss: 70.464888215065, loss : 2.056152820587158\n",
            "epoch: 7, classification_loss: 1.782586693763733, Val Loss: 70.48095321655273, loss : 2.0483477115631104\n",
            "epoch: 8, classification_loss: 1.7935559749603271, Val Loss: 70.46549963951111, loss : 2.050358772277832\n",
            "epoch: 9, classification_loss: 1.7855162620544434, Val Loss: 70.44133424758911, loss : 2.038309097290039\n",
            "epoch: 10, classification_loss: 1.7905089855194092, Val Loss: 70.46277332305908, loss : 2.0350027084350586\n",
            "epoch: 11, classification_loss: 1.8011059761047363, Val Loss: 70.4805155992508, loss : 2.038424015045166\n",
            "epoch: 12, classification_loss: 1.8023663759231567, Val Loss: 70.47107791900635, loss : 2.0390024185180664\n",
            "epoch: 13, classification_loss: 1.788151502609253, Val Loss: 70.45415115356445, loss : 2.030428886413574\n",
            "epoch: 14, classification_loss: 1.795696496963501, Val Loss: 70.51695430278778, loss : 2.035166025161743\n",
            "epoch: 15, classification_loss: 1.785317301750183, Val Loss: 70.45728600025177, loss : 2.022918462753296\n",
            "epoch: 16, classification_loss: 1.7953041791915894, Val Loss: 70.47807812690735, loss : 2.026393413543701\n",
            "epoch: 17, classification_loss: 1.7904797792434692, Val Loss: 70.4754296541214, loss : 2.024199962615967\n",
            "epoch: 18, classification_loss: 1.7983590364456177, Val Loss: 70.48456239700317, loss : 2.0238230228424072\n",
            "epoch: 19, classification_loss: 1.8073501586914062, Val Loss: 70.50940072536469, loss : 2.0391507148742676\n",
            "Batch: 129, Test Acc: 0.5716145833333334\n",
            "Batch: 130:\n",
            "epoch: 0, classification_loss: 1.8142430782318115, Val Loss: 70.43463051319122, loss : 1.8142430782318115\n",
            "epoch: 1, classification_loss: 1.7713496685028076, Val Loss: 70.39268743991852, loss : 2.1081244945526123\n",
            "epoch: 2, classification_loss: 1.755358099937439, Val Loss: 70.49991345405579, loss : 2.0125701427459717\n",
            "epoch: 3, classification_loss: 1.7769145965576172, Val Loss: 70.48997592926025, loss : 2.054664134979248\n",
            "epoch: 4, classification_loss: 1.8027442693710327, Val Loss: 70.45711469650269, loss : 2.0445170402526855\n",
            "epoch: 5, classification_loss: 1.7867425680160522, Val Loss: 70.44414925575256, loss : 2.049015522003174\n",
            "epoch: 6, classification_loss: 1.7737280130386353, Val Loss: 70.41968476772308, loss : 2.037266254425049\n",
            "epoch: 7, classification_loss: 1.7824264764785767, Val Loss: 70.45384657382965, loss : 2.0429468154907227\n",
            "epoch: 8, classification_loss: 1.7867779731750488, Val Loss: 70.45043528079987, loss : 2.0327961444854736\n",
            "epoch: 9, classification_loss: 1.7780723571777344, Val Loss: 70.47158181667328, loss : 2.026418924331665\n",
            "epoch: 10, classification_loss: 1.7916698455810547, Val Loss: 70.4667854309082, loss : 2.031672239303589\n",
            "epoch: 11, classification_loss: 1.7826688289642334, Val Loss: 70.48108804225922, loss : 2.020707368850708\n",
            "epoch: 12, classification_loss: 1.7870159149169922, Val Loss: 70.45560348033905, loss : 2.0187957286834717\n",
            "epoch: 13, classification_loss: 1.7876423597335815, Val Loss: 70.44936978816986, loss : 2.023413896560669\n",
            "epoch: 14, classification_loss: 1.7923524379730225, Val Loss: 70.52416980266571, loss : 2.018561840057373\n",
            "epoch: 15, classification_loss: 1.7782548666000366, Val Loss: 70.53741645812988, loss : 2.0101685523986816\n",
            "epoch: 16, classification_loss: 1.7887790203094482, Val Loss: 70.49514031410217, loss : 2.007519483566284\n",
            "epoch: 17, classification_loss: 1.7942088842391968, Val Loss: 70.4746503829956, loss : 2.0232725143432617\n",
            "epoch: 18, classification_loss: 1.787941575050354, Val Loss: 70.52516007423401, loss : 2.0008912086486816\n",
            "epoch: 19, classification_loss: 1.7955572605133057, Val Loss: 70.52985560894012, loss : 2.022613763809204\n",
            "Batch: 130, Test Acc: 0.5740184294871795\n",
            "Batch: 131:\n",
            "epoch: 0, classification_loss: 1.8553736209869385, Val Loss: 70.3928691148758, loss : 1.8553736209869385\n",
            "epoch: 1, classification_loss: 1.8267300128936768, Val Loss: 70.44366085529327, loss : 2.1537961959838867\n",
            "epoch: 2, classification_loss: 1.804121971130371, Val Loss: 70.44504880905151, loss : 2.069366931915283\n",
            "epoch: 3, classification_loss: 1.8183268308639526, Val Loss: 70.41229951381683, loss : 2.1003472805023193\n",
            "epoch: 4, classification_loss: 1.8596088886260986, Val Loss: 70.42541241645813, loss : 2.100095748901367\n",
            "epoch: 5, classification_loss: 1.8431293964385986, Val Loss: 70.50889146327972, loss : 2.106729507446289\n",
            "epoch: 6, classification_loss: 1.8303943872451782, Val Loss: 70.55173707008362, loss : 2.0931713581085205\n",
            "epoch: 7, classification_loss: 1.832452654838562, Val Loss: 70.4903575181961, loss : 2.096151828765869\n",
            "epoch: 8, classification_loss: 1.8338731527328491, Val Loss: 70.48646676540375, loss : 2.0914723873138428\n",
            "epoch: 9, classification_loss: 1.8278905153274536, Val Loss: 70.45249629020691, loss : 2.0861191749572754\n",
            "epoch: 10, classification_loss: 1.8490725755691528, Val Loss: 70.51234757900238, loss : 2.101968765258789\n",
            "epoch: 11, classification_loss: 1.8371509313583374, Val Loss: 70.47264683246613, loss : 2.088132858276367\n",
            "epoch: 12, classification_loss: 1.841874599456787, Val Loss: 70.50121331214905, loss : 2.088020086288452\n",
            "epoch: 13, classification_loss: 1.846766471862793, Val Loss: 70.48758280277252, loss : 2.100337266921997\n",
            "epoch: 14, classification_loss: 1.840709924697876, Val Loss: 70.54565024375916, loss : 2.087003231048584\n",
            "epoch: 15, classification_loss: 1.8374531269073486, Val Loss: 70.45908415317535, loss : 2.0908641815185547\n",
            "epoch: 16, classification_loss: 1.832194447517395, Val Loss: 70.48018097877502, loss : 2.0843122005462646\n",
            "epoch: 17, classification_loss: 1.8230974674224854, Val Loss: 70.5527560710907, loss : 2.0804316997528076\n",
            "epoch: 18, classification_loss: 1.8366979360580444, Val Loss: 70.49594116210938, loss : 2.0806596279144287\n",
            "epoch: 19, classification_loss: 1.8567556142807007, Val Loss: 70.51071643829346, loss : 2.101435661315918\n",
            "Batch: 131, Test Acc: 0.5765224358974359\n",
            "Batch: 132:\n",
            "epoch: 0, classification_loss: 1.8144153356552124, Val Loss: 70.41190826892853, loss : 1.8144153356552124\n",
            "epoch: 1, classification_loss: 1.7790696620941162, Val Loss: 70.46969747543335, loss : 2.115687847137451\n",
            "epoch: 2, classification_loss: 1.755013346672058, Val Loss: 70.48246884346008, loss : 2.0324814319610596\n",
            "epoch: 3, classification_loss: 1.7869099378585815, Val Loss: 70.45244073867798, loss : 2.0625197887420654\n",
            "epoch: 4, classification_loss: 1.8151713609695435, Val Loss: 70.40455794334412, loss : 2.0605127811431885\n",
            "epoch: 5, classification_loss: 1.798099160194397, Val Loss: 70.45160448551178, loss : 2.0598084926605225\n",
            "epoch: 6, classification_loss: 1.782185435295105, Val Loss: 70.44137501716614, loss : 2.048283338546753\n",
            "epoch: 7, classification_loss: 1.7880293130874634, Val Loss: 70.45806765556335, loss : 2.049936294555664\n",
            "epoch: 8, classification_loss: 1.787232518196106, Val Loss: 70.44371545314789, loss : 2.0419392585754395\n",
            "epoch: 9, classification_loss: 1.7858964204788208, Val Loss: 70.445587515831, loss : 2.039172649383545\n",
            "epoch: 10, classification_loss: 1.8035438060760498, Val Loss: 70.46920418739319, loss : 2.042586326599121\n",
            "epoch: 11, classification_loss: 1.7915418148040771, Val Loss: 70.50744664669037, loss : 2.0340938568115234\n",
            "epoch: 12, classification_loss: 1.798963189125061, Val Loss: 70.44864857196808, loss : 2.036187171936035\n",
            "epoch: 13, classification_loss: 1.7849493026733398, Val Loss: 70.44231462478638, loss : 2.026825428009033\n",
            "epoch: 14, classification_loss: 1.7890841960906982, Val Loss: 70.502614736557, loss : 2.0235466957092285\n",
            "epoch: 15, classification_loss: 1.7848610877990723, Val Loss: 70.51845121383667, loss : 2.0232388973236084\n",
            "epoch: 16, classification_loss: 1.79366135597229, Val Loss: 70.47367143630981, loss : 2.0251893997192383\n",
            "epoch: 17, classification_loss: 1.7947320938110352, Val Loss: 70.46053504943848, loss : 2.0255300998687744\n",
            "epoch: 18, classification_loss: 1.7967169284820557, Val Loss: 70.47767388820648, loss : 2.0199081897735596\n",
            "epoch: 19, classification_loss: 1.7958917617797852, Val Loss: 70.46526992321014, loss : 2.0295567512512207\n",
            "Batch: 132, Test Acc: 0.5765224358974359\n",
            "Batch: 133:\n",
            "epoch: 0, classification_loss: 1.7942379713058472, Val Loss: 70.4378491640091, loss : 1.7942379713058472\n",
            "epoch: 1, classification_loss: 1.752059817314148, Val Loss: 70.4373254776001, loss : 2.0848052501678467\n",
            "epoch: 2, classification_loss: 1.7283861637115479, Val Loss: 70.43461668491364, loss : 1.9948171377182007\n",
            "epoch: 3, classification_loss: 1.7672476768493652, Val Loss: 70.43672704696655, loss : 2.0444178581237793\n",
            "epoch: 4, classification_loss: 1.7777286767959595, Val Loss: 70.42777812480927, loss : 2.022796392440796\n",
            "epoch: 5, classification_loss: 1.7933827638626099, Val Loss: 70.45893394947052, loss : 2.0486066341400146\n",
            "epoch: 6, classification_loss: 1.7637360095977783, Val Loss: 70.42767930030823, loss : 2.028715133666992\n",
            "epoch: 7, classification_loss: 1.7687134742736816, Val Loss: 70.45419096946716, loss : 2.0381433963775635\n",
            "epoch: 8, classification_loss: 1.7594077587127686, Val Loss: 70.44317150115967, loss : 2.029038667678833\n",
            "epoch: 9, classification_loss: 1.7516049146652222, Val Loss: 70.4243038892746, loss : 2.014749050140381\n",
            "epoch: 10, classification_loss: 1.7611541748046875, Val Loss: 70.42341220378876, loss : 2.0151500701904297\n",
            "epoch: 11, classification_loss: 1.774285078048706, Val Loss: 70.42661833763123, loss : 2.020409107208252\n",
            "epoch: 12, classification_loss: 1.780635952949524, Val Loss: 70.47740399837494, loss : 2.024554967880249\n",
            "epoch: 13, classification_loss: 1.774577260017395, Val Loss: 70.4481543302536, loss : 2.027285099029541\n",
            "epoch: 14, classification_loss: 1.7683080434799194, Val Loss: 70.46085953712463, loss : 2.020150899887085\n",
            "epoch: 15, classification_loss: 1.7673835754394531, Val Loss: 70.40016496181488, loss : 2.017997980117798\n",
            "epoch: 16, classification_loss: 1.7787529230117798, Val Loss: 70.44144880771637, loss : 2.0212762355804443\n",
            "epoch: 17, classification_loss: 1.7725918292999268, Val Loss: 70.49844479560852, loss : 2.0177152156829834\n",
            "epoch: 18, classification_loss: 1.7729132175445557, Val Loss: 70.45405149459839, loss : 2.0163111686706543\n",
            "epoch: 19, classification_loss: 1.7781215906143188, Val Loss: 70.43133664131165, loss : 2.025560140609741\n",
            "Batch: 133, Test Acc: 0.575020032051282\n",
            "Batch: 134:\n",
            "epoch: 0, classification_loss: 1.8005037307739258, Val Loss: 70.50320756435394, loss : 1.8005037307739258\n",
            "epoch: 1, classification_loss: 1.7616357803344727, Val Loss: 70.47651672363281, loss : 2.0907466411590576\n",
            "epoch: 2, classification_loss: 1.7558101415634155, Val Loss: 70.41641783714294, loss : 2.0035593509674072\n",
            "epoch: 3, classification_loss: 1.7931532859802246, Val Loss: 70.42228960990906, loss : 2.0669333934783936\n",
            "epoch: 4, classification_loss: 1.7915490865707397, Val Loss: 70.45609128475189, loss : 2.0189740657806396\n",
            "epoch: 5, classification_loss: 1.7808045148849487, Val Loss: 70.46474611759186, loss : 2.047481060028076\n",
            "epoch: 6, classification_loss: 1.7651909589767456, Val Loss: 70.42483270168304, loss : 2.0417070388793945\n",
            "epoch: 7, classification_loss: 1.7764004468917847, Val Loss: 70.44467163085938, loss : 2.036233425140381\n",
            "epoch: 8, classification_loss: 1.7789647579193115, Val Loss: 70.44307291507721, loss : 2.048676013946533\n",
            "epoch: 9, classification_loss: 1.7842864990234375, Val Loss: 70.47425937652588, loss : 2.0350801944732666\n",
            "epoch: 10, classification_loss: 1.782441258430481, Val Loss: 70.46926963329315, loss : 2.037513256072998\n",
            "epoch: 11, classification_loss: 1.7755043506622314, Val Loss: 70.44804036617279, loss : 2.0170326232910156\n",
            "epoch: 12, classification_loss: 1.7786908149719238, Val Loss: 70.4333838224411, loss : 2.0240800380706787\n",
            "epoch: 13, classification_loss: 1.7761893272399902, Val Loss: 70.47370874881744, loss : 2.0218868255615234\n",
            "epoch: 14, classification_loss: 1.7771143913269043, Val Loss: 70.49465799331665, loss : 2.0224273204803467\n",
            "epoch: 15, classification_loss: 1.782647728919983, Val Loss: 70.44391524791718, loss : 2.0244369506835938\n",
            "epoch: 16, classification_loss: 1.7833727598190308, Val Loss: 70.48210847377777, loss : 2.014721393585205\n",
            "epoch: 17, classification_loss: 1.7846009731292725, Val Loss: 70.45731687545776, loss : 2.013789415359497\n",
            "epoch: 18, classification_loss: 1.777764081954956, Val Loss: 70.4806958436966, loss : 2.006889581680298\n",
            "epoch: 19, classification_loss: 1.7842423915863037, Val Loss: 70.4545509815216, loss : 2.0201876163482666\n",
            "Batch: 134, Test Acc: 0.5768229166666666\n",
            "Batch: 135:\n",
            "epoch: 0, classification_loss: 1.759786605834961, Val Loss: 70.4034731388092, loss : 1.759786605834961\n",
            "epoch: 1, classification_loss: 1.7400692701339722, Val Loss: 70.42899227142334, loss : 2.057410717010498\n",
            "epoch: 2, classification_loss: 1.7220995426177979, Val Loss: 70.44860672950745, loss : 1.952697515487671\n",
            "epoch: 3, classification_loss: 1.7603449821472168, Val Loss: 70.46721279621124, loss : 2.02473783493042\n",
            "epoch: 4, classification_loss: 1.7652385234832764, Val Loss: 70.43131816387177, loss : 1.9829118251800537\n",
            "epoch: 5, classification_loss: 1.7498899698257446, Val Loss: 70.48239493370056, loss : 2.015563488006592\n",
            "epoch: 6, classification_loss: 1.742045283317566, Val Loss: 70.50578045845032, loss : 2.0095176696777344\n",
            "epoch: 7, classification_loss: 1.7385817766189575, Val Loss: 70.51622331142426, loss : 1.9864566326141357\n",
            "epoch: 8, classification_loss: 1.763892412185669, Val Loss: 70.50747740268707, loss : 2.0116796493530273\n",
            "epoch: 9, classification_loss: 1.7518739700317383, Val Loss: 70.54746687412262, loss : 1.9886001348495483\n",
            "epoch: 10, classification_loss: 1.7440942525863647, Val Loss: 70.5650532245636, loss : 1.9886648654937744\n",
            "epoch: 11, classification_loss: 1.7417852878570557, Val Loss: 70.52664959430695, loss : 1.984114408493042\n",
            "epoch: 12, classification_loss: 1.745473861694336, Val Loss: 70.5627110004425, loss : 1.98274827003479\n",
            "epoch: 13, classification_loss: 1.752793550491333, Val Loss: 70.54010677337646, loss : 1.9821381568908691\n",
            "epoch: 14, classification_loss: 1.7479023933410645, Val Loss: 70.5372873544693, loss : 1.9822853803634644\n",
            "epoch: 15, classification_loss: 1.7316546440124512, Val Loss: 70.56611430644989, loss : 1.964359998703003\n",
            "epoch: 16, classification_loss: 1.7438937425613403, Val Loss: 70.69036257266998, loss : 1.9734909534454346\n",
            "epoch: 17, classification_loss: 1.7406508922576904, Val Loss: 70.58269202709198, loss : 1.9691039323806763\n",
            "epoch: 18, classification_loss: 1.7561110258102417, Val Loss: 70.51218640804291, loss : 1.97661292552948\n",
            "epoch: 19, classification_loss: 1.7412201166152954, Val Loss: 70.60536706447601, loss : 1.9690812826156616\n",
            "Batch: 135, Test Acc: 0.5785256410256411\n",
            "Batch: 136:\n",
            "epoch: 0, classification_loss: 1.8520170450210571, Val Loss: 70.4444637298584, loss : 1.8520170450210571\n",
            "epoch: 1, classification_loss: 1.8169749975204468, Val Loss: 70.42726039886475, loss : 2.140939235687256\n",
            "epoch: 2, classification_loss: 1.79598867893219, Val Loss: 70.43020331859589, loss : 2.047600507736206\n",
            "epoch: 3, classification_loss: 1.8351755142211914, Val Loss: 70.40905153751373, loss : 2.1021604537963867\n",
            "epoch: 4, classification_loss: 1.8423371315002441, Val Loss: 70.41401493549347, loss : 2.0746421813964844\n",
            "epoch: 5, classification_loss: 1.840989589691162, Val Loss: 70.43782019615173, loss : 2.0915937423706055\n",
            "epoch: 6, classification_loss: 1.8214198350906372, Val Loss: 70.4323970079422, loss : 2.0847578048706055\n",
            "epoch: 7, classification_loss: 1.8356209993362427, Val Loss: 70.41252744197845, loss : 2.0800890922546387\n",
            "epoch: 8, classification_loss: 1.828276515007019, Val Loss: 70.41426432132721, loss : 2.0771327018737793\n",
            "epoch: 9, classification_loss: 1.8254599571228027, Val Loss: 70.40212333202362, loss : 2.0728061199188232\n",
            "epoch: 10, classification_loss: 1.8322582244873047, Val Loss: 70.43243682384491, loss : 2.074814796447754\n",
            "epoch: 11, classification_loss: 1.834246277809143, Val Loss: 70.43953025341034, loss : 2.0732264518737793\n",
            "epoch: 12, classification_loss: 1.83320951461792, Val Loss: 70.43686258792877, loss : 2.0614917278289795\n",
            "epoch: 13, classification_loss: 1.8304462432861328, Val Loss: 70.42492210865021, loss : 2.0696470737457275\n",
            "epoch: 14, classification_loss: 1.8337886333465576, Val Loss: 70.43150079250336, loss : 2.0689663887023926\n",
            "epoch: 15, classification_loss: 1.8354442119598389, Val Loss: 70.46314299106598, loss : 2.0723655223846436\n",
            "epoch: 16, classification_loss: 1.835294246673584, Val Loss: 70.43018198013306, loss : 2.065284013748169\n",
            "epoch: 17, classification_loss: 1.8404806852340698, Val Loss: 70.44856631755829, loss : 2.0800538063049316\n",
            "epoch: 18, classification_loss: 1.8298091888427734, Val Loss: 70.5132497549057, loss : 2.0548880100250244\n",
            "epoch: 19, classification_loss: 1.822371244430542, Val Loss: 70.42984676361084, loss : 2.0615146160125732\n",
            "Batch: 136, Test Acc: 0.5745192307692307\n",
            "Batch: 137:\n",
            "epoch: 0, classification_loss: 1.820656180381775, Val Loss: 70.4935497045517, loss : 1.820656180381775\n",
            "epoch: 1, classification_loss: 1.7878875732421875, Val Loss: 70.64782249927521, loss : 2.1108312606811523\n",
            "epoch: 2, classification_loss: 1.768493413925171, Val Loss: 70.45808291435242, loss : 2.03676700592041\n",
            "epoch: 3, classification_loss: 1.7963528633117676, Val Loss: 70.4509847164154, loss : 2.0647647380828857\n",
            "epoch: 4, classification_loss: 1.8245508670806885, Val Loss: 70.40394473075867, loss : 2.0630431175231934\n",
            "epoch: 5, classification_loss: 1.808881402015686, Val Loss: 70.38954031467438, loss : 2.0622098445892334\n",
            "epoch: 6, classification_loss: 1.7970343828201294, Val Loss: 70.42172241210938, loss : 2.05649995803833\n",
            "epoch: 7, classification_loss: 1.7860732078552246, Val Loss: 70.44241142272949, loss : 2.0368337631225586\n",
            "epoch: 8, classification_loss: 1.8048526048660278, Val Loss: 70.4505707025528, loss : 2.053941249847412\n",
            "epoch: 9, classification_loss: 1.7977042198181152, Val Loss: 70.43080031871796, loss : 2.049569606781006\n",
            "epoch: 10, classification_loss: 1.8031392097473145, Val Loss: 70.48545908927917, loss : 2.0407867431640625\n",
            "epoch: 11, classification_loss: 1.8080657720565796, Val Loss: 70.45360553264618, loss : 2.0518200397491455\n",
            "epoch: 12, classification_loss: 1.807566523551941, Val Loss: 70.4354876279831, loss : 2.040555238723755\n",
            "epoch: 13, classification_loss: 1.8040169477462769, Val Loss: 70.43650448322296, loss : 2.0456345081329346\n",
            "epoch: 14, classification_loss: 1.8015599250793457, Val Loss: 70.45625376701355, loss : 2.0394411087036133\n",
            "epoch: 15, classification_loss: 1.8014284372329712, Val Loss: 70.47243320941925, loss : 2.0518851280212402\n",
            "epoch: 16, classification_loss: 1.7920820713043213, Val Loss: 70.47882199287415, loss : 2.0281333923339844\n",
            "epoch: 17, classification_loss: 1.8019447326660156, Val Loss: 70.45888543128967, loss : 2.0445542335510254\n",
            "epoch: 18, classification_loss: 1.8143309354782104, Val Loss: 70.47097373008728, loss : 2.032041072845459\n",
            "epoch: 19, classification_loss: 1.792802333831787, Val Loss: 70.46317076683044, loss : 2.040300130844116\n",
            "Batch: 137, Test Acc: 0.5777243589743589\n",
            "Batch: 138:\n",
            "epoch: 0, classification_loss: 1.8129099607467651, Val Loss: 70.44493079185486, loss : 1.8129099607467651\n",
            "epoch: 1, classification_loss: 1.781631350517273, Val Loss: 70.46002948284149, loss : 2.08697772026062\n",
            "epoch: 2, classification_loss: 1.7618262767791748, Val Loss: 70.41155576705933, loss : 1.9913780689239502\n",
            "epoch: 3, classification_loss: 1.793277382850647, Val Loss: 70.40256154537201, loss : 2.053161859512329\n",
            "epoch: 4, classification_loss: 1.794884204864502, Val Loss: 70.38190305233002, loss : 2.0166501998901367\n",
            "epoch: 5, classification_loss: 1.7943074703216553, Val Loss: 70.42869651317596, loss : 2.051576614379883\n",
            "epoch: 6, classification_loss: 1.7807852029800415, Val Loss: 70.42311358451843, loss : 2.036522626876831\n",
            "epoch: 7, classification_loss: 1.7920019626617432, Val Loss: 70.37415790557861, loss : 2.0416905879974365\n",
            "epoch: 8, classification_loss: 1.792612910270691, Val Loss: 70.41447615623474, loss : 2.03501296043396\n",
            "epoch: 9, classification_loss: 1.783278226852417, Val Loss: 70.42734551429749, loss : 2.027284860610962\n",
            "epoch: 10, classification_loss: 1.7904183864593506, Val Loss: 70.44264376163483, loss : 2.02221417427063\n",
            "epoch: 11, classification_loss: 1.7922885417938232, Val Loss: 70.41557562351227, loss : 2.021573781967163\n",
            "epoch: 12, classification_loss: 1.790173888206482, Val Loss: 70.41289460659027, loss : 2.01628041267395\n",
            "epoch: 13, classification_loss: 1.789129614830017, Val Loss: 70.41263020038605, loss : 2.031783103942871\n",
            "epoch: 14, classification_loss: 1.7841506004333496, Val Loss: 70.40369212627411, loss : 2.015565872192383\n",
            "epoch: 15, classification_loss: 1.7872862815856934, Val Loss: 70.41709554195404, loss : 2.0221080780029297\n",
            "epoch: 16, classification_loss: 1.8039863109588623, Val Loss: 70.47911620140076, loss : 2.0308704376220703\n",
            "epoch: 17, classification_loss: 1.7946187257766724, Val Loss: 70.47027111053467, loss : 2.020575761795044\n",
            "epoch: 18, classification_loss: 1.794046401977539, Val Loss: 70.44061994552612, loss : 2.0199520587921143\n",
            "epoch: 19, classification_loss: 1.7817648649215698, Val Loss: 70.41538262367249, loss : 2.0161831378936768\n",
            "Batch: 138, Test Acc: 0.5774238782051282\n",
            "Batch: 139:\n",
            "epoch: 0, classification_loss: 1.7677072286605835, Val Loss: 70.47984528541565, loss : 1.7677072286605835\n",
            "epoch: 1, classification_loss: 1.748214602470398, Val Loss: 70.52113080024719, loss : 2.066185474395752\n",
            "epoch: 2, classification_loss: 1.7241955995559692, Val Loss: 70.49088549613953, loss : 1.9645593166351318\n",
            "epoch: 3, classification_loss: 1.7671868801116943, Val Loss: 70.43018233776093, loss : 2.0248029232025146\n",
            "epoch: 4, classification_loss: 1.77131187915802, Val Loss: 70.45125150680542, loss : 1.9882912635803223\n",
            "epoch: 5, classification_loss: 1.7575750350952148, Val Loss: 70.49199974536896, loss : 2.010969638824463\n",
            "epoch: 6, classification_loss: 1.7398767471313477, Val Loss: 70.50557208061218, loss : 1.9966202974319458\n",
            "epoch: 7, classification_loss: 1.7492088079452515, Val Loss: 70.47654163837433, loss : 1.9911091327667236\n",
            "epoch: 8, classification_loss: 1.763884425163269, Val Loss: 70.52171123027802, loss : 2.000720739364624\n",
            "epoch: 9, classification_loss: 1.7655577659606934, Val Loss: 70.4732357263565, loss : 1.9962401390075684\n",
            "epoch: 10, classification_loss: 1.7464836835861206, Val Loss: 70.43169832229614, loss : 1.9799728393554688\n",
            "epoch: 11, classification_loss: 1.7664639949798584, Val Loss: 70.48789930343628, loss : 1.9943703413009644\n",
            "epoch: 12, classification_loss: 1.759182095527649, Val Loss: 70.4833779335022, loss : 1.9821003675460815\n",
            "epoch: 13, classification_loss: 1.7563796043395996, Val Loss: 70.4608598947525, loss : 1.9836877584457397\n",
            "epoch: 14, classification_loss: 1.7566336393356323, Val Loss: 70.48500335216522, loss : 1.9769173860549927\n",
            "epoch: 15, classification_loss: 1.7541532516479492, Val Loss: 70.53991889953613, loss : 1.9779754877090454\n",
            "epoch: 16, classification_loss: 1.764531135559082, Val Loss: 70.50129687786102, loss : 1.9777791500091553\n",
            "epoch: 17, classification_loss: 1.74966561794281, Val Loss: 70.50550174713135, loss : 1.9846899509429932\n",
            "epoch: 18, classification_loss: 1.7473043203353882, Val Loss: 70.48266005516052, loss : 1.9671941995620728\n",
            "epoch: 19, classification_loss: 1.7516995668411255, Val Loss: 70.52161312103271, loss : 1.9795297384262085\n",
            "Batch: 139, Test Acc: 0.5744190705128205\n",
            "Batch: 140:\n",
            "epoch: 0, classification_loss: 1.8443403244018555, Val Loss: 70.41772627830505, loss : 1.8443403244018555\n",
            "epoch: 1, classification_loss: 1.8162732124328613, Val Loss: 70.44944107532501, loss : 2.124434232711792\n",
            "epoch: 2, classification_loss: 1.797371506690979, Val Loss: 70.45376932621002, loss : 2.03951358795166\n",
            "epoch: 3, classification_loss: 1.8244856595993042, Val Loss: 70.4441956281662, loss : 2.0895071029663086\n",
            "epoch: 4, classification_loss: 1.831911325454712, Val Loss: 70.4255245923996, loss : 2.053978443145752\n",
            "epoch: 5, classification_loss: 1.8313844203948975, Val Loss: 70.40314209461212, loss : 2.086214780807495\n",
            "epoch: 6, classification_loss: 1.8157382011413574, Val Loss: 70.44367849826813, loss : 2.0795164108276367\n",
            "epoch: 7, classification_loss: 1.8152762651443481, Val Loss: 70.4691458940506, loss : 2.0579726696014404\n",
            "epoch: 8, classification_loss: 1.8309732675552368, Val Loss: 70.45834481716156, loss : 2.0806336402893066\n",
            "epoch: 9, classification_loss: 1.8201239109039307, Val Loss: 70.43907928466797, loss : 2.0619494915008545\n",
            "epoch: 10, classification_loss: 1.8216921091079712, Val Loss: 70.53693664073944, loss : 2.0707643032073975\n",
            "epoch: 11, classification_loss: 1.8237740993499756, Val Loss: 70.43229055404663, loss : 2.068369150161743\n",
            "epoch: 12, classification_loss: 1.816941738128662, Val Loss: 70.45662641525269, loss : 2.046346664428711\n",
            "epoch: 13, classification_loss: 1.8187470436096191, Val Loss: 70.47376596927643, loss : 2.0573949813842773\n",
            "epoch: 14, classification_loss: 1.8168085813522339, Val Loss: 70.50756275653839, loss : 2.0483713150024414\n",
            "epoch: 15, classification_loss: 1.820841908454895, Val Loss: 70.44591760635376, loss : 2.0612452030181885\n",
            "epoch: 16, classification_loss: 1.8212156295776367, Val Loss: 70.44281363487244, loss : 2.050276756286621\n",
            "epoch: 17, classification_loss: 1.8338173627853394, Val Loss: 70.4930647611618, loss : 2.0640170574188232\n",
            "epoch: 18, classification_loss: 1.8309314250946045, Val Loss: 70.50645518302917, loss : 2.048445224761963\n",
            "epoch: 19, classification_loss: 1.8234976530075073, Val Loss: 70.44187128543854, loss : 2.0530660152435303\n",
            "Batch: 140, Test Acc: 0.578926282051282\n",
            "Batch: 141:\n",
            "epoch: 0, classification_loss: 1.8268661499023438, Val Loss: 70.40767824649811, loss : 1.8268661499023438\n",
            "epoch: 1, classification_loss: 1.7826776504516602, Val Loss: 70.56973230838776, loss : 2.1064772605895996\n",
            "epoch: 2, classification_loss: 1.7670437097549438, Val Loss: 70.50165832042694, loss : 2.0481693744659424\n",
            "epoch: 3, classification_loss: 1.7843587398529053, Val Loss: 70.46459066867828, loss : 2.0558359622955322\n",
            "epoch: 4, classification_loss: 1.8128379583358765, Val Loss: 70.3868625164032, loss : 2.0521228313446045\n",
            "epoch: 5, classification_loss: 1.8035603761672974, Val Loss: 70.39837265014648, loss : 2.0533881187438965\n",
            "epoch: 6, classification_loss: 1.793813943862915, Val Loss: 70.42702746391296, loss : 2.0559518337249756\n",
            "epoch: 7, classification_loss: 1.7905828952789307, Val Loss: 70.419189453125, loss : 2.055238723754883\n",
            "epoch: 8, classification_loss: 1.7961682081222534, Val Loss: 70.44096970558167, loss : 2.048023223876953\n",
            "epoch: 9, classification_loss: 1.790313959121704, Val Loss: 70.4483894109726, loss : 2.0471136569976807\n",
            "epoch: 10, classification_loss: 1.7931556701660156, Val Loss: 70.42780494689941, loss : 2.039217472076416\n",
            "epoch: 11, classification_loss: 1.7925773859024048, Val Loss: 70.42195248603821, loss : 2.029756546020508\n",
            "epoch: 12, classification_loss: 1.7824726104736328, Val Loss: 70.40318512916565, loss : 2.020151376724243\n",
            "epoch: 13, classification_loss: 1.7972469329833984, Val Loss: 70.43076062202454, loss : 2.035686731338501\n",
            "epoch: 14, classification_loss: 1.7995691299438477, Val Loss: 70.42778897285461, loss : 2.038760185241699\n",
            "epoch: 15, classification_loss: 1.793681025505066, Val Loss: 70.42017066478729, loss : 2.026752471923828\n",
            "epoch: 16, classification_loss: 1.7975008487701416, Val Loss: 70.44189608097076, loss : 2.031339406967163\n",
            "epoch: 17, classification_loss: 1.803332805633545, Val Loss: 70.48133969306946, loss : 2.0363662242889404\n",
            "epoch: 18, classification_loss: 1.802873969078064, Val Loss: 70.45774233341217, loss : 2.0323164463043213\n",
            "epoch: 19, classification_loss: 1.8010263442993164, Val Loss: 70.40654444694519, loss : 2.029737710952759\n",
            "Batch: 141, Test Acc: 0.5796274038461539\n",
            "Batch: 142:\n",
            "epoch: 0, classification_loss: 1.836625099182129, Val Loss: 70.47649359703064, loss : 1.836625099182129\n",
            "epoch: 1, classification_loss: 1.793493390083313, Val Loss: 70.46142554283142, loss : 2.130401134490967\n",
            "epoch: 2, classification_loss: 1.7761648893356323, Val Loss: 70.40909337997437, loss : 2.055293321609497\n",
            "epoch: 3, classification_loss: 1.7975836992263794, Val Loss: 70.42065262794495, loss : 2.0822787284851074\n",
            "epoch: 4, classification_loss: 1.8159539699554443, Val Loss: 70.40979552268982, loss : 2.0744051933288574\n",
            "epoch: 5, classification_loss: 1.8126150369644165, Val Loss: 70.37183952331543, loss : 2.0657708644866943\n",
            "epoch: 6, classification_loss: 1.7964125871658325, Val Loss: 70.47172713279724, loss : 1.7964125871658325\n",
            "epoch: 7, classification_loss: 1.764955997467041, Val Loss: 70.5388400554657, loss : 2.1150102615356445\n",
            "epoch: 8, classification_loss: 1.7435098886489868, Val Loss: 70.46448969841003, loss : 2.0521445274353027\n",
            "epoch: 9, classification_loss: 1.7748650312423706, Val Loss: 70.40712201595306, loss : 2.029278516769409\n",
            "epoch: 10, classification_loss: 1.7993086576461792, Val Loss: 70.38240969181061, loss : 2.05635929107666\n",
            "epoch: 11, classification_loss: 1.790000557899475, Val Loss: 70.43665945529938, loss : 2.031292200088501\n",
            "epoch: 12, classification_loss: 1.7682777643203735, Val Loss: 70.42895710468292, loss : 2.0332584381103516\n",
            "epoch: 13, classification_loss: 1.7848433256149292, Val Loss: 70.39649748802185, loss : 2.0506784915924072\n",
            "epoch: 14, classification_loss: 1.7766366004943848, Val Loss: 70.41594362258911, loss : 2.035107374191284\n",
            "epoch: 15, classification_loss: 1.786547064781189, Val Loss: 70.409827709198, loss : 2.023421287536621\n",
            "epoch: 16, classification_loss: 1.778122901916504, Val Loss: 70.40853106975555, loss : 2.0338525772094727\n",
            "epoch: 17, classification_loss: 1.7763009071350098, Val Loss: 70.39234411716461, loss : 2.018598794937134\n",
            "epoch: 18, classification_loss: 1.7718522548675537, Val Loss: 70.45558273792267, loss : 2.025144100189209\n",
            "epoch: 19, classification_loss: 1.7863128185272217, Val Loss: 70.40282320976257, loss : 2.0203657150268555\n",
            "Batch: 142, Test Acc: 0.5788261217948718\n",
            "Batch: 143:\n",
            "epoch: 0, classification_loss: 1.8367217779159546, Val Loss: 70.4109628200531, loss : 1.8367217779159546\n",
            "epoch: 1, classification_loss: 1.7997627258300781, Val Loss: 70.3969259262085, loss : 2.143028736114502\n",
            "epoch: 2, classification_loss: 1.7727373838424683, Val Loss: 70.41951441764832, loss : 2.053046941757202\n",
            "epoch: 3, classification_loss: 1.7976446151733398, Val Loss: 70.40991151332855, loss : 2.087679147720337\n",
            "epoch: 4, classification_loss: 1.819122314453125, Val Loss: 70.44994056224823, loss : 2.0905301570892334\n",
            "epoch: 5, classification_loss: 1.8350257873535156, Val Loss: 70.43139731884003, loss : 2.092711925506592\n",
            "epoch: 6, classification_loss: 1.8063467741012573, Val Loss: 70.3934371471405, loss : 2.087547540664673\n",
            "epoch: 7, classification_loss: 1.7966922521591187, Val Loss: 70.43029749393463, loss : 2.064267158508301\n",
            "epoch: 8, classification_loss: 1.7987382411956787, Val Loss: 70.45308494567871, loss : 2.0840044021606445\n",
            "epoch: 9, classification_loss: 1.7922388315200806, Val Loss: 70.41778874397278, loss : 2.0685977935791016\n",
            "epoch: 10, classification_loss: 1.796371340751648, Val Loss: 70.4129821062088, loss : 2.056575298309326\n",
            "epoch: 11, classification_loss: 1.8172625303268433, Val Loss: 70.42497289180756, loss : 2.073336362838745\n",
            "epoch: 12, classification_loss: 1.800856113433838, Val Loss: 70.48382294178009, loss : 2.04689884185791\n",
            "epoch: 13, classification_loss: 1.806880235671997, Val Loss: 70.40412902832031, loss : 2.065082311630249\n",
            "epoch: 14, classification_loss: 1.8062204122543335, Val Loss: 70.38993442058563, loss : 2.0539591312408447\n",
            "epoch: 15, classification_loss: 1.8036130666732788, Val Loss: 70.40819728374481, loss : 2.0619709491729736\n",
            "epoch: 16, classification_loss: 1.794146180152893, Val Loss: 70.42888259887695, loss : 2.0367772579193115\n",
            "epoch: 17, classification_loss: 1.7997767925262451, Val Loss: 70.4267657995224, loss : 2.0455899238586426\n",
            "epoch: 18, classification_loss: 1.8081809282302856, Val Loss: 70.42502224445343, loss : 2.0490190982818604\n",
            "epoch: 19, classification_loss: 1.8001693487167358, Val Loss: 70.39952421188354, loss : 2.0476622581481934\n",
            "Batch: 143, Test Acc: 0.5753205128205128\n",
            "Batch: 144:\n",
            "epoch: 0, classification_loss: 1.8028852939605713, Val Loss: 70.44036960601807, loss : 1.8028852939605713\n",
            "epoch: 1, classification_loss: 1.7726565599441528, Val Loss: 70.45307803153992, loss : 2.093078851699829\n",
            "epoch: 2, classification_loss: 1.7407258749008179, Val Loss: 70.48946452140808, loss : 1.9981114864349365\n",
            "epoch: 3, classification_loss: 1.7790008783340454, Val Loss: 70.40224599838257, loss : 2.054987668991089\n",
            "epoch: 4, classification_loss: 1.7893744707107544, Val Loss: 70.42837464809418, loss : 2.017522096633911\n",
            "epoch: 5, classification_loss: 1.780519962310791, Val Loss: 70.44902622699738, loss : 2.056281566619873\n",
            "epoch: 6, classification_loss: 1.7668498754501343, Val Loss: 70.40952551364899, loss : 2.0436010360717773\n",
            "epoch: 7, classification_loss: 1.78411865234375, Val Loss: 70.41958940029144, loss : 2.038377523422241\n",
            "epoch: 8, classification_loss: 1.7831859588623047, Val Loss: 70.46903133392334, loss : 2.036151170730591\n",
            "epoch: 9, classification_loss: 1.773620843887329, Val Loss: 70.51707804203033, loss : 2.02388858795166\n",
            "epoch: 10, classification_loss: 1.7767308950424194, Val Loss: 70.43212068080902, loss : 2.0251529216766357\n",
            "epoch: 11, classification_loss: 1.782274603843689, Val Loss: 70.46690368652344, loss : 2.025705575942993\n",
            "epoch: 12, classification_loss: 1.7781075239181519, Val Loss: 70.46851444244385, loss : 2.015655279159546\n",
            "epoch: 13, classification_loss: 1.7809675931930542, Val Loss: 70.51644718647003, loss : 2.0205905437469482\n",
            "epoch: 14, classification_loss: 1.786744475364685, Val Loss: 70.49555838108063, loss : 2.0226268768310547\n",
            "epoch: 15, classification_loss: 1.7837834358215332, Val Loss: 70.46943366527557, loss : 2.027254104614258\n",
            "epoch: 16, classification_loss: 1.7887146472930908, Val Loss: 70.46850991249084, loss : 2.0225021839141846\n",
            "epoch: 17, classification_loss: 1.7805224657058716, Val Loss: 70.4599038362503, loss : 2.0091164112091064\n",
            "epoch: 18, classification_loss: 1.784264087677002, Val Loss: 70.46820521354675, loss : 2.0104005336761475\n",
            "epoch: 19, classification_loss: 1.7805871963500977, Val Loss: 70.47847497463226, loss : 2.014249086380005\n",
            "Batch: 144, Test Acc: 0.5744190705128205\n",
            "Batch: 145:\n",
            "epoch: 0, classification_loss: 1.8367056846618652, Val Loss: 70.4267008304596, loss : 1.8367056846618652\n",
            "epoch: 1, classification_loss: 1.8149453401565552, Val Loss: 70.54627776145935, loss : 2.140784978866577\n",
            "epoch: 2, classification_loss: 1.7897307872772217, Val Loss: 70.52795910835266, loss : 2.043928861618042\n",
            "epoch: 3, classification_loss: 1.8142828941345215, Val Loss: 70.40380036830902, loss : 2.093808174133301\n",
            "epoch: 4, classification_loss: 1.8450121879577637, Val Loss: 70.4236820936203, loss : 2.0848188400268555\n",
            "epoch: 5, classification_loss: 1.8217079639434814, Val Loss: 70.45854806900024, loss : 2.088350534439087\n",
            "epoch: 6, classification_loss: 1.8201004266738892, Val Loss: 70.43607807159424, loss : 2.1012558937072754\n",
            "epoch: 7, classification_loss: 1.8019306659698486, Val Loss: 70.42343759536743, loss : 2.0699234008789062\n",
            "epoch: 8, classification_loss: 1.8265482187271118, Val Loss: 70.46888768672943, loss : 2.0783488750457764\n",
            "epoch: 9, classification_loss: 1.8253144025802612, Val Loss: 70.45652866363525, loss : 2.0800633430480957\n",
            "epoch: 10, classification_loss: 1.826278805732727, Val Loss: 70.43673622608185, loss : 2.0683155059814453\n",
            "epoch: 11, classification_loss: 1.822762370109558, Val Loss: 70.43570375442505, loss : 2.0749948024749756\n",
            "epoch: 12, classification_loss: 1.8096907138824463, Val Loss: 70.44498383998871, loss : 2.05059814453125\n",
            "epoch: 13, classification_loss: 1.829360008239746, Val Loss: 70.4447431564331, loss : 2.082979202270508\n",
            "epoch: 14, classification_loss: 1.8172472715377808, Val Loss: 70.42685317993164, loss : 2.0615549087524414\n",
            "epoch: 15, classification_loss: 1.822594404220581, Val Loss: 70.45446109771729, loss : 2.06796932220459\n",
            "epoch: 16, classification_loss: 1.8211134672164917, Val Loss: 70.49271047115326, loss : 2.0604500770568848\n",
            "epoch: 17, classification_loss: 1.8224937915802002, Val Loss: 70.47009921073914, loss : 2.063500165939331\n",
            "epoch: 18, classification_loss: 1.8177392482757568, Val Loss: 70.4266881942749, loss : 2.0570785999298096\n",
            "epoch: 19, classification_loss: 1.823276162147522, Val Loss: 70.43118226528168, loss : 2.058164119720459\n",
            "Batch: 145, Test Acc: 0.5751201923076923\n",
            "Batch: 146:\n",
            "epoch: 0, classification_loss: 1.8154444694519043, Val Loss: 70.4102953672409, loss : 1.8154444694519043\n",
            "epoch: 1, classification_loss: 1.7799488306045532, Val Loss: 70.4265296459198, loss : 2.109476327896118\n",
            "epoch: 2, classification_loss: 1.7631099224090576, Val Loss: 70.41942822933197, loss : 2.034602642059326\n",
            "epoch: 3, classification_loss: 1.7765369415283203, Val Loss: 70.41265428066254, loss : 2.053684949874878\n",
            "epoch: 4, classification_loss: 1.8058122396469116, Val Loss: 70.4101631641388, loss : 2.0531692504882812\n",
            "epoch: 5, classification_loss: 1.7945326566696167, Val Loss: 70.44258975982666, loss : 2.0511255264282227\n",
            "epoch: 6, classification_loss: 1.7882119417190552, Val Loss: 70.41517341136932, loss : 2.055696725845337\n",
            "epoch: 7, classification_loss: 1.789664626121521, Val Loss: 70.42328631877899, loss : 2.0469977855682373\n",
            "epoch: 8, classification_loss: 1.7882333993911743, Val Loss: 70.42146694660187, loss : 2.0352985858917236\n",
            "epoch: 9, classification_loss: 1.7904382944107056, Val Loss: 70.42964768409729, loss : 2.040722131729126\n",
            "epoch: 10, classification_loss: 1.7939558029174805, Val Loss: 70.44170260429382, loss : 2.0344085693359375\n",
            "epoch: 11, classification_loss: 1.8000396490097046, Val Loss: 70.4900871515274, loss : 2.044081687927246\n",
            "epoch: 12, classification_loss: 1.7907533645629883, Val Loss: 70.39944624900818, loss : 2.027945041656494\n",
            "epoch: 13, classification_loss: 1.787532091140747, Val Loss: 70.43912780284882, loss : 2.0286993980407715\n",
            "epoch: 14, classification_loss: 1.7963169813156128, Val Loss: 70.50253713130951, loss : 2.0358481407165527\n",
            "epoch: 15, classification_loss: 1.7900700569152832, Val Loss: 70.47717797756195, loss : 2.0276718139648438\n",
            "epoch: 16, classification_loss: 1.7994987964630127, Val Loss: 70.40915274620056, loss : 2.0446319580078125\n",
            "epoch: 17, classification_loss: 1.7914042472839355, Val Loss: 70.49256789684296, loss : 2.0313045978546143\n",
            "epoch: 18, classification_loss: 1.7934802770614624, Val Loss: 70.47466218471527, loss : 2.0284008979797363\n",
            "epoch: 19, classification_loss: 1.806923508644104, Val Loss: 70.42971813678741, loss : 2.036226272583008\n",
            "Batch: 146, Test Acc: 0.5747195512820513\n",
            "Batch: 147:\n",
            "epoch: 0, classification_loss: 1.7968393564224243, Val Loss: 70.36584603786469, loss : 1.7968393564224243\n",
            "epoch: 1, classification_loss: 1.7598248720169067, Val Loss: 70.5423834323883, loss : 1.7598248720169067\n",
            "epoch: 2, classification_loss: 1.7017436027526855, Val Loss: 70.58751034736633, loss : 2.065152645111084\n",
            "epoch: 3, classification_loss: 1.6957178115844727, Val Loss: 70.50434064865112, loss : 2.0257580280303955\n",
            "epoch: 4, classification_loss: 1.708857774734497, Val Loss: 70.41342115402222, loss : 2.0062317848205566\n",
            "epoch: 5, classification_loss: 1.763566255569458, Val Loss: 70.36695063114166, loss : 2.0262436866760254\n",
            "epoch: 6, classification_loss: 1.7363557815551758, Val Loss: 70.41502368450165, loss : 1.9869403839111328\n",
            "epoch: 7, classification_loss: 1.7239553928375244, Val Loss: 70.39651203155518, loss : 2.0061237812042236\n",
            "epoch: 8, classification_loss: 1.716424822807312, Val Loss: 70.38866400718689, loss : 2.002033233642578\n",
            "epoch: 9, classification_loss: 1.7254396677017212, Val Loss: 70.42748081684113, loss : 1.9974814653396606\n",
            "epoch: 10, classification_loss: 1.7360422611236572, Val Loss: 70.4022889137268, loss : 1.9968706369400024\n",
            "epoch: 11, classification_loss: 1.7488094568252563, Val Loss: 70.38784146308899, loss : 2.0005781650543213\n",
            "epoch: 12, classification_loss: 1.7370994091033936, Val Loss: 70.37514567375183, loss : 1.9905569553375244\n",
            "epoch: 13, classification_loss: 1.728517770767212, Val Loss: 70.36594033241272, loss : 1.9826686382293701\n",
            "epoch: 14, classification_loss: 1.7250101566314697, Val Loss: 70.40422439575195, loss : 1.9780173301696777\n",
            "epoch: 15, classification_loss: 1.729087233543396, Val Loss: 70.44353890419006, loss : 1.9771265983581543\n",
            "epoch: 16, classification_loss: 1.7321135997772217, Val Loss: 70.35800671577454, loss : 1.974393606185913\n",
            "epoch: 17, classification_loss: 1.7282017469406128, Val Loss: 70.46811091899872, loss : 1.7282017469406128\n",
            "epoch: 18, classification_loss: 1.7126206159591675, Val Loss: 70.45372104644775, loss : 2.0445361137390137\n",
            "epoch: 19, classification_loss: 1.6939727067947388, Val Loss: 70.44542813301086, loss : 1.9497243165969849\n",
            "Batch: 147, Test Acc: 0.5759214743589743\n",
            "Batch: 148:\n",
            "epoch: 0, classification_loss: 1.8268449306488037, Val Loss: 70.43076753616333, loss : 1.8268449306488037\n",
            "epoch: 1, classification_loss: 1.7879911661148071, Val Loss: 70.46528601646423, loss : 2.1666970252990723\n",
            "epoch: 2, classification_loss: 1.7728173732757568, Val Loss: 70.44269335269928, loss : 2.1101937294006348\n",
            "epoch: 3, classification_loss: 1.7756569385528564, Val Loss: 70.44781446456909, loss : 2.0460758209228516\n",
            "epoch: 4, classification_loss: 1.8006551265716553, Val Loss: 70.46384978294373, loss : 2.081861972808838\n",
            "epoch: 5, classification_loss: 1.8070553541183472, Val Loss: 70.40772235393524, loss : 2.0492656230926514\n",
            "epoch: 6, classification_loss: 1.8034629821777344, Val Loss: 70.46520698070526, loss : 2.0739855766296387\n",
            "epoch: 7, classification_loss: 1.7934397459030151, Val Loss: 70.45663857460022, loss : 2.068596839904785\n",
            "epoch: 8, classification_loss: 1.7879081964492798, Val Loss: 70.4689189195633, loss : 2.0384106636047363\n",
            "epoch: 9, classification_loss: 1.8128031492233276, Val Loss: 70.44485652446747, loss : 2.0634796619415283\n",
            "epoch: 10, classification_loss: 1.8037793636322021, Val Loss: 70.44107019901276, loss : 2.044865131378174\n",
            "epoch: 11, classification_loss: 1.8046475648880005, Val Loss: 70.44826662540436, loss : 2.044748067855835\n",
            "epoch: 12, classification_loss: 1.796440839767456, Val Loss: 70.46455574035645, loss : 2.0374605655670166\n",
            "epoch: 13, classification_loss: 1.8003671169281006, Val Loss: 70.4894791841507, loss : 2.032059669494629\n",
            "epoch: 14, classification_loss: 1.7949093580245972, Val Loss: 70.45245039463043, loss : 2.0294525623321533\n",
            "epoch: 15, classification_loss: 1.7892396450042725, Val Loss: 70.44677102565765, loss : 2.029006004333496\n",
            "epoch: 16, classification_loss: 1.7952691316604614, Val Loss: 70.50845348834991, loss : 2.021470069885254\n",
            "epoch: 17, classification_loss: 1.8119044303894043, Val Loss: 70.4560843706131, loss : 2.0413451194763184\n",
            "epoch: 18, classification_loss: 1.800888180732727, Val Loss: 70.47654521465302, loss : 2.023890733718872\n",
            "epoch: 19, classification_loss: 1.791853427886963, Val Loss: 70.50794875621796, loss : 2.0292327404022217\n",
            "Batch: 148, Test Acc: 0.5755208333333334\n",
            "Batch: 149:\n",
            "epoch: 0, classification_loss: 1.8487180471420288, Val Loss: 70.41249430179596, loss : 1.8487180471420288\n",
            "epoch: 1, classification_loss: 1.8221710920333862, Val Loss: 70.43588960170746, loss : 2.153541326522827\n",
            "epoch: 2, classification_loss: 1.7849297523498535, Val Loss: 70.45894169807434, loss : 2.059933662414551\n",
            "epoch: 3, classification_loss: 1.8193097114562988, Val Loss: 70.41550326347351, loss : 2.0902392864227295\n",
            "epoch: 4, classification_loss: 1.8272782564163208, Val Loss: 70.37132120132446, loss : 2.0693769454956055\n",
            "epoch: 5, classification_loss: 1.8195691108703613, Val Loss: 70.35041272640228, loss : 2.078803777694702\n",
            "epoch: 6, classification_loss: 1.8167403936386108, Val Loss: 70.42677593231201, loss : 1.8167403936386108\n",
            "epoch: 7, classification_loss: 1.7768408060073853, Val Loss: 70.4734799861908, loss : 2.109292984008789\n",
            "epoch: 8, classification_loss: 1.7540814876556396, Val Loss: 70.42217803001404, loss : 2.032471179962158\n",
            "epoch: 9, classification_loss: 1.785894513130188, Val Loss: 70.37678623199463, loss : 2.0348827838897705\n",
            "epoch: 10, classification_loss: 1.8052083253860474, Val Loss: 70.42113482952118, loss : 2.0424981117248535\n",
            "epoch: 11, classification_loss: 1.7995260953903198, Val Loss: 70.43225276470184, loss : 2.0375254154205322\n",
            "epoch: 12, classification_loss: 1.7855219841003418, Val Loss: 70.4188621044159, loss : 2.0390162467956543\n",
            "epoch: 13, classification_loss: 1.7949084043502808, Val Loss: 70.4277229309082, loss : 2.03786039352417\n",
            "epoch: 14, classification_loss: 1.799126148223877, Val Loss: 70.41440749168396, loss : 2.055572032928467\n",
            "epoch: 15, classification_loss: 1.7806932926177979, Val Loss: 70.46416425704956, loss : 2.0303444862365723\n",
            "epoch: 16, classification_loss: 1.7892193794250488, Val Loss: 70.41395854949951, loss : 2.0412521362304688\n",
            "epoch: 17, classification_loss: 1.791724681854248, Val Loss: 70.4107860326767, loss : 2.033743381500244\n",
            "epoch: 18, classification_loss: 1.7919871807098389, Val Loss: 70.42317080497742, loss : 2.039881467819214\n",
            "epoch: 19, classification_loss: 1.7894800901412964, Val Loss: 70.43213999271393, loss : 2.0268399715423584\n",
            "Batch: 149, Test Acc: 0.5754206730769231\n",
            "Batch: 150:\n",
            "epoch: 0, classification_loss: 1.8158316612243652, Val Loss: 70.51589679718018, loss : 1.8158316612243652\n",
            "epoch: 1, classification_loss: 1.7775845527648926, Val Loss: 70.4228982925415, loss : 2.122753381729126\n",
            "epoch: 2, classification_loss: 1.7555835247039795, Val Loss: 70.37392210960388, loss : 2.0234036445617676\n",
            "epoch: 3, classification_loss: 1.7797136306762695, Val Loss: 70.36193907260895, loss : 2.0807290077209473\n",
            "epoch: 4, classification_loss: 1.792144536972046, Val Loss: 70.42539286613464, loss : 2.0645477771759033\n",
            "epoch: 5, classification_loss: 1.7871925830841064, Val Loss: 70.40138828754425, loss : 2.0559256076812744\n",
            "epoch: 6, classification_loss: 1.7690279483795166, Val Loss: 70.45149755477905, loss : 2.055861711502075\n",
            "epoch: 7, classification_loss: 1.7733181715011597, Val Loss: 70.39157199859619, loss : 2.0447142124176025\n",
            "epoch: 8, classification_loss: 1.766351342201233, Val Loss: 70.39935982227325, loss : 2.045555591583252\n",
            "epoch: 9, classification_loss: 1.7789846658706665, Val Loss: 70.3661322593689, loss : 2.0424065589904785\n",
            "epoch: 10, classification_loss: 1.7796354293823242, Val Loss: 70.40371596813202, loss : 2.0527102947235107\n",
            "epoch: 11, classification_loss: 1.7841191291809082, Val Loss: 70.44504809379578, loss : 2.0440802574157715\n",
            "epoch: 12, classification_loss: 1.795323133468628, Val Loss: 70.45843720436096, loss : 2.0492782592773438\n",
            "epoch: 13, classification_loss: 1.7838249206542969, Val Loss: 70.39221239089966, loss : 2.0379626750946045\n",
            "epoch: 14, classification_loss: 1.7928467988967896, Val Loss: 70.41700983047485, loss : 2.041630744934082\n",
            "epoch: 15, classification_loss: 1.7832581996917725, Val Loss: 70.43751573562622, loss : 2.037597417831421\n",
            "epoch: 16, classification_loss: 1.7815356254577637, Val Loss: 70.39714634418488, loss : 2.0257019996643066\n",
            "epoch: 17, classification_loss: 1.792977213859558, Val Loss: 70.4138753414154, loss : 2.050217628479004\n",
            "epoch: 18, classification_loss: 1.7813438177108765, Val Loss: 70.45094239711761, loss : 2.0300846099853516\n",
            "epoch: 19, classification_loss: 1.7888596057891846, Val Loss: 70.43092036247253, loss : 2.0301706790924072\n",
            "Batch: 150, Test Acc: 0.5778245192307693\n",
            "Batch: 151:\n",
            "epoch: 0, classification_loss: 1.8113486766815186, Val Loss: 70.45597779750824, loss : 1.8113486766815186\n",
            "epoch: 1, classification_loss: 1.7647106647491455, Val Loss: 70.47464227676392, loss : 2.102576732635498\n",
            "epoch: 2, classification_loss: 1.758002758026123, Val Loss: 70.40272986888885, loss : 2.0246503353118896\n",
            "epoch: 3, classification_loss: 1.7815369367599487, Val Loss: 70.41559278964996, loss : 2.062648296356201\n",
            "epoch: 4, classification_loss: 1.8025050163269043, Val Loss: 70.40186262130737, loss : 2.046919107437134\n",
            "epoch: 5, classification_loss: 1.7841687202453613, Val Loss: 70.37964475154877, loss : 2.044759511947632\n",
            "epoch: 6, classification_loss: 1.7801986932754517, Val Loss: 70.4101175069809, loss : 2.055752992630005\n",
            "epoch: 7, classification_loss: 1.7769062519073486, Val Loss: 70.41976737976074, loss : 2.041506290435791\n",
            "epoch: 8, classification_loss: 1.7850255966186523, Val Loss: 70.4533656835556, loss : 2.039912700653076\n",
            "epoch: 9, classification_loss: 1.7993403673171997, Val Loss: 70.44934034347534, loss : 2.0450613498687744\n",
            "epoch: 10, classification_loss: 1.7878994941711426, Val Loss: 70.38943183422089, loss : 2.0372211933135986\n",
            "epoch: 11, classification_loss: 1.7901830673217773, Val Loss: 70.42194354534149, loss : 2.0362255573272705\n",
            "epoch: 12, classification_loss: 1.7928498983383179, Val Loss: 70.42939364910126, loss : 2.0432028770446777\n",
            "epoch: 13, classification_loss: 1.7868058681488037, Val Loss: 70.4843932390213, loss : 2.031111478805542\n",
            "epoch: 14, classification_loss: 1.7838428020477295, Val Loss: 70.42435300350189, loss : 2.030282735824585\n",
            "epoch: 15, classification_loss: 1.7995353937149048, Val Loss: 70.43097186088562, loss : 2.0308642387390137\n",
            "epoch: 16, classification_loss: 1.792805790901184, Val Loss: 70.45731627941132, loss : 2.0406010150909424\n",
            "epoch: 17, classification_loss: 1.7851402759552002, Val Loss: 70.40799975395203, loss : 2.017883777618408\n",
            "epoch: 18, classification_loss: 1.7941515445709229, Val Loss: 70.42696380615234, loss : 2.0405514240264893\n",
            "epoch: 19, classification_loss: 1.7798126935958862, Val Loss: 70.46773886680603, loss : 2.0186080932617188\n",
            "Batch: 151, Test Acc: 0.5764222756410257\n",
            "Batch: 152:\n",
            "epoch: 0, classification_loss: 1.8136990070343018, Val Loss: 70.54932200908661, loss : 1.8136990070343018\n",
            "epoch: 1, classification_loss: 1.7754160165786743, Val Loss: 70.53093647956848, loss : 2.117969512939453\n",
            "epoch: 2, classification_loss: 1.745233416557312, Val Loss: 70.4052187204361, loss : 2.0257859230041504\n",
            "epoch: 3, classification_loss: 1.7869666814804077, Val Loss: 70.4093941450119, loss : 2.070915699005127\n",
            "epoch: 4, classification_loss: 1.8078209161758423, Val Loss: 70.43833136558533, loss : 2.072441577911377\n",
            "epoch: 5, classification_loss: 1.7956424951553345, Val Loss: 70.4401752948761, loss : 2.048694133758545\n",
            "epoch: 6, classification_loss: 1.7923203706741333, Val Loss: 70.4786947965622, loss : 2.078900098800659\n",
            "epoch: 7, classification_loss: 1.7830957174301147, Val Loss: 70.4683780670166, loss : 2.056096315383911\n",
            "epoch: 8, classification_loss: 1.7845344543457031, Val Loss: 70.46075451374054, loss : 2.053114891052246\n",
            "epoch: 9, classification_loss: 1.7996114492416382, Val Loss: 70.47039842605591, loss : 2.0641298294067383\n",
            "epoch: 10, classification_loss: 1.796177864074707, Val Loss: 70.4857040643692, loss : 2.0465993881225586\n",
            "epoch: 11, classification_loss: 1.787114143371582, Val Loss: 70.49184036254883, loss : 2.04406476020813\n",
            "epoch: 12, classification_loss: 1.7954251766204834, Val Loss: 70.48383057117462, loss : 2.044567823410034\n",
            "epoch: 13, classification_loss: 1.7905645370483398, Val Loss: 70.50969624519348, loss : 2.0462465286254883\n",
            "epoch: 14, classification_loss: 1.7938709259033203, Val Loss: 70.4654791355133, loss : 2.044959306716919\n",
            "epoch: 15, classification_loss: 1.8006081581115723, Val Loss: 70.50413143634796, loss : 2.0477209091186523\n",
            "epoch: 16, classification_loss: 1.8005595207214355, Val Loss: 70.5217205286026, loss : 2.041358709335327\n",
            "epoch: 17, classification_loss: 1.7959915399551392, Val Loss: 70.50861895084381, loss : 2.035282850265503\n",
            "epoch: 18, classification_loss: 1.7997084856033325, Val Loss: 70.50904667377472, loss : 2.0398590564727783\n",
            "epoch: 19, classification_loss: 1.79173743724823, Val Loss: 70.56082737445831, loss : 2.0333218574523926\n",
            "Batch: 152, Test Acc: 0.5744190705128205\n",
            "Batch: 153:\n",
            "epoch: 0, classification_loss: 1.8505513668060303, Val Loss: 70.53278636932373, loss : 1.8505513668060303\n",
            "epoch: 1, classification_loss: 1.816658616065979, Val Loss: 70.48689067363739, loss : 2.1509406566619873\n",
            "epoch: 2, classification_loss: 1.7998539209365845, Val Loss: 70.36751782894135, loss : 2.066469430923462\n",
            "epoch: 3, classification_loss: 1.8252533674240112, Val Loss: 70.39847755432129, loss : 2.116668224334717\n",
            "epoch: 4, classification_loss: 1.8312339782714844, Val Loss: 70.46645045280457, loss : 2.089712381362915\n",
            "epoch: 5, classification_loss: 1.8303699493408203, Val Loss: 70.42644321918488, loss : 2.1016483306884766\n",
            "epoch: 6, classification_loss: 1.8161473274230957, Val Loss: 70.38340294361115, loss : 2.099382162094116\n",
            "epoch: 7, classification_loss: 1.8336793184280396, Val Loss: 70.44695711135864, loss : 2.0965731143951416\n",
            "epoch: 8, classification_loss: 1.8285971879959106, Val Loss: 70.42593693733215, loss : 2.0918209552764893\n",
            "epoch: 9, classification_loss: 1.8294504880905151, Val Loss: 70.41241478919983, loss : 2.093499183654785\n",
            "epoch: 10, classification_loss: 1.8486593961715698, Val Loss: 70.42423152923584, loss : 2.1087615489959717\n",
            "epoch: 11, classification_loss: 1.8192397356033325, Val Loss: 70.4379768371582, loss : 2.0826704502105713\n",
            "epoch: 12, classification_loss: 1.834547996520996, Val Loss: 70.41781175136566, loss : 2.0856895446777344\n",
            "epoch: 13, classification_loss: 1.824270486831665, Val Loss: 70.3944342136383, loss : 2.0739660263061523\n",
            "epoch: 14, classification_loss: 1.815618634223938, Val Loss: 70.4363477230072, loss : 2.067607879638672\n",
            "epoch: 15, classification_loss: 1.8355433940887451, Val Loss: 70.53298485279083, loss : 2.0852274894714355\n",
            "epoch: 16, classification_loss: 1.8220733404159546, Val Loss: 70.4219480752945, loss : 2.068267345428467\n",
            "epoch: 17, classification_loss: 1.822597622871399, Val Loss: 70.43485760688782, loss : 2.073615550994873\n",
            "epoch: 18, classification_loss: 1.8228920698165894, Val Loss: 70.45127177238464, loss : 2.0703999996185303\n",
            "epoch: 19, classification_loss: 1.8232338428497314, Val Loss: 70.5755615234375, loss : 2.076873302459717\n",
            "Batch: 153, Test Acc: 0.5724158653846154\n",
            "Batch: 154:\n",
            "epoch: 0, classification_loss: 1.7984554767608643, Val Loss: 70.44065916538239, loss : 1.7984554767608643\n",
            "epoch: 1, classification_loss: 1.7598215341567993, Val Loss: 70.39679038524628, loss : 2.0867490768432617\n",
            "epoch: 2, classification_loss: 1.747081995010376, Val Loss: 70.376145362854, loss : 2.022118091583252\n",
            "epoch: 3, classification_loss: 1.773374319076538, Val Loss: 70.39212822914124, loss : 2.0465216636657715\n",
            "epoch: 4, classification_loss: 1.7961102724075317, Val Loss: 70.42812788486481, loss : 2.0370125770568848\n",
            "epoch: 5, classification_loss: 1.783445119857788, Val Loss: 70.40814423561096, loss : 2.0414652824401855\n",
            "epoch: 6, classification_loss: 1.778390645980835, Val Loss: 70.41261970996857, loss : 2.0507755279541016\n",
            "epoch: 7, classification_loss: 1.761765480041504, Val Loss: 70.41227149963379, loss : 2.0170981884002686\n",
            "epoch: 8, classification_loss: 1.7818015813827515, Val Loss: 70.37390494346619, loss : 2.037680149078369\n",
            "epoch: 9, classification_loss: 1.7761625051498413, Val Loss: 70.37085270881653, loss : 2.0268125534057617\n",
            "epoch: 10, classification_loss: 1.7814544439315796, Val Loss: 70.43196535110474, loss : 2.025057315826416\n",
            "epoch: 11, classification_loss: 1.7968274354934692, Val Loss: 70.4331374168396, loss : 2.035022258758545\n",
            "epoch: 12, classification_loss: 1.7787739038467407, Val Loss: 70.4014800786972, loss : 2.0189850330352783\n",
            "epoch: 13, classification_loss: 1.7827589511871338, Val Loss: 70.41476142406464, loss : 2.0171446800231934\n",
            "epoch: 14, classification_loss: 1.78067946434021, Val Loss: 70.4282739162445, loss : 2.0234038829803467\n",
            "epoch: 15, classification_loss: 1.7709009647369385, Val Loss: 70.42839074134827, loss : 2.0081753730773926\n",
            "epoch: 16, classification_loss: 1.766845941543579, Val Loss: 70.4165780544281, loss : 1.998338222503662\n",
            "epoch: 17, classification_loss: 1.7823063135147095, Val Loss: 70.42290425300598, loss : 2.012615203857422\n",
            "epoch: 18, classification_loss: 1.785923719406128, Val Loss: 70.45583403110504, loss : 2.013702154159546\n",
            "epoch: 19, classification_loss: 1.789562702178955, Val Loss: 70.43993937969208, loss : 2.0144622325897217\n",
            "Batch: 154, Test Acc: 0.5771233974358975\n",
            "Batch: 155:\n",
            "epoch: 0, classification_loss: 1.8152204751968384, Val Loss: 70.4004100561142, loss : 1.8152204751968384\n",
            "epoch: 1, classification_loss: 1.77200448513031, Val Loss: 70.39408659934998, loss : 2.113974094390869\n",
            "epoch: 2, classification_loss: 1.75105619430542, Val Loss: 70.42445039749146, loss : 2.0170905590057373\n",
            "epoch: 3, classification_loss: 1.7862374782562256, Val Loss: 70.39452147483826, loss : 2.048198938369751\n",
            "epoch: 4, classification_loss: 1.8205008506774902, Val Loss: 70.36249387264252, loss : 2.0546224117279053\n",
            "epoch: 5, classification_loss: 1.7926607131958008, Val Loss: 70.38982808589935, loss : 2.054304361343384\n",
            "epoch: 6, classification_loss: 1.7756929397583008, Val Loss: 70.42628693580627, loss : 2.0487442016601562\n",
            "epoch: 7, classification_loss: 1.7857437133789062, Val Loss: 70.39761221408844, loss : 2.0395102500915527\n",
            "epoch: 8, classification_loss: 1.7948906421661377, Val Loss: 70.38204622268677, loss : 2.0379018783569336\n",
            "epoch: 9, classification_loss: 1.7874034643173218, Val Loss: 70.3831034898758, loss : 2.031616687774658\n",
            "epoch: 10, classification_loss: 1.7844047546386719, Val Loss: 70.44099605083466, loss : 2.026153087615967\n",
            "epoch: 11, classification_loss: 1.795819640159607, Val Loss: 70.42525398731232, loss : 2.0389652252197266\n",
            "epoch: 12, classification_loss: 1.7914462089538574, Val Loss: 70.39083421230316, loss : 2.0300304889678955\n",
            "epoch: 13, classification_loss: 1.778562068939209, Val Loss: 70.40291392803192, loss : 2.0162148475646973\n",
            "epoch: 14, classification_loss: 1.7923208475112915, Val Loss: 70.46299433708191, loss : 2.031447649002075\n",
            "epoch: 15, classification_loss: 1.780279278755188, Val Loss: 70.42443680763245, loss : 2.0193891525268555\n",
            "epoch: 16, classification_loss: 1.7869797945022583, Val Loss: 70.3829847574234, loss : 2.023160457611084\n",
            "epoch: 17, classification_loss: 1.782999873161316, Val Loss: 70.40101540088654, loss : 2.0210494995117188\n",
            "epoch: 18, classification_loss: 1.7859840393066406, Val Loss: 70.46273458003998, loss : 2.013793706893921\n",
            "epoch: 19, classification_loss: 1.7956050634384155, Val Loss: 70.41406452655792, loss : 2.030409812927246\n",
            "Batch: 155, Test Acc: 0.575020032051282\n",
            "#########################################################\n",
            "Batch: 0:\n",
            "epoch: 0, classification_loss: 1.6266683340072632, Val Loss: 70.52071261405945, loss : 1.6266683340072632\n",
            "epoch: 1, classification_loss: 1.613120198249817, Val Loss: 71.099862575531, loss : 1.613120198249817\n",
            "epoch: 2, classification_loss: 1.5749870538711548, Val Loss: 72.25712382793427, loss : 1.5749870538711548\n",
            "epoch: 3, classification_loss: 1.5505353212356567, Val Loss: 73.48973333835602, loss : 1.5505353212356567\n",
            "epoch: 4, classification_loss: 1.5329554080963135, Val Loss: 74.40219259262085, loss : 1.5329554080963135\n",
            "epoch: 5, classification_loss: 1.5193771123886108, Val Loss: 74.85967350006104, loss : 1.5193771123886108\n",
            "epoch: 6, classification_loss: 1.507840871810913, Val Loss: 75.15159511566162, loss : 1.507840871810913\n",
            "epoch: 7, classification_loss: 1.5054669380187988, Val Loss: 75.2879478931427, loss : 1.5054669380187988\n",
            "epoch: 8, classification_loss: 1.4957823753356934, Val Loss: 75.25955605506897, loss : 1.4957823753356934\n",
            "epoch: 9, classification_loss: 1.487026333808899, Val Loss: 75.17795813083649, loss : 1.487026333808899\n",
            "epoch: 10, classification_loss: 1.4783657789230347, Val Loss: 75.16668093204498, loss : 1.4783657789230347\n",
            "epoch: 11, classification_loss: 1.4748367071151733, Val Loss: 75.16180980205536, loss : 1.4748367071151733\n",
            "epoch: 12, classification_loss: 1.4717023372650146, Val Loss: 75.18315172195435, loss : 1.4717023372650146\n",
            "epoch: 13, classification_loss: 1.4716204404830933, Val Loss: 75.22539973258972, loss : 1.4716204404830933\n",
            "epoch: 14, classification_loss: 1.469708800315857, Val Loss: 75.26992070674896, loss : 1.469708800315857\n",
            "epoch: 15, classification_loss: 1.4683316946029663, Val Loss: 75.35880446434021, loss : 1.4683316946029663\n",
            "epoch: 16, classification_loss: 1.4689849615097046, Val Loss: 75.44485855102539, loss : 1.4689849615097046\n",
            "epoch: 17, classification_loss: 1.468488335609436, Val Loss: 75.52304911613464, loss : 1.468488335609436\n",
            "epoch: 18, classification_loss: 1.4698433876037598, Val Loss: 75.56267762184143, loss : 1.4698433876037598\n",
            "epoch: 19, classification_loss: 1.4688435792922974, Val Loss: 75.55340123176575, loss : 1.4688435792922974\n",
            "Batch: 0, Test Acc: 0.49939903846153844\n",
            "Batch: 1:\n",
            "epoch: 0, classification_loss: 1.5176435708999634, Val Loss: 70.54983866214752, loss : 1.5176435708999634\n",
            "epoch: 1, classification_loss: 1.49834406375885, Val Loss: 70.72190725803375, loss : 1.49834406375885\n",
            "epoch: 2, classification_loss: 1.4843003749847412, Val Loss: 71.09278321266174, loss : 1.4843003749847412\n",
            "epoch: 3, classification_loss: 1.4807250499725342, Val Loss: 71.52481496334076, loss : 1.4807250499725342\n",
            "epoch: 4, classification_loss: 1.4781585931777954, Val Loss: 71.9671573638916, loss : 1.4781585931777954\n",
            "epoch: 5, classification_loss: 1.4762948751449585, Val Loss: 72.3734256029129, loss : 1.4762948751449585\n",
            "epoch: 6, classification_loss: 1.4749441146850586, Val Loss: 72.73144721984863, loss : 1.4749441146850586\n",
            "epoch: 7, classification_loss: 1.473393201828003, Val Loss: 73.06273102760315, loss : 1.473393201828003\n",
            "epoch: 8, classification_loss: 1.4740040302276611, Val Loss: 73.33742988109589, loss : 1.4740040302276611\n",
            "epoch: 9, classification_loss: 1.4727638959884644, Val Loss: 73.56530332565308, loss : 1.4727638959884644\n",
            "epoch: 10, classification_loss: 1.4711793661117554, Val Loss: 73.72655642032623, loss : 1.4711793661117554\n",
            "epoch: 11, classification_loss: 1.470969319343567, Val Loss: 73.84217178821564, loss : 1.470969319343567\n",
            "epoch: 12, classification_loss: 1.4700368642807007, Val Loss: 73.91721415519714, loss : 1.4700368642807007\n",
            "epoch: 13, classification_loss: 1.4694958925247192, Val Loss: 73.968869805336, loss : 1.4694958925247192\n",
            "epoch: 14, classification_loss: 1.4695484638214111, Val Loss: 73.98116290569305, loss : 1.4695484638214111\n",
            "epoch: 15, classification_loss: 1.469576358795166, Val Loss: 73.9720379114151, loss : 1.469576358795166\n",
            "epoch: 16, classification_loss: 1.4692957401275635, Val Loss: 73.9436742067337, loss : 1.4692957401275635\n",
            "epoch: 17, classification_loss: 1.4696576595306396, Val Loss: 73.88776504993439, loss : 1.4696576595306396\n",
            "epoch: 18, classification_loss: 1.470159649848938, Val Loss: 73.82487893104553, loss : 1.470159649848938\n",
            "epoch: 19, classification_loss: 1.4686952829360962, Val Loss: 73.7678245306015, loss : 1.4686952829360962\n",
            "Batch: 1, Test Acc: 0.5367588141025641\n",
            "Batch: 2:\n",
            "epoch: 0, classification_loss: 1.6505708694458008, Val Loss: 70.55502617359161, loss : 1.6505708694458008\n",
            "epoch: 1, classification_loss: 1.6295175552368164, Val Loss: 70.66524004936218, loss : 1.6295175552368164\n",
            "epoch: 2, classification_loss: 1.607206106185913, Val Loss: 70.90910625457764, loss : 1.607206106185913\n",
            "epoch: 3, classification_loss: 1.58010733127594, Val Loss: 71.38271760940552, loss : 1.58010733127594\n",
            "epoch: 4, classification_loss: 1.558180570602417, Val Loss: 72.12264585494995, loss : 1.558180570602417\n",
            "epoch: 5, classification_loss: 1.5386786460876465, Val Loss: 73.04298007488251, loss : 1.5386786460876465\n",
            "epoch: 6, classification_loss: 1.5320804119110107, Val Loss: 74.04837667942047, loss : 1.5320804119110107\n",
            "epoch: 7, classification_loss: 1.522737979888916, Val Loss: 74.93326497077942, loss : 1.522737979888916\n",
            "epoch: 8, classification_loss: 1.5214043855667114, Val Loss: 75.62903106212616, loss : 1.5214043855667114\n",
            "epoch: 9, classification_loss: 1.5200474262237549, Val Loss: 76.1666624546051, loss : 1.5200474262237549\n",
            "epoch: 10, classification_loss: 1.5134024620056152, Val Loss: 76.47976660728455, loss : 1.5134024620056152\n",
            "epoch: 11, classification_loss: 1.5076732635498047, Val Loss: 76.54898643493652, loss : 1.5076732635498047\n",
            "epoch: 12, classification_loss: 1.5043779611587524, Val Loss: 76.40784776210785, loss : 1.5043779611587524\n",
            "epoch: 13, classification_loss: 1.5000766515731812, Val Loss: 76.12565302848816, loss : 1.5000766515731812\n",
            "epoch: 14, classification_loss: 1.4957053661346436, Val Loss: 75.77684164047241, loss : 1.4957053661346436\n",
            "epoch: 15, classification_loss: 1.492374062538147, Val Loss: 75.53072941303253, loss : 1.492374062538147\n",
            "epoch: 16, classification_loss: 1.4895095825195312, Val Loss: 75.4232587814331, loss : 1.4895095825195312\n",
            "epoch: 17, classification_loss: 1.4868534803390503, Val Loss: 75.3980280160904, loss : 1.4868534803390503\n",
            "epoch: 18, classification_loss: 1.483909249305725, Val Loss: 75.41125535964966, loss : 1.483909249305725\n",
            "epoch: 19, classification_loss: 1.4835786819458008, Val Loss: 75.3758841753006, loss : 1.4835786819458008\n",
            "Batch: 2, Test Acc: 0.5091145833333334\n",
            "Batch: 3:\n",
            "epoch: 0, classification_loss: 1.642217755317688, Val Loss: 70.53888618946075, loss : 1.642217755317688\n",
            "epoch: 1, classification_loss: 1.633917212486267, Val Loss: 70.68384683132172, loss : 1.633917212486267\n",
            "epoch: 2, classification_loss: 1.6075413227081299, Val Loss: 70.97849893569946, loss : 1.6075413227081299\n",
            "epoch: 3, classification_loss: 1.5814696550369263, Val Loss: 71.47928559780121, loss : 1.5814696550369263\n",
            "epoch: 4, classification_loss: 1.5669492483139038, Val Loss: 72.09637939929962, loss : 1.5669492483139038\n",
            "epoch: 5, classification_loss: 1.5475497245788574, Val Loss: 72.75422751903534, loss : 1.5475497245788574\n",
            "epoch: 6, classification_loss: 1.5373843908309937, Val Loss: 73.35116457939148, loss : 1.5373843908309937\n",
            "epoch: 7, classification_loss: 1.5262682437896729, Val Loss: 73.89933466911316, loss : 1.5262682437896729\n",
            "epoch: 8, classification_loss: 1.5180288553237915, Val Loss: 74.4029188156128, loss : 1.5180288553237915\n",
            "epoch: 9, classification_loss: 1.511539340019226, Val Loss: 74.71510589122772, loss : 1.511539340019226\n",
            "epoch: 10, classification_loss: 1.5045077800750732, Val Loss: 74.88965594768524, loss : 1.5045077800750732\n",
            "epoch: 11, classification_loss: 1.5005474090576172, Val Loss: 74.93229508399963, loss : 1.5005474090576172\n",
            "epoch: 12, classification_loss: 1.49667227268219, Val Loss: 74.92140233516693, loss : 1.49667227268219\n",
            "epoch: 13, classification_loss: 1.4938734769821167, Val Loss: 74.85599339008331, loss : 1.4938734769821167\n",
            "epoch: 14, classification_loss: 1.4923348426818848, Val Loss: 74.78875541687012, loss : 1.4923348426818848\n",
            "epoch: 15, classification_loss: 1.4910396337509155, Val Loss: 74.68903934955597, loss : 1.4910396337509155\n",
            "epoch: 16, classification_loss: 1.4891183376312256, Val Loss: 74.5402113199234, loss : 1.4891183376312256\n",
            "epoch: 17, classification_loss: 1.4881161451339722, Val Loss: 74.39020383358002, loss : 1.4881161451339722\n",
            "epoch: 18, classification_loss: 1.4874515533447266, Val Loss: 74.24540078639984, loss : 1.4874515533447266\n",
            "epoch: 19, classification_loss: 1.4839075803756714, Val Loss: 74.14225578308105, loss : 1.4839075803756714\n",
            "Batch: 3, Test Acc: 0.534354967948718\n",
            "Batch: 4:\n",
            "epoch: 0, classification_loss: 1.667199730873108, Val Loss: 70.66693472862244, loss : 1.667199730873108\n",
            "epoch: 1, classification_loss: 1.6361514329910278, Val Loss: 70.52651333808899, loss : 1.7940399646759033\n",
            "epoch: 2, classification_loss: 1.6613402366638184, Val Loss: 70.48385059833527, loss : 1.9120876789093018\n",
            "epoch: 3, classification_loss: 1.663260579109192, Val Loss: 70.5986407995224, loss : 1.8952791690826416\n",
            "epoch: 4, classification_loss: 1.6438138484954834, Val Loss: 70.5902693271637, loss : 1.8925738334655762\n",
            "epoch: 5, classification_loss: 1.6475623846054077, Val Loss: 70.49577915668488, loss : 1.9388806819915771\n",
            "epoch: 6, classification_loss: 1.6499114036560059, Val Loss: 70.49664902687073, loss : 1.8973759412765503\n",
            "epoch: 7, classification_loss: 1.6565700769424438, Val Loss: 70.48370373249054, loss : 1.9254238605499268\n",
            "epoch: 8, classification_loss: 1.6470754146575928, Val Loss: 70.52840888500214, loss : 1.8833134174346924\n",
            "epoch: 9, classification_loss: 1.6563506126403809, Val Loss: 70.52968454360962, loss : 1.9233492612838745\n",
            "epoch: 10, classification_loss: 1.6340996026992798, Val Loss: 70.4757376909256, loss : 1.8887076377868652\n",
            "epoch: 11, classification_loss: 1.641556978225708, Val Loss: 70.47086238861084, loss : 1.8985786437988281\n",
            "epoch: 12, classification_loss: 1.6620993614196777, Val Loss: 70.50005185604095, loss : 1.890998363494873\n",
            "epoch: 13, classification_loss: 1.6590412855148315, Val Loss: 70.49927473068237, loss : 1.907712697982788\n",
            "epoch: 14, classification_loss: 1.6346051692962646, Val Loss: 70.4806547164917, loss : 1.8876571655273438\n",
            "epoch: 15, classification_loss: 1.655685544013977, Val Loss: 70.46621477603912, loss : 1.8978804349899292\n",
            "epoch: 16, classification_loss: 1.6506410837173462, Val Loss: 70.47884941101074, loss : 1.8895589113235474\n",
            "epoch: 17, classification_loss: 1.6464933156967163, Val Loss: 70.48447406291962, loss : 1.8716394901275635\n",
            "epoch: 18, classification_loss: 1.644007921218872, Val Loss: 70.47179079055786, loss : 1.8858122825622559\n",
            "epoch: 19, classification_loss: 1.6445850133895874, Val Loss: 70.46849596500397, loss : 1.8704452514648438\n",
            "Batch: 4, Test Acc: 0.5787259615384616\n",
            "Batch: 5:\n",
            "epoch: 0, classification_loss: 1.756413459777832, Val Loss: 70.55968117713928, loss : 1.756413459777832\n",
            "epoch: 1, classification_loss: 1.7161879539489746, Val Loss: 70.54382920265198, loss : 2.0852952003479004\n",
            "epoch: 2, classification_loss: 1.695671796798706, Val Loss: 70.56980407238007, loss : 1.990817666053772\n",
            "epoch: 3, classification_loss: 1.72916841506958, Val Loss: 70.53210699558258, loss : 2.0351762771606445\n",
            "epoch: 4, classification_loss: 1.750999927520752, Val Loss: 70.52499127388, loss : 2.02498722076416\n",
            "epoch: 5, classification_loss: 1.7420010566711426, Val Loss: 70.4946299791336, loss : 2.009765386581421\n",
            "epoch: 6, classification_loss: 1.7173198461532593, Val Loss: 70.48974895477295, loss : 2.0183515548706055\n",
            "epoch: 7, classification_loss: 1.7184250354766846, Val Loss: 70.64167010784149, loss : 2.0087451934814453\n",
            "epoch: 8, classification_loss: 1.7327755689620972, Val Loss: 70.58639860153198, loss : 2.007829189300537\n",
            "epoch: 9, classification_loss: 1.7293577194213867, Val Loss: 70.48743855953217, loss : 2.0051193237304688\n",
            "epoch: 10, classification_loss: 1.7306981086730957, Val Loss: 70.5073333978653, loss : 2.0021984577178955\n",
            "epoch: 11, classification_loss: 1.732683539390564, Val Loss: 70.60196840763092, loss : 1.9969291687011719\n",
            "epoch: 12, classification_loss: 1.7392998933792114, Val Loss: 70.54611599445343, loss : 2.0025951862335205\n",
            "epoch: 13, classification_loss: 1.7344574928283691, Val Loss: 70.50719058513641, loss : 2.001462459564209\n",
            "epoch: 14, classification_loss: 1.7361782789230347, Val Loss: 70.53393566608429, loss : 2.0080180168151855\n",
            "epoch: 15, classification_loss: 1.7325255870819092, Val Loss: 70.532830119133, loss : 1.9979685544967651\n",
            "epoch: 16, classification_loss: 1.7377253770828247, Val Loss: 70.54689931869507, loss : 1.9956235885620117\n",
            "epoch: 17, classification_loss: 1.7192968130111694, Val Loss: 70.58273494243622, loss : 1.9816362857818604\n",
            "epoch: 18, classification_loss: 1.7460613250732422, Val Loss: 70.63987386226654, loss : 2.00677490234375\n",
            "epoch: 19, classification_loss: 1.7374671697616577, Val Loss: 70.52125632762909, loss : 1.9920073747634888\n",
            "Batch: 5, Test Acc: 0.5782251602564102\n",
            "Batch: 6:\n",
            "epoch: 0, classification_loss: 1.7586219310760498, Val Loss: 70.49365568161011, loss : 1.7586219310760498\n",
            "epoch: 1, classification_loss: 1.7424631118774414, Val Loss: 70.46722388267517, loss : 2.0799570083618164\n",
            "epoch: 2, classification_loss: 1.7236285209655762, Val Loss: 70.58596122264862, loss : 1.9723063707351685\n",
            "epoch: 3, classification_loss: 1.7450580596923828, Val Loss: 70.57300424575806, loss : 2.033179521560669\n",
            "epoch: 4, classification_loss: 1.7630451917648315, Val Loss: 70.45651566982269, loss : 2.02274751663208\n",
            "epoch: 5, classification_loss: 1.7379777431488037, Val Loss: 70.44713759422302, loss : 2.0129776000976562\n",
            "epoch: 6, classification_loss: 1.7374221086502075, Val Loss: 70.54129540920258, loss : 1.7374221086502075\n",
            "epoch: 7, classification_loss: 1.7122911214828491, Val Loss: 70.49274170398712, loss : 2.029510974884033\n",
            "epoch: 8, classification_loss: 1.7035995721817017, Val Loss: 70.46172332763672, loss : 1.9372916221618652\n",
            "epoch: 9, classification_loss: 1.7312067747116089, Val Loss: 70.4412271976471, loss : 1.9997637271881104\n",
            "epoch: 10, classification_loss: 1.7361880540847778, Val Loss: 70.49642050266266, loss : 1.9678218364715576\n",
            "epoch: 11, classification_loss: 1.7079709768295288, Val Loss: 70.53684318065643, loss : 2.001108407974243\n",
            "epoch: 12, classification_loss: 1.6968380212783813, Val Loss: 70.47758567333221, loss : 2.013270616531372\n",
            "epoch: 13, classification_loss: 1.710774540901184, Val Loss: 70.44665360450745, loss : 1.9705801010131836\n",
            "epoch: 14, classification_loss: 1.7354226112365723, Val Loss: 70.44337093830109, loss : 2.00136661529541\n",
            "epoch: 15, classification_loss: 1.7334173917770386, Val Loss: 70.49090480804443, loss : 1.991142749786377\n",
            "epoch: 16, classification_loss: 1.714918613433838, Val Loss: 70.47487604618073, loss : 1.9738534688949585\n",
            "epoch: 17, classification_loss: 1.7117904424667358, Val Loss: 70.44167578220367, loss : 1.9873347282409668\n",
            "epoch: 18, classification_loss: 1.7257165908813477, Val Loss: 70.43736600875854, loss : 1.983838438987732\n",
            "epoch: 19, classification_loss: 1.7270069122314453, Val Loss: 70.44846725463867, loss : 1.9740419387817383\n",
            "Batch: 6, Test Acc: 0.5746193910256411\n",
            "Batch: 7:\n",
            "epoch: 0, classification_loss: 1.7764487266540527, Val Loss: 70.55231392383575, loss : 1.7764487266540527\n",
            "epoch: 1, classification_loss: 1.7483364343643188, Val Loss: 70.56531524658203, loss : 2.1011452674865723\n",
            "epoch: 2, classification_loss: 1.729906439781189, Val Loss: 70.5125869512558, loss : 2.0061986446380615\n",
            "epoch: 3, classification_loss: 1.750503420829773, Val Loss: 70.47763121128082, loss : 2.02821683883667\n",
            "epoch: 4, classification_loss: 1.7837084531784058, Val Loss: 70.5531040430069, loss : 2.038468360900879\n",
            "epoch: 5, classification_loss: 1.7631211280822754, Val Loss: 70.45581877231598, loss : 2.0240440368652344\n",
            "epoch: 6, classification_loss: 1.7595020532608032, Val Loss: 70.44791328907013, loss : 2.0296573638916016\n",
            "epoch: 7, classification_loss: 1.7552311420440674, Val Loss: 70.45380115509033, loss : 2.0174689292907715\n",
            "epoch: 8, classification_loss: 1.767109751701355, Val Loss: 70.52298641204834, loss : 2.0296435356140137\n",
            "epoch: 9, classification_loss: 1.7615437507629395, Val Loss: 70.46887123584747, loss : 2.020911931991577\n",
            "epoch: 10, classification_loss: 1.7634527683258057, Val Loss: 70.45857059955597, loss : 2.018442392349243\n",
            "epoch: 11, classification_loss: 1.7469947338104248, Val Loss: 70.5078786611557, loss : 2.00142502784729\n",
            "epoch: 12, classification_loss: 1.761648178100586, Val Loss: 70.49766433238983, loss : 2.011462926864624\n",
            "epoch: 13, classification_loss: 1.753393292427063, Val Loss: 70.44087088108063, loss : 2.010524272918701\n",
            "epoch: 14, classification_loss: 1.7677274942398071, Val Loss: 70.47206246852875, loss : 2.01214599609375\n",
            "epoch: 15, classification_loss: 1.7550138235092163, Val Loss: 70.53353309631348, loss : 2.0090551376342773\n",
            "epoch: 16, classification_loss: 1.7678152322769165, Val Loss: 70.4904214143753, loss : 2.0078065395355225\n",
            "epoch: 17, classification_loss: 1.7656058073043823, Val Loss: 70.51094114780426, loss : 2.015838384628296\n",
            "epoch: 18, classification_loss: 1.7547039985656738, Val Loss: 70.55382418632507, loss : 2.0017971992492676\n",
            "epoch: 19, classification_loss: 1.7508834600448608, Val Loss: 70.4376529455185, loss : 2.0049023628234863\n",
            "Batch: 7, Test Acc: 0.5776241987179487\n",
            "Batch: 8:\n",
            "epoch: 0, classification_loss: 1.783151626586914, Val Loss: 70.57922375202179, loss : 1.783151626586914\n",
            "epoch: 1, classification_loss: 1.74613356590271, Val Loss: 70.52319097518921, loss : 2.0949087142944336\n",
            "epoch: 2, classification_loss: 1.7216670513153076, Val Loss: 70.5915904045105, loss : 1.9876518249511719\n",
            "epoch: 3, classification_loss: 1.7691646814346313, Val Loss: 70.45463502407074, loss : 2.041713237762451\n",
            "epoch: 4, classification_loss: 1.7839158773422241, Val Loss: 70.43801140785217, loss : 2.0229246616363525\n",
            "epoch: 5, classification_loss: 1.7564188241958618, Val Loss: 70.45850360393524, loss : 2.0307204723358154\n",
            "epoch: 6, classification_loss: 1.7489718198776245, Val Loss: 70.45039331912994, loss : 2.03277325630188\n",
            "epoch: 7, classification_loss: 1.743435025215149, Val Loss: 70.48572254180908, loss : 2.0050840377807617\n",
            "epoch: 8, classification_loss: 1.7612111568450928, Val Loss: 70.39669382572174, loss : 2.0186219215393066\n",
            "epoch: 9, classification_loss: 1.7481064796447754, Val Loss: 70.44370877742767, loss : 1.9956806898117065\n",
            "epoch: 10, classification_loss: 1.7539762258529663, Val Loss: 70.39739060401917, loss : 2.00443959236145\n",
            "epoch: 11, classification_loss: 1.7627485990524292, Val Loss: 70.45339238643646, loss : 2.002467632293701\n",
            "epoch: 12, classification_loss: 1.7614210844039917, Val Loss: 70.42943298816681, loss : 2.0063254833221436\n",
            "epoch: 13, classification_loss: 1.751254916191101, Val Loss: 70.41554033756256, loss : 1.9965274333953857\n",
            "epoch: 14, classification_loss: 1.7613908052444458, Val Loss: 70.39010715484619, loss : 2.007695198059082\n",
            "epoch: 15, classification_loss: 1.7636348009109497, Val Loss: 70.47953629493713, loss : 1.7636348009109497\n",
            "epoch: 16, classification_loss: 1.7317947149276733, Val Loss: 70.48204958438873, loss : 2.067532539367676\n",
            "epoch: 17, classification_loss: 1.7104721069335938, Val Loss: 70.41900491714478, loss : 1.9547315835952759\n",
            "epoch: 18, classification_loss: 1.7521336078643799, Val Loss: 70.41995310783386, loss : 2.026611804962158\n",
            "epoch: 19, classification_loss: 1.770151138305664, Val Loss: 70.48090291023254, loss : 1.9975063800811768\n",
            "Batch: 8, Test Acc: 0.578125\n",
            "Batch: 9:\n",
            "epoch: 0, classification_loss: 1.8349584341049194, Val Loss: 70.53921663761139, loss : 1.8349584341049194\n",
            "epoch: 1, classification_loss: 1.8121113777160645, Val Loss: 70.52136385440826, loss : 2.221616268157959\n",
            "epoch: 2, classification_loss: 1.780889630317688, Val Loss: 70.4589695930481, loss : 2.1381099224090576\n",
            "epoch: 3, classification_loss: 1.7832186222076416, Val Loss: 70.50358152389526, loss : 2.094930648803711\n",
            "epoch: 4, classification_loss: 1.820716381072998, Val Loss: 70.52806878089905, loss : 2.142016887664795\n",
            "epoch: 5, classification_loss: 1.8365612030029297, Val Loss: 70.44364249706268, loss : 2.105863571166992\n",
            "epoch: 6, classification_loss: 1.8164403438568115, Val Loss: 70.45730030536652, loss : 2.1115005016326904\n",
            "epoch: 7, classification_loss: 1.8008449077606201, Val Loss: 70.4358583688736, loss : 2.1079328060150146\n",
            "epoch: 8, classification_loss: 1.7881370782852173, Val Loss: 70.5026570558548, loss : 2.089977741241455\n",
            "epoch: 9, classification_loss: 1.8123687505722046, Val Loss: 70.5090583562851, loss : 2.0959887504577637\n",
            "epoch: 10, classification_loss: 1.812555193901062, Val Loss: 70.44272601604462, loss : 2.088595390319824\n",
            "epoch: 11, classification_loss: 1.815148949623108, Val Loss: 70.41557800769806, loss : 2.0765388011932373\n",
            "epoch: 12, classification_loss: 1.8206851482391357, Val Loss: 70.43496298789978, loss : 2.0800139904022217\n",
            "epoch: 13, classification_loss: 1.8164616823196411, Val Loss: 70.47677111625671, loss : 2.073216676712036\n",
            "epoch: 14, classification_loss: 1.8056737184524536, Val Loss: 70.48382151126862, loss : 2.059217691421509\n",
            "epoch: 15, classification_loss: 1.8076478242874146, Val Loss: 70.41004168987274, loss : 2.063744306564331\n",
            "epoch: 16, classification_loss: 1.812351942062378, Val Loss: 70.43962669372559, loss : 2.0528409481048584\n",
            "epoch: 17, classification_loss: 1.8143424987792969, Val Loss: 70.46658480167389, loss : 2.067384719848633\n",
            "epoch: 18, classification_loss: 1.8118913173675537, Val Loss: 70.4624617099762, loss : 2.053982734680176\n",
            "epoch: 19, classification_loss: 1.8174223899841309, Val Loss: 70.50202667713165, loss : 2.068110704421997\n",
            "Batch: 9, Test Acc: 0.5765224358974359\n",
            "Batch: 10:\n",
            "epoch: 0, classification_loss: 1.7551946640014648, Val Loss: 70.58587217330933, loss : 1.7551946640014648\n",
            "epoch: 1, classification_loss: 1.7296202182769775, Val Loss: 70.43478429317474, loss : 2.0647048950195312\n",
            "epoch: 2, classification_loss: 1.7043060064315796, Val Loss: 70.44100844860077, loss : 1.963525414466858\n",
            "epoch: 3, classification_loss: 1.7422144412994385, Val Loss: 70.41504633426666, loss : 2.019967794418335\n",
            "epoch: 4, classification_loss: 1.7570592164993286, Val Loss: 70.36611688137054, loss : 2.00545597076416\n",
            "epoch: 5, classification_loss: 1.7403379678726196, Val Loss: 70.36297643184662, loss : 1.9994488954544067\n",
            "epoch: 6, classification_loss: 1.735778570175171, Val Loss: 70.4341756105423, loss : 2.0039799213409424\n",
            "epoch: 7, classification_loss: 1.7383434772491455, Val Loss: 70.48565399646759, loss : 1.991145133972168\n",
            "epoch: 8, classification_loss: 1.7495598793029785, Val Loss: 70.39795351028442, loss : 2.006495952606201\n",
            "epoch: 9, classification_loss: 1.7467377185821533, Val Loss: 70.37912631034851, loss : 1.997435212135315\n",
            "epoch: 10, classification_loss: 1.7405142784118652, Val Loss: 70.4387423992157, loss : 1.9902148246765137\n",
            "epoch: 11, classification_loss: 1.747121810913086, Val Loss: 70.45873844623566, loss : 1.9910651445388794\n",
            "epoch: 12, classification_loss: 1.7388818264007568, Val Loss: 70.39292538166046, loss : 1.9922281503677368\n",
            "epoch: 13, classification_loss: 1.730947494506836, Val Loss: 70.40751075744629, loss : 1.97829008102417\n",
            "epoch: 14, classification_loss: 1.736895203590393, Val Loss: 70.41624653339386, loss : 1.986236333847046\n",
            "epoch: 15, classification_loss: 1.7445743083953857, Val Loss: 70.39868175983429, loss : 1.9820865392684937\n",
            "epoch: 16, classification_loss: 1.7466360330581665, Val Loss: 70.40451967716217, loss : 1.9913667440414429\n",
            "epoch: 17, classification_loss: 1.7273948192596436, Val Loss: 70.40123653411865, loss : 1.9648430347442627\n",
            "epoch: 18, classification_loss: 1.7341375350952148, Val Loss: 70.40989637374878, loss : 1.9710395336151123\n"
          ]
        }
      ],
      "source": [
        "model = VGG().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "best_model_wts = None\n",
        "\n",
        "while (True):\n",
        "    try:\n",
        "        print(\"#########################################################\")\n",
        "        best_model_wts = train(model, 20, criterion, optimizer)\n",
        "    except KeyboardInterrupt:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
