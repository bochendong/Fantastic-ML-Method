{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 256\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(batch_size=64):\n",
    "    # Old data\n",
    "    train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=tf)\n",
    "\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    source_indices = [i for i, (_, label) in enumerate(train_dataset) if label < 8]\n",
    "    target_indices = [i for i, (_, label) in enumerate(train_dataset) if (label == 9)]\n",
    "\n",
    "    source_set = Subset(train_dataset, source_indices)\n",
    "    target_set = Subset(train_dataset, target_indices)\n",
    "\n",
    "    source_dl_train = DataLoader(source_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    target_dl_train = DataLoader(target_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_dl= DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=tf)\n",
    "    \n",
    "    test_dl =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "\n",
    "    return source_dl_train, target_dl_train, test_dl, val_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, out_dim = 10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, out_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(criterion, optimizer, model, num_epochs, trainloader, testloader, device):\n",
    "    train_loss_history = []\n",
    "    test_acc_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_acc = correct / total\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Test Accuracy: {epoch_acc:.4f}\")\n",
    "            test_acc_history.append(epoch_acc)\n",
    "            train_loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "    return train_loss_history, test_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('source.pth'):\n",
    "        source_dl_train, target_dl_train, test_dl, val_dl = load_data_set(batch_size=batch_size)\n",
    "        model = VGG()\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "\n",
    "        num_epochs = 30\n",
    "        loss_history, test_acc_history = pre_train(criterion, optimizer, model, num_epochs, source_dl_train, test_dl, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "                torch.save(model.state_dict(), 'source.pth')\n",
    "else:\n",
    "        model = VGG().to(device)\n",
    "        model.load_state_dict(torch.load('source.pth'))\n",
    "        print(\"Loaded model from file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(criterion, optimizer, model, num_epochs, trainloader, testloader, valloader, device):\n",
    "    best_model_wts = None\n",
    "    leader = VGG().to(device)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_num = 0\n",
    "        if (best_model_wts):\n",
    "            model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "            model.train()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            '''reg_loss = 0\n",
    "            for lead_para, follower_para in zip(leader.parameters(), model.parameters()):\n",
    "                reg_loss += torch.norm(follower_para - lead_para, p = 2)'''\n",
    "            \n",
    "            classification_loss = criterion(outputs, labels)\n",
    "            loss = classification_loss #+ reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in valloader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    outputs = model(val_inputs)\n",
    "                    batch_loss = criterion(outputs, val_labels)\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    leader.load_state_dict(best_model_wts)\n",
    "            \n",
    "            print(f\"Batch num: {batch_num}, classification_loss: {classification_loss.item()}, Val Loss: {val_loss}, loss : {loss.item()}\")\n",
    "            batch_num += 1\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_acc = correct / total\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Test Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "source_dl_train, target_dl_train, test_dl, val_dl = load_data_set(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num: 0, classification_loss: 2.4564502239227295, Val Loss: 69.08209574222565, loss : 2.4564502239227295\n",
      "Batch num: 1, classification_loss: 2.456946611404419, Val Loss: 69.77781546115875, loss : 2.456946611404419\n",
      "Batch num: 2, classification_loss: 2.4577531814575195, Val Loss: 70.71295011043549, loss : 2.4577531814575195\n",
      "Batch num: 3, classification_loss: 2.4576566219329834, Val Loss: 71.45107674598694, loss : 2.4576566219329834\n",
      "Batch num: 4, classification_loss: 2.4570491313934326, Val Loss: 72.54166460037231, loss : 2.4570491313934326\n",
      "Batch num: 5, classification_loss: 2.4578373432159424, Val Loss: 73.22082340717316, loss : 2.4578373432159424\n",
      "Batch num: 6, classification_loss: 2.456967830657959, Val Loss: 73.78667771816254, loss : 2.456967830657959\n",
      "Batch num: 7, classification_loss: 2.456411123275757, Val Loss: 74.47651863098145, loss : 2.456411123275757\n",
      "Batch num: 8, classification_loss: 2.4575998783111572, Val Loss: 74.75889432430267, loss : 2.4575998783111572\n",
      "Batch num: 9, classification_loss: 2.4572951793670654, Val Loss: 75.117950797081, loss : 2.4572951793670654\n",
      "Batch num: 10, classification_loss: 2.457495927810669, Val Loss: 75.63699793815613, loss : 2.457495927810669\n",
      "Batch num: 11, classification_loss: 2.4566359519958496, Val Loss: 75.78456914424896, loss : 2.4566359519958496\n",
      "Batch num: 12, classification_loss: 2.4574739933013916, Val Loss: 75.74804306030273, loss : 2.4574739933013916\n",
      "Batch num: 13, classification_loss: 2.4563210010528564, Val Loss: 75.74822044372559, loss : 2.4563210010528564\n",
      "Batch num: 14, classification_loss: 2.455742120742798, Val Loss: 75.80757510662079, loss : 2.455742120742798\n",
      "Epoch [1/30], Loss: 2.4557, Test Accuracy: 0.4682\n",
      "Batch num: 0, classification_loss: 2.4587340354919434, Val Loss: 69.39583623409271, loss : 2.4587340354919434\n",
      "Batch num: 1, classification_loss: 2.456529140472412, Val Loss: 69.74706244468689, loss : 2.456529140472412\n",
      "Batch num: 2, classification_loss: 2.4575841426849365, Val Loss: 70.07892835140228, loss : 2.4575841426849365\n",
      "Batch num: 3, classification_loss: 2.457240104675293, Val Loss: 70.43391835689545, loss : 2.457240104675293\n",
      "Batch num: 4, classification_loss: 2.4572081565856934, Val Loss: 70.60056483745575, loss : 2.4572081565856934\n",
      "Batch num: 5, classification_loss: 2.4572925567626953, Val Loss: 70.8056606054306, loss : 2.4572925567626953\n",
      "Batch num: 6, classification_loss: 2.4567296504974365, Val Loss: 71.04166352748871, loss : 2.4567296504974365\n",
      "Batch num: 7, classification_loss: 2.4571523666381836, Val Loss: 71.27407944202423, loss : 2.4571523666381836\n",
      "Batch num: 8, classification_loss: 2.45682692527771, Val Loss: 71.4773827791214, loss : 2.45682692527771\n",
      "Batch num: 9, classification_loss: 2.4556236267089844, Val Loss: 71.67006838321686, loss : 2.4556236267089844\n",
      "Batch num: 10, classification_loss: 2.457753896713257, Val Loss: 71.88397300243378, loss : 2.457753896713257\n",
      "Batch num: 11, classification_loss: 2.4564409255981445, Val Loss: 72.16134452819824, loss : 2.4564409255981445\n",
      "Batch num: 12, classification_loss: 2.4570870399475098, Val Loss: 72.5402694940567, loss : 2.4570870399475098\n",
      "Batch num: 13, classification_loss: 2.4562394618988037, Val Loss: 72.64146876335144, loss : 2.4562394618988037\n",
      "Batch num: 14, classification_loss: 2.4561917781829834, Val Loss: 72.79113447666168, loss : 2.4561917781829834\n",
      "Epoch [2/30], Loss: 2.4562, Test Accuracy: 0.5272\n",
      "Batch num: 0, classification_loss: 2.457585573196411, Val Loss: 69.17478108406067, loss : 2.457585573196411\n",
      "Batch num: 1, classification_loss: 2.456791639328003, Val Loss: 69.21087265014648, loss : 2.456791639328003\n",
      "Batch num: 2, classification_loss: 2.4567222595214844, Val Loss: 69.2425057888031, loss : 2.4567222595214844\n",
      "Batch num: 3, classification_loss: 2.4573185443878174, Val Loss: 69.26128828525543, loss : 2.4573185443878174\n",
      "Batch num: 4, classification_loss: 2.4560065269470215, Val Loss: 69.24854791164398, loss : 2.4560065269470215\n",
      "Batch num: 5, classification_loss: 2.456981897354126, Val Loss: 69.31801891326904, loss : 2.456981897354126\n",
      "Batch num: 6, classification_loss: 2.456824779510498, Val Loss: 69.49560880661011, loss : 2.456824779510498\n",
      "Batch num: 7, classification_loss: 2.457705497741699, Val Loss: 69.60138928890228, loss : 2.457705497741699\n",
      "Batch num: 8, classification_loss: 2.4568002223968506, Val Loss: 69.54798829555511, loss : 2.4568002223968506\n",
      "Batch num: 9, classification_loss: 2.4576215744018555, Val Loss: 69.65438258647919, loss : 2.4576215744018555\n",
      "Batch num: 10, classification_loss: 2.456017255783081, Val Loss: 69.8092349767685, loss : 2.456017255783081\n",
      "Batch num: 11, classification_loss: 2.457188129425049, Val Loss: 70.23368442058563, loss : 2.457188129425049\n",
      "Batch num: 12, classification_loss: 2.4568495750427246, Val Loss: 70.63924753665924, loss : 2.4568495750427246\n",
      "Batch num: 13, classification_loss: 2.456010580062866, Val Loss: 71.15061271190643, loss : 2.456010580062866\n",
      "Batch num: 14, classification_loss: 2.4566030502319336, Val Loss: 71.38569140434265, loss : 2.4566030502319336\n",
      "Epoch [3/30], Loss: 2.4566, Test Accuracy: 0.5645\n",
      "Batch num: 0, classification_loss: 2.4581093788146973, Val Loss: 68.9542680978775, loss : 2.4581093788146973\n",
      "Batch num: 1, classification_loss: 2.456468105316162, Val Loss: 68.85679817199707, loss : 2.456468105316162\n",
      "Batch num: 2, classification_loss: 2.4568350315093994, Val Loss: 68.90502393245697, loss : 2.4568350315093994\n",
      "Batch num: 3, classification_loss: 2.456690549850464, Val Loss: 68.82347548007965, loss : 2.456690549850464\n",
      "Batch num: 4, classification_loss: 2.4569218158721924, Val Loss: 68.86935782432556, loss : 2.4569218158721924\n",
      "Batch num: 5, classification_loss: 2.4581170082092285, Val Loss: 68.93007218837738, loss : 2.4581170082092285\n",
      "Batch num: 6, classification_loss: 2.4559624195098877, Val Loss: 69.04608011245728, loss : 2.4559624195098877\n",
      "Batch num: 7, classification_loss: 2.4566004276275635, Val Loss: 69.10016202926636, loss : 2.4566004276275635\n",
      "Batch num: 8, classification_loss: 2.456498622894287, Val Loss: 69.19130396842957, loss : 2.456498622894287\n",
      "Batch num: 9, classification_loss: 2.4569060802459717, Val Loss: 69.3205281496048, loss : 2.4569060802459717\n",
      "Batch num: 10, classification_loss: 2.4562957286834717, Val Loss: 69.50907063484192, loss : 2.4562957286834717\n",
      "Batch num: 11, classification_loss: 2.457937240600586, Val Loss: 69.80301594734192, loss : 2.457937240600586\n",
      "Batch num: 12, classification_loss: 2.455876111984253, Val Loss: 70.16381132602692, loss : 2.455876111984253\n",
      "Batch num: 13, classification_loss: 2.455151081085205, Val Loss: 70.56443703174591, loss : 2.455151081085205\n",
      "Batch num: 14, classification_loss: 2.456742286682129, Val Loss: 71.05123257637024, loss : 2.456742286682129\n",
      "Epoch [4/30], Loss: 2.4567, Test Accuracy: 0.5746\n",
      "Batch num: 0, classification_loss: 2.4576022624969482, Val Loss: 68.91792917251587, loss : 2.4576022624969482\n",
      "Batch num: 1, classification_loss: 2.457587718963623, Val Loss: 69.10333693027496, loss : 2.457587718963623\n",
      "Batch num: 2, classification_loss: 2.4577715396881104, Val Loss: 69.42796194553375, loss : 2.4577715396881104\n",
      "Batch num: 3, classification_loss: 2.4567084312438965, Val Loss: 69.85000395774841, loss : 2.4567084312438965\n",
      "Batch num: 4, classification_loss: 2.4574196338653564, Val Loss: 70.29084944725037, loss : 2.4574196338653564\n",
      "Batch num: 5, classification_loss: 2.457298755645752, Val Loss: 70.575963139534, loss : 2.457298755645752\n",
      "Batch num: 6, classification_loss: 2.4559640884399414, Val Loss: 71.01735162734985, loss : 2.4559640884399414\n",
      "Batch num: 7, classification_loss: 2.4568912982940674, Val Loss: 71.2854654788971, loss : 2.4568912982940674\n",
      "Batch num: 8, classification_loss: 2.4559080600738525, Val Loss: 71.45198833942413, loss : 2.4559080600738525\n",
      "Batch num: 9, classification_loss: 2.456758737564087, Val Loss: 71.47387659549713, loss : 2.456758737564087\n",
      "Batch num: 10, classification_loss: 2.4562482833862305, Val Loss: 71.69798505306244, loss : 2.4562482833862305\n",
      "Batch num: 11, classification_loss: 2.456660747528076, Val Loss: 71.97588789463043, loss : 2.456660747528076\n",
      "Batch num: 12, classification_loss: 2.4550299644470215, Val Loss: 72.17082238197327, loss : 2.4550299644470215\n",
      "Batch num: 13, classification_loss: 2.455395221710205, Val Loss: 72.30674874782562, loss : 2.455395221710205\n",
      "Batch num: 14, classification_loss: 2.456644296646118, Val Loss: 72.56237351894379, loss : 2.456644296646118\n",
      "Epoch [5/30], Loss: 2.4566, Test Accuracy: 0.5463\n",
      "Batch num: 0, classification_loss: 2.457907199859619, Val Loss: 68.8933492898941, loss : 2.457907199859619\n",
      "Batch num: 1, classification_loss: 2.4571659564971924, Val Loss: 68.92860615253448, loss : 2.4571659564971924\n",
      "Batch num: 2, classification_loss: 2.457047700881958, Val Loss: 69.0571699142456, loss : 2.457047700881958\n",
      "Batch num: 3, classification_loss: 2.457489490509033, Val Loss: 69.23472845554352, loss : 2.457489490509033\n",
      "Batch num: 4, classification_loss: 2.4571940898895264, Val Loss: 69.41650474071503, loss : 2.4571940898895264\n",
      "Batch num: 5, classification_loss: 2.4572174549102783, Val Loss: 69.67832934856415, loss : 2.4572174549102783\n",
      "Batch num: 6, classification_loss: 2.4558956623077393, Val Loss: 70.00266683101654, loss : 2.4558956623077393\n",
      "Batch num: 7, classification_loss: 2.457350254058838, Val Loss: 70.26409065723419, loss : 2.457350254058838\n",
      "Batch num: 8, classification_loss: 2.4561421871185303, Val Loss: 70.64889109134674, loss : 2.4561421871185303\n",
      "Batch num: 9, classification_loss: 2.4565675258636475, Val Loss: 70.89610862731934, loss : 2.4565675258636475\n",
      "Batch num: 10, classification_loss: 2.4560861587524414, Val Loss: 71.0276505947113, loss : 2.4560861587524414\n",
      "Batch num: 11, classification_loss: 2.4569952487945557, Val Loss: 71.2858054637909, loss : 2.4569952487945557\n",
      "Batch num: 12, classification_loss: 2.455984592437744, Val Loss: 71.43465518951416, loss : 2.455984592437744\n",
      "Batch num: 13, classification_loss: 2.4566738605499268, Val Loss: 71.62600016593933, loss : 2.4566738605499268\n",
      "Batch num: 14, classification_loss: 2.4564733505249023, Val Loss: 71.9800193309784, loss : 2.4564733505249023\n",
      "Epoch [6/30], Loss: 2.4565, Test Accuracy: 0.5507\n",
      "Batch num: 0, classification_loss: 2.458034038543701, Val Loss: 68.6975349187851, loss : 2.458034038543701\n",
      "Batch num: 1, classification_loss: 2.4573543071746826, Val Loss: 68.7415179014206, loss : 2.4573543071746826\n",
      "Batch num: 2, classification_loss: 2.457362174987793, Val Loss: 68.84035396575928, loss : 2.457362174987793\n",
      "Batch num: 3, classification_loss: 2.4570000171661377, Val Loss: 68.9918247461319, loss : 2.4570000171661377\n",
      "Batch num: 4, classification_loss: 2.4578661918640137, Val Loss: 69.1736067533493, loss : 2.4578661918640137\n",
      "Batch num: 5, classification_loss: 2.4552605152130127, Val Loss: 69.31180763244629, loss : 2.4552605152130127\n",
      "Batch num: 6, classification_loss: 2.4562132358551025, Val Loss: 69.47641146183014, loss : 2.4562132358551025\n",
      "Batch num: 7, classification_loss: 2.4571800231933594, Val Loss: 69.6848109960556, loss : 2.4571800231933594\n",
      "Batch num: 8, classification_loss: 2.4568517208099365, Val Loss: 69.96215558052063, loss : 2.4568517208099365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.0005\u001b[39m)\n\u001b[0;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(criterion, optimizer, model, num_epochs, trainloader, testloader, valloader, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_inputs, val_labels \u001b[38;5;129;01min\u001b[39;00m valloader:\n\u001b[0;32m     31\u001b[0m         val_inputs, val_labels \u001b[38;5;241m=\u001b[39m val_inputs\u001b[38;5;241m.\u001b[39mto(device), val_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(val_inputs)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55366\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\functional.py:151\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    150\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 151\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    153\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.0005)\n",
    "num_epochs = 30\n",
    "fine_tune(criterion, optimizer, model, num_epochs, target_dl_train, test_dl, val_dl, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
